{"Conclusions: Paying it Off": ["5.2\nWhen Correlations No Longer Correlate\nMachine learning systems often have a dif\ufb01cult time distinguishing the impact of correlated features.\nThis may not seem like a major problem: if two features are always correlated, but only one is truly\ncausal, it may still seem okay to ascribe credit to both and rely on their observed co-occurrence.\nHowever, if the world suddenly stops making these features co-occur, prediction behavior may\nchange signi\ufb01cantly. The full range of ML strategies for teasing apart correlation effects is be-\nyond our scope; some excellent suggestions and references are given in [2]. For the purpose of this\npaper, we note that non-causal correlations are another source of hidden debt.\n5.3\nMonitoring and Testing\nUnit testing of individual components and end-to-end tests of running systems are valuable, but in\nthe face of a changing world such tests are not suf\ufb01cient to provide evidence that a system is working\nas intended. Live monitoring of system behavior in real time is critical.\nThe key question is: what to monitor? It can be dif\ufb01cult to establish useful invariants, given that the\npurpose of machine learning systems is to adapt over time. We offer two reasonable starting points.\nPrediction Bias. In a system that is working as intended, it should usually be the case that the\ndistribution of predicted labels is equal to the distribution of observed labels. This is by no means\na comprehensive test, as it can be met by a null model that simply predicts average values of label\noccurrences without regard to the input features. However, it is a surprisingly useful diagnostic, and\nchanges in metrics such as this are often indicative of an issue that requires attention. For example,\nthis method can help to detect cases in which the world behavior suddenly changes, making training\ndistributions drawn from historical data no longer re\ufb02ective of current reality. Slicing prediction\nbias by various dimensions isolate issues quickly, and can also be used for automated alerting.\nAction Limits. In systems that are used to take actions in the real world, it can be useful to set\nand enforce action limits as a sanity check. These limits should be broad enough not to trigger\nspuriously. If the system hits a limit for a given action, automated alerts should \ufb01re and trigger\nmanual intervention or investigation.\n6\nConclusions: Paying it Off\nThis paper has highlighted a number of areas where machine learning systems can create technical\ndebt, sometimes in surprising ways. This is not to say that machine learning is bad, or even that\ntechnical debt is something to be avoided at all costs. It may be reasonable to take on moderate\ntechnical debt for the bene\ufb01t of moving quickly in the short term, but this must be recognized and\naccounted for lest it quickly grow unmanageable.\nPerhaps the most important insight to be gained is that technical debt is an issue that both engineers\nand researchers need to be aware of. Research solutions that provide a tiny accuracy bene\ufb01t at the\ncost of massive increases in system complexity are rarely wise practice. Even the addition of one or\ntwo seemingly innocuous data dependencies can slow further progress.\nPaying down technical debt is not always as exciting as proving a new theorem, but it is a critical part\nof consistently strong innovation. And developing holistic, elegant solutions for complex machine\nlearning systems is deeply rewarding work.\nAcknowledgments\nThis paper owes much to the important lessons learned day to day in a culture that values both\ninnovative ML research and strong engineering practice. Many colleagues have helped shape our\nthoughts here, and the bene\ufb01t of accumulated folk wisdom cannot be overstated. We would like to\nspeci\ufb01cally recognize the following: Luis Cobo, Sharat Chikkerur, Jean-Francois Crespo, Jeff Dean,\nDan Dennison, Philip Henderson, Arnar Mar Hrafnkelsson, Ankur Jain, Joe Kovac, Jeremy Kubica,\nH. Brendan McMahan, Satyaki Mahalanabis, Lan Nie, Michael Pohl, Abdul Salem, Sajid Siddiqi,\nRicky Shan, Alan Skelly, Cory Williams, and Andrew Young.\n8\n", []], "Monitoring and Testing": ["5.2\nWhen Correlations No Longer Correlate\nMachine learning systems often have a dif\ufb01cult time distinguishing the impact of correlated features.\nThis may not seem like a major problem: if two features are always correlated, but only one is truly\ncausal, it may still seem okay to ascribe credit to both and rely on their observed co-occurrence.\nHowever, if the world suddenly stops making these features co-occur, prediction behavior may\nchange signi\ufb01cantly. The full range of ML strategies for teasing apart correlation effects is be-\nyond our scope; some excellent suggestions and references are given in [2]. For the purpose of this\npaper, we note that non-causal correlations are another source of hidden debt.\n5.3\nMonitoring and Testing\nUnit testing of individual components and end-to-end tests of running systems are valuable, but in\nthe face of a changing world such tests are not suf\ufb01cient to provide evidence that a system is working\nas intended. Live monitoring of system behavior in real time is critical.\nThe key question is: what to monitor? It can be dif\ufb01cult to establish useful invariants, given that the\npurpose of machine learning systems is to adapt over time. We offer two reasonable starting points.\nPrediction Bias. In a system that is working as intended, it should usually be the case that the\ndistribution of predicted labels is equal to the distribution of observed labels. This is by no means\na comprehensive test, as it can be met by a null model that simply predicts average values of label\noccurrences without regard to the input features. However, it is a surprisingly useful diagnostic, and\nchanges in metrics such as this are often indicative of an issue that requires attention. For example,\nthis method can help to detect cases in which the world behavior suddenly changes, making training\ndistributions drawn from historical data no longer re\ufb02ective of current reality. Slicing prediction\nbias by various dimensions isolate issues quickly, and can also be used for automated alerting.\nAction Limits. In systems that are used to take actions in the real world, it can be useful to set\nand enforce action limits as a sanity check. These limits should be broad enough not to trigger\nspuriously. If the system hits a limit for a given action, automated alerts should \ufb01re and trigger\nmanual intervention or investigation.\n6\nConclusions: Paying it Off\nThis paper has highlighted a number of areas where machine learning systems can create technical\ndebt, sometimes in surprising ways. This is not to say that machine learning is bad, or even that\ntechnical debt is something to be avoided at all costs. It may be reasonable to take on moderate\ntechnical debt for the bene\ufb01t of moving quickly in the short term, but this must be recognized and\naccounted for lest it quickly grow unmanageable.\nPerhaps the most important insight to be gained is that technical debt is an issue that both engineers\nand researchers need to be aware of. Research solutions that provide a tiny accuracy bene\ufb01t at the\ncost of massive increases in system complexity are rarely wise practice. Even the addition of one or\ntwo seemingly innocuous data dependencies can slow further progress.\nPaying down technical debt is not always as exciting as proving a new theorem, but it is a critical part\nof consistently strong innovation. And developing holistic, elegant solutions for complex machine\nlearning systems is deeply rewarding work.\nAcknowledgments\nThis paper owes much to the important lessons learned day to day in a culture that values both\ninnovative ML research and strong engineering practice. Many colleagues have helped shape our\nthoughts here, and the bene\ufb01t of accumulated folk wisdom cannot be overstated. We would like to\nspeci\ufb01cally recognize the following: Luis Cobo, Sharat Chikkerur, Jean-Francois Crespo, Jeff Dean,\nDan Dennison, Philip Henderson, Arnar Mar Hrafnkelsson, Ankur Jain, Joe Kovac, Jeremy Kubica,\nH. Brendan McMahan, Satyaki Mahalanabis, Lan Nie, Michael Pohl, Abdul Salem, Sajid Siddiqi,\nRicky Shan, Alan Skelly, Cory Williams, and Andrew Young.\n8\n", []], "When Correlations No Longer Correlate": ["5.2\nWhen Correlations No Longer Correlate\nMachine learning systems often have a dif\ufb01cult time distinguishing the impact of correlated features.\nThis may not seem like a major problem: if two features are always correlated, but only one is truly\ncausal, it may still seem okay to ascribe credit to both and rely on their observed co-occurrence.\nHowever, if the world suddenly stops making these features co-occur, prediction behavior may\nchange signi\ufb01cantly. The full range of ML strategies for teasing apart correlation effects is be-\nyond our scope; some excellent suggestions and references are given in [2]. For the purpose of this\npaper, we note that non-causal correlations are another source of hidden debt.\n5.3\nMonitoring and Testing\nUnit testing of individual components and end-to-end tests of running systems are valuable, but in\nthe face of a changing world such tests are not suf\ufb01cient to provide evidence that a system is working\nas intended. Live monitoring of system behavior in real time is critical.\nThe key question is: what to monitor? It can be dif\ufb01cult to establish useful invariants, given that the\npurpose of machine learning systems is to adapt over time. We offer two reasonable starting points.\nPrediction Bias. In a system that is working as intended, it should usually be the case that the\ndistribution of predicted labels is equal to the distribution of observed labels. This is by no means\na comprehensive test, as it can be met by a null model that simply predicts average values of label\noccurrences without regard to the input features. However, it is a surprisingly useful diagnostic, and\nchanges in metrics such as this are often indicative of an issue that requires attention. For example,\nthis method can help to detect cases in which the world behavior suddenly changes, making training\ndistributions drawn from historical data no longer re\ufb02ective of current reality. Slicing prediction\nbias by various dimensions isolate issues quickly, and can also be used for automated alerting.\nAction Limits. In systems that are used to take actions in the real world, it can be useful to set\nand enforce action limits as a sanity check. These limits should be broad enough not to trigger\nspuriously. If the system hits a limit for a given action, automated alerts should \ufb01re and trigger\nmanual intervention or investigation.\n6\nConclusions: Paying it Off\nThis paper has highlighted a number of areas where machine learning systems can create technical\ndebt, sometimes in surprising ways. This is not to say that machine learning is bad, or even that\ntechnical debt is something to be avoided at all costs. It may be reasonable to take on moderate\ntechnical debt for the bene\ufb01t of moving quickly in the short term, but this must be recognized and\naccounted for lest it quickly grow unmanageable.\nPerhaps the most important insight to be gained is that technical debt is an issue that both engineers\nand researchers need to be aware of. Research solutions that provide a tiny accuracy bene\ufb01t at the\ncost of massive increases in system complexity are rarely wise practice. Even the addition of one or\ntwo seemingly innocuous data dependencies can slow further progress.\nPaying down technical debt is not always as exciting as proving a new theorem, but it is a critical part\nof consistently strong innovation. And developing holistic, elegant solutions for complex machine\nlearning systems is deeply rewarding work.\nAcknowledgments\nThis paper owes much to the important lessons learned day to day in a culture that values both\ninnovative ML research and strong engineering practice. Many colleagues have helped shape our\nthoughts here, and the bene\ufb01t of accumulated folk wisdom cannot be overstated. We would like to\nspeci\ufb01cally recognize the following: Luis Cobo, Sharat Chikkerur, Jean-Francois Crespo, Jeff Dean,\nDan Dennison, Philip Henderson, Arnar Mar Hrafnkelsson, Ankur Jain, Joe Kovac, Jeremy Kubica,\nH. Brendan McMahan, Satyaki Mahalanabis, Lan Nie, Michael Pohl, Abdul Salem, Sajid Siddiqi,\nRicky Shan, Alan Skelly, Cory Williams, and Andrew Young.\n8\n", []], "Fixed Thresholds in Dynamic Systems": ["with vary over time; a redesign and a rewrite of some pieces may be needed periodically in order to\nmove forward ef\ufb01ciently.\nAs a real-world anecdote, in a recent cleanup effort of one important machine learning system at\nGoogle, it was found possible to rip out tens of thousands of lines of unused experimental code-\npaths. A follow-on rewrite with a tighter API allowed experimentation with new algorithms to be\nperformed with dramatically reduced effort and production risk and minimal incremental system\ncomplexity.\n4.4\nCon\ufb01guration Debt\nAnother potentially surprising area where debt can accumulate is in the con\ufb01guration of machine\nlearning systems. Any large system has a wide range of con\ufb01gurable options, including which fea-\ntures are used, how data is selected, a wide variety of algorithm-speci\ufb01c learning settings, potential\npre- or post-processing, veri\ufb01cation methods, etc.\nMany engineers do a great job of thinking hard about abstractions and unit tests in production code,\nbut may treat con\ufb01guration (and extension of con\ufb01guration) as an afterthought. Indeed, veri\ufb01cation\nor testing of con\ufb01gurations may not even be seen as important. Con\ufb01guration by its very nature\ntends to be the place where real-world messiness intrudes on beautiful algorithms.\nConsider the following examples. Feature A was incorrectly logged from 9/14 to 9/17. Feature\nB is not available on data before 10/7. The code used to compute feature C has to change for\ndata before and after 11/1 because of changes to the logging format. Feature D is not available\nin production, so a substitute features D\u2032 and D\u2032\u2032 must be used when querying the model in a live\nsetting. If feature Z is used, then jobs for training must be given extra memory due to lookup\ntables or they will train inef\ufb01ciently. Feature Q precludes the use of feature R because of latency\nconstraints. All this messiness makes con\ufb01guration hard to modify correctly, and hard to reason\nabout. However, mistakes in con\ufb01guration can be costly, leading to serious loss of time, waste of\ncomputing resources, or production issues.\nAlso, in a mature system which is being actively developed, the number of lines of con\ufb01guration\ncan far exceed the number of lines of the code that actually does machine learning. Each line has a\npotential for mistakes, and con\ufb01gurations are by their nature ephemeral and less well tested.\nAssertions about con\ufb01guration invariants can be critical to prevent mistakes, but careful thought is\nneeded about what kind of assertions will be useful. Another useful tool is the ability to present\nvisual side-by-side differences (diffs) of two con\ufb01gurations. Because con\ufb01gurations are often copy-\nand-pasted with small modi\ufb01cations, such diffs highlight important changes. And clearly, con\ufb01gu-\nrations should be treated with the same level of seriousness as code changes, and be carefully code\nreviewed by peers.\n5\nDealing with Changes in the External World\nOne of the things that makes machine learning systems so fascinating is that they often interact\ndirectly with the external world. Experience has shown that the external world is rarely stable.\nIndeed, the changing nature of the world is one of the sources of technical debt in machine learning\nsystems.\n5.1\nFixed Thresholds in Dynamic Systems\nIt is often necessary to pick a decision threshold for a given model to perform some action: to predict\ntrue or false, to mark an email as spam or not spam, to show or not show a given ad. One classic\napproach in machine learning is to choose a threshold from a set of possible thresholds, in order to\nget good tradeoffs on certain metrics, such as precision and recall. However, such thresholds are\noften manually set. Thus if a model updates on new data, the old manually set threshold may be\ninvalid. Manually updating many thresholds across many models is time-consuming and brittle.\nA useful mitigation strategy for this kind of problem appears in [8], in which thresholds are learned\nvia simple evaluation on heldout validation data.\n7\n", []], "Dealing with Changes in the External World": ["with vary over time; a redesign and a rewrite of some pieces may be needed periodically in order to\nmove forward ef\ufb01ciently.\nAs a real-world anecdote, in a recent cleanup effort of one important machine learning system at\nGoogle, it was found possible to rip out tens of thousands of lines of unused experimental code-\npaths. A follow-on rewrite with a tighter API allowed experimentation with new algorithms to be\nperformed with dramatically reduced effort and production risk and minimal incremental system\ncomplexity.\n4.4\nCon\ufb01guration Debt\nAnother potentially surprising area where debt can accumulate is in the con\ufb01guration of machine\nlearning systems. Any large system has a wide range of con\ufb01gurable options, including which fea-\ntures are used, how data is selected, a wide variety of algorithm-speci\ufb01c learning settings, potential\npre- or post-processing, veri\ufb01cation methods, etc.\nMany engineers do a great job of thinking hard about abstractions and unit tests in production code,\nbut may treat con\ufb01guration (and extension of con\ufb01guration) as an afterthought. Indeed, veri\ufb01cation\nor testing of con\ufb01gurations may not even be seen as important. Con\ufb01guration by its very nature\ntends to be the place where real-world messiness intrudes on beautiful algorithms.\nConsider the following examples. Feature A was incorrectly logged from 9/14 to 9/17. Feature\nB is not available on data before 10/7. The code used to compute feature C has to change for\ndata before and after 11/1 because of changes to the logging format. Feature D is not available\nin production, so a substitute features D\u2032 and D\u2032\u2032 must be used when querying the model in a live\nsetting. If feature Z is used, then jobs for training must be given extra memory due to lookup\ntables or they will train inef\ufb01ciently. Feature Q precludes the use of feature R because of latency\nconstraints. All this messiness makes con\ufb01guration hard to modify correctly, and hard to reason\nabout. However, mistakes in con\ufb01guration can be costly, leading to serious loss of time, waste of\ncomputing resources, or production issues.\nAlso, in a mature system which is being actively developed, the number of lines of con\ufb01guration\ncan far exceed the number of lines of the code that actually does machine learning. Each line has a\npotential for mistakes, and con\ufb01gurations are by their nature ephemeral and less well tested.\nAssertions about con\ufb01guration invariants can be critical to prevent mistakes, but careful thought is\nneeded about what kind of assertions will be useful. Another useful tool is the ability to present\nvisual side-by-side differences (diffs) of two con\ufb01gurations. Because con\ufb01gurations are often copy-\nand-pasted with small modi\ufb01cations, such diffs highlight important changes. And clearly, con\ufb01gu-\nrations should be treated with the same level of seriousness as code changes, and be carefully code\nreviewed by peers.\n5\nDealing with Changes in the External World\nOne of the things that makes machine learning systems so fascinating is that they often interact\ndirectly with the external world. Experience has shown that the external world is rarely stable.\nIndeed, the changing nature of the world is one of the sources of technical debt in machine learning\nsystems.\n5.1\nFixed Thresholds in Dynamic Systems\nIt is often necessary to pick a decision threshold for a given model to perform some action: to predict\ntrue or false, to mark an email as spam or not spam, to show or not show a given ad. One classic\napproach in machine learning is to choose a threshold from a set of possible thresholds, in order to\nget good tradeoffs on certain metrics, such as precision and recall. However, such thresholds are\noften manually set. Thus if a model updates on new data, the old manually set threshold may be\ninvalid. Manually updating many thresholds across many models is time-consuming and brittle.\nA useful mitigation strategy for this kind of problem appears in [8], in which thresholds are learned\nvia simple evaluation on heldout validation data.\n7\n", []], "Configuration Debt": ["with vary over time; a redesign and a rewrite of some pieces may be needed periodically in order to\nmove forward ef\ufb01ciently.\nAs a real-world anecdote, in a recent cleanup effort of one important machine learning system at\nGoogle, it was found possible to rip out tens of thousands of lines of unused experimental code-\npaths. A follow-on rewrite with a tighter API allowed experimentation with new algorithms to be\nperformed with dramatically reduced effort and production risk and minimal incremental system\ncomplexity.\n4.4\nCon\ufb01guration Debt\nAnother potentially surprising area where debt can accumulate is in the con\ufb01guration of machine\nlearning systems. Any large system has a wide range of con\ufb01gurable options, including which fea-\ntures are used, how data is selected, a wide variety of algorithm-speci\ufb01c learning settings, potential\npre- or post-processing, veri\ufb01cation methods, etc.\nMany engineers do a great job of thinking hard about abstractions and unit tests in production code,\nbut may treat con\ufb01guration (and extension of con\ufb01guration) as an afterthought. Indeed, veri\ufb01cation\nor testing of con\ufb01gurations may not even be seen as important. Con\ufb01guration by its very nature\ntends to be the place where real-world messiness intrudes on beautiful algorithms.\nConsider the following examples. Feature A was incorrectly logged from 9/14 to 9/17. Feature\nB is not available on data before 10/7. The code used to compute feature C has to change for\ndata before and after 11/1 because of changes to the logging format. Feature D is not available\nin production, so a substitute features D\u2032 and D\u2032\u2032 must be used when querying the model in a live\nsetting. If feature Z is used, then jobs for training must be given extra memory due to lookup\ntables or they will train inef\ufb01ciently. Feature Q precludes the use of feature R because of latency\nconstraints. All this messiness makes con\ufb01guration hard to modify correctly, and hard to reason\nabout. However, mistakes in con\ufb01guration can be costly, leading to serious loss of time, waste of\ncomputing resources, or production issues.\nAlso, in a mature system which is being actively developed, the number of lines of con\ufb01guration\ncan far exceed the number of lines of the code that actually does machine learning. Each line has a\npotential for mistakes, and con\ufb01gurations are by their nature ephemeral and less well tested.\nAssertions about con\ufb01guration invariants can be critical to prevent mistakes, but careful thought is\nneeded about what kind of assertions will be useful. Another useful tool is the ability to present\nvisual side-by-side differences (diffs) of two con\ufb01gurations. Because con\ufb01gurations are often copy-\nand-pasted with small modi\ufb01cations, such diffs highlight important changes. And clearly, con\ufb01gu-\nrations should be treated with the same level of seriousness as code changes, and be carefully code\nreviewed by peers.\n5\nDealing with Changes in the External World\nOne of the things that makes machine learning systems so fascinating is that they often interact\ndirectly with the external world. Experience has shown that the external world is rarely stable.\nIndeed, the changing nature of the world is one of the sources of technical debt in machine learning\nsystems.\n5.1\nFixed Thresholds in Dynamic Systems\nIt is often necessary to pick a decision threshold for a given model to perform some action: to predict\ntrue or false, to mark an email as spam or not spam, to show or not show a given ad. One classic\napproach in machine learning is to choose a threshold from a set of possible thresholds, in order to\nget good tradeoffs on certain metrics, such as precision and recall. However, such thresholds are\noften manually set. Thus if a model updates on new data, the old manually set threshold may be\ninvalid. Manually updating many thresholds across many models is time-consuming and brittle.\nA useful mitigation strategy for this kind of problem appears in [8], in which thresholds are learned\nvia simple evaluation on heldout validation data.\n7\n", []], "Dead Experimental Codepaths": ["imentation with other machine learning approaches prohibitively expensive, resulting in an ongoing\ntax on innovation.\nGlue code can be reduced by choosing to re-implement speci\ufb01c algorithms within the broader system\narchitecture. At \ufb01rst, this may seem like a high cost to pay\u2014re-implementing a machine learning\npackage in C++ or Java that is already available in R or matlab, for example, may appear to be\na waste of effort. But the resulting system may require dramatically less glue code to integrate in\nthe overall system, be easier to test, be easier to maintain, and be better designed to allow alternate\napproaches to be plugged in and empirically tested. Problem-speci\ufb01c machine learning code can\nalso be tweaked with problem-speci\ufb01c knowledge that is hard to support in general packages.\nIt may be surprising to the academic community to know that only a tiny fraction of the code in\nmany machine learning systems is actually doing \u201cmachine learning\u201d. When we recognize that a\nmature system might end up being (at most) 5% machine learning code and (at least) 95% glue code,\nreimplementation rather than reuse of a clumsy API looks like a much better strategy.\n4.2\nPipeline Jungles\nAs a special case of glue code, pipeline jungles often appear in data preparation. These can evolve\norganically, as new signals are identi\ufb01ed and new information sources added. Without care, the\nresulting system for preparing data in an ML-friendly format may become a jungle of scrapes, joins,\nand sampling steps, often with intermediate \ufb01les output. Managing these pipelines, detecting errors\nand recovering from failures are all dif\ufb01cult and costly [1]. Testing such pipelines often requires\nexpensive end-to-end integration tests. All of this adds to technical debt of a system and makes\nfurther innovation more costly.\nPipeline jungles can only be avoided by thinking holistically about data collection and feature ex-\ntraction. The clean-slate approach of scrapping a pipeline jungle and redesigning from the ground\nup is indeed a major investment of engineering effort, but one that can dramatically reduce ongoing\ncosts and speed further innovation.\nIt\u2019s worth noting that glue code and pipeline jungles are symptomatic of integration issues that may\nhave a root cause in overly separated \u201cresearch\u201d and \u201cengineering\u201d roles. When machine learning\npackages are developed in an ivory-tower setting, the resulting packages may appear to be more\nlike black boxes to the teams that actually employ them in practice. At Google, a hybrid research\napproach where engineers and researchers are embedded together on the same teams (and indeed,\nare often the same people) has helped reduce this source of friction signi\ufb01cantly [10]. But even\nwhen a fully integrated team structure is not possible, it can be advantageous to have close, active\ncollaborations.\n4.3\nDead Experimental Codepaths\nA common reaction to the hardening of glue code or pipeline jungles is that it becomes more and\nmore tempting to perform experiments with alternative algorithms or tweaks by implementing these\nexperimental codepaths as conditional branches within the main production code. For any individ-\nual change, the cost of experimenting in this manner is relatively low\u2014none of the surrounding\ninfrastructure needs to be reworked. However, over time, these accumulated codepaths can create\na growing debt. Maintaining backward compatibility with experimental codepaths is a burden for\nmaking more substantive changes. Furthermore, obsolete experimental codepaths can interact with\neach other in unpredictable ways, and tracking which combinations are incompatible quickly results\nin an exponential blowup in system complexity. A famous example of the dangers here was Knight\nCapital\u2019s system losing $465 million in 45 minutes apparently because of unexpected behavior from\nobsolete experimental codepaths [9].\nAs with the case of dead \ufb02ags in traditional software [7], it is often bene\ufb01cial to periodically re-\nexamine each experimental branch to see what can be ripped out. Very often only a small subset of\nthe possible branches is actually used; many others may have been tested once and abandoned.\nDead experimental codepaths are a symptom of a more fundamental issue: in a healthy machine\nlearning system, experimental code should be well isolated, not leaving tendrils in multiple modules.\nThis may require rethinking code APIs. In our experience, the kinds of things we want to experiment\n6\n", []], "Pipeline Jungles": ["imentation with other machine learning approaches prohibitively expensive, resulting in an ongoing\ntax on innovation.\nGlue code can be reduced by choosing to re-implement speci\ufb01c algorithms within the broader system\narchitecture. At \ufb01rst, this may seem like a high cost to pay\u2014re-implementing a machine learning\npackage in C++ or Java that is already available in R or matlab, for example, may appear to be\na waste of effort. But the resulting system may require dramatically less glue code to integrate in\nthe overall system, be easier to test, be easier to maintain, and be better designed to allow alternate\napproaches to be plugged in and empirically tested. Problem-speci\ufb01c machine learning code can\nalso be tweaked with problem-speci\ufb01c knowledge that is hard to support in general packages.\nIt may be surprising to the academic community to know that only a tiny fraction of the code in\nmany machine learning systems is actually doing \u201cmachine learning\u201d. When we recognize that a\nmature system might end up being (at most) 5% machine learning code and (at least) 95% glue code,\nreimplementation rather than reuse of a clumsy API looks like a much better strategy.\n4.2\nPipeline Jungles\nAs a special case of glue code, pipeline jungles often appear in data preparation. These can evolve\norganically, as new signals are identi\ufb01ed and new information sources added. Without care, the\nresulting system for preparing data in an ML-friendly format may become a jungle of scrapes, joins,\nand sampling steps, often with intermediate \ufb01les output. Managing these pipelines, detecting errors\nand recovering from failures are all dif\ufb01cult and costly [1]. Testing such pipelines often requires\nexpensive end-to-end integration tests. All of this adds to technical debt of a system and makes\nfurther innovation more costly.\nPipeline jungles can only be avoided by thinking holistically about data collection and feature ex-\ntraction. The clean-slate approach of scrapping a pipeline jungle and redesigning from the ground\nup is indeed a major investment of engineering effort, but one that can dramatically reduce ongoing\ncosts and speed further innovation.\nIt\u2019s worth noting that glue code and pipeline jungles are symptomatic of integration issues that may\nhave a root cause in overly separated \u201cresearch\u201d and \u201cengineering\u201d roles. When machine learning\npackages are developed in an ivory-tower setting, the resulting packages may appear to be more\nlike black boxes to the teams that actually employ them in practice. At Google, a hybrid research\napproach where engineers and researchers are embedded together on the same teams (and indeed,\nare often the same people) has helped reduce this source of friction signi\ufb01cantly [10]. But even\nwhen a fully integrated team structure is not possible, it can be advantageous to have close, active\ncollaborations.\n4.3\nDead Experimental Codepaths\nA common reaction to the hardening of glue code or pipeline jungles is that it becomes more and\nmore tempting to perform experiments with alternative algorithms or tweaks by implementing these\nexperimental codepaths as conditional branches within the main production code. For any individ-\nual change, the cost of experimenting in this manner is relatively low\u2014none of the surrounding\ninfrastructure needs to be reworked. However, over time, these accumulated codepaths can create\na growing debt. Maintaining backward compatibility with experimental codepaths is a burden for\nmaking more substantive changes. Furthermore, obsolete experimental codepaths can interact with\neach other in unpredictable ways, and tracking which combinations are incompatible quickly results\nin an exponential blowup in system complexity. A famous example of the dangers here was Knight\nCapital\u2019s system losing $465 million in 45 minutes apparently because of unexpected behavior from\nobsolete experimental codepaths [9].\nAs with the case of dead \ufb02ags in traditional software [7], it is often bene\ufb01cial to periodically re-\nexamine each experimental branch to see what can be ripped out. Very often only a small subset of\nthe possible branches is actually used; many others may have been tested once and abandoned.\nDead experimental codepaths are a symptom of a more fundamental issue: in a healthy machine\nlearning system, experimental code should be well isolated, not leaving tendrils in multiple modules.\nThis may require rethinking code APIs. In our experience, the kinds of things we want to experiment\n6\n", []], "Glue Code": ["dictionary must be changed; in a large company, it may not be easy even to \ufb01nd all the consumers\nof the dictionary. Or suppose that for ef\ufb01ciency a particular signal will no longer be computed; are\nall former consumers of the signal done with it? Even if there are no references to it in the current\nversion of the codebase, are there still production instances with older binaries that use it? Making\nchanges safely can be dif\ufb01cult without automatic tooling.\nA remarkably useful automated feature management tool was described in [6], which enables data\nsources and features to be annotated. Automated checks can then be run to ensure that all depen-\ndencies have the appropriate annotations, and dependency trees can be fully resolved. Since its\nadoption, this approach has regularly allowed a team at Google to safely delete thousands of lines of\nfeature-related code per quarter, and has made veri\ufb01cation of versions and other issues automatic.\nThe system has on many occasions prevented accidental use of deprecated or broken features in new\nmodels.\n3.4\nCorrection Cascades\nThere are often situations in which model a for problem A exists, but a solution for a slightly\ndifferent problem A\u2032 is required. In this case, it can be tempting to learn a model a\u2032(a) that takes a\nas input and learns a small correction. This can appear to be a fast, low-cost win, as the correction\nmodel is likely very small and can often be done by a completely independent team. It is easy and\nquick to create a \ufb01rst version.\nHowever, this correction model has created a system dependency on a, making it signi\ufb01cantly more\nexpensive to analyze improvements to that model in the future. Things get even worse if correc-\ntion models are cascaded, with a model for problem A\u2032\u2032 learned on top of a\u2032, and so on. This can\neasily happen for closely related problems, such as calibrating outputs to slightly different test dis-\ntributions. It is not at all unlikely that a correction cascade will create a situation where improving\nthe accuracy of a actually leads to system-level detriments. Additionally, such systems may create\ndeadlock, where the coupled ML system is in a poor local optimum, and no component model may\nbe individually improved. At this point, the independent development that was initially attractive\nnow becomes a large barrier to progress.\nA mitigation strategy is to augment a to learn the corrections directly within the same model by\nadding features that help the model distinguish among the various use-cases. At test time, the model\nmay be queried with the appropriate features for the appropriate test distributions. This is not a free\nsolution\u2014the solutions for the various related problems remain coupled via CACE, but it may be\neasier to make updates and evaluate their impact.\n4\nSystem-level Spaghetti\nIt is unfortunately common for systems that incorporate machine learning methods to end up with\nhigh-debt design patterns. In this section, we examine several system-design anti-patterns [3] that\ncan surface in machine learning systems and which should be avoided or refactored where possible.\n4.1\nGlue Code\nMachine learning researchers tend to develop general purpose solutions as self-contained packages.\nA wide variety of these are available as open-source packages at places like mloss.org, or from\nin-house code, proprietary packages, and cloud-based platforms. Using self-contained solutions\noften results in a glue code system design pattern, in which a massive amount of supporting code is\nwritten to get data into and out of general-purpose packages.\nThis glue code design pattern can be costly in the long term, as it tends to freeze a system to the\npeculiarities of a speci\ufb01c package. General purpose solutions often have different design goals: they\nseek to provide one learning system to solve many problems, but many practical software systems\nare highly engineered to apply to one large-scale problem, for which many experimental solutions\nare sought. While generic systems might make it possible to interchange optimization algorithms,\nit is quite often refactoring of the construction of the problem space which yields the most bene\ufb01t\nto mature systems. The glue code pattern implicitly embeds this construction in supporting code\ninstead of in principally designed components. As a result, the glue code pattern often makes exper-\n5\n", []], "System-level Spaghetti": ["dictionary must be changed; in a large company, it may not be easy even to \ufb01nd all the consumers\nof the dictionary. Or suppose that for ef\ufb01ciency a particular signal will no longer be computed; are\nall former consumers of the signal done with it? Even if there are no references to it in the current\nversion of the codebase, are there still production instances with older binaries that use it? Making\nchanges safely can be dif\ufb01cult without automatic tooling.\nA remarkably useful automated feature management tool was described in [6], which enables data\nsources and features to be annotated. Automated checks can then be run to ensure that all depen-\ndencies have the appropriate annotations, and dependency trees can be fully resolved. Since its\nadoption, this approach has regularly allowed a team at Google to safely delete thousands of lines of\nfeature-related code per quarter, and has made veri\ufb01cation of versions and other issues automatic.\nThe system has on many occasions prevented accidental use of deprecated or broken features in new\nmodels.\n3.4\nCorrection Cascades\nThere are often situations in which model a for problem A exists, but a solution for a slightly\ndifferent problem A\u2032 is required. In this case, it can be tempting to learn a model a\u2032(a) that takes a\nas input and learns a small correction. This can appear to be a fast, low-cost win, as the correction\nmodel is likely very small and can often be done by a completely independent team. It is easy and\nquick to create a \ufb01rst version.\nHowever, this correction model has created a system dependency on a, making it signi\ufb01cantly more\nexpensive to analyze improvements to that model in the future. Things get even worse if correc-\ntion models are cascaded, with a model for problem A\u2032\u2032 learned on top of a\u2032, and so on. This can\neasily happen for closely related problems, such as calibrating outputs to slightly different test dis-\ntributions. It is not at all unlikely that a correction cascade will create a situation where improving\nthe accuracy of a actually leads to system-level detriments. Additionally, such systems may create\ndeadlock, where the coupled ML system is in a poor local optimum, and no component model may\nbe individually improved. At this point, the independent development that was initially attractive\nnow becomes a large barrier to progress.\nA mitigation strategy is to augment a to learn the corrections directly within the same model by\nadding features that help the model distinguish among the various use-cases. At test time, the model\nmay be queried with the appropriate features for the appropriate test distributions. This is not a free\nsolution\u2014the solutions for the various related problems remain coupled via CACE, but it may be\neasier to make updates and evaluate their impact.\n4\nSystem-level Spaghetti\nIt is unfortunately common for systems that incorporate machine learning methods to end up with\nhigh-debt design patterns. In this section, we examine several system-design anti-patterns [3] that\ncan surface in machine learning systems and which should be avoided or refactored where possible.\n4.1\nGlue Code\nMachine learning researchers tend to develop general purpose solutions as self-contained packages.\nA wide variety of these are available as open-source packages at places like mloss.org, or from\nin-house code, proprietary packages, and cloud-based platforms. Using self-contained solutions\noften results in a glue code system design pattern, in which a massive amount of supporting code is\nwritten to get data into and out of general-purpose packages.\nThis glue code design pattern can be costly in the long term, as it tends to freeze a system to the\npeculiarities of a speci\ufb01c package. General purpose solutions often have different design goals: they\nseek to provide one learning system to solve many problems, but many practical software systems\nare highly engineered to apply to one large-scale problem, for which many experimental solutions\nare sought. While generic systems might make it possible to interchange optimization algorithms,\nit is quite often refactoring of the construction of the problem space which yields the most bene\ufb01t\nto mature systems. The glue code pattern implicitly embeds this construction in supporting code\ninstead of in principally designed components. As a result, the glue code pattern often makes exper-\n5\n", []], "Correction Cascades": ["dictionary must be changed; in a large company, it may not be easy even to \ufb01nd all the consumers\nof the dictionary. Or suppose that for ef\ufb01ciency a particular signal will no longer be computed; are\nall former consumers of the signal done with it? Even if there are no references to it in the current\nversion of the codebase, are there still production instances with older binaries that use it? Making\nchanges safely can be dif\ufb01cult without automatic tooling.\nA remarkably useful automated feature management tool was described in [6], which enables data\nsources and features to be annotated. Automated checks can then be run to ensure that all depen-\ndencies have the appropriate annotations, and dependency trees can be fully resolved. Since its\nadoption, this approach has regularly allowed a team at Google to safely delete thousands of lines of\nfeature-related code per quarter, and has made veri\ufb01cation of versions and other issues automatic.\nThe system has on many occasions prevented accidental use of deprecated or broken features in new\nmodels.\n3.4\nCorrection Cascades\nThere are often situations in which model a for problem A exists, but a solution for a slightly\ndifferent problem A\u2032 is required. In this case, it can be tempting to learn a model a\u2032(a) that takes a\nas input and learns a small correction. This can appear to be a fast, low-cost win, as the correction\nmodel is likely very small and can often be done by a completely independent team. It is easy and\nquick to create a \ufb01rst version.\nHowever, this correction model has created a system dependency on a, making it signi\ufb01cantly more\nexpensive to analyze improvements to that model in the future. Things get even worse if correc-\ntion models are cascaded, with a model for problem A\u2032\u2032 learned on top of a\u2032, and so on. This can\neasily happen for closely related problems, such as calibrating outputs to slightly different test dis-\ntributions. It is not at all unlikely that a correction cascade will create a situation where improving\nthe accuracy of a actually leads to system-level detriments. Additionally, such systems may create\ndeadlock, where the coupled ML system is in a poor local optimum, and no component model may\nbe individually improved. At this point, the independent development that was initially attractive\nnow becomes a large barrier to progress.\nA mitigation strategy is to augment a to learn the corrections directly within the same model by\nadding features that help the model distinguish among the various use-cases. At test time, the model\nmay be queried with the appropriate features for the appropriate test distributions. This is not a free\nsolution\u2014the solutions for the various related problems remain coupled via CACE, but it may be\neasier to make updates and evaluate their impact.\n4\nSystem-level Spaghetti\nIt is unfortunately common for systems that incorporate machine learning methods to end up with\nhigh-debt design patterns. In this section, we examine several system-design anti-patterns [3] that\ncan surface in machine learning systems and which should be avoided or refactored where possible.\n4.1\nGlue Code\nMachine learning researchers tend to develop general purpose solutions as self-contained packages.\nA wide variety of these are available as open-source packages at places like mloss.org, or from\nin-house code, proprietary packages, and cloud-based platforms. Using self-contained solutions\noften results in a glue code system design pattern, in which a massive amount of supporting code is\nwritten to get data into and out of general-purpose packages.\nThis glue code design pattern can be costly in the long term, as it tends to freeze a system to the\npeculiarities of a speci\ufb01c package. General purpose solutions often have different design goals: they\nseek to provide one learning system to solve many problems, but many practical software systems\nare highly engineered to apply to one large-scale problem, for which many experimental solutions\nare sought. While generic systems might make it possible to interchange optimization algorithms,\nit is quite often refactoring of the construction of the problem space which yields the most bene\ufb01t\nto mature systems. The glue code pattern implicitly embeds this construction in supporting code\ninstead of in principally designed components. As a result, the glue code pattern often makes exper-\n5\n", []], "Static Analysis of Data Dependencies": ["behavior over time. This can happen implicitly, when the input signal comes from another machine\nlearning model itself that updates over time, or a data-dependent lookup table, such as for computing\nTF/IDF scores or semantic mappings. It can also happen explicitly, when the engineering ownership\nof the input signal is separate from the engineering ownership of the model that consumes it. In such\ncases, changes and improvements to the input signal may be regularly rolled out, without regard\nfor how the machine learning system may be affected. As noted above in the CACE principle,\n\u201cimprovements\u201d to input signals may have arbitrary, sometimes deleterious, effects that are costly\nto diagnose and address.\nOne common mitigation strategy for unstable data dependencies is to create a versioned copy of a\ngiven signal. For example, rather than allowing a semantic mapping of words to topic clusters to\nchange over time, it might be reasonable to create a frozen version of this mapping and use it until\nsuch a time as an updated version has been fully vetted. Versioning carries its own costs, however,\nsuch as potential staleness. And the requirement to maintain multiple versions of the same signal\nover time is a contributor to technical debt in its own right.\n3.2\nUnderutilized Data Dependencies\nIn code, underutilized dependencies are packages that are mostly unneeded [7]. Similarly, under-\nutilized data dependencies include input features or signals that provide little incremental value in\nterms of accuracy. Underutilized dependencies are costly, since they make the system unnecessarily\nvulnerable to changes.\nUnderutilized dependencies can creep into a machine learning model in several ways.\nLegacy Features. The most common is that a feature F is included in a model early in its develop-\nment. As time goes on, other features are added that make F mostly or entirely redundant, but this\nis not detected.\nBundled Features. Sometimes, a group of features is evaluated and found to be bene\ufb01cial. Because\nof deadline pressures or similar effects, all the features in the bundle are added to the model together.\nThis form of process can hide features that add little or no value.\n\u01eb-Features. As machine learning researchers, it is satisfying to improve model accuracy. It can be\ntempting to add a new feature to a model that improves accuracy, even when the accuracy gain is\nvery small or when the complexity overhead might be high.\nIn all these cases, features could be removed from the model with little or no loss in accuracy. But\nbecause they are still present, the model will likely assign them some weight, and the system is\ntherefore vulnerable, sometimes catastrophically so, to changes in these unnecessary features.\nAs an example, suppose that after a team merger, to ease the transition from an old product number-\ning scheme to new product numbers, both schemes are left in the system as features. New products\nget only a new number, but old products may have both. The machine learning algorithm knows\nof no reason to reduce its reliance on the old numbers. A year later, someone acting with good\nintent cleans up the code that stops populating the database with the old numbers. This change goes\nundetected by regression tests because no one else is using them any more. This will not be a good\nday for the maintainers of the machine learning system.\nA common mitigation strategy for under-utilized dependencies is to regularly evaluate the effect\nof removing individual features from a given model and act on this information whenever possi-\nble. More broadly, it may be important to build cultural awareness about the long-term bene\ufb01t of\nunderutilized dependency cleanup.\n3.3\nStatic Analysis of Data Dependencies\nOne of the key issues in data dependency debt is the dif\ufb01culty of performing static analysis. While\ncompilers and build systems typically provide such functionality for code, data dependencies may\nrequire additional tooling to track. Without this, it can be dif\ufb01cult to manually track the use of\ndata in a system. On teams with many engineers, or if there are multiple interacting teams, not\neveryone knows the status of every single feature, and it can be dif\ufb01cult for any individual human\nto know every last place where the feature was used. For example, suppose that the version of a\n4\n", []], "Underutilized Data Dependencies": ["behavior over time. This can happen implicitly, when the input signal comes from another machine\nlearning model itself that updates over time, or a data-dependent lookup table, such as for computing\nTF/IDF scores or semantic mappings. It can also happen explicitly, when the engineering ownership\nof the input signal is separate from the engineering ownership of the model that consumes it. In such\ncases, changes and improvements to the input signal may be regularly rolled out, without regard\nfor how the machine learning system may be affected. As noted above in the CACE principle,\n\u201cimprovements\u201d to input signals may have arbitrary, sometimes deleterious, effects that are costly\nto diagnose and address.\nOne common mitigation strategy for unstable data dependencies is to create a versioned copy of a\ngiven signal. For example, rather than allowing a semantic mapping of words to topic clusters to\nchange over time, it might be reasonable to create a frozen version of this mapping and use it until\nsuch a time as an updated version has been fully vetted. Versioning carries its own costs, however,\nsuch as potential staleness. And the requirement to maintain multiple versions of the same signal\nover time is a contributor to technical debt in its own right.\n3.2\nUnderutilized Data Dependencies\nIn code, underutilized dependencies are packages that are mostly unneeded [7]. Similarly, under-\nutilized data dependencies include input features or signals that provide little incremental value in\nterms of accuracy. Underutilized dependencies are costly, since they make the system unnecessarily\nvulnerable to changes.\nUnderutilized dependencies can creep into a machine learning model in several ways.\nLegacy Features. The most common is that a feature F is included in a model early in its develop-\nment. As time goes on, other features are added that make F mostly or entirely redundant, but this\nis not detected.\nBundled Features. Sometimes, a group of features is evaluated and found to be bene\ufb01cial. Because\nof deadline pressures or similar effects, all the features in the bundle are added to the model together.\nThis form of process can hide features that add little or no value.\n\u01eb-Features. As machine learning researchers, it is satisfying to improve model accuracy. It can be\ntempting to add a new feature to a model that improves accuracy, even when the accuracy gain is\nvery small or when the complexity overhead might be high.\nIn all these cases, features could be removed from the model with little or no loss in accuracy. But\nbecause they are still present, the model will likely assign them some weight, and the system is\ntherefore vulnerable, sometimes catastrophically so, to changes in these unnecessary features.\nAs an example, suppose that after a team merger, to ease the transition from an old product number-\ning scheme to new product numbers, both schemes are left in the system as features. New products\nget only a new number, but old products may have both. The machine learning algorithm knows\nof no reason to reduce its reliance on the old numbers. A year later, someone acting with good\nintent cleans up the code that stops populating the database with the old numbers. This change goes\nundetected by regression tests because no one else is using them any more. This will not be a good\nday for the maintainers of the machine learning system.\nA common mitigation strategy for under-utilized dependencies is to regularly evaluate the effect\nof removing individual features from a given model and act on this information whenever possi-\nble. More broadly, it may be important to build cultural awareness about the long-term bene\ufb01t of\nunderutilized dependency cleanup.\n3.3\nStatic Analysis of Data Dependencies\nOne of the key issues in data dependency debt is the dif\ufb01culty of performing static analysis. While\ncompilers and build systems typically provide such functionality for code, data dependencies may\nrequire additional tooling to track. Without this, it can be dif\ufb01cult to manually track the use of\ndata in a system. On teams with many engineers, or if there are multiple interacting teams, not\neveryone knows the status of every single feature, and it can be dif\ufb01cult for any individual human\nto know every last place where the feature was used. For example, suppose that the version of a\n4\n", []], "Unstable Data Dependencies": ["2.2\nHidden Feedback Loops\nAnother worry for real-world systems lies in hidden feedback loops. Systems that learn from world\nbehavior are clearly intended to be part of a feedback loop. For example, a system for predicting the\nclick through rate (CTR) of news headlines on a website likely relies on user clicks as training labels,\nwhich in turn depend on previous predictions from the model. This leads to issues in analyzing\nsystem performance, but these are the obvious kinds of statistical challenges that machine learning\nresearchers may \ufb01nd natural to investigate [2].\nAs an example of a hidden loop, now imagine that one of the input features used in this CTR model\nis a feature xweek that reports how many news headlines the given user has clicked on in the past\nweek. If the CTR model is improved, it is likely that all users are given better recommendations and\nmany users will click on more headlines. However, the result of this effect may not fully surface for\nat least a week, as the xweek feature adjusts. Furthermore, if the model is updated on the new data,\neither in batch mode at a later time or in streaming fashion with online updates, the model may later\nadjust its opinion of the xweek feature in response. In such a setting, the system will slowly change\nbehavior, potentially over a time scale much longer than a week. Gradual changes not visible in\nquick experiments make analyzing the effect of proposed changes extremely dif\ufb01cult, and add cost\nto even simple improvements.\nWe recommend looking carefully for hidden feedback loops and removing them whenever feasible.\n2.3\nUndeclared Consumers\nOftentimes, a prediction from a machine learning model A is made accessible to a wide variety\nof systems, either at runtime or by writing to logs that may later be consumed by other systems.\nIn more classical software engineering, these issues are referred to as visibility debt [7]. Without\naccess controls, it is possible for some of these consumers to be undeclared consumers, consuming\nthe output of a given prediction model as an input to another component of the system. Undeclared\nconsumers are expensive at best and dangerous at worst.\nThe expense of undeclared consumers is drawn from the sudden tight coupling of model A to other\nparts of the stack. Changes to A will very likely impact these other parts, sometimes in ways that are\nunintended, poorly understood, or detrimental. In practice, this has the effect of making it dif\ufb01cult\nand expensive to make any changes to A at all.\nThe danger of undeclared consumers is that they may introduce additional hidden feedback loops.\nImagine in our news headline CTR prediction system that there is another component of the system\nin charge of \u201cintelligently\u201d determining the size of the font used for the headline. If this font-size\nmodule starts consuming CTR as an input signal, and font-size has an effect on user propensity to\nclick, then the inclusion of CTR in font-size adds a new hidden feedback loop. It\u2019s easy to imagine\na case where such a system would gradually and endlessly increase the size of all headlines.\nUndeclared consumers may be dif\ufb01cult to detect unless the system is speci\ufb01cally designed to guard\nagainst this case. In the absence of barriers, engineers may naturally grab for the most convenient\nsignal, especially when there are deadline pressures.\n3\nData Dependencies Cost More than Code Dependencies\nIn [7], dependency debt is noted as a key contributor to code complexity and technical debt in\nclassical software engineering settings. We argue here that data dependencies in machine learning\nsystems carry a similar capacity for building debt. Furthermore, while code dependencies can be\nrelatively easy to identify via static analysis, linkage graphs, and the like, it is far less common that\ndata dependencies have similar analysis tools. Thus, it can be inappropriately easy to build large\ndata-dependency chains that can be dif\ufb01cult to untangle.\n3.1\nUnstable Data Dependencies\nTo move quickly, it is often convenient to consume signals as input features that are produced by\nother systems. However, some input signals are unstable, meaning that they qualitatively change\n3\n", []], "Data Dependencies Cost More than Code Dependencies": ["2.2\nHidden Feedback Loops\nAnother worry for real-world systems lies in hidden feedback loops. Systems that learn from world\nbehavior are clearly intended to be part of a feedback loop. For example, a system for predicting the\nclick through rate (CTR) of news headlines on a website likely relies on user clicks as training labels,\nwhich in turn depend on previous predictions from the model. This leads to issues in analyzing\nsystem performance, but these are the obvious kinds of statistical challenges that machine learning\nresearchers may \ufb01nd natural to investigate [2].\nAs an example of a hidden loop, now imagine that one of the input features used in this CTR model\nis a feature xweek that reports how many news headlines the given user has clicked on in the past\nweek. If the CTR model is improved, it is likely that all users are given better recommendations and\nmany users will click on more headlines. However, the result of this effect may not fully surface for\nat least a week, as the xweek feature adjusts. Furthermore, if the model is updated on the new data,\neither in batch mode at a later time or in streaming fashion with online updates, the model may later\nadjust its opinion of the xweek feature in response. In such a setting, the system will slowly change\nbehavior, potentially over a time scale much longer than a week. Gradual changes not visible in\nquick experiments make analyzing the effect of proposed changes extremely dif\ufb01cult, and add cost\nto even simple improvements.\nWe recommend looking carefully for hidden feedback loops and removing them whenever feasible.\n2.3\nUndeclared Consumers\nOftentimes, a prediction from a machine learning model A is made accessible to a wide variety\nof systems, either at runtime or by writing to logs that may later be consumed by other systems.\nIn more classical software engineering, these issues are referred to as visibility debt [7]. Without\naccess controls, it is possible for some of these consumers to be undeclared consumers, consuming\nthe output of a given prediction model as an input to another component of the system. Undeclared\nconsumers are expensive at best and dangerous at worst.\nThe expense of undeclared consumers is drawn from the sudden tight coupling of model A to other\nparts of the stack. Changes to A will very likely impact these other parts, sometimes in ways that are\nunintended, poorly understood, or detrimental. In practice, this has the effect of making it dif\ufb01cult\nand expensive to make any changes to A at all.\nThe danger of undeclared consumers is that they may introduce additional hidden feedback loops.\nImagine in our news headline CTR prediction system that there is another component of the system\nin charge of \u201cintelligently\u201d determining the size of the font used for the headline. If this font-size\nmodule starts consuming CTR as an input signal, and font-size has an effect on user propensity to\nclick, then the inclusion of CTR in font-size adds a new hidden feedback loop. It\u2019s easy to imagine\na case where such a system would gradually and endlessly increase the size of all headlines.\nUndeclared consumers may be dif\ufb01cult to detect unless the system is speci\ufb01cally designed to guard\nagainst this case. In the absence of barriers, engineers may naturally grab for the most convenient\nsignal, especially when there are deadline pressures.\n3\nData Dependencies Cost More than Code Dependencies\nIn [7], dependency debt is noted as a key contributor to code complexity and technical debt in\nclassical software engineering settings. We argue here that data dependencies in machine learning\nsystems carry a similar capacity for building debt. Furthermore, while code dependencies can be\nrelatively easy to identify via static analysis, linkage graphs, and the like, it is far less common that\ndata dependencies have similar analysis tools. Thus, it can be inappropriately easy to build large\ndata-dependency chains that can be dif\ufb01cult to untangle.\n3.1\nUnstable Data Dependencies\nTo move quickly, it is often convenient to consume signals as input features that are produced by\nother systems. However, some input signals are unstable, meaning that they qualitatively change\n3\n", []], "Undeclared Consumers": ["2.2\nHidden Feedback Loops\nAnother worry for real-world systems lies in hidden feedback loops. Systems that learn from world\nbehavior are clearly intended to be part of a feedback loop. For example, a system for predicting the\nclick through rate (CTR) of news headlines on a website likely relies on user clicks as training labels,\nwhich in turn depend on previous predictions from the model. This leads to issues in analyzing\nsystem performance, but these are the obvious kinds of statistical challenges that machine learning\nresearchers may \ufb01nd natural to investigate [2].\nAs an example of a hidden loop, now imagine that one of the input features used in this CTR model\nis a feature xweek that reports how many news headlines the given user has clicked on in the past\nweek. If the CTR model is improved, it is likely that all users are given better recommendations and\nmany users will click on more headlines. However, the result of this effect may not fully surface for\nat least a week, as the xweek feature adjusts. Furthermore, if the model is updated on the new data,\neither in batch mode at a later time or in streaming fashion with online updates, the model may later\nadjust its opinion of the xweek feature in response. In such a setting, the system will slowly change\nbehavior, potentially over a time scale much longer than a week. Gradual changes not visible in\nquick experiments make analyzing the effect of proposed changes extremely dif\ufb01cult, and add cost\nto even simple improvements.\nWe recommend looking carefully for hidden feedback loops and removing them whenever feasible.\n2.3\nUndeclared Consumers\nOftentimes, a prediction from a machine learning model A is made accessible to a wide variety\nof systems, either at runtime or by writing to logs that may later be consumed by other systems.\nIn more classical software engineering, these issues are referred to as visibility debt [7]. Without\naccess controls, it is possible for some of these consumers to be undeclared consumers, consuming\nthe output of a given prediction model as an input to another component of the system. Undeclared\nconsumers are expensive at best and dangerous at worst.\nThe expense of undeclared consumers is drawn from the sudden tight coupling of model A to other\nparts of the stack. Changes to A will very likely impact these other parts, sometimes in ways that are\nunintended, poorly understood, or detrimental. In practice, this has the effect of making it dif\ufb01cult\nand expensive to make any changes to A at all.\nThe danger of undeclared consumers is that they may introduce additional hidden feedback loops.\nImagine in our news headline CTR prediction system that there is another component of the system\nin charge of \u201cintelligently\u201d determining the size of the font used for the headline. If this font-size\nmodule starts consuming CTR as an input signal, and font-size has an effect on user propensity to\nclick, then the inclusion of CTR in font-size adds a new hidden feedback loop. It\u2019s easy to imagine\na case where such a system would gradually and endlessly increase the size of all headlines.\nUndeclared consumers may be dif\ufb01cult to detect unless the system is speci\ufb01cally designed to guard\nagainst this case. In the absence of barriers, engineers may naturally grab for the most convenient\nsignal, especially when there are deadline pressures.\n3\nData Dependencies Cost More than Code Dependencies\nIn [7], dependency debt is noted as a key contributor to code complexity and technical debt in\nclassical software engineering settings. We argue here that data dependencies in machine learning\nsystems carry a similar capacity for building debt. Furthermore, while code dependencies can be\nrelatively easy to identify via static analysis, linkage graphs, and the like, it is far less common that\ndata dependencies have similar analysis tools. Thus, it can be inappropriately easy to build large\ndata-dependency chains that can be dif\ufb01cult to untangle.\n3.1\nUnstable Data Dependencies\nTo move quickly, it is often convenient to consume signals as input features that are produced by\nother systems. However, some input signals are unstable, meaning that they qualitatively change\n3\n", []], "Hidden Feedback Loops": ["2.2\nHidden Feedback Loops\nAnother worry for real-world systems lies in hidden feedback loops. Systems that learn from world\nbehavior are clearly intended to be part of a feedback loop. For example, a system for predicting the\nclick through rate (CTR) of news headlines on a website likely relies on user clicks as training labels,\nwhich in turn depend on previous predictions from the model. This leads to issues in analyzing\nsystem performance, but these are the obvious kinds of statistical challenges that machine learning\nresearchers may \ufb01nd natural to investigate [2].\nAs an example of a hidden loop, now imagine that one of the input features used in this CTR model\nis a feature xweek that reports how many news headlines the given user has clicked on in the past\nweek. If the CTR model is improved, it is likely that all users are given better recommendations and\nmany users will click on more headlines. However, the result of this effect may not fully surface for\nat least a week, as the xweek feature adjusts. Furthermore, if the model is updated on the new data,\neither in batch mode at a later time or in streaming fashion with online updates, the model may later\nadjust its opinion of the xweek feature in response. In such a setting, the system will slowly change\nbehavior, potentially over a time scale much longer than a week. Gradual changes not visible in\nquick experiments make analyzing the effect of proposed changes extremely dif\ufb01cult, and add cost\nto even simple improvements.\nWe recommend looking carefully for hidden feedback loops and removing them whenever feasible.\n2.3\nUndeclared Consumers\nOftentimes, a prediction from a machine learning model A is made accessible to a wide variety\nof systems, either at runtime or by writing to logs that may later be consumed by other systems.\nIn more classical software engineering, these issues are referred to as visibility debt [7]. Without\naccess controls, it is possible for some of these consumers to be undeclared consumers, consuming\nthe output of a given prediction model as an input to another component of the system. Undeclared\nconsumers are expensive at best and dangerous at worst.\nThe expense of undeclared consumers is drawn from the sudden tight coupling of model A to other\nparts of the stack. Changes to A will very likely impact these other parts, sometimes in ways that are\nunintended, poorly understood, or detrimental. In practice, this has the effect of making it dif\ufb01cult\nand expensive to make any changes to A at all.\nThe danger of undeclared consumers is that they may introduce additional hidden feedback loops.\nImagine in our news headline CTR prediction system that there is another component of the system\nin charge of \u201cintelligently\u201d determining the size of the font used for the headline. If this font-size\nmodule starts consuming CTR as an input signal, and font-size has an effect on user propensity to\nclick, then the inclusion of CTR in font-size adds a new hidden feedback loop. It\u2019s easy to imagine\na case where such a system would gradually and endlessly increase the size of all headlines.\nUndeclared consumers may be dif\ufb01cult to detect unless the system is speci\ufb01cally designed to guard\nagainst this case. In the absence of barriers, engineers may naturally grab for the most convenient\nsignal, especially when there are deadline pressures.\n3\nData Dependencies Cost More than Code Dependencies\nIn [7], dependency debt is noted as a key contributor to code complexity and technical debt in\nclassical software engineering settings. We argue here that data dependencies in machine learning\nsystems carry a similar capacity for building debt. Furthermore, while code dependencies can be\nrelatively easy to identify via static analysis, linkage graphs, and the like, it is far less common that\ndata dependencies have similar analysis tools. Thus, it can be inappropriately easy to build large\ndata-dependency chains that can be dif\ufb01cult to untangle.\n3.1\nUnstable Data Dependencies\nTo move quickly, it is often convenient to consume signals as input features that are produced by\nother systems. However, some input signals are unstable, meaning that they qualitatively change\n3\n", []], "Entanglement": ["Indeed, a remarkable portion of real-world \u201cmachine learning\u201d work is devoted to tackling issues\nof this form. Paying down technical debt may initially appear less glamorous than research results\nusually reported in academic ML conferences. But it is critical for long-term system health and\nenables algorithmic advances and other cutting-edge improvements.\n2\nComplex Models Erode Boundaries\nTraditional software engineering practice has shown that strong abstraction boundaries using en-\ncapsulation and modular design help create maintainable code in which it is easy to make isolated\nchanges and improvements. Strict abstraction boundaries help express the invariants and logical\nconsistency of the information inputs and outputs from an given component [4].\nUnfortunately, it is dif\ufb01cult to enforce strict abstraction boundaries for machine learning systems\nby requiring these systems to adhere to speci\ufb01c intended behavior. Indeed, arguably the most im-\nportant reason for using a machine learning system is precisely that the desired behavior cannot be\neffectively implemented in software logic without dependency on external data. There is little way to\nseparate abstract behavioral invariants from quirks of data. The resulting erosion of boundaries can\ncause signi\ufb01cant increases in technical debt. In this section we look at several issues of this form.\n2.1\nEntanglement\nFrom a high level perspective, a machine learning package is a tool for mixing data sources together.\nThat is, machine learning models are machines for creating entanglement and making the isolation\nof improvements effectively impossible.\nTo make this concrete, imagine we have a system that uses features x1, ...xn in a model. If we\nchange the input distribution of values in x1, the importance, weights, or use of the remaining n \u22121\nfeatures may all change\u2014this is true whether the model is retrained fully in a batch style or allowed\nto adapt in an online fashion. Adding a new feature xn+1 can cause similar changes, as can removing\nany feature xj. No inputs are ever really independent. We refer to this here as the CACE principle:\nChanging Anything Changes Everything.\nThe net result of such changes is that prediction behavior may alter, either subtly or dramatically,\non various slices of the distribution. The same principle applies to hyper-parameters. Changes in\nregularization strength, learning settings, sampling methods in training, convergence thresholds, and\nessentially every other possible tweak can have similarly wide ranging effects.\nOne possible mitigation strategy is to isolate models and serve ensembles. This approach is useful\nin situations such as [8], in which sub-problems decompose naturally, or in which the cost of main-\ntaining separate models is outweighed by the bene\ufb01ts of enforced modularity. However, in many\nlarge-scale settings such a strategy may prove unscalable. And within a given model, the issues of\nentanglement may still be present.\nA second possible mitigation strategy is to develop methods of gaining deep insights into the be-\nhavior of model predictions. One such method was proposed in [6], in which a high-dimensional\nvisualization tool was used to allow researchers to quickly see effects across many dimensions and\nslicings. Metrics that operate on a slice-by-slice basis may also be extremely useful.\nA third possibility is to attempt to use more sophisticated regularization methods to enforce that any\nchanges in prediction performance carry a cost in the objective function used in training [5]. Like\nany other regularization approach, this kind of approach can be useful but is far from a guarantee and\nmay add more debt via increased system complexity than is reduced via decreased entanglement.\nThe above mitigation strategies may help, but this issue of entanglement is in some sense innate to\nmachine learning, regardless of the particular learning algorithm being used. In practice, this all too\noften means that shipping the \ufb01rst version of a machine learning system is easy, but that making\nsubsequent improvements is unexpectedly dif\ufb01cult. This consideration should be weighed carefully\nagainst deadline pressures for version 1.0 of any ML system.\n2\n", []], "Complex Models Erode Boundaries": ["Indeed, a remarkable portion of real-world \u201cmachine learning\u201d work is devoted to tackling issues\nof this form. Paying down technical debt may initially appear less glamorous than research results\nusually reported in academic ML conferences. But it is critical for long-term system health and\nenables algorithmic advances and other cutting-edge improvements.\n2\nComplex Models Erode Boundaries\nTraditional software engineering practice has shown that strong abstraction boundaries using en-\ncapsulation and modular design help create maintainable code in which it is easy to make isolated\nchanges and improvements. Strict abstraction boundaries help express the invariants and logical\nconsistency of the information inputs and outputs from an given component [4].\nUnfortunately, it is dif\ufb01cult to enforce strict abstraction boundaries for machine learning systems\nby requiring these systems to adhere to speci\ufb01c intended behavior. Indeed, arguably the most im-\nportant reason for using a machine learning system is precisely that the desired behavior cannot be\neffectively implemented in software logic without dependency on external data. There is little way to\nseparate abstract behavioral invariants from quirks of data. The resulting erosion of boundaries can\ncause signi\ufb01cant increases in technical debt. In this section we look at several issues of this form.\n2.1\nEntanglement\nFrom a high level perspective, a machine learning package is a tool for mixing data sources together.\nThat is, machine learning models are machines for creating entanglement and making the isolation\nof improvements effectively impossible.\nTo make this concrete, imagine we have a system that uses features x1, ...xn in a model. If we\nchange the input distribution of values in x1, the importance, weights, or use of the remaining n \u22121\nfeatures may all change\u2014this is true whether the model is retrained fully in a batch style or allowed\nto adapt in an online fashion. Adding a new feature xn+1 can cause similar changes, as can removing\nany feature xj. No inputs are ever really independent. We refer to this here as the CACE principle:\nChanging Anything Changes Everything.\nThe net result of such changes is that prediction behavior may alter, either subtly or dramatically,\non various slices of the distribution. The same principle applies to hyper-parameters. Changes in\nregularization strength, learning settings, sampling methods in training, convergence thresholds, and\nessentially every other possible tweak can have similarly wide ranging effects.\nOne possible mitigation strategy is to isolate models and serve ensembles. This approach is useful\nin situations such as [8], in which sub-problems decompose naturally, or in which the cost of main-\ntaining separate models is outweighed by the bene\ufb01ts of enforced modularity. However, in many\nlarge-scale settings such a strategy may prove unscalable. And within a given model, the issues of\nentanglement may still be present.\nA second possible mitigation strategy is to develop methods of gaining deep insights into the be-\nhavior of model predictions. One such method was proposed in [6], in which a high-dimensional\nvisualization tool was used to allow researchers to quickly see effects across many dimensions and\nslicings. Metrics that operate on a slice-by-slice basis may also be extremely useful.\nA third possibility is to attempt to use more sophisticated regularization methods to enforce that any\nchanges in prediction performance carry a cost in the objective function used in training [5]. Like\nany other regularization approach, this kind of approach can be useful but is far from a guarantee and\nmay add more debt via increased system complexity than is reduced via decreased entanglement.\nThe above mitigation strategies may help, but this issue of entanglement is in some sense innate to\nmachine learning, regardless of the particular learning algorithm being used. In practice, this all too\noften means that shipping the \ufb01rst version of a machine learning system is easy, but that making\nsubsequent improvements is unexpectedly dif\ufb01cult. This consideration should be weighed carefully\nagainst deadline pressures for version 1.0 of any ML system.\n2\n", []], "Machine Learning and Complex Systems": ["Machine Learning:\nThe High-Interest Credit Card of Technical Debt\nD. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov,\nTodd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young\n{dsculley,gholt,dgg,edavydov}@google.com\n{toddphillips,ebner,vchaudhary,mwyoung}@google.com\nGoogle, Inc\nAbstract\nMachine learning offers a fantastically powerful toolkit for building complex sys-\ntems quickly. This paper argues that it is dangerous to think of these quick wins\nas coming for free. Using the framework of technical debt, we note that it is re-\nmarkably easy to incur massive ongoing maintenance costs at the system level\nwhen applying machine learning. The goal of this paper is highlight several ma-\nchine learning speci\ufb01c risk factors and design patterns to be avoided or refactored\nwhere possible. These include boundary erosion, entanglement, hidden feedback\nloops, undeclared consumers, data dependencies, changes in the external world,\nand a variety of system-level anti-patterns.\n1\nMachine Learning and Complex Systems\nReal world software engineers are often faced with the challenge of moving quickly to ship new\nproducts or services, which can lead to a dilemma between speed of execution and quality of en-\ngineering. The concept of technical debt was \ufb01rst introduced by Ward Cunningham in 1992 as a\nway to help quantify the cost of such decisions. Like incurring \ufb01scal debt, there are often sound\nstrategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does\ntend to compound. Deferring the work to pay it off results in increasing costs, system brittleness,\nand reduced rates of innovation.\nTraditional methods of paying off technical debt include refactoring, increasing coverage of unit\ntests, deleting dead code, reducing dependencies, tightening APIs, and improving documentation\n[4]. The goal of these activities is not to add new functionality, but to make it easier to add future\nimprovements, be cheaper to maintain, and reduce the likelihood of bugs.\nOne of the basic arguments in this paper is that machine learning packages have all the basic code\ncomplexity issues as normal code, but also have a larger system-level complexity that can create\nhidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time\nwell spent but does not necessarily address debt at a systems level.\nIn this paper, we focus on the system-level interaction between machine learning code and larger sys-\ntems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine\nlearning model may subtly erode abstraction boundaries. It may be tempting to re-use input sig-\nnals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning\npackages may often be treated as black boxes, resulting in large masses of \u201cglue code\u201d or calibra-\ntion layers that can lock in assumptions. Changes in the external world may make models or input\nsignals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any\ndebt. Even monitoring that the system as a whole is operating as intended may be dif\ufb01cult without\ncareful design.\n1\n", []]}