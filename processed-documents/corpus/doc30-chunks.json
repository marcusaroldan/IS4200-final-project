{"References": ["Entropy 2021, 23, 18\n38 of 45\nTable A2. Repository Link (2).\nTool\nRepository Link\nHSJA\nhttps://github.com/Jianbo-Lab/HSJA\nin\ufb02uence-release\nhttps://github.com/kohpangwei/in\ufb02uence-release\niNNvestigate\nhttps://github.com/albermax/innvestigate\nIntegrated Gradients\nhttps://github.com/ankurtaly/Integrated-Gradients\nInterpretML\nhttps://github.com/interpretml/interpret\nL2X\nhttps://github.com/Jianbo-Lab/L2X\nlime\nhttps://github.com/marcotcr/lime\nML-fairness-gym\nhttps://github.com/google/ml-fairness-gym\nNattack\nhttps://github.com/gaussian-attack/Nattack\nnettack\nhttps://github.com/danielzuegner/nettack\nnlp_adversarial_examples\nhttps://github.com/nesl/nlp_adversarial_examples\nnn_robust_attacks\nhttps://github.com/carlini/nn_robust_attacks\none-pixel-attack-keras\nhttps://github.com/Hyperparticle/one-pixel-attack-keras\nPDPbox\nhttps://github.com/SauceCat/PDPbox\nprocedurally_fair_learning\nhttps://github.com/nina-gh/procedurally_fair_learning\npyBreakDown\nhttps://github.com/MI2DataLab/pyBreakDown\nPyCEbox\nhttps://github.com/AustinRochford/PyCEbox\nrationale\nhttps://github.com/taolei87/rcnn/tree/master/code/rationale\nRISE\nhttps://github.com/eclique/RISE\nscpn\nhttps://github.com/miyyer/scpn\nSALib\nhttps://github.com/SALib/SALib\nshap\nhttps://github.com/slundberg/shap\nSkater\nhttps://github.com/oracle/Skater\nSlim\nhttps://github.com/ustunb/slim-python\nstAdv\nhttps://github.com/rakutentech/stAdv\ntcav\nhttps://github.com/tensor\ufb02ow/tcav\nTextAttack\nhttps://github.com/QData/TextAttack\nTextFooler\nhttps://github.com/jind11/TextFooler\ntf-explain\nhttps://github.com/sicara/tf-explain\nThe LRP Toolbox\nhttps://github.com/sebastian-lapuschkin/lrp_toolbox\nthemis-ml\nhttps://github.com/cosmicBboy/themis-ml\ntransferability-advdnn-pub\nhttps://github.com/sunblaze-ucb/transferability-advdnn-pub\nUAN\nhttps://github.com/jhayes14/UAN\nuniversal\nhttps://github.com/LTS4/universal\nWordAdver\nhttps://github.com/AnyiRao/WordAdver\nZOO-Attack\nhttps://github.com/huanzhang12/ZOO-Attack\n", []], "Repository Links": ["Entropy 2021, 23, 18\n37 of 45\nimg\nImage Data\ntxt\nText Data\ngraph\nGraph Data\nAppendix A. Repository Links\nTable A1. Repository Links.\nTool\nRepository Link\naccessorize-to-a-crime\nhttps://github.com/mahmoods01/accessorize-to-a-crime\nadversarial-squad\nhttps://github.com/robinjia/adversarial-squad\nadversarial_text\nhttps://github.com/aonotas/adversarial_text\nadversarial_training\nhttps://github.com/WangJiuniu/adversarial_training\nadversarial_training_methods\nhttps://github.com/enry12/adversarial_training_methods\naequitas\nhttps://github.com/dssg/aequitas\nAIF360\nhttps://github.com/Trusted-AI/AIF360\nAIX360\nhttps://github.com/Trusted-AI/AIX360\nalibi\nhttps://github.com/SeldonIO/alibi\nAnalysisBySynthesis\nhttps://github.com/bethgelab/AnalysisBySynthesis\nAnchor\nhttps://github.com/marcotcr/anchor\nboundary-attack\nhttps://github.com/greentfrapp/boundary-attack\nCAM\nhttps://github.com/zhoubolei/CAM\ncleverhans\nhttps://github.com/tensor\ufb02ow/cleverhans\ndebiaswe\nhttps://github.com/tolga-b/debiaswe\nDeep Visualization Toolbox\nhttps://github.com/yosinski/deep-visualization-toolbox\nDeepExplain\nhttps://github.com/marcoancona/DeepExplain\nDeepLift\nhttps://github.com/kundajelab/deeplift\nDLIME\nhttps://github.com/rehmanzafar/dlime_experiments\nEli5\nhttps://github.com/TeamHG-Memex/eli5\nequalized_odds_and_calibration\nhttps://github.com/gpleiss/equalized_odds_and_calibration\nfair-classi\ufb01cation\nhttps://github.com/mbilalzafar/fair-classi\ufb01cation\nfairlearn\nhttps://github.com/fairlearn/fairlearn\nFairMachineLearning\nhttps://github.com/jtcho/FairMachineLearning\nfairml\nhttps://github.com/adebayoj/fairml\nfairness\nhttps://github.com/dodger487/fairness\nfairness-comparison\nhttps://github.com/algofairness/fairness-comparison\nfairness-in-ml\nhttps://github.com/equialgo/fairness-in-ml\nfoolbox\nhttps://github.com/bethgelab/foolbox\nGerryFair\nhttps://github.com/algowatchpenn/GerryFair\ngnn-meta-attack\nhttps://github.com/danielzuegner/gnn-meta-attack\nGrad-CAM\nhttps://github.com/ramprs/grad-cam\nGrad-CAM ++\nhttps://github.com/adityac94/Grad_CAM_plus_plus\ngraph_adversarial_attack\nhttps://github.com/Hanjun-Dai/graph_adversarial_attack\n", []], "Discussion and Conclusions": ["Entropy 2021, 23, 18\n35 of 45\nTable 6. Cont.\nRef\nTool\nCategory\nLocal vs.\nGlobal\nModel Speci\ufb01c vs.\nModel Agnostic\nData\nType\nCitations/\nYear\nYear\n[159]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n31.7\n2018\n[130]\nHSJA\nS\nL & G\nAgnostic\nimg\n31.5\n2019\n[160]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n29.5\n2019\n[122]\nNattack\nS\nL & G\nAgnostic\nimg\n29\n2019\n[129]\nfoolbox\nS\nL & G\nAgnostic\nimg\n26.8\n2016\n[157]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n26.7\n2018\n[161]\nTextAttack\nTextFooler\nS\nL & G\nSpeci\ufb01c\ntxt\n21\n2019\n[125]\nnn_robust_attacks\nS\nL & G\nAgnostic\nimg\n11\n2017\n[165]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n10\n2020\n[142]\nUAN\nS\nL & G\nAgnostic\nimg\n9.7\n2018\n[154]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n9.3\n2018\n[136]\nfoolbox\nS\nL & G\nAgnostic\nimg\n6.5\n2019\n[155]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n6.5\n2019\n[163]\nTextAttack\nS\nL & G\nSpeci\ufb01c\ntxt\n5\n2020\n[164]\nTextAttack\nS\nL & G\nAgnostic\ntxt\n5\n2020\n[162]\nTextAttack\nS\nL & G\nSpeci\ufb01c\ntxt\n4\n2020\n[144]\nfoolbox\nS\nL & G\nAgnostic\nimg\n1.5\n2019\n4. Discussion and Conclusions\nThe main contribution of this study is a taxonomy of the existing machine learning\ninterpretability methods that allows for a multi-perspective comparison among them.\nUnder this taxonomy, four major categories for interpretability methods were identi\ufb01ed:\nmethods for explaining complex black-box models, methods for creating white-box models,\nmethods that promote fairness and restrict the existence of discrimination, and, lastly,\nmethods for analysing the sensitivity of model predictions.\nAs a result of the high attention that is paid by the research community to deep learn-\ning, the literature around interpretability methods has been largely dominated by neural\nnetworks and their applications to computer vision and natural language processing. Most\ninterpretability methods for explaining deep learning models refer to image classi\ufb01cation\nand produce saliency maps, highlighting the impact of the different image regions. In\nmany cases, this is achieved through exploiting the gradient information \ufb02owing through\nthe layers of the network, Grad-CAM [35], a direct extension of [34], being a prime and\nmost in\ufb02uential example in terms of citations per year. Another way of creating saliency\nmaps, and the most in\ufb02uential overall while using the same metric, is through the adoption\nof deconvolutional neural networks [32]. In terms of explaining any black-box model, the\nLIME [45] and SHAP [48] methods are, by far, the most comprehensive and dominant\nacross the literature methods for visualising feature interactions and feature importance,\nwhile Friedman\u2019s PDPs [59], although much older and not as sophisticated, still remains a\npopular choice. The LIME and SHAP methods are not only model-agnostic, but they have\nbeen demonstrated to be applicable to any type of data.\nWhite-box highly performing models are very hard to create, especially in computer\nvision and natural language processing, where the gap in performance against deep\nlearning models is unbridgeable. Furthermore, because models are more than ever expected\n", []], "Adversarial Example-based Sensitivity Analysis": ["Entropy 2021, 23, 18\n28 of 45\nthe unconditional output distribution and output distribution when conditioned on one\nor more input variables. Building on from the work of Borgonovo, Plischke et al. [113]\nintroduced a new class of estimators for approximating density-based sensitivity measures,\nindependent of the sampling generation method used.\nBeing introduced by Sobol and Kucherenko [114], the method of derivative-based\nglobal sensitivity measures (DGSM) is based on \ufb01nding the averages of local derivatives\nwhile using Monte Carlo or Quasi Monte Carlo sampling methods. DGSM, which can\nbe seen as the generalization of the Morris method, are much easier to implement and\nevaluate when compared to the Sobol sensitivity indices.\n3.4.2. Adversarial Example-based Sensitivity Analysis\nAdversarial examples are datapoints whose features have been perturbed by a subtle\nyet suf\ufb01cient amount, enough to cause a machine learning model make incorrect predictions\nabout them. Adversarial examples are like counterfactual examples; however, they do not\nfocus on explaining the model, but on misleading it. Adversarial example-based sensitivity\nanalysis methods are methods that create adversarial examples for different kinds of data\nsuch as images or test.\nIt was Szegedy et al. [115] who \ufb01rst discovered that the functions learnt by deep neural\nnetworks can be signi\ufb01cantly discontinuous, thus their output is very fragile to certain\ninput perturbations. The term \u201cadversarial examples\u201d was coined for such perturbations\nand it was found that adversarial examples can be shared among neural networks with\ndifferent architectures, trained on different subsets, disjoint or not, of the same data: the\nvery same input perturbations that caused one network to misclassify can cause a different\nnetwork to also alter its output dramatically. The problem of \ufb01nding the minimal necessary\nperturbations was formulated as a box-constrained L2-norm optimisation problem and\nthe L-BFGS optimisation algorithm was employed in order to approximate its solution.\nGoodfellow et al. [116] argued that high-con\ufb01dence neural network misclassi\ufb01cations that\nare caused by small, yet intentionally, worst-case datapoint perturbations, were not due to\nnonlinearity or over\ufb01tting, but instead due to neural networks\u2019 linear nature. In addition to\ntheir \ufb01ndings, they also proposed a fast and simple yet powerful gradient-based method of\ngenerating adversarial examples while using the L\u221enorm, called fast gradient sign method\n(FGSM). Figure 8 illustrates the effectiveness of the FGSM method, where instances of\nthe MNIST dataset are perturbed while using different values of \u03f5, resulting in the model\nmisclassifying them.\nIn order to test the sensitivity of deep learning models, Moosavi-Dezfooli et al. pro-\nposed DeepFool [117], a method that generates minimum-perturbation adversarial exam-\nples that are optimised for the L2 norm. By making the simplifying assumptions, DeepFool\nemploys an iterative process of classi\ufb01er linearisation, producing adversaries that work\nwell against both binary and multi-class classi\ufb01ers. Moosavi-Dezfooli et al. [118] also\ncame up with a formulation that is able to produce a single perturbation, such that the\nclassi\ufb01er mis-classi\ufb01es most of the instances. The existence of these so called \u201cuniversal\nadversarial examples\u201d exposed the inherent weaknesses of deep neural networks on all\nof the inputs. Papernot et al. [119] conducted a thorough investigation regarding the\nadversarial behaviour within the deep learning framework and proposed a new class of\nalgorithms able to generate adversarial instances. More speci\ufb01cally, the method exploiting\nthe mathematical relationship between the inputs and outputs of deep neural networks\nto compute forward derivatives and subsequently construct adversarial saliency maps.\nFinally, the authors pointed towards the development and utilisation of a distance metric\nbetween non-adversarial inputs and the corresponding target labels as a way to defend\nagainst adversarial examples. Kurakin et al. [120] highlighted that, although in most\nstudies regarding machine learning sensitivity it is assumed the adversary examples can\nbe input directly into the classi\ufb01er, this assumption does not always hold true for classi\ufb01ers\nengaging with the physical world, such as those receiving input in the form of signals from\nother devices. To this end, among the other methods used, a new method that improves\n", [183]], "Traditional Sensitivity Analysis Methods": ["Entropy 2021, 23, 18\n26 of 45\nFigure 7. Comparison of the level of race bias (bias disparity) among different groups in the sample population.\nIn conclusion, fairness is a relatively new domain of machine learning interpretability,\nyet the progress made in the last few years has been tremendous. Various methods have\nbeen created in order to protect disadvantaged demographic segments against social bias\nand ensure fair allocation of resources. These different methods concern data manipulations\nprior to model training, algorithmic modi\ufb01cations within the training process as well as\npost-hoc adjustments. However, most of these methods, regardless of which step of\nthe process they are applied, focus too much on group-level fairness and often ignore\nindividual-level factors both within the groups and at a global scale, potentially mistreating\nindividuals in favour of groups. Furthermore, only a tiny portion of the scienti\ufb01c literature\nis concerned with fairness in non-tabular data, such as images and text; therefore, a large\ngap exists in these unexplored areas that are to be \ufb01lled in the coming years.\n3.4. Interpretability Methods to Analyse the Sensitivity of Machine Learning Model Predictions\nThis category includes interpretability methods that attempt to assess and challenge\nthe machine learning models in order to ensure that their predictions are trustworthy and\nreliable. These methods apply some form of sensitivity analysis, as models are tested with\nrespect to the stability of their learnt functions and how sensitive their output predictions\nare with respect to subtle yet intentional changes in the corresponding inputs. Sensitivity\nanalysis can be both a global and local interpretation technique, depending on whether\nthe change to the output is analysed with respect to a single example or across all exam-\nples in the dataset. Traditional and adversarial example-based sensitivity methods are\npresented and discussed in this section, while their corresponding summaries are provided\nin Tables 5 and 6 respectively.\n3.4.1. Traditional Sensitivity Analysis Methods\nTraditional sensitivity analysis methods try to represent each input variable with a\nnumeric value, which is called the sensitivity index. Sensitivity indices can be \ufb01rst-order\nindices, measuring the contribution of a single input variable to the output variance or\nsecond, third of higher-order indices, measuring the contribution of the interaction between\ntwo, three, or more input variables to the output variance respectively. the total-effect\nindices, combining the contributions of \ufb01rst-order and higher-order interactions with\nrespect to the output variance.\nAn output variance sensitivity analysis that is based on the ANOVA decomposi-\ntion was formalised by Sobol, who proposed the approximation of sensitivity indices of\n\ufb01rst and higher order while using Monte-Carlo methods [101], while Saltelli [102] and\n", [164]], "Interpretability Methods to Analyse the Sensitivity of Machine Learning Model Predictions": ["Entropy 2021, 23, 18\n26 of 45\nFigure 7. Comparison of the level of race bias (bias disparity) among different groups in the sample population.\nIn conclusion, fairness is a relatively new domain of machine learning interpretability,\nyet the progress made in the last few years has been tremendous. Various methods have\nbeen created in order to protect disadvantaged demographic segments against social bias\nand ensure fair allocation of resources. These different methods concern data manipulations\nprior to model training, algorithmic modi\ufb01cations within the training process as well as\npost-hoc adjustments. However, most of these methods, regardless of which step of\nthe process they are applied, focus too much on group-level fairness and often ignore\nindividual-level factors both within the groups and at a global scale, potentially mistreating\nindividuals in favour of groups. Furthermore, only a tiny portion of the scienti\ufb01c literature\nis concerned with fairness in non-tabular data, such as images and text; therefore, a large\ngap exists in these unexplored areas that are to be \ufb01lled in the coming years.\n3.4. Interpretability Methods to Analyse the Sensitivity of Machine Learning Model Predictions\nThis category includes interpretability methods that attempt to assess and challenge\nthe machine learning models in order to ensure that their predictions are trustworthy and\nreliable. These methods apply some form of sensitivity analysis, as models are tested with\nrespect to the stability of their learnt functions and how sensitive their output predictions\nare with respect to subtle yet intentional changes in the corresponding inputs. Sensitivity\nanalysis can be both a global and local interpretation technique, depending on whether\nthe change to the output is analysed with respect to a single example or across all exam-\nples in the dataset. Traditional and adversarial example-based sensitivity methods are\npresented and discussed in this section, while their corresponding summaries are provided\nin Tables 5 and 6 respectively.\n3.4.1. Traditional Sensitivity Analysis Methods\nTraditional sensitivity analysis methods try to represent each input variable with a\nnumeric value, which is called the sensitivity index. Sensitivity indices can be \ufb01rst-order\nindices, measuring the contribution of a single input variable to the output variance or\nsecond, third of higher-order indices, measuring the contribution of the interaction between\ntwo, three, or more input variables to the output variance respectively. the total-effect\nindices, combining the contributions of \ufb01rst-order and higher-order interactions with\nrespect to the output variance.\nAn output variance sensitivity analysis that is based on the ANOVA decomposi-\ntion was formalised by Sobol, who proposed the approximation of sensitivity indices of\n\ufb01rst and higher order while using Monte-Carlo methods [101], while Saltelli [102] and\n", [164]], "Interpretability Methods to Restrict Discrimination and Enhance Fairness in Machine Learning Models": ["Entropy 2021, 23, 18\n18 of 45\nand dependencies. Under the proposed approach, a GLM is re-\ufb01t as rules are created, thus\nallowing for existing rules to be re-weighted, ultimately producing a weighted combination\nof rules.\nHind et al. [71] introduced TED, a framework for producing local explanations that\nsatisfy the complexity mental model of a domain. The goal of TED is not to dig into the\nreasoning process of a model, but, instead, to mirror the reasoning process of a human\nexpert in a speci\ufb01c domain, who effectively creates an domain-speci\ufb01c explanation system.\nIn summary, not a lot of progress has been made in recent years towards developing\nwhite-box models. This is most likely the result of the immense complexity modern\napplications require, in combination with the inherent limitations of such models in terms\nof predictive power\u2014especially in computer vision and natural language processing, where\nthe difference in performance when compared to deep learning models is unbridgeable.\nFurthermore, because models are increasingly expected to perform well on more than one\ntasks and transfer of knowledge from one domain to another is becoming a common theme,\nwhite-box models, currently being able to perform well only in a single task, are losing\ntraction within the literature and they are dropping further behind in terms of interest.\nTable 3. Interpretability Methods to Create White-Box Models.\nRef\nTool\nCategory\nLocal vs.\nGlobal\nModel Speci\ufb01c vs.\nModel Agnostic\nData Type\nCitations/Year\nYear\n[65]\nInterpretML\nW\nG\nSpeci\ufb01c\ntab\n129.5\n2015\n[64]\nSlim\nW\nG\nSpeci\ufb01c\ntab\n35.2\n2016\n[68]\nAIX360\nW\nG\nSpeci\ufb01c\ntab\n12.3\n2018\n[71]\nAIX360\nW\nL\nSpeci\ufb01c\ntab\n12\n2019\n[69]\nAIX360\nW\nG\nSpeci\ufb01c\ntab\n5\n2019\n3.3. Interpretability Methods to Restrict Discrimination and Enhance Fairness in Machine\nLearning Models\nBecause machine learning systems are increasingly adopted in real life applications,\nany inequities or discrimination that are promoted by those systems have the potential\nto directly affect human lives. Machine Learning Fairness is a sub-domain of machine\nlearning interpretability that focuses solely on the social and ethical impact of machine\nlearning algorithms by evaluating them in terms impartiality and discrimination. The study\nof fairness in machine learning is becoming more broad and diverse, and it is progressing\nrapidly. Traditionally, the fairness of a machine learning system has been evaluated by\nchecking the models\u2019 predictions and errors across certain demographic segments, for\nexample, groups of a speci\ufb01c ethnicity or gender. In terms of dealing with a lack of fairness,\na number of techniques have been developed both to remove bias from training data and\nfrom model predictions and to train models that learn to make fair predictions in the\n\ufb01rst place. In this section, the most widely-used machine learning fairness methods are\npresented, discussed and \ufb01nally summarised in Table 4.\nDisparate impact testing [72] is a model agnostic method that is able to assess the\nfairness of a model, but is not able to provide any insight or detail regarding the causes of\nany discovered bias. The method conducts a series of simple experiments that highlight\nany differences in terms of model predictions and errors across different demographic\ngroups. More speci\ufb01cally, it can detect biases regarding ethnicity, gender, disability status,\nmarital status, or any other demographic. While straightforward and ef\ufb01cient when it\ncomes to selecting the most fair model, the method, due to the simplicity of its tests, might\nfail to pick up on local occurrences of discrimination, especially in complex models.\nA way to ensure fairness in machine learning models is to alter the model construc-\ntion process. In [73], three different data preprocessing techniques to ensure fairness in\n", [159, 164]], "Interpretability Methods to Create White-Box Models": ["Entropy 2021, 23, 18\n17 of 45\nneed, as most methods focus on either a speci\ufb01c type of model, or a speci\ufb01c type of data,\nor their scope is either local or global, but not both. Of the methods presented, SHAP is\nthe most complete method, providing explanations for any model and any type of data,\ndoing so at both a global and local scope. However, SHAP is not without shortcomings:\nThe kernel version of SHAP, KernelSHAP, like most permutation based methods, does not\ntake feature dependence into account, thus often over-weighing unlikely data points and,\nwhile TreeSHAP, the tree version of SHAP, solves this problem, its reliance on conditional\nexpected predictions is known to produce non-intuitive feature importance values as\nfeatures with no impact on predictions can be assigned an importance value that is different\nto zero.\n3.2. Interpretability Methods to Create White-Box Models\nThis category encompasses methods that create interpretable and easily understand-\nable from humans models. The models in this category are often called intrinsic, transpar-\nent, or white-box models. Such models include the linear, decision tree, and rule-based\nmodels and some other more complex and sophisticated models that are equally trans-\nparent and, therefore, promising for the interpretability \ufb01eld. This work will focus on\nmore complex models, as the linear, decision tree and elementary rule-based models have\nbeen extensively discussed in many other scienti\ufb01c studies. A summary of the discussed\ninterpretability methods to create white-box models can be found in Table 3.\nUstun and Rudin [64] proposed Supersparse Linear Integer Models (SLIM), a type of\npredictive system that only allows for additions, subtraction, and multiplications of input\nfeatures to generate predictions, thus being highly interpretable.\nIn [65], Microsoft presented two case studies on real medical data, where naturally\ninterpretable generalized additive models with pairwise interactions (GA2Ms), as originally\nproposed in [66], achieved state-of-the-art accuracy, showing that GA2Ms are the \ufb01rst step\ntowards deploying interpretable high-accuracy models in applications like healthcare,\nwhere interpretability is of utmost importance. GA2Ms are generalized additive models\n(GAM) [67], but with a few tweaks that set them apart, in terms of predictive power, from\ntraditional GAMs. More speci\ufb01cally, GA2Ms are trained while using modern machine\nlearning techniques such as bagging and boosting, while their boosting procedure uses\na round-robin approach through features in order to reduce the undesirable effects of\nco-linearity. Furthermore, any pairwise interaction terms are automatically identi\ufb01ed\nand, therefore, included, which further increases their predictive power. In terms of\ninterpretability, as additive models, GA2Ms are naturally interpretable, being able to\ncalculate the contributions of each feature towards the \ufb01nal prediction in a modular way,\nthus making it easy for humans to understand the degree of impact of each feature and\ngain useful insight into the model\u2019s predictions.\nBoolean Rule Column Generation [68] is a technique that utilises Boolean rules, either\nin their disjunctive normal form (DNF) or in their conjunctive normal form (CNF), in\norder to create predictive models. In this case, interpretability is achieved through rule\nsimplicity: a low number Boolean rules with few clauses and conditions in each clause can\nmore easily be understood and interpreted by humans. The authors highlighted that most\ncolumn generation algorithms, although ef\ufb01cient, can lead to computational issues when\nit comes to learning rules for large datasets, due to the exponential size of the rule-space,\nwhich corresponds to all possible conjunctions or disjunctions of the input features. As\na solution, they introduced an approximate column-generation algorithm that employs\nrandomization in order to ef\ufb01ciently search the rule-space and learn interpretable DNF\nor CNF classi\ufb01cation rules while optimally balancing the tradeoff between classi\ufb01cation\naccuracy and rule simplicity.\nGeneralized Linear Rule Models [69], which are often referred to as rule ensembles,\nare Generalized Linear Models (GLMs) [70] that are linear combinations of rule-based\nfeatures. The bene\ufb01t of such models is that they are naturally interpretable, while also\nbeing relatively complex and \ufb02exible, since rules are able to capture nonlinear relationships\n", []], "Interpretability Methods to Explain any Black-Box Model": ["Entropy 2021, 23, 18\n11 of 45\nFigure 3. Comparison of Interpretability Methods to Explain Deep Learning Models on ImageNet sample images, using the\ninnvestigate package.\n3.1.2. Interpretability Methods to Explain any Black-Box Model\nThis section focuses on interpretability techniques, which can be applied to any black-\nbox model. First introduced in [45], the local interpretable model-agnostic explanations\n(LIME) method is one of the most popular interpretability methods for black-box models.\nFollowing a simple yet powerful approach, LIME can generate interpretations for single\nprediction scores produced by any classi\ufb01er. For any given instance and its corresponding\nprediction, simulated randomly-sampled data around the neighbourhood of input instance,\nfor which the prediction was produced, are generated. Subsequently, while using the\nmodel in question, new predictions are made for generated instances and weighted by their\nproximity to the input instance. Lastly, a simple, interpretable model, such as a decision\ntree, is trained on this newly-created dataset of perturbed instances. By interpreting this\nlocal model, the initial black box model is consequently interpreted. Although LIME is\npowerful and straightforward, it has its drawbacks. In 2020, the \ufb01rst theoretical analysis\nof LIME [46] was published, validating the signi\ufb01cance and meaningfulness of LIME, but\nalso proving that poor choices in terms of parameters could lead LIME to missing out\non important features. Figure 4 illustrates the application of the LIME method, in order\nto explain the rationale behind the classi\ufb01cation of an instance of the Quora Insincere\nQuestions Dataset.\nZafar and Khan [47] supported that the random perturbation and feature selection\nmethods that LIME utilises result in unstable generated interpretations. This is because, for\nthe same prediction, different interpretations can be generated, which can be problematic\nfor deployment. In order to address this uncertainty, a deterministic version of LIME,\nDLIME is proposed. In this version, random perturbation is replaced with hierarchical\nclustering to group the data and k-nearest neighbours (KNN) to select the cluster that is\nbelieved where the instance in question belongs. Using three medical datasets among\nmultiple explanations, they demonstrate the superiority of DLIME over LIME in terms of\nthe Jacart Similarity.\n", [109, 115, 126]], "Interpretability Methods to Explain Deep Learning Models": ["Entropy 2021, 23, 18\n6 of 45\nsuch as deep neural networks. That is also why they sometimes are referred to as post-hoc\ninterpretability methods in the related scienti\ufb01c literature.\nUnder this taxonomy, this category, due to the volume of scienti\ufb01c work around\ndeep learning related interpretability methodologies, is split into two sub-categories, one\nspeci\ufb01cally for deep learning methods and one concerning all other black-box models.\nFor each of these sub-categories, a summary of the included methods is shown in Tables 1\nand 2 respectively.\n3.1.1. Interpretability Methods to Explain Deep Learning Models\nThe widespread adoption of deep learning methods, combined with the fact that\nit is in their very nature to produce black-box machine learning systems, has led to a\nconsiderable amount of experiments and scienti\ufb01c work around them and, therefore, tools\nregarding their interpretability. A substantial portion of attention regarding python tools\nis focused on deep learning for images more speci\ufb01cally on the concept of saliency in\nimages, as initially proposed in [22]. Saliency refers to unique features, such as pixels or\nresolution of the image in the context of visual processing. These unique features depict the\nvisually alluring locations in an image and a saliency map is a topographical representation\nof them.\nGradients: \ufb01rst proposed in [23], the gradients explanation technique, as its name\nsuggests, is gradient-based attribution method, according to which each gradient quanti\ufb01es\nhow much a change in each input dimension would a change the predictions in a small\nneighborhood around the input. Consequently, the method computes an image-speci\ufb01c\nclass saliency map corresponding to the gradient of an output neuron with respect to\nthe input, highlighting the areas of the given image, discriminative with respect to the\ngiven class. An improvement over the initial method was proposed in [24], where the\nwell-known Krizhevsky network [25] was utilised in order to outperform state-of-the-art\nsaliency models by a large margin, increasing the amount of explained information by 67%\nwhen compared to state-of-the art. Furthermore, in [26], a task-speci\ufb01c pre-training scheme\nwas designed in order to make the multi-context modeling suited for saliency detection.\nIntegrated Gradients [27] is gradient-based attribution a method that attempts to\nexplain predictions that are made by deep neural network by attributing them to the\nnetwork\u2019s input features. It is essentially is a variation on calculating the gradient of the\nprediction output with respect to the features of the input, as implemented by the simpler\nGradients method. Under this variation, a much desired property, which is known as\ncompleteness or Ef\ufb01ciency [28] or Summation to Delta [29], is satis\ufb01ed: the attributions\nsum up to the target output minus the target output that was evaluated at the baseline.\nMoreover, two fundamental axioms that attribution methods ought to satisfy are identi\ufb01ed:\nsensitivity and implementation invariance. Upon highlighting that most known attribution\nmethods do not satisfy these axioms, they propose the integrated gradients method as\na simple way obtain great interpretability results. Another work, closely related to the\nintegrated gradients method, was proposed in [30], where attributions are used in order to\nhelp identify weaknesses of three question-answer models better than the conventional\nmodels, while also to provide work\ufb02ow superiority.\nDeepLIFT [29] is a popular algorithm that was designed to be applied on top of deep\nneural network predictions. The method, as described in [29], is an improvement over its\n\ufb01rst form [29], also known as the \u201cGradient \u2217Input\u201d method, where it was observed that\nsaliency maps that were obtained using the gradient method can be greatly enhanced by\nmultiplying the gradient with the input signal\u2014an operation that is essentially a \ufb01rst-order\nTaylor approximation of how the output would change if the input were set to zero. The\nmethod\u2019s superiority was demonstrated by showing considerable bene\ufb01ts over gradient-\nbased methods when applied to models that were trained on natural images and genomics\ndata. By observing the activation of each neuron, it assigns them contribution scores,\ncalculated by comparing the difference of the output from some reference output to the\ndifferences of the inputs from their reference inputs. By optionally giving separate consid-\n", [109]], "Interpretability Methods to Explain Black-Box Models": ["Entropy 2021, 23, 18\n5 of 45\nA especally important separation of interpretability methods could happen based on\nthe type of algorithms that could be applied. If their application is only restricted to a\nspeci\ufb01c family of algorithms, then these methods are called model-speci\ufb01c. In contrast,\nthe methods that could be applied in every possible algorithm are called model agnostic.\nAdditionally, one crucial aspect of dividing the interpretability methods is based on the\nscale of interpretation. If the method provides an explanation only for a speci\ufb01c instance,\nthen it is a local one and, if the method explains the whole model, then it is global. At\nlast, one crucial factor that should be taken into consideration is the type of data on which\nthese methods could be applied. The most common types of data are tabular and images,\nbut there are also some methods for text data. Figure 2 presents a summarized mind-\nmap, which visualizes the different aspects by which an interpretability method could be\nclassi\ufb01ed. These aspects should always be taken into consideration by practitioners, in\norder for the ideal method with respect to their needs to be identi\ufb01ed.\nFigure 2. Taxonomy mind-map of Machine Learning Interpretability Techniques.\nThis taxonomy focuses on the purpose that these methods were created to serve\nand the ways through which they accomplish this purpose. As a result, according to\nthe presented taxonomy, four major categories for interpretability methods are identi\ufb01ed:\nmethods for explaining complex black-box models, methods for creating white-box models,\nmethods that promote fairness and restrict the existence of discrimination, and, lastly,\nmethods for analysing the sensitivity of model predictions.\n3.1. Interpretability Methods to Explain Black-Box Models\nThis \ufb01rst category encompasses methods that are concerned with black-box pre-\ntrained machine learning models. More speci\ufb01cally, such methods do not try to create\ninterpretable models, but, instead, try to interpret already trained, often complex models,\n", [85]], "Different Scopes of Machine Learning Interpretability: A Taxonomy of Methods": ["Entropy 2021, 23, 18\n4 of 45\nbut also highlights the lack of combinatorial approaches, which would attempt to merge\ndifferent techniques of explanation, claiming that such types of methods would result in\nbetter explanations.\nAdadi and Berrada [17] conducted an extensive literature review, collecting and\nanalysing 381 different scienti\ufb01c papers between 2004 and 2018. They arranged all of\nthe scienti\ufb01c work in the \ufb01eld of explainable AI along four main axes and stressed the\nneed for more formalism to be introduced in the \ufb01eld of XAI and for more interaction\nbetween humans and machines. After highlighting the trend of the community to explore\nexplainability only in terms of modelling, they proposed embracing explainability in other\naspects of machine learning. Finally, they suggested a potential research direction that\nwould be towards the composition of existing explainability methods.\nAnother survey that attempted to categorise the existing explainability methods is\nthis of Guidotti et al. [19]. Firstly, the authors identi\ufb01ed four categories for each method\nbased on the type of problem that they were created to tackle. One category for explaining\nblack-box models, one for inspecting them, one for explaining their outcomes, and, \ufb01nally,\none for creating transparent black box models. Subsequently, they proposed a taxonomy\nthat takes into account the type of underlying explanation model (explanator), the type of\ndata used as input, the problem the method encounters, as well as the black box model\nthat was \u201copened\u201d. As with works previously discussed, the lack of formality and need\nfor a de\ufb01nition of metrics for evaluating the performance of interpretability methods was\nhighlighted once again, while the incapacity of most black-box explainability methods\nto interpret models that make decisions based on unknown or latent features was also\nraised. Lastly, the lack of interpretability techniques in the \ufb01eld of recommender systems\nis identi\ufb01ed and an approach according to which models could be learned directly from\nexplanations is proposed.\nUpon identifying the lack of formality and ways to measure the performance of inter-\npretability methods, Murdoch et al. [20] published a survey in 2019, in which they created\nan interpretability framework in the hope that it would help to bridge the aforementioned\ngap in the \ufb01eld. The Predictive, Descriptive, Relevant (PDR) framework introduced three\ntypes of metrics for rating the interpretability methods, predictive accuracy, descriptive\naccuracy, and relevancy. To conclude, they dealt with transparent models and post-hoc\ninterpretation, as they believed that post-hoc interpretability could be used to elevate the\npredictive accuracy of a model and that transparent models could increase their use cases\nby increasing predictive accuracy\u2014making clear, that, in some cases, the combination of\nthe two methods is ideal.\nA more recent study carried out by Arrieta et al. [21] introduced a different type\nof arrangement that initially distinguishes transparent and post-hoc methods and subse-\nquently created sub-categories. An alternative taxonomy speci\ufb01cally for the deep learning\ninterpretability methods, due to their high volume, was developed. Under this taxonomy,\nfour categories were proposed: one for providing explanations regarding deep network\nprocessing, one in relation to the explanation of deep network representation, one con-\ncerned with the explanation of producing systems, and one encompassing hybrids of\ntransparent and black-box methods. Finally, the authors dived into the concept of Respon-\nsible Arti\ufb01cial Intelligence, a methodology introducing a series of criteria for implementing\nAI in organizations.\n3. Different Scopes of Machine Learning Interpretability: A Taxonomy of Methods\nDifferent view-points exist when it comes to looking at the the emerging landscape of\ninterpretability methods, such as the type of data these methods deal with or whether they\nrefer to global or local properties. The classi\ufb01cation of machine learning interpretability\ntechniques should not be one-sided. There are exist different points of view, which distin-\nguish and could further divide these methods. Hence, in order for a practitioner to identify\nthe ideal method for the speci\ufb01c criteria of each problem encountered, all aspects of each\nmethod should be taken into consideration.\n", [85]], "Related Work": ["Entropy 2021, 23, 18\n3 of 45\nsystem\u2019s inputs and outputs. For example, in image recognition tasks, part of the reason\nthat led a system to decide that a speci\ufb01c object is part of an image (output) could be certain\ndominant patterns in the image (input). Explainability, on the other hand, is associated\nwith the internal logic and mechanics that are inside a machine learning system. The\nmore explainable a model, the deeper the understanding that humans achieve in terms of\nthe internal procedures that take place while the model is training or making decisions.\nAn interpretable model does not necessarily translate to one that humans are able to\nunderstand the internal logic of or its underlying processes. Therefore, regarding machine\nlearning systems, interpretability does not axiomatically entail explainability, or vice versa.\nAs a result, Gilpin et al. [16] supported that interpretability alone is insuf\ufb01cient and that\nthe presence of explainability is also of fundamental importance. Mostly aligned with the\nwork of Doshi-Velez and Kim [15], this study considers interpretability to be a broader\nterm than explainability.\n2.2. Evaluation of Machine Learning Interpretability\nDoshi-Velez and Kim [15] proposed the following classi\ufb01cation of evaluation methods\nfor interpretability: application-grounded, human-grounded, and functionally-grounded,\nsubsequently discussing the potential trade-offs among them. Application-grounded eval-\nuation concerns itself with how the results of the interpretation process affect the human,\ndomain expert, end-user in terms of a speci\ufb01c and well-de\ufb01ned task or application. Con-\ncrete examples under this type of evaluation include whether an interpretability method\nresults in better identi\ufb01cation of errors or less discrimination. Human-grounded evaluation\nis similar to application-grounded evaluation; however, there are two main differences:\n\ufb01rst, the tester in this case does not have be a domain expert, but can be any human end-\nuser and secondly, the end goal is not to evaluate a produced interpretation with respect\nto its \ufb01tness for a speci\ufb01c application, but rather to test the quality of produced interpre-\ntation in a more general setting and measure how well the general notions are captured.\nAn example of measuring how well an interpretation captures the abstract notion of an\ninput would be for humans to be presented with different interpretations of the input, and\nthem selecting the one that they believe best encapsulates the essence of it. Functionally-\ngrounded evaluation does not require any experiments that involve humans, but instead\nuses formal, well-de\ufb01ned mathematical de\ufb01nitions of interpretability to evaluate quality\nof an interpretability method. This type of evaluation usually follows the other two types\nof evaluation: once a class of models has already passed some interpretability criteria via\nhuman-grounded or application-grounded experiments, then mathematical de\ufb01nitions can\nbe used to further rank the quality of the interpretability models. Functionally-grounded\nevaluation is also appropriate when experiments that involve humans cannot be applied\nfor some reason (e.g ethical considerations) or when the proposed method has not reached\na mature enough stage to be evaluated by human users. That said, determining the right\nmeasurement criteria and metric for each case is challenging and remains an open problem.\n2.3. Related Work\nThe concepts of interpretability and explainability are hard to rigorously de\ufb01ne;\nhowever, multiple attempts have been made towards that goal, the most emblematic works\nbeing [14,15].\nThe work of Gilpin et al. [16] constitutes another attempt to de\ufb01ne the key concepts\naround interpretability in machine learning. The authors, while focusing mostly on deep\nlearning, also proposed a taxonomy, by which the interpretability methods for neural\nnetworks could be classi\ufb01ed into three different categories. The \ufb01rst one encompasses\nmethods that emulate the processing of data in order to create insights for the connections\nbetween inputs and outputs of the model. The second category contains approaches\nthat try to explain the representation of data inside a network, while the last category\nconsists of transparent networks that explain themselves. Lastly, the author recognises the\npromising nature of the progress achieved in the \ufb01eld of explaining deep neural networks,\n", []], "Evaluation of Machine Learning Interpretability": ["Entropy 2021, 23, 18\n3 of 45\nsystem\u2019s inputs and outputs. For example, in image recognition tasks, part of the reason\nthat led a system to decide that a speci\ufb01c object is part of an image (output) could be certain\ndominant patterns in the image (input). Explainability, on the other hand, is associated\nwith the internal logic and mechanics that are inside a machine learning system. The\nmore explainable a model, the deeper the understanding that humans achieve in terms of\nthe internal procedures that take place while the model is training or making decisions.\nAn interpretable model does not necessarily translate to one that humans are able to\nunderstand the internal logic of or its underlying processes. Therefore, regarding machine\nlearning systems, interpretability does not axiomatically entail explainability, or vice versa.\nAs a result, Gilpin et al. [16] supported that interpretability alone is insuf\ufb01cient and that\nthe presence of explainability is also of fundamental importance. Mostly aligned with the\nwork of Doshi-Velez and Kim [15], this study considers interpretability to be a broader\nterm than explainability.\n2.2. Evaluation of Machine Learning Interpretability\nDoshi-Velez and Kim [15] proposed the following classi\ufb01cation of evaluation methods\nfor interpretability: application-grounded, human-grounded, and functionally-grounded,\nsubsequently discussing the potential trade-offs among them. Application-grounded eval-\nuation concerns itself with how the results of the interpretation process affect the human,\ndomain expert, end-user in terms of a speci\ufb01c and well-de\ufb01ned task or application. Con-\ncrete examples under this type of evaluation include whether an interpretability method\nresults in better identi\ufb01cation of errors or less discrimination. Human-grounded evaluation\nis similar to application-grounded evaluation; however, there are two main differences:\n\ufb01rst, the tester in this case does not have be a domain expert, but can be any human end-\nuser and secondly, the end goal is not to evaluate a produced interpretation with respect\nto its \ufb01tness for a speci\ufb01c application, but rather to test the quality of produced interpre-\ntation in a more general setting and measure how well the general notions are captured.\nAn example of measuring how well an interpretation captures the abstract notion of an\ninput would be for humans to be presented with different interpretations of the input, and\nthem selecting the one that they believe best encapsulates the essence of it. Functionally-\ngrounded evaluation does not require any experiments that involve humans, but instead\nuses formal, well-de\ufb01ned mathematical de\ufb01nitions of interpretability to evaluate quality\nof an interpretability method. This type of evaluation usually follows the other two types\nof evaluation: once a class of models has already passed some interpretability criteria via\nhuman-grounded or application-grounded experiments, then mathematical de\ufb01nitions can\nbe used to further rank the quality of the interpretability models. Functionally-grounded\nevaluation is also appropriate when experiments that involve humans cannot be applied\nfor some reason (e.g ethical considerations) or when the proposed method has not reached\na mature enough stage to be evaluated by human users. That said, determining the right\nmeasurement criteria and metric for each case is challenging and remains an open problem.\n2.3. Related Work\nThe concepts of interpretability and explainability are hard to rigorously de\ufb01ne;\nhowever, multiple attempts have been made towards that goal, the most emblematic works\nbeing [14,15].\nThe work of Gilpin et al. [16] constitutes another attempt to de\ufb01ne the key concepts\naround interpretability in machine learning. The authors, while focusing mostly on deep\nlearning, also proposed a taxonomy, by which the interpretability methods for neural\nnetworks could be classi\ufb01ed into three different categories. The \ufb01rst one encompasses\nmethods that emulate the processing of data in order to create insights for the connections\nbetween inputs and outputs of the model. The second category contains approaches\nthat try to explain the representation of data inside a network, while the last category\nconsists of transparent networks that explain themselves. Lastly, the author recognises the\npromising nature of the progress achieved in the \ufb01eld of explaining deep neural networks,\n", []], "Explainability and Interpretability": ["Entropy 2021, 23, 18\n2 of 45\nproduce explainable results\u2014with common examples, including linear [11] and decision-\ntree based [12] models. Although more explainable and interpretable, the latter models\nare not as powerful and they fail achieve state-of-the-art performance when compared\nto the former. Both their poor performance and the ability to be well-interpreted and\neasily-explained come down to the same reason: their frugal design.\nSystems whose decisions cannot be well-interpreted are dif\ufb01cult to be trusted, espe-\ncially in sectors, such as healthcare or self-driving cars, where also moral and fairness issues\nhave naturally arisen. This need for trustworthy, fair, robust, high performing models for\nreal-world applications led to the revival of the \ufb01eld of eXplainable Arti\ufb01cial Intelligence\n(XAI) [13]\u2014a \ufb01eld focused on the understanding and interpretation of the behaviour of\nAI systems, which. in the years prior to its revival, had lost the attention of the scienti\ufb01c\ncommunity, as most research focused on the predictive power of algorithms rather than the\nunderstanding behind these predictions. The popularity of the search term \u201cExplainable\nAI\u201d throughout the years, as measured by Google Trends, is illustrated in Figure 1. The\nnoticeable spike in recent years, indicating the of rejuvenation of the \ufb01eld, is also re\ufb02ected\nin the increased research output of the same period.\nFigure 1. Google Trends Popularity Index (Max value is 100) of the term \u201cExplainable AI\u201d over the last ten years (2011\u20132020).\nThe Contribution of this Survey\nAs the demand for more explainable machine learning models with interpretable\npredictions rises, so does the need for methods that can help to achieve these goals. This\nsurvey will focus on providing an extensive and in-depth identi\ufb01cation, analysis, and\ncomparison of machine learning interpretability methods. The end goal of the survey is\nto serve as a reference point for both theorists and practitioners not only by providing a\ntaxonomy of the existing methods, but also by scoping the best use cases for each of the\nmethods and also providing links to their programming implementations\u2013the latter being\nfound in the Appendix A section.\n2. Fundamental Concepts and Background\n2.1. Explainability and Interpretability\nThe terms interpretability and explainability are usually used by researchers inter-\nchangeably; however, while these terms are very closely related, some works identify\ntheir differences and distinguish these two concepts. There is not a concrete mathematical\nde\ufb01nition for interpretability or explainability, nor have they been measured by some\nmetric; however, a number of attempts have been made [14\u201316] in order to clarify not\nonly these two terms, but also related concepts such as comprehensibility. However, all\nthese de\ufb01nitions lack mathematical formality and rigorousness [17]. One of the most\npopular de\ufb01nitions of interpretability is the one of Doshi-Velez and Kim, who, in their\nwork [15], de\ufb01ne it as \u201cthe ability to explain or to present in understandable terms to a\nhuman\u201d. Another popular de\ufb01nition came from Miller in his work [18], where he de\ufb01nes\ninterpretability as \u201cthe degree to which a human can understand the cause of a decision\u201d.\nAlthough intuitive, these de\ufb01nitions lack mathematical formality and rigorousness [17].\nBased on the above, interpretability is mostly connected with the intuition behind\nthe outputs of a model [17]; with the idea being that the more interpretable a machine\nlearning system is, the easier it is to identify cause-and-effect relationships within the\n", [74]], "Fundamental Concepts and Background": ["Entropy 2021, 23, 18\n2 of 45\nproduce explainable results\u2014with common examples, including linear [11] and decision-\ntree based [12] models. Although more explainable and interpretable, the latter models\nare not as powerful and they fail achieve state-of-the-art performance when compared\nto the former. Both their poor performance and the ability to be well-interpreted and\neasily-explained come down to the same reason: their frugal design.\nSystems whose decisions cannot be well-interpreted are dif\ufb01cult to be trusted, espe-\ncially in sectors, such as healthcare or self-driving cars, where also moral and fairness issues\nhave naturally arisen. This need for trustworthy, fair, robust, high performing models for\nreal-world applications led to the revival of the \ufb01eld of eXplainable Arti\ufb01cial Intelligence\n(XAI) [13]\u2014a \ufb01eld focused on the understanding and interpretation of the behaviour of\nAI systems, which. in the years prior to its revival, had lost the attention of the scienti\ufb01c\ncommunity, as most research focused on the predictive power of algorithms rather than the\nunderstanding behind these predictions. The popularity of the search term \u201cExplainable\nAI\u201d throughout the years, as measured by Google Trends, is illustrated in Figure 1. The\nnoticeable spike in recent years, indicating the of rejuvenation of the \ufb01eld, is also re\ufb02ected\nin the increased research output of the same period.\nFigure 1. Google Trends Popularity Index (Max value is 100) of the term \u201cExplainable AI\u201d over the last ten years (2011\u20132020).\nThe Contribution of this Survey\nAs the demand for more explainable machine learning models with interpretable\npredictions rises, so does the need for methods that can help to achieve these goals. This\nsurvey will focus on providing an extensive and in-depth identi\ufb01cation, analysis, and\ncomparison of machine learning interpretability methods. The end goal of the survey is\nto serve as a reference point for both theorists and practitioners not only by providing a\ntaxonomy of the existing methods, but also by scoping the best use cases for each of the\nmethods and also providing links to their programming implementations\u2013the latter being\nfound in the Appendix A section.\n2. Fundamental Concepts and Background\n2.1. Explainability and Interpretability\nThe terms interpretability and explainability are usually used by researchers inter-\nchangeably; however, while these terms are very closely related, some works identify\ntheir differences and distinguish these two concepts. There is not a concrete mathematical\nde\ufb01nition for interpretability or explainability, nor have they been measured by some\nmetric; however, a number of attempts have been made [14\u201316] in order to clarify not\nonly these two terms, but also related concepts such as comprehensibility. However, all\nthese de\ufb01nitions lack mathematical formality and rigorousness [17]. One of the most\npopular de\ufb01nitions of interpretability is the one of Doshi-Velez and Kim, who, in their\nwork [15], de\ufb01ne it as \u201cthe ability to explain or to present in understandable terms to a\nhuman\u201d. Another popular de\ufb01nition came from Miller in his work [18], where he de\ufb01nes\ninterpretability as \u201cthe degree to which a human can understand the cause of a decision\u201d.\nAlthough intuitive, these de\ufb01nitions lack mathematical formality and rigorousness [17].\nBased on the above, interpretability is mostly connected with the intuition behind\nthe outputs of a model [17]; with the idea being that the more interpretable a machine\nlearning system is, the easier it is to identify cause-and-effect relationships within the\n", [74]], "Introduction": ["entropy\nReview\nExplainable AI: A Review of Machine Learning\nInterpretability Methods\nPantelis Linardatos \u2217\n, Vasilis Papastefanopoulos\nand Sotiris Kotsiantis\n\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\n\u0001\u0002\u0003\u0004\u0005\u0006\u0007\nCitation:\nLinardatos,\nP.;\nPapastefanopoulos, V.; Kotsiantis, S.\nExplainable AI: A Review of Machine\nLearning Interpretability Methods.\nEntropy 2021, 23, 18. https://dx.doi.org/\n10.3390/e23010018\nReceived: 8 December 2020\nAccepted: 22 December 2020\nPublished: 25 December 2020\nPublisher\u2019s Note: MDPI stays neu-\ntral with regard to jurisdictional claims\nin published maps and institutional\naf\ufb01liations.\nCopyright: \u00a9 2020 by the authors. Li-\ncensee MDPI, Basel, Switzerland. This\narticle is an open access article distributed\nunder the terms and conditions of the\nCreative Commons Attribution (CC BY)\nlicense (https://creativecommons.org/\nlicenses/by/4.0/).\nDepartment of Mathematics, University of Patras, 26504 Patras, Greece;\nvasileios.papastefanopoulos@upatras.gr (V.P.); sotos@math.upatras.gr (S.K.)\n* Correspondence: p.linardatos@upnet.gr\nAbstract: Recent advances in arti\ufb01cial intelligence (AI) have led to its widespread industrial adoption,\nwith machine learning systems demonstrating superhuman performance in a signi\ufb01cant number\nof tasks. However, this surge in performance, has often been achieved through increased model\ncomplexity, turning such systems into \u201cblack box\u201d approaches and causing uncertainty regarding\nthe way they operate and, ultimately, the way that they come to decisions. This ambiguity has\nmade it problematic for machine learning systems to be adopted in sensitive yet critical domains,\nwhere their value could be immense, such as healthcare. As a result, scienti\ufb01c interest in the \ufb01eld\nof Explainable Arti\ufb01cial Intelligence (XAI), a \ufb01eld that is concerned with the development of new\nmethods that explain and interpret machine learning models, has been tremendously reignited over\nrecent years. This study focuses on machine learning interpretability methods; more speci\ufb01cally, a\nliterature review and taxonomy of these methods are presented, as well as links to their programming\nimplementations, in the hope that this survey would serve as a reference point for both theorists\nand practitioners.\nKeywords: xai; machine learning; explainability; interpretability; fairness; sensitivity; black-box\n1. Introduction\nArti\ufb01cial intelligence (AI) had for many years mostly been a \ufb01eld focused heavily\non theory, without many applications of real-world impact. This has radically changed\nover the past decade as a combination of more powerful machines, improved learning\nalgorithms, as well as easier access to vast amounts of data enabled advances in Machine\nLearning (ML) and led to its widespread industrial adoption[1]. Around 2012 Deep\nLearning methods [2] started to dominate accuracy benchmarks, achieving superhuman\nresults and further improving in the subsequent years. As a result, today, a lot of real-world\nproblems in different domains, stretching from retail and banking [3,4] to medicine and\nhealthcare [5\u20137], are tackled while using machine learning models.\nHowever, this improved predictive accuracy has often been achieved through in-\ncreased model complexity. A prime example is the deep learning paradigm, which is at the\nheart of most state-of-the-art machine learning systems. It allows for machines to automati-\ncally discover, learn, and extract the hierarchical data representations that are needed for\ndetection or classi\ufb01cation tasks. This hierarchy of increasing complexity combined with\nthe fact that vast amounts of data are used to train and develop such complex systems,\nwhile, in most cases, boosts the systems\u2019 predictive power, inherently reducing their ability\nto explain their inner workings and mechanisms. As a consequence, the rationale behind\ntheir decisions becomes quite hard to understand and, therefore, their predictions hard\nto interpret.\nThere is clear trade-off between the performance of a machine learning model and\nits ability to produce explainable and interpretable predictions. On the one hand, there\nare the so called black-box models, which include deep learning [2] and ensembles [8\u201310].\nOn the other hand, there are the so called white-box or glass-box models, which easily\nEntropy 2021, 23, 18. https://dx.doi.org/10.3390/e23010018\nhttps://www.mdpi.com/journal/entropy\n", [42, 43, 44, 45, 69, 74]]}