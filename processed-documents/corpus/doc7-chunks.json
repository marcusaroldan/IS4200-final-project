{"Conclusion": ["It is important to emphasize that SARIMA and kNN-TSPI did not present SSD concerning three\nparametric (ARIMA, MA and MHW) and three non-parametric (SVM, LSTM and MLP) methods.\nWe can consider MLP, SVM and kNN-TSPI the most suitable algorithms to model and pre-\ndict chaotic time series (Fig. 40(b.3)).\nMLP obtained the smallest errors and the best POCID\nrates, followed by SVM and kNN-TSPI. We observe a similar behavior on the 40 synthetic datasets\n(Fig. 40(b.4)), where SVM, MLP and kNN-TSPI were the most appropriate methods both concerning\nthe low prediction error and the high hit rates on the projection horizons trends.\nSARIMA, SVM, kNN-TSPI, and ARIMA are the most promising algorithms for real time series\n(Fig. 40(b.5)). ARIMA\u2019s performance was very close to kNN-TSPI. Also, SARIMA and kNN-TSPI\nshowed the highest POCID values.\nFor all the 95 datasets (Fig. 40(b.6)), SVM surpassed kNN-TSPI and SARIMA by a small margin.\nThese three methods did not exhibit large performance di\ufb00erences regarding MLP. Besides, they\nachieved similar POCID results.\nConsidering both projection strategies, SVM was the algorithm that obtained, regarding prediction\nerror and POCID rates, the most stable performance for deterministic time series (Fig. 40 \u2013 (a.1) and\n(b.1)). In contrast, SARIMA and kNN-TSPI were the best models for stochastic data (Fig. 40 \u2013\n(a.2) and (b.2)). Although kNN-TSPI was more accurate concerning the projection horizons trends,\nSARIMA provided the lowest error rates. Regarding the chaotic time series (Fig. 40 \u2013 (a.3) and (b.3)),\nSVM and kNN-TSPI were more stable than the other predictors.\nThe learning algorithms kNN-TSPI and SVM outperformed the state-of-the-art statistical methods\n(SARIMA and ARIMA) when examining all the 40 synthetic datasets (Fig. 40 \u2013 (a.4) and (b.4)).\nGiven the 55 real datasets (Fig. 40 \u2013 (a.5) and (b.5)), SARIMA and SVM showed very similar\nperformances. kNN-TSPI also provided good results for both projection strategies and without SSD\nin comparison to SARIMA and SVM.\nOn the overall comparison involving 95 datasets (Fig. 40 \u2013 (a.6) and (b.6)), the multi-criteria\nanalysis indicates that SARIMA, SVM and kNN-TSPI are the most promising methods for temporal\ndata modeling and prediction.\nTo compare the prediction quality of SARIMA, MLP, SVM, and kNN-TSPI, we show eight exam-\nples in Fig. 41. We chose these time series to illustrate some cases where the modeling is challenging.\nIn Fig. 41, we can see the impact of error propagation in the multi-step-ahead projection with ap-\nproximate iteration when compared to the updated iteration. Among the four algorithms, kNN-TSPI\nstands out for its robustness and stability concerning the projection horizons trends. An interesting\ncase where the algorithm performed very well is the example illustrated in Fig. 41(d.1).\nAlthough SARIMA and SVM have achieved about the same prediction accuracy as the similarity-\nbased method, the algorithm with invariances is simpler to understand, encode, and adjust. While\nSARIMA has seven parameters and SVM has three, kNN-TSPI have only two. Most importantly,\nthe two input arguments of kNN-TSPI are intuitive and can be easily estimated based on the data\nseasonality. The \ufb01rst parameter (l) is the query length in number of observations, and the second (k)\nis the number of similar subsequences required to make a prediction.\nThe use of baseline and topline methods is an essential guideline for conducting empirical as-\nsessments. In this direction, we suggest MA and HW as baseline models. MA is quite simple and\ngenerally performs better than the na\u00a8\u0131ve (one-step-ahead) technique. HW methods, besides being in-\ndicated for time series with trend and seasonality, show reasonable results concerning the projections\nhorizons trends. In respect to topline models, we emphasize the performance of SARIMA, SVM, and\nkNN-TSPI.\nWe are certain that the study of machine learning methods is right now at the same maturity stage\nas temporal data modeling researchers using statistical models. From the practical point of view, the\nresults put forward in this work will serve as a reference for the advance of time series prediction \ufb01eld.\n8. Conclusion\nThe comprehensive review performed in this work allowed us to identify an important gap in the\nliterature: the lack of an objective comparison between parametric and non-parametric models for\ntime series prediction. In this sense, to present a relevant contribution to the areas of statistics and\n33\n", [2832, 2833, 2834]], "Limitations, Recommendations, and Practical Implications of the Outcomes": ["the SARIMA, MLP, SVM, and kNN-TSPI models. Each one of these algorithms, regardless of the\nprojection strategy employed, recorded a POCID average of approximately 64.89% (SD = 23.33%).\nUpdated iteration\nApproximate iteration\nPOCID\nPrediction Methods\n30\n50\n70\n90\n110\n10\nFigure 38: Averages and SD of POCID obtained by the\nprediction methods on synthetic and real time series\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nSVM\nSARIMA\nkNN-TSPI\nLSTM\nMLP\nHES\nSES\nAHW\nMHW\nARIMA\nMA\n(a) Approximate iteration\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nSVM\nkNN-TSPI\nSARIMA\nMLP\nARIMA\nMA\nSES\nHES\nMHW\nLSTM\nAHW\n(b) Updated iteration\nFigure 39: MCPM over synthetic and real time series\nFig. 39 illustrates the CD diagrams with respect to the MCPM total area values for synthetic\nand real data. The outcomes of this multi-criteria analysis indicate that SARIMA, MLP, SVM, and\nkNN-TSPI are the most promising methods for time series modeling and prediction. Such fact is\nin line with our initial hypothesis, i.e., that machine learning algorithms o\ufb00er results similar to or\nbetter than those reached by state-of-the-art statistical methods with fewer parameters and without\nthe requirement of a priori knowledge of data distribution.\n7. Limitations, Recommendations, and Practical Implications of the Outcomes\nWe limited our study to predictors that deal with univariate data. Such methods receive as input\nunidimensional time series without considering possible explanatory variables. In future works, we\nwant to explore the multivariate scenario which still constitutes an important gap in the literature.\nTable 4 summarizes the main characteristics of the statistical and machine learning models dis-\ncussed and evaluated in this paper. The table exhibits the following information: the algorithm\u2019s\nname, the prediction approach, the number of parameters (#P) required by each predictor, the sam-\npling method for the parameter estimation, if the algorithm supports the multi-step-ahead projection\nstrategy (h > 1), and if the method is prepared to deal with non-stationarity, trend (T), and season-\nality (S). Note that the computational complexity is empirically related to the number of parameters\nof a model. Thus, SARIMA is the most expensive and complex among the evaluated methods.\nTable 4: Prediction algorithms and their properties\nAlgorithm\nPrediction\n#P\nTechnique for\nh > 1\nNon-stationarity\nT\nS\nApproach\nParameter Estimation\nMA\nParametric\n1\nHoldout Validation\n\u2713\nSES\nParametric\n1\nHoldout Validation\nHES\nParametric\n2\nHoldout Validation\n\u2713\n\u2713\nAHW\nParametric\n4\nHoldout Validation\n\u2713\n\u2713\n\u2713\nMHW\nParametric\n4\nHoldout Validation\n\u2713\n\u2713\n\u2713\nARIMA\nParametric\n4\nBox-Jenkins Method\n\u2713\n\u2713\n\u2713\nSARIMA\nParametric\n7\nBox-Jenkins Method\n\u2713\n\u2713\n\u2713\n\u2713\nMLP\nNon-parametric\n5\nCross-validation\n\u2713\n\u2713\n\u2713\n\u2713\nLSTM\nNon-parametric\n6\nHoldout Validation\n\u2713\n\u2713\n\u2713\n\u2713\nSVM\nNon-parametric\n3\nCross-validation\n\u2713\n\u2713\n\u2713\n\u2713\nkNN-TSPI\nNon-parametric\n2\nHoldout Validation\n\u2713\n\u2713\n\u2713\n\u2713\nFig. 40 summarizes the key results of this work. This \ufb01gure shows, for each time series group, the\nrank position of the predictors given the multi-criteria performance measure. Such illustration can\nbe an interesting tool to guide practitioners and researchers to choose the most promising method\naccording to the data characteristics and the type of multi-step-ahead projection strategy desired.\nThe multi-step-ahead projection with approximate iteration faces some di\ufb03culties, such as reduced\nperformance and increase uncertainty given the error accumulation. These problems become more\nvisible as the prediction horizon grows and the predicted values are not replaced by the actual values\n31\n", []], "Overall Comparison": ["CD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nARIMA\nkNN-TSPI\nLSTM\nHES\nAHW\nMHW\nMLP\nSES\nMA\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nARIMA\nLSTM\nkNN-TSPI\nHES\nAHW\nMHW\nMLP\nSES\nMA\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nkNN-TSPI\nMLP\nSVM\nMHW\nSES\nARIMA\nMA\nHES\nAHW\nLSTM\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nARIMA\nkNN-TSPI\nMLP\nMA\nHES\nMHW\nAHW\nLSTM\nSES\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nARIMA\nkNN-TSPI\nMLP\nMA\nHES\nMHW\nAHW\nLSTM\nSES\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nkNN-TSPI\nSVM\nMLP\nLSTM\nSES\nMA\nHES\nMHW\nARIMA\nAHW\n(f) Updated iteration \u2013 POCID\nFigure 32: CD diagrams for the MSE, TU, and POCID values coming from the predictors on real time series\nIn Fig. 32, we can note that SARIMA achieved the best results for both projection strategies given\nthe three evaluated measures. In general, models based on simple exponential smoothing obtained\nthe worst results, being that the MA method maintained the poorest overall performance. We did not\nverify SSD between the machine learning algorithms concerning the ARIMA and SARIMA models.\nThe four ranges of TU values indicated in Fig. 33 show that, using approximate iteration\n(Fig. 33(a)), SARIMA was adequate to predict 30 (20 + 10) of 55 datasets (TU < 1). The SVM\nalgorithm was suitable for modeling 27 (11 + 16) time series, while the ARIMA and kNN-TSPI\nmethods were preferable to the trivial model for 22 (12 + 10) of 55 datasets.\nTU > 1.00\nTU = 1.00\n0.55 < TU < 1.00\nTU \u22640.55\n5\n15\n25\n35\n45\n55\nDatasets\nPrediction Methods\n(a) Approximate iteration\n5\n15\n25\n35\n45\n55\nDatasets\nPrediction Methods\n(b) Updated iteration\nFigure 33: Performance of the prediction methods for the four ranges of TU values in real time series\nFor the updated iteration (Fig. 33(b)), SARIMA was convenient (TU < 1) to predict 43 (22 + 21)\ndatasets. Over this total, 22 provided a solid modeling which is reliable to make future projections\n(TU \u22640.55). SVM and kNN-TSPI were favorable (TU < 1) to predict the values of 42 (18 + 24) and\n38 (16 + 22) datasets, respectively.\nIn Fig. 34 we display the POCID results for real time series. We can see that when using approx-\nimate iteration, kNN-TSPI obtained an average hit rate on the prediction horizons trends equivalent\nto 61.86% (SD = 18.83%), competing with SARIMA \u2013 61.64% (SD = 26.29%) \u2013 and MLP \u2013 58.07%\n(SD = 18.04%). SARIMA and kNN-TSPI, both with updated iteration, achieved the highest POCID\nvalues, i.e., 61.27% (SD = 18.20%) and 59.11% (SD = 18.34%), respectively.\nFig. 35 portrays the CD diagrams designed from the MCPM total area values. SARIMA and\nSVM culminated in the best overall results. kNN-TSPI, in turn, also provided good results for both\nprojection strategies and without SSD in comparison to SARIMA, MLP, and SVM.\n6.3. Overall Comparison\nOur last comparison comprises all the 95 datasets, i.e., involves synthetic and real datasets with\nhigh variability in their characteristics. We evaluated a total of 2,090 con\ufb01gurations (11 predictors \u00d7\n2 projection strategies \u00d7 95 datasets). The CD diagrams of Fig. 36 summarizes such results according\n29\nTU > 1.00\nTU = 1.00\n0.55 < TU < 1.00\nTU \u22640.55\n2\n3\n4\n5\n6\n8\n7\n1\nDatasets\nPrediction Methods\n(a) Approximate iteration\n2\n3\n4\n5\n6\n8\n7\n1\nDatasets\nPrediction Methods\n(b) Updated iteration\nFigure 25: Performance of the prediction methods for the four ranges of TU values in chaotic time series\n\u2013, while SES \u2013 0.98% (SD = 1.62%) \u2013 and MA \u2013 33.76% (SD = 20.85%) \u2013 showed the poorest hit\nrates on the projection horizons trends. As for the projection strategy with updated iteration, the\nbest POCID values were achieved by MLP \u2013 97.41% (SD = 3.30%) \u2013, followed by SVM \u2013 95.93% (SD\n= 9.93%). The MA model maintained the poorest overall performance \u2013 67.78% (SD = 31.16%).\nUpdated iteration\nApproximate iteration\nPOCID\nPrediction Methods\n20\n40\n60\n80\n100\n120\nFigure 26: Averages and SD of POCID obtained by the\nprediction methods on chaotic time series\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nSVM\nLSTM\nkNN-TSPI\nMA\nSES\nAHW\nARIMA\nHES\nMHW\nMLP\nSARIMA\n(a) Approximate iteration\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nMLP\nSVM\nkNN-TSPI\nHES\nAHW\nMA\nSARIMA\nARIMA\nSES\nMHW\nLSTM\n(b) Updated iteration\nFigure 27: MCPM over chaotic time series\nFig. 27 illustrates the CD diagrams with respect to the MCPM total area values. Inspecting both\nprojection strategies, we can note that SVM and kNN-TSPI were more stable than the other methods\nfor predicting chaotic data.\n6.1.4. Overall Comparison\nAfter analyzing and discussing the results according to the di\ufb00erent characteristics of the data, we\nare now in a position to interpreting the outcomes considering all the synthetic data. The experiments\nperformed using all synthetic time series encompassed 880 con\ufb01gurations (11 predictors \u00d7 2 projection\nstrategies \u00d7 40 datasets). The CD diagrams of Fig. 28 summarizes such results.\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nLSTM\nHES\nARIMA\nMHW\nAHW\nSES\nMA\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nLSTM\nMLP\nHES\nARIMA\nMHW\nAHW\nSES\nMA\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nSARIMA\nMLP\nkNN-TSPI\nLSTM\nSES\nMA\nARIMA\nHES\nMHW\nAHW\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nMLP\nkNN-TSPI\nHES\nSARIMA\nMA\nSES\nARIMA\nLSTM\nMHW\nAHW\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nMLP\nkNN-TSPI\nHES\nAHW\nMA\nSES\nARIMA\nSARIMA\nMHW\nLSTM\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nMLP\nkNN-TSPI\nAHW\nLSTM\nSES\nMA\nARIMA\nSARIMA\nHES\nMHW\n(f) Updated iteration \u2013 POCID\nFigure 28: CD diagrams for the MSE, TU, and POCID values coming from the predictors on synthetic time series\n27\n", []], "Real Data": ["In agreement with these diagrams, SARIMA with approximate iteration showed the best values of\nMSE (Fig. 28(a)) and TU (Fig. 28(b)). Di\ufb00erently, when we examined the hit rates on the projection\nhorizons trends (Fig. 28(c)), such con\ufb01guration occupied the second position without SSD in compar-\nison with SVM. Considering the updated iteration, SVM, MLP and kNN-TSPI assumed, in this order\nand by a small di\ufb00erence margin, the \ufb01rst, second and third positions in the rankings derived from\nMSE (Fig. 28(d)), TU (Fig. 28(e)), and POCID (Fig. 28(f)).\nLooking at both projection strategies in Fig. 28, kNN-TSPI exhibited the third best performance\nfor all evaluation measures, except when treated of the POCID index for the projection strategy with\nupdated iteration. Besides, SVM was very competitive regarding the results obtained from SARIMA.\nFig. 29 highlights this result in terms of TU values.\nTU > 1.00\nTU = 1.00\n0.55 < TU < 1.00\nTU \u22640.55\n10\n20\n30\n40\nDatasets\nPrediction Methods\n(a) Approximate iteration\n10\n20\n30\n40\nDatasets\nPrediction Methods\n(b) Updated iteration\nFigure 29: Performance of the prediction methods for the four ranges of TU values in synthetic time series\nAnalyzing the approximate iteration (Fig. 29(a)), SARIMA was adequate in 27 (25 + 2) datasets\n(TU < 1), which 25 were very well modeled (TU \u22640.55). In contrast, HES failed to outperform the\nna\u00a8\u0131ve method for no one of the 40 datasets. Considering the updated iteration (Fig. 29(b)), the use\nof MLP, SVM and kNN-TSPI was appropriate for 39 datasets (TU < 1), which 26 resulted in reliable\npredictive models (TU \u22640.55).\nFig. 30 demonstrates that the machine learning methods MLP, SVM, and kNN-TSPI were able to\nachieve approximately 73.55% of precision (SD = 26.19%) on the projection horizons trends, regardless\nof the projection strategy adopted.\nUpdated iteration\nApproximate iteration\n30\n50\n70\n90\nPOCID\n110\nPrediction Methods\n10\nFigure 30: Averages and SD of POCID obtained by the\nprediction methods on synthetic time series\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nkNN-TSPI\nSVM\nSARIMA\nMLP\nLSTM\nHES\nARIMA\nSES\nMHW\nAHW\nMA\n(a) Approximate iteration\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nSVM\nMLP\nkNN-TSPI\nAHW\nMHW\nMA\nSES\nARIMA\nSARIMA\nHES\nLSTM\n(b) Updated iteration\nFigure 31: MCPM over synthetic time series\nFig. 31 contemplates the CD diagrams concerning the MCPM values considering all synthetic\ndatasets.\nWe can see that kNN-TSPI with approximate iteration achieved the best multi-criteria\nresults (Fig. 31(a)), surpassing by a minimum di\ufb00erence SVM, SARIMA, MLP, and LSTM. On the\nother hand, SVM with updated iteration showed the smallest prediction errors and the highest hit\nrates on the projection horizons trends (Fig. 31(b)), being very competitive with MLP and kNN-TSPI.\nIn relation to stability, the machine learning algorithms outperformed the state-of-the-art statistical\nmethods.\n6.2. Real Data\nThe computational tests conducted using real data totaled 1,210 con\ufb01gurations (11 predictors \u00d7 2\nprojection strategies \u00d7 55 datasets). Fig. 32 presents the CD diagrams regarding the MSE, TU, and\nPOCID measures obtained from the aforementioned experiments.\n28\n", []], "Chaotic Time Series": ["Updated iteration\nApproximate iteration\nPOCID\nPrediction Methods\n20\n40\n60\n80\n100\nFigure 22: Averages and SD of POCID obtained by the\nprediction methods on stochastic time series\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nkNN-TSPI\nSARIMA\nSVM\nLSTM\nMLP\nHES\nSES\nMHW\nAHW\nARIMA\nMA\n(a) Approximate iteration\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11\nSARIMA\nkNN-TSPI\nARIMA\nSVM\nLSTM\nHES\nSES\nAHW\nMLP\nMHW\nMA\n(b) Updated iteration\nFigure 23: MCPM over stochastic time series\nrates. This fact is due to SARIMA\u2019s own structure, which often includes an MA procedure that covers\nestimates of the innovation factor (white noise) that cannot be explained by the model.\n6.1.3. Chaotic Time Series\nThe computational tests conducted using chaotic data involved 176 con\ufb01gurations (11 predictors \u00d7\n2 projection strategies \u00d7 8 datasets). As depicted in the CD diagrams of Fig. 24, the algorithms with\napproximate iteration MA, SVM, SES and LSTM showed, in increasing order and without SSD, the\nbest results of MSE (Fig. 24(a)) and TU (Fig. 24(b)). The models with approximate iteration SVM,\nLSTM, and MLP assumed respectively the \ufb01rst, second, and third positions in the POCID average\nranking (Fig. 24(c)).\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nLSTM\nSES\nSVM\nMA\nSARIMA\nAHW\nHES\nARIMA\nMLP\nMHW\nkNN-TSPI\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nLSTM\nSES\nSVM\nMA\nSARIMA\nAHW\nHES\nARIMA\nMLP\nMHW\nkNN-TSPI\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nLSTM\nMLP\nHES\nkNN-TSPI\nSES\nMA\nAHW\nARIMA\nMHW\nSARIMA\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nSVM\nkNN-TSPI\nHES\nAHW\nMA\nMHW\nSARIMA\nARIMA\nSES\nLSTM\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nSVM\nkNN-TSPI\nHES\nAHW\nMA\nSARIMA\nMHW\nARIMA\nSES\nLSTM\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nSVM\nHES\nkNN-TSPI\nAHW\nMA\nSES\nMHW\nSARIMA\nARIMA\nLSTM\n(f) Updated iteration \u2013 POCID\nFigure 24: CD diagrams for the MSE, TU, and POCID values coming from the predictors on chaotic time series\nConcerning the updated iteration, MLP and SVM achieved the best results in the three perfor-\nmance measures (Fig. 24 \u2013 (d), (e), and (f)). On the other hand, MA exhibited the highest prediction\nerrors and the lowest hit rates on the projection horizons trends.\nIn Fig. 25, the statistics derived from TU coe\ufb03cient show that, using the approximate iter-\nation (Fig.25(a)), the predictive models were convenient to predict, approximately, two of eight\ndatasets (TU < 1). In this scenario, HES, AHW and MLP obtained, for all datasets, a performance\nlower than the trivial model (TU > 1).\nSuch results reinforce the di\ufb03culty in predicting chaotic\ntime series, especially when the employed projection strategy favors the error propagation along the\nprediction horizon.\nExamining the updated iteration in Fig. 25(b), we can see that, in general, MLP, SVM and kNN-\nTSPI were reliable for modeling seven of eight datasets (TU \u22640.55). We expected such results since\nmachine learning models are non-parametric. Although this premise is also valid for LSTM, we tend\nto have over\ufb01tting with such a network since it has many parameters and our datasets are not so\nlarge.\nIn Fig. 26, we expose the POCID averages and their respective SD. Analyzing the projection\nstrategy with approximate iteration, SVM presented the best POCID values \u2013 72.90% (SD = 14.47%)\n26\n", []], "Stochastic Time Series": ["prediction error and hit rate on the projection horizons trends.\n6.1.2. Stochastic Time Series\nThe experiments performed from stochastic data encompassed 330 con\ufb01gurations (11 predictors\n\u00d7 2 projection strategies \u00d7 15 datasets). The CD diagrams portrayed in Fig 20 summarizes these\ncomputational tests according to MSE, TU, and POCID.\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nLSTM\nARIMA\nHES\nMHW\nSES\nAHW\nMA\nMLP\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nLSTM\nMLP\nHES\nMHW\nSES\nAHW\nMA\nARIMA\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nLSTM\nSVM\nSARIMA\nMLP\nMHW\nSES\nMA\nARIMA\nkNN-TSPI\nAHW\nHES\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nARIMA\nSVM\nkNN-TSPI\nLSTM\nAHW\nHES\nMHW\nMA\nSES\nMLP\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nARIMA\nSVM\nkNN-TSPI\nLSTM\nAHW\nHES\nMHW\nMA\nSES\nMLP\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nkNN-TSPI\nLSTM\nSARIMA\nMHW\nSVM\nSES\nHES\nARIMA\nAHW\nMA\nMLP\n(f) Updated iteration \u2013 POCID\nFigure 20: CD diagrams for the MSE, TU, and POCID values coming from the predictors on stochastic time series\nIn Fig 20, kNN-TSPI with approximate iteration occupied the third position in the rankings of\nMSE (Fig. 20(a)) and TU (Fig. 20(b)).\nSuch con\ufb01guration has lost to SARIMA and SVM by a\nsmall di\ufb00erence margin. On average, LSTM with approximate iteration presented the best POCID\nrates (Fig. 20(c)), followed by SVM.\nConsidering the updated iteration, SVM achieved the third best result of MSE (Fig. 20(d)) and\nTU (Fig. 20(e)). The \ufb01rst two ranking positions were occupied by SARIMA and ARIMA, respectively.\nAs for the POCID values (Fig. 20(f)), kNN-TSPI outperformed LSTM and SARIMA without SSD.\nIn Fig. 21, the ranges of values derived from TU coe\ufb03cient evidence that SARIMA, with approx-\nimate and updated iterations, culminated in the most promising method to predict time series with\nstochastic behavior.\nTU > 1.00\nTU = 1.00\n0.55 < TU < 1.00\nTU \u22640.55\n3\n5\n7\n9\n11\n15\n13\n1\nDatasets\nPrediction Methods\n(a) Approximate iteration\n3\n5\n7\n9\n11\n15\n13\n1\nDatasets\nPrediction Methods\n(b) Updated iteration\nFigure 21: Performance of the prediction methods for the four ranges of TU values in stochastic time series\nIn Fig. 22, we can note that when employing approximate iteration, the highest hit rates were\nreached by MHW \u2013 51.71% (SD = 5.39%) \u2013, AHW \u2013 50.73% (SD = 3.51%) \u2013, and HES \u2013 50.65%\n(SD = 3.41%). kNN-TSPI exhibited the eighth best result, i.e., an average hit rate of 47.83% (SD =\n8.46%). In contrast, for the updated iteration, it showed the highest average hit rate on the prediction\nhorizons trends \u2013 49.20% (SD = 6.47%). The poorest POCID values were obtained by SES \u2013 27.94%\n(SD = 3.52%) \u2013, HES \u2013 31.47% (SD = 2.41%) \u2013, and ARIMA \u2013 37.02% (SD = 14.09%).\nFig. 23 covers the CD diagrams designed from the MCPM total area values. According to the\nmulti-criteria analysis, SARIMA and kNN-TSPI were the best algorithms regardless of the employed\nprojection strategy. Nevertheless, they did not present SSD regarding the ARIMA, MLP, LSTM, and\nSVM methods. Although kNN-TSPI has been more accurate, SARIMA provided the lowest error\n25\n", []], "Deterministic Time Series": ["triangle. Lower values of MCPM correspond to a better predictive performance for an algorithm, as\nportrayed in the right side of Fig. 15.\nFrom the values of the adopted evaluation measures, it was possible to compare the investigated\nalgorithms objectively. Friedman\u2019s non-parametric statistical test for paired data and multiple com-\nparisons, with a signi\ufb01cance level of 5% (p-value < 0.05), followed by Nemenyi posthoc test3, was\nemployed to compare the results.\nThe experimental protocol execution contemplated the use of the following programming languages:\nMATLAB and GNU Octave, as well as their packages of functions for time series prediction; R with\nForecast package; and Java with Weka library.\n6. Results and Discussion\nWe present and discuss the empirical results arranged into three large comparative studies: (i) pre-\ndictive models applied to synthetic datasets \u2013 including an isolated assessment of deterministic,\nstochastic, and chaotic data; (ii) predictive models applied to real datasets; and (iii) predictive models\napplied to both synthetic and real datasets. Note that the sequence of our discussion accompanies the\ncomplexity increment to model the data and the di\ufb03culty of the prediction task.\nWe used Critical Di\ufb00erence (CD) diagrams to show the statistical validation results. In these\ndiagrams, the scale indicates the rank position of each predictor according to their average perfor-\nmance [12]. Algorithms connected by a thick line have not presented Statistically Signi\ufb01cant Di\ufb00er-\nences (SSD) in quality. In the supplementary material or at the ICMC-USP Time Series Prediction\nRepository [32], we can see detailed results of MSE, TU and POCID, as well the values found in the\nstep of parameters estimation.\nIn a complementary way and due to the particularities of TU and POCID indexes, they had\ntheir values summarized in full stacked area charts and bar charts with Standard Deviations (SD),\nrespectively. We also employed a MCPM to provide an overview of the results.\n6.1. Synthetic Data\nThis collection of experiments covers 40 synthetic time series, of which 17 are deterministic, 15\nstochastic, and 8 chaotic. In the next subsections we detail our analysis according to these categories.\n6.1.1. Deterministic Time Series\nThe computational tests conducted using deterministic data involved 374 con\ufb01gurations (11 pre-\ndictors \u00d7 2 projection strategies \u00d7 17 datasets). Fig. 16 presents the CD diagrams concerning the\nMSE, TU, and POCID indexes.\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nAHW\nARIMA\nHES\nLSTM\nMHW\nSES\nMA\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nAHW\nARIMA\nHES\nMHW\nSES\nLSTM\nMA\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nkNN-TSPI\nSVM\nMLP\nAHW\nSES\nMA\nLSTM\nHES\nARIMA\nMHW\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nAHW\nSVM\nHES\nMHW\nMA\nARIMA\nSARIMA\nSES\nLSTM\nkNN-TSPI\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nAHW\nSVM\nHES\nMHW\nMA\nARIMA\nSARIMA\nSES\nLSTM\nkNN-TSPI\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nMLP\nAHW\nHES\nMHW\nMA\nARIMA\nSES\nSARIMA\nLSTM\nkNN-TSPI\n(f) Updated iteration \u2013 POCID\nFigure 16: CD diagrams for the MSE, TU, and POCID values coming from the predictors on deterministic time series\n3All statistical tests were performed using KEEL Software Tool for Windows, http://www.keel.es.\n23\n", []], "Synthetic Data": ["triangle. Lower values of MCPM correspond to a better predictive performance for an algorithm, as\nportrayed in the right side of Fig. 15.\nFrom the values of the adopted evaluation measures, it was possible to compare the investigated\nalgorithms objectively. Friedman\u2019s non-parametric statistical test for paired data and multiple com-\nparisons, with a signi\ufb01cance level of 5% (p-value < 0.05), followed by Nemenyi posthoc test3, was\nemployed to compare the results.\nThe experimental protocol execution contemplated the use of the following programming languages:\nMATLAB and GNU Octave, as well as their packages of functions for time series prediction; R with\nForecast package; and Java with Weka library.\n6. Results and Discussion\nWe present and discuss the empirical results arranged into three large comparative studies: (i) pre-\ndictive models applied to synthetic datasets \u2013 including an isolated assessment of deterministic,\nstochastic, and chaotic data; (ii) predictive models applied to real datasets; and (iii) predictive models\napplied to both synthetic and real datasets. Note that the sequence of our discussion accompanies the\ncomplexity increment to model the data and the di\ufb03culty of the prediction task.\nWe used Critical Di\ufb00erence (CD) diagrams to show the statistical validation results. In these\ndiagrams, the scale indicates the rank position of each predictor according to their average perfor-\nmance [12]. Algorithms connected by a thick line have not presented Statistically Signi\ufb01cant Di\ufb00er-\nences (SSD) in quality. In the supplementary material or at the ICMC-USP Time Series Prediction\nRepository [32], we can see detailed results of MSE, TU and POCID, as well the values found in the\nstep of parameters estimation.\nIn a complementary way and due to the particularities of TU and POCID indexes, they had\ntheir values summarized in full stacked area charts and bar charts with Standard Deviations (SD),\nrespectively. We also employed a MCPM to provide an overview of the results.\n6.1. Synthetic Data\nThis collection of experiments covers 40 synthetic time series, of which 17 are deterministic, 15\nstochastic, and 8 chaotic. In the next subsections we detail our analysis according to these categories.\n6.1.1. Deterministic Time Series\nThe computational tests conducted using deterministic data involved 374 con\ufb01gurations (11 pre-\ndictors \u00d7 2 projection strategies \u00d7 17 datasets). Fig. 16 presents the CD diagrams concerning the\nMSE, TU, and POCID indexes.\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nAHW\nARIMA\nHES\nLSTM\nMHW\nSES\nMA\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nAHW\nARIMA\nHES\nMHW\nSES\nLSTM\nMA\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nkNN-TSPI\nSVM\nMLP\nAHW\nSES\nMA\nLSTM\nHES\nARIMA\nMHW\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nAHW\nSVM\nHES\nMHW\nMA\nARIMA\nSARIMA\nSES\nLSTM\nkNN-TSPI\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nAHW\nSVM\nHES\nMHW\nMA\nARIMA\nSARIMA\nSES\nLSTM\nkNN-TSPI\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nMLP\nAHW\nHES\nMHW\nMA\nARIMA\nSES\nSARIMA\nLSTM\nkNN-TSPI\n(f) Updated iteration \u2013 POCID\nFigure 16: CD diagrams for the MSE, TU, and POCID values coming from the predictors on deterministic time series\n3All statistical tests were performed using KEEL Software Tool for Windows, http://www.keel.es.\n23\n", []], "Results and Discussion": ["triangle. Lower values of MCPM correspond to a better predictive performance for an algorithm, as\nportrayed in the right side of Fig. 15.\nFrom the values of the adopted evaluation measures, it was possible to compare the investigated\nalgorithms objectively. Friedman\u2019s non-parametric statistical test for paired data and multiple com-\nparisons, with a signi\ufb01cance level of 5% (p-value < 0.05), followed by Nemenyi posthoc test3, was\nemployed to compare the results.\nThe experimental protocol execution contemplated the use of the following programming languages:\nMATLAB and GNU Octave, as well as their packages of functions for time series prediction; R with\nForecast package; and Java with Weka library.\n6. Results and Discussion\nWe present and discuss the empirical results arranged into three large comparative studies: (i) pre-\ndictive models applied to synthetic datasets \u2013 including an isolated assessment of deterministic,\nstochastic, and chaotic data; (ii) predictive models applied to real datasets; and (iii) predictive models\napplied to both synthetic and real datasets. Note that the sequence of our discussion accompanies the\ncomplexity increment to model the data and the di\ufb03culty of the prediction task.\nWe used Critical Di\ufb00erence (CD) diagrams to show the statistical validation results. In these\ndiagrams, the scale indicates the rank position of each predictor according to their average perfor-\nmance [12]. Algorithms connected by a thick line have not presented Statistically Signi\ufb01cant Di\ufb00er-\nences (SSD) in quality. In the supplementary material or at the ICMC-USP Time Series Prediction\nRepository [32], we can see detailed results of MSE, TU and POCID, as well the values found in the\nstep of parameters estimation.\nIn a complementary way and due to the particularities of TU and POCID indexes, they had\ntheir values summarized in full stacked area charts and bar charts with Standard Deviations (SD),\nrespectively. We also employed a MCPM to provide an overview of the results.\n6.1. Synthetic Data\nThis collection of experiments covers 40 synthetic time series, of which 17 are deterministic, 15\nstochastic, and 8 chaotic. In the next subsections we detail our analysis according to these categories.\n6.1.1. Deterministic Time Series\nThe computational tests conducted using deterministic data involved 374 con\ufb01gurations (11 pre-\ndictors \u00d7 2 projection strategies \u00d7 17 datasets). Fig. 16 presents the CD diagrams concerning the\nMSE, TU, and POCID indexes.\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nAHW\nARIMA\nHES\nLSTM\nMHW\nSES\nMA\n(a) Approximate iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nSVM\nkNN-TSPI\nMLP\nAHW\nARIMA\nHES\nMHW\nSES\nLSTM\nMA\n(b) Approximate iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSARIMA\nkNN-TSPI\nSVM\nMLP\nAHW\nSES\nMA\nLSTM\nHES\nARIMA\nMHW\n(c) Approximate iteration \u2013 POCID\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nAHW\nSVM\nHES\nMHW\nMA\nARIMA\nSARIMA\nSES\nLSTM\nkNN-TSPI\n(d) Updated iteration \u2013 MSE\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nMLP\nAHW\nSVM\nHES\nMHW\nMA\nARIMA\nSARIMA\nSES\nLSTM\nkNN-TSPI\n(e) Updated iteration \u2013 TU\nCD\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nSVM\nMLP\nAHW\nHES\nMHW\nMA\nARIMA\nSES\nSARIMA\nLSTM\nkNN-TSPI\n(f) Updated iteration \u2013 POCID\nFigure 16: CD diagrams for the MSE, TU, and POCID values coming from the predictors on deterministic time series\n3All statistical tests were performed using KEEL Software Tool for Windows, http://www.keel.es.\n23\n", []], "Empirical Evaluation": ["Algorithm 3: Box-Jenkins Method\n// S represents a training subsequence\n/* max ord1 e max ord2 are three-position vectors whose values indicate the maximum lag lengths of the\nnon-seasonal and seasonal parts, respectively\n*/\n/* max p is an upper bound for the number of observations constituting a seasonal station in the historical\nseries\n*/\n/* P comprises the parameters list which resulted in the least prediction error\n*/\nInput:\nS, max ord1, max ord2, max p\nOutput:\nP\n1 begin\n2\nbest aic = \u221e;\n3\nn = length(S);\n4\nfor p \u21900 : max ord1[1] do\n5\nfor d \u21900 : max ord1[2] do\n6\nfor q \u21900 : max ord1[3] do\n7\nfor P \u21900 : max ord2[1] do\n8\nfor D \u21900 : max ord2[2] do\n9\nfor Q \u21900 : max ord2[3] do\n10\nif\nd + D \u22641 then\n11\nfit = sarima(S, [p, d, q], [P, D, Q], max p, \u03b4 = TRUE);\n12\nelse\n13\nfit = sarima(S, [p, d, q], [P, D, Q], max p, \u03b4 = FALSE);\n14\nend\n15\nfit aic = \u22122 \u2217fit.loglik + (log(n) + 1) \u2217length(fit.coef);\n16\nif\nfit aic < best aic then\n17\nbest aic \u2190fit aic;\n18\nbest fit \u2190fit;\n19\nbest model \u2190[p, d, q, P, D, Q, max p];\n20\nend\n21\nend\n22\nend\n23\nend\n24\nend\n25\nend\n26\nend\n27\nP \u2190{best aic, best fit, best model};\n28\nreturn P;\n29 end\nseasonal period in the series. In situations where this information is not clearly visible, we can apply\nthe technique of scatter plot to obtain max p. Given that the constant \u03b4 represents the initial level\nof the model, a condition for inclusion or omission of \u03b4 was inserted respecting the di\ufb00erentiation\nrules for SARIMA in the 10th line. In the 16th line, the most promising parameters are chosen so to\nminimize the AIC (Eq. 27).\nThe values of d and D can be established in a preprocessing step integrating the time series until\nits variance becomes smaller than its original version (undi\ufb00erentiated). The prior identi\ufb01cation of\nthese values is important to reduce the processing time of Algorithm 3.\n5. Empirical Evaluation\nThe protocol of our performance evaluation was organized in three steps, as outlined in Fig. 14.\nStep 2\nParameter\nEstimation\nPrediction of\nValues\nStep 1\nTemporal\nDatasets\nICMC-USP\nTime Series Prediction Repository\n\u2013 \uf0a3\uf0d1\n\u2013 \uf0a3\uf0d1\nSynthetic\nReal\nStep 3\nPerformance\nEvaluation\ny(t)\nt\nMSE\nU-Theil\nPOCID\nMulti-Criteria\nPerformance \nMeasure\nCross-validation\nHoldout\nValidation\nBox-Jenkins Method\nMachine\nLearning \nMethods\nMA\nSES\nHES\nAHW\nMHW\nARIMA\nSARIMA\nStatistical\nMethods\nSVM\nMLP\nkNN-TSPI\nLSTM\nFigure 14: Experimental setup\n19\n", [1869, 1870, 1871, 1872, 1885, 1886]], "Box-Jenkins Method": ["observations of the subsequence S, i.e., z1, . . . , zm\u2212h (S \u2208Z). Afterward, the model is extrapolated\nto a horizon prediction h\u2032 whose length is equivalent to the validation subsequence (sn\u2212h\u2032+1, . . . , sn).\nAt the end of this search, the most promising parameters (P) are those which minimize the error\nbetween the predicted and validation subsequences. Many measures can measure this error, but the\nmost common is the Mean Square Error (MSE).\nThere are di\ufb00erent ways to choose the number of observations covered by h\u2032. We have adopted\nh\u2032 = (max p + h) \u00f7 2, where h indicates the number of values to be projected in the test step by the\nprediction method using the best set of parameters found.\n4.3.2. Cross-validation\nCross-validation is a sampling technique broadly used for evaluating models in machine learning.\nFor time series prediction, the technique searches for the most adequate parameter values for global\napproach methods. Algorithm 2 details this sampling technique to search values for the parameters l,\nC, and \u03c3 of the SVM regression algorithm.\nAlgorithm 2: Cross-validation\n// S represents a training subsequence\n/* max p specifies an upper bound for the number of observations constituting a seasonal station in the\nhistorical series\n*/\n/* kFolds is the number of partitions on which the training data sample will be split\n*/\n/* P comprises the parameters list which resulted in the least prediction error\n*/\nInput:\nS, max p, h\nOutput:\nP\n1 begin\n2\nmin error = \u221e;\n3\nfor l \u21903 : 2 : max p do\n4\nT \u2190generate data table(S, l);\n5\nfor C \u21900 : 0.25 : 1 do\n6\nfor \u03c3 \u21900.005 : 0.05 : 0.25 do\n7\nerror = cross validation(T, kFolds, C, \u03c3, model = \u201cSVM\u201d);\n8\nif error < min error then\n9\nmin error = error;\n10\nlbest = l;\n11\nCbest = C;\n12\n\u03c3best = \u03c3;\n13\nend\n14\nend\n15\nend\n16\nend\n17\nP \u2190{lbest, Cbest, \u03c3best};\n18\nreturn P;\n19 end\nIn Algorithm 2, the idea behind the search is similar to that observed in Algorithm 1, except by\nthe content of the 4th and 7th lines. In the 4th line occurs the transposition of the training subsequence\nS for the attribute-value format using a sliding window of length l, such as illustrated by the training\nset in Fig. 7. In the 7th line, the attribute-value table T is randomly divided into k samples (kFolds)\nmutually exclusive, all of approximately the same size. The kth sample is used as a validation set and\nthe k \u22121 remaining samples are the training set. For each combination of k \u22121 samples, a model is\nconstructed and adjusted according to the current parameters combination. The prediction error is\ncomputed on the validation set k by a loss function as MSE. Evidently, when we use the MSE, the\nparameterization error is given by the average of the MSE of the k generated models and is considered\nan estimative of the true error expected on independent data.\n4.3.3. Box-Jenkins Method\nWe can determine the parameters of an ARIMA model via a search mechanism guided by an\ninformation criterion that penalizes the models\u2019 adjustment with many parameters. This method is\ncalled Box-Jenkins. Algorithm 3 exempli\ufb01es the use of this method to identify all SARIMA parameters\nof order (p, d, q)\u00d7(P, D, Q)s.\nIn Algorithm 3, p, d, q, P, D, and Q are relevant time delays of the time series within a search\nspace pre-determined by the user. The value of max p corresponds, in number of observations, a\n18\n", [1869, 1870, 1871, 1872]], "Cross-validation": ["observations of the subsequence S, i.e., z1, . . . , zm\u2212h (S \u2208Z). Afterward, the model is extrapolated\nto a horizon prediction h\u2032 whose length is equivalent to the validation subsequence (sn\u2212h\u2032+1, . . . , sn).\nAt the end of this search, the most promising parameters (P) are those which minimize the error\nbetween the predicted and validation subsequences. Many measures can measure this error, but the\nmost common is the Mean Square Error (MSE).\nThere are di\ufb00erent ways to choose the number of observations covered by h\u2032. We have adopted\nh\u2032 = (max p + h) \u00f7 2, where h indicates the number of values to be projected in the test step by the\nprediction method using the best set of parameters found.\n4.3.2. Cross-validation\nCross-validation is a sampling technique broadly used for evaluating models in machine learning.\nFor time series prediction, the technique searches for the most adequate parameter values for global\napproach methods. Algorithm 2 details this sampling technique to search values for the parameters l,\nC, and \u03c3 of the SVM regression algorithm.\nAlgorithm 2: Cross-validation\n// S represents a training subsequence\n/* max p specifies an upper bound for the number of observations constituting a seasonal station in the\nhistorical series\n*/\n/* kFolds is the number of partitions on which the training data sample will be split\n*/\n/* P comprises the parameters list which resulted in the least prediction error\n*/\nInput:\nS, max p, h\nOutput:\nP\n1 begin\n2\nmin error = \u221e;\n3\nfor l \u21903 : 2 : max p do\n4\nT \u2190generate data table(S, l);\n5\nfor C \u21900 : 0.25 : 1 do\n6\nfor \u03c3 \u21900.005 : 0.05 : 0.25 do\n7\nerror = cross validation(T, kFolds, C, \u03c3, model = \u201cSVM\u201d);\n8\nif error < min error then\n9\nmin error = error;\n10\nlbest = l;\n11\nCbest = C;\n12\n\u03c3best = \u03c3;\n13\nend\n14\nend\n15\nend\n16\nend\n17\nP \u2190{lbest, Cbest, \u03c3best};\n18\nreturn P;\n19 end\nIn Algorithm 2, the idea behind the search is similar to that observed in Algorithm 1, except by\nthe content of the 4th and 7th lines. In the 4th line occurs the transposition of the training subsequence\nS for the attribute-value format using a sliding window of length l, such as illustrated by the training\nset in Fig. 7. In the 7th line, the attribute-value table T is randomly divided into k samples (kFolds)\nmutually exclusive, all of approximately the same size. The kth sample is used as a validation set and\nthe k \u22121 remaining samples are the training set. For each combination of k \u22121 samples, a model is\nconstructed and adjusted according to the current parameters combination. The prediction error is\ncomputed on the validation set k by a loss function as MSE. Evidently, when we use the MSE, the\nparameterization error is given by the average of the MSE of the k generated models and is considered\nan estimative of the true error expected on independent data.\n4.3.3. Box-Jenkins Method\nWe can determine the parameters of an ARIMA model via a search mechanism guided by an\ninformation criterion that penalizes the models\u2019 adjustment with many parameters. This method is\ncalled Box-Jenkins. Algorithm 3 exempli\ufb01es the use of this method to identify all SARIMA parameters\nof order (p, d, q)\u00d7(P, D, Q)s.\nIn Algorithm 3, p, d, q, P, D, and Q are relevant time delays of the time series within a search\nspace pre-determined by the user. The value of max p corresponds, in number of observations, a\n18\n", []], "Holdout Validation": ["t\nZ\n1\nm\n\ud835\udc4d\n\ud835\udc44\n\ud835\udc461..\ud835\udc59\n(\ud835\udc58)\n\ud835\udc46\ud835\udc59+1\n(\ud835\udc57)\n\ud835\udc67\ud835\udc5a+1\nFigure 13: An example that illustrates the kNN algorithm for time series prediction with l = 25 and k = 3\nhighly nonlinear and complex time series patterns [33, 48]. A recent study proposed a novel and\npromising modi\ufb01cation of the kNN algorithm for time series prediction, namely kNN - Time Series\nPrediction with Invariances (kNN-TSPI) [33]. This proposal di\ufb00ers from the literature by incorpo-\nrating techniques for amplitude and o\ufb00set invariance, complexity invariance, and treatment of trivial\nmatches. According to the authors, these three invariances when combined allow more meaningful\nmatching between the reference queries and temporal data subsequences.\n4.3. Techniques for Parameter Estimation\nOne of the main di\ufb03culties faced by researchers in the time series prediction is the search for the\nbest parameter setting to \ufb01t a model according to a dataset.\nIn theoretical terms, the establishment of all parameters of a model could need the full exploitation\nof the state space. As this procedure is impractical for most real-world datasets, search algorithms are\nused to \ufb01nd a suboptimal solution with satisfactory performance and acceptable computational cost.\nNext, we present the main parameter estimation methods for time series prediction.\n4.3.1. Holdout Validation\nThe holdout validation technique performs the search for parameters values that minimize the\npredictive model error over a di\ufb00erent number of intervals in the training data. Algorithm 1 describes\nthe logic of this sampling technique to \ufb01nd adequate values for the parameters l and k of the kNN-TSPI\nmethod.\nAlgorithm 1: Holdout Validation\n/* S represents a training subsequence of length n extracted from a time series Z of size m\n*/\n/* max p is an upper bound for number of observations constituting a seasonal station in the historical\nseries\n*/\n/* h indicates the amount of values to be predicted by the best model identified\n*/\n/* P comprises the parameters list which resulted in the least prediction error\n*/\nInput:\nS, max p, h\nOutput:\nP\n1 begin\n2\nmin error = \u221e;\n3\nh\u2032 = (max p + h) \u00f7 2;\n4\nfor l \u21903 : 2 : max p do\n5\nfor k \u21901 : 2 : 9 do\n6\nerror = knn tspi(S, l, k, h\u2032);\n7\nif error < min error then\n8\nmin error = error;\n9\nlbest = l;\n10\nkbest = k;\n11\nend\n12\nend\n13\nend\n14\nP \u2190{lbest, kbest};\n15\nreturn P;\n16 end\nAlgorithm 1 iteratively evaluates a set of previously de\ufb01ned parameters. At each iteration and\naccording to the current parameters combination, a new model is built and adjusted on the m \u2212h\n17\n", [1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857]], "Techniques for Parameter Estimation": ["t\nZ\n1\nm\n\ud835\udc4d\n\ud835\udc44\n\ud835\udc461..\ud835\udc59\n(\ud835\udc58)\n\ud835\udc46\ud835\udc59+1\n(\ud835\udc57)\n\ud835\udc67\ud835\udc5a+1\nFigure 13: An example that illustrates the kNN algorithm for time series prediction with l = 25 and k = 3\nhighly nonlinear and complex time series patterns [33, 48]. A recent study proposed a novel and\npromising modi\ufb01cation of the kNN algorithm for time series prediction, namely kNN - Time Series\nPrediction with Invariances (kNN-TSPI) [33]. This proposal di\ufb00ers from the literature by incorpo-\nrating techniques for amplitude and o\ufb00set invariance, complexity invariance, and treatment of trivial\nmatches. According to the authors, these three invariances when combined allow more meaningful\nmatching between the reference queries and temporal data subsequences.\n4.3. Techniques for Parameter Estimation\nOne of the main di\ufb03culties faced by researchers in the time series prediction is the search for the\nbest parameter setting to \ufb01t a model according to a dataset.\nIn theoretical terms, the establishment of all parameters of a model could need the full exploitation\nof the state space. As this procedure is impractical for most real-world datasets, search algorithms are\nused to \ufb01nd a suboptimal solution with satisfactory performance and acceptable computational cost.\nNext, we present the main parameter estimation methods for time series prediction.\n4.3.1. Holdout Validation\nThe holdout validation technique performs the search for parameters values that minimize the\npredictive model error over a di\ufb00erent number of intervals in the training data. Algorithm 1 describes\nthe logic of this sampling technique to \ufb01nd adequate values for the parameters l and k of the kNN-TSPI\nmethod.\nAlgorithm 1: Holdout Validation\n/* S represents a training subsequence of length n extracted from a time series Z of size m\n*/\n/* max p is an upper bound for number of observations constituting a seasonal station in the historical\nseries\n*/\n/* h indicates the amount of values to be predicted by the best model identified\n*/\n/* P comprises the parameters list which resulted in the least prediction error\n*/\nInput:\nS, max p, h\nOutput:\nP\n1 begin\n2\nmin error = \u221e;\n3\nh\u2032 = (max p + h) \u00f7 2;\n4\nfor l \u21903 : 2 : max p do\n5\nfor k \u21901 : 2 : 9 do\n6\nerror = knn tspi(S, l, k, h\u2032);\n7\nif error < min error then\n8\nmin error = error;\n9\nlbest = l;\n10\nkbest = k;\n11\nend\n12\nend\n13\nend\n14\nP \u2190{lbest, kbest};\n15\nreturn P;\n16 end\nAlgorithm 1 iteratively evaluates a set of previously de\ufb01ned parameters. At each iteration and\naccording to the current parameters combination, a new model is built and adjusted on the m \u2212h\n17\n", [1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857]], "k-Nearest Neighbors": ["Support\nVectors\nSeparation\nMargin\nSupport\nVector\nSupport\nHyperplanes\nOptimal Separation\nHyperplane\nx2\nx1\nFigure 12: Optimal separation hyperplane and its supporting hyperplanes. The ordered axes x1 and x2 represent the\ndimensions of the samples in a 2D space\nkernel requires two parameters [17]: (i) C, which is a regularization term that imposes a weight on the\ntraining set errors minimization regarding the model complexity minimization; and (ii) \u03c3, that re\ufb02ects\nthe Gaussian\u2019s width of the kernel function.\nThe number of radial functions and their respective\ncenters is determined by the support vectors found.\nThe construction of an SVM implies in solving a quadratic problem with linear constraints which\ndepends on the set of input data, parameters, and of the separation margin. During the training\nphase, the Lagrange multipliers that characterize the support vectors are obtained. These support\nvectors de\ufb01ne the edges of the optimal separation hyperplane.\n4.2.3. k-Nearest Neighbors\nSimilarity-based methods, like the kNN classi\ufb01er, are characterized by not constructing a model\nthat explicitly describes the training dataset behavior. The model is built by simply storing the data\nsample. The generalization on the training set is performed every time we asked the algorithm for a\nnew classi\ufb01cation.\nThe general idea behind the adaptation of kNN for time series prediction is very intuitive. Given\na series Z = (z1, z2, . . . , zm) in which zt \u2208\u211c, the problem is to predict the value zm+h, where h is\nthe prediction horizon. For simplicity, but without loss of generality, the idea will be discussed in a\nunitary horizon (h = 1), i.e., considering only the prediction of the next value (zm+1).\nThe modi\ufb01ed method uses the last l observations as query Q, and searches for the k most similar\nsubsequences to Q, using a sliding window of size l.\nGiven S(1)\n1..l, . . . , S(k)\n1..l as the k most similar\nsubsequences of Q, the algorithm uses the next observations of each subsequence S(j)\nl+1 with 1 \u2264j \u2264k\nto predict zm+1. Thereby, the values of S(j)\nl+1 are provided as input to a prediction function f to\napproximate the value of zm+1. Eq. 29 is an example of ensemble function.\nf(S) = 1\nk\nk\nX\nj=1\nS(j)\nl+1\n(29)\nIn Eq. 29, the prediction function f has the argument S that denotes the set of k most similar\nsubsequences, and S(j) refers to jth nearest neighbor.\nThis is the simplest way of combining the\nprojections since the predictions average considers that all projected values are equally probable to\noccur in the future.\nFig. 13 displays an application of the described method with l = 25 and k = 3. The dotted line in\ngray represents the observations that belong to the time series; the green line indicates the subsequence\nof length 25 taken as reference query; the blue dotted lines express the most similar subsequences found\nusing some similarity measure, in this case, the Euclidean distance; the blue circles correspond to the\nobservations used for making the prediction; and the red square re\ufb02ects the value to be predicted.\nThe prediction made by similarity-based methods considers only the previous l observations. Thus,\nthe temporal dependence is restricted to a limited number of previous observations, since usually a\ncertain value is not in\ufb02uenced by observations that happened a long time ago.\nSeveral surveys were conducted to analyze the performance of kNN with di\ufb00erent ensemble func-\ntions and distance measures. Moreover, a few papers showed that these methods are useful to predict\n16\n", [1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857]], "Support Vector Machines": ["We can categorize the Perceptron and MLP models as feed-forward neural networks because in\nthem the neuron-to-neuron signals \ufb02ow only in one direction: from input to output. Di\ufb00erently, in\nRecurrent Neural Networks (RNN), connections between neurons form a cycle, and the signals are\nable to move in di\ufb00erent directions. For example, in the Simple Recurrent Network (SRN) [13], the\nstate of the hidden layer at a given time is conditioned on its previous state by a context layer, as\nillustrated in Fig. 11. This recursion implies a short-term memory, which allows the network to store\ncomplex signals for arbitrarily long time periods.\nOutput Layer\nHidden Layer\nContext Layer\nInput Layer\n1\nFigure 11: Structure of SRN. Recurrent link between hidden and context layer\nThe ability to model temporal dependencies makes RNN architectures especially suitable for tasks\nas speech classi\ufb01cation and prediction, where input and/or output covers data sequences that are\ndependent. Among the several improved RNN, we can cite Bidirectional RNN (BRNN) [42], and\nNonlinear Autoregressive RNN with Exogenous Inputs (NARX) [43].\nIn recent years, the concept of fuzzy logic and wavelets has become very attractive in the \ufb01eld\nof RNN [28].\nFuzzy logic enables us to reduce the complexity of the data and to deal with un-\ncertainty [7]; wavelet transform allows us to analyze non-stationary signals to discover their local\ndetails [36]; RNN has self-learning characteristic that increases the accuracy of the model.\nTheir\ncombination contributes to the development of models with fast learning capability that can describe\nnonlinear systems characterized by uncertainty.\nWhile theoretically powerful, the recurrent models aforementioned were widely considered to be\nhard to train due to the so-called vanishing and exploding gradient problems [20, 4]. A popular solution\nto deal with this issue is adopting gated architectures, like Long Short-Term Memory (LSTM) and\nGated Recurrent Unit (GRU), which were speci\ufb01cally designed to allow the network to learn much\nlonger-range dependencies [21].\nAlthough LSTM is responsible for several models considered state-of-the-art in the literature,\nits performance depends heavily on the amount of data available. Besides, the parameterization of\nLSTM-based networks is still very complex and expensive.\n4.2.2. Support Vector Machines\nSVM constitutes a machine learning technique based on the statistical learning theory [45] with\nthe ability to solve distinct problems as classi\ufb01cation and regression [17].\nAlthough SVM models have a similar structure to ANN, they di\ufb00er in how the learning is con-\nducted. While ANN work by minimizing the empirical risk, i.e., the error minimization of the induced\nmodel on the training data, SVM are grounded on the principle of structural risk minimization, which\nseeks the lowest training error while minimizing an upper bound on the generalization error of the\nmodel (model error when applied to test data) [45].\nThe generalization concept is best understood in the case of binary classi\ufb01cation. Thus, given a\nset of points belonging two classes, an SVM determines the hyperplane that separates them placing\nthe largest possible number of points of a class on the same side, while the distance from each class to\nthe decision surface is maximized. Fig. 12 shows a set of straight lines that discriminate the data into\ntwo classes. Between these straight lines, only one maximizes the separation margin (distance between\nthe hyperplane and the nearest sample of each class). The straight line with maximum margin, called\noptimal separation hyperplane, is the object to be searched during the model training.\nThe technique indicated in Fig. 12 is restricted to linearly separable problems. However, in situa-\ntions where the samples are not linearly separable, the solution focuses on mapping the input data to\na higher-dimensional space (feature space). We can achieve this mapping using a kernel function.\nLinear, polynomial, and Radial Basis Function (RBF) kernels are the most applied in practice.\nEach one of them covers parameters that need to be set. For example, the SVM model with RBF\n15\n", [1821, 1822, 1823, 1824, 1825, 1826]], "Artificial Neural Networks": ["h = 4\nz\nz1\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nz11\nz12\nz13\nz14\nz15\nt\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nl = 3\n\u2460\n\u0177\n\u1e91t+1\n\u1e9112\n\u1e9113\n\u1e9114\n\u1e9115\ny\nzt+1\nz12\nz13\nz14\nz15\nTime \nSeries\nTest\nDataset\n\u2462\nParameter\nEstimation\nModel Building\nand Model Fit\nPrediction of\nValues\nConventional\nMachine Learning\nx1\nzt-2\nz1\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nz11\nz12\ny\nzt+1\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nz11\nz12\nz13\nz14\nz15\nxl\nzt\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nz11\nz12\nz13\nz14\n\u2461\nAttribute-value\nTable\nx2\nzt-1\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nz10\nz11\nz12\nz13\nTraining\nDataset\nFigure 7: Exempli\ufb01cation of global approach for time series prediction\nthe time series in order to collect all the subsequences formed by l consecutive observations. Each\nextracted subsequence refers to a pair (Xi, yi) where: Xi = (xi1, xi2, xil) and corresponds to the\ntemporal pattern of length l; and yi indicates the subsequent value to Xi, observed at the instant\nl + 1. The set of pairs (Xij, yij), where j \u2208[1, m \u2212l], constitutes the attribute-value table. The idea\nbehind this conversion is to use observations from the past to predict an observation in the future.\nAssuming a prediction horizon h = 4, the data of the table are partitioned into two sets: (i) training\nset, used to build the model; and (ii) test set, used for model performance evaluation. The prediction\naccuracy is estimated by comparing the predicted values \u02c6yk with their actual values yk, where k \u2208[1, h].\nFig. 8 exempli\ufb01es an application of the multi-step-ahead projection strategy, with approximate and\nupdated iterations, to calculate the estimates \u02c6yk.\nx2\nzt-1\nz10\nz11\n\u1e9112\n\u1e9113\nx1\nzt-2\nz9\nz10\nz11\n\u1e9112\ny\nzt+1\nz12\nz13\nz14\nz15\nxl\nzt\nz11\n\u1e9112\n\u1e9113\n\u1e9114\n\u0177\n\u1e91t+1\n\u1e9112\n\u1e9113\n\u1e9114\n\u1e9115\n\u2192\n\u2192 \n\u2192 \n\u2192 \n(a) Approximate iteration\nx2\nzt-1\nz10\nz11\nz12\nz13\nx1\nzt-2\nz9\nz10\nz11\nz12\ny\nzt+1\nz12\nz13\nz14\nz15\nxl\nzt\nz11\nz12\nz13\nz14\n\u0177\n\u1e91t+1\n\u1e9112\n\u1e9113\n\u1e9114\n\u1e9115\n\u2192\n\u2192 \n\u2192 \n\u2192 \n(b) Updated iteration\nFigure 8: Representation of the multi-step-ahead prediction strategy for the global approach\nDespite its simplicity, the global approach is not exempt from limitations. The most obvious of\nthem is the fact that the pairs (Xij, yij) are considered independent and identically distributed by\ntraditional machine learning algorithms. This assumption leads to a loss of temporal information,\nwhich implies in the performance degradation of the resulting regression model. Among the methods\nthat apply this approach, we can cite those that consider polynomial and rational functions. Moreover,\nthere are also those grounded in ANN [46] and SVM [37, 40].\nThe local approach comprises machine learning algorithms that have been adapted to include tem-\nporal information in the prediction process. Such methods partition the time series into subsequences\nwhose the closest or most important values related to the current value are combined to produce the\nfuture value. These combinations are undertaken by approximation functions, such as simple local\naverage and weighted. Examples of methods that use the local approach are variations of the kNN\nalgorithm [33, 48].\nIn the following subsections, we present three state-of-the-art machine learning algorithms that we\nconsider in our experimental evaluation.\n4.2.1. Arti\ufb01cial Neural Networks\nANN are computational models inspired by the information processing performed by the human\nbrain [19]. The Perceptron, exhibited in Fig. 9, is the simplest form of an ANN used for, besides other\ntasks, the classi\ufb01cation of linearly separable classes [30, 38].\nIn Fig. 9, the single neuron comprises l data inputs xi \u2208X. The ith element of X, eventually\nprovided by adjacent neurons, is associated with a synaptic weight wi. This weight can assume a\nnegative or positive value that re\ufb02ects the importance of the input. The linear combination of inputs\n13\n", [1777, 1778, 1779, 1780, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1821, 1822, 1823, 1824, 1825, 1826]], "Non-parametric Methods": ["Presupposition \nbased on the \ncharacteristics of \nthe data\nCorrelograms or\ninformation\ncriteria\nNonlinear\nestimation\ntechniques\nResidues\nanalysis\nSelection of\nModel Structure\nIdentification of\nModel Orders\nEstimation of\nModel Coefficients\nDiagnosis of the\nFitted Model\nFigure 6: Activity \ufb02ow diagram for building an ARIMA or SARIMA model\nof the di\ufb00erentiated time series in the lags 1, 2, 3, . . . to obtain the value of p and q, and in the\nlags s, s\u00d72, s\u00d73, . . . to obtain the value of P and Q. An alternative way to \ufb01nd these parameters\nvalues lies in the application of information criteria. The Akaike Information Criterion (AIC),\nexpressed by Eq. 27 [31], penalizes the adjustment quality of models with many parameters.\nAIC = \u22122 \u00d7 LL + (log(n) + 1) \u00d7 NP\n(27)\nwhere LL is the likelihood function logarithm, n refers to the number of observations of the\ntraining series, and NP comprises the number of parameters. The idea is to select the model\nthat achieves the lower AIC. Obviously, a model with more parameters may have a better \ufb01t,\nbut not necessarily will be preferred regarding AIC;\n3. Estimation of Model Coe\ufb03cients: The preliminary estimates of \u03c6p and \u03a6P , of the autoregressive\ncomponent, and \u03b8q and \u0398Q, of the MA component, can be obtained using the autocorrelations\nof the time series integrated into the model identi\ufb01cation step. After the assignment of initial\nvalues, the coe\ufb03cients are estimated by maximizing the likelihood function. As the least squares\nestimators can approximate the maximum likelihood estimators, the said function is typically\nmaximized by nonlinear least squares using the Levenberg-Marquardt algorithm;\n4. Diagnosis of the Fitted Model: The model identi\ufb01ed as the most promising is examined to\nensure that the data dynamic was satisfactorily represented. In practice, the estimates of errors\n(residues) are analyzed by residual autocorrelation tests. These tests are intended to verify if the\nresidues present white noise behavior, i.e. if their autocorrelations behave randomly and are not\nsigni\ufb01cant. In the a\ufb03rmative case, we can extrapolate the model to future times. In the negative\ncase, it will be necessary to select another model and repeat the identi\ufb01cation, estimation, and\ndiagnosis steps.\nThe employment of ARIMA models requires expertise both in the application domain and in the\ncomputational mathematics. Moreover, the analyst perception and experience are essential for the\nmodeling process becomes more practical and less expensive.\n4.2. Non-parametric Methods\nMachine learning prediction methods, as opposed to statistical models, describe the data properties\nwithout the prior knowledge of their distribution. Because they do not depend explicitly on param-\neters to model the phenomenon\u2019s behavior, these methods are simpler to adjust and show reliable\nperformance even when applied to complex and highly nonlinear series. We can divide the machine\nlearning predictors into two approaches [24]: (i) global and (ii) local.\nIn the global approach, the machine learning methods consider all observations of the training\nseries to build a model. It normally involves the transposition of the data sequence into an attribute-\nvalue table which is used as input to machine learning regression algorithms. Fig. 7 shows how to\nemploy this approach.\nIn Fig. 7, the transposition procedure of the sequence Z with size m = 15 into the attribute-value\nformat is obtained sweeping a sliding window of length l = 3. This window is iteratively shifted on\n12\n", [1747, 1777, 1778, 1779, 1780]], "ARIMA and SARIMA Models": ["S1 = z1\nLs\n, S2 = z2\nLs\n, . . . , Ss = zs\nLs\n(15)\nThe Additive model of HW (AHW) has greater explanatory power in series where the di\ufb00erence\nbetween the highest and the lowest demand value within the stations remains constant over time. The\nalgorithm that implements this model uses the following equations:\nLt = \u03b1(zt \u2212St\u2212s) + (1 \u2212\u03b1)(Lt\u22121 + Tt\u22121)\n(16)\nTt = \u03b2(Lt \u2212Lt\u22121) + (1 \u2212\u03b2)Tt\u22121\n(17)\nSt = \u03b3(zt \u2212Lt) + (1 \u2212\u03b3)St\u2212s\n(18)\nzt+h = Lt + hTt + St\u2212s+h\n(19)\nEq. 10 of MHW is identical to Eq. 17 of AHW. The di\ufb00erence lies in the use of the other equations,\nin which the seasonal indexes are summed and subtracted, rather than multiplied and divided as in\nthe multiplicative model.\nThe variables L and T are commonly initialized by applying the same equations of MHW. The\nseasonal indexes are calculated according to Eq. 20.\nS1 = z1 \u2212Ls, S2 = z2 \u2212Ls, . . . , Ss = zs \u2212Ls\n(20)\nThe correct choice of HW models is associated with the morphology of the seasonal variations in\nthe time series, regardless of the existence of the trend component. In these models, when \u03b3 = 0 does\nnot mean that the series is devoid of seasonality, but that seasonal rates have been initialized with\nvalues that do not need to be \ufb01xed along the prediction.\n4.1.5. ARIMA and SARIMA Models\nThe ARIMA models of order (p, d, q), i.e. ARIMA(p, d, q), result from the combination of three\nprocedures [5]: (i) Autoregression (AR(p)), (ii) integration2, and (iii) Moving Averages (MA(q)).\nThe simultaneous use of these three components is not a rule to model time series with absence of\nseasonal patterns, once they can be executed in a conjugated way, i.e., one complementing the other.\nBy this perspective and as the integration procedure can be performed in a preprocessing step, the\nARIMA nomenclature is also used to refer to the following structures: ARIMA(p, 0, 0) = AR(p);\nARIMA(0, 0, q) = MA(q); and ARIMA(p, 0, q) = ARMA(p, q).\nThe main bene\ufb01t of ARMA is that, to adjust its structure to complex stationary time series, it\noften uses a number of terms lower than required by models purely AR or purely MA. When a time\nseries is non-stationary, it can be transformed using a data di\ufb00erentiation procedure which ensures\nsuch property. This procedure when added to the ARMA structure results in the ARIMA model with\norder (p, d, q), ARIMA(p, d, q), de\ufb01ned by Eq. 21.\nI\u2032\nt = \u03b4 +\np\nX\ni=1\n\u03c6iI\u2032\nt\u2212i +\nq\nX\ni=1\n\u03b8iet\u2212i + et\n(21)\nIn Eq. 21, I\u2032\nt = \u2206dzt = \u2206(\u2206d\u22121zt) and d indicates the di\ufb00erence operator degree; \u03c6p and \u03b8q are,\nin this order, the parameters of the procedures: autoregressive, with lag length p, and MA, with lag\nlength q; \u03b4 re\ufb02ects the initial level of the model (performs the same function as the intercept in linear\nregression) and is calculated according to Eq. 22, where \u00b5 represents the stationary process average;\nand et is the white noise in a distribution with zero average and constant variance \u03c32\ne. Besides, for\neach time instant t, it is assumed that et is independent of the time series past values (zt\u22121, zt\u22122, . . .,\nzt\u2212m+1).\n2An operation which consists of taking successive di\ufb00erences from the original series Z. The \ufb01rst di\ufb00erence is denoted\nby \u2206zt = zt \u2212zt\u22121; the second di\ufb00erence is de\ufb01ned as \u22062zt = \u2206(\u2206zt) = \u2206(zt \u2212zt\u22121); \ufb01nally, the dth di\ufb00erence equals\n\u2206dzt = \u2206(\u2206d\u22121zt).\n10\n", [1747]], "Holt-Winters' Seasonal Exponential Smoothing": ["The HES model structure, de\ufb01ned by Eqs. 6 and 7 [22], is similar to the SES method. However,\nbesides to use the parameter \u03b1 to soften the level component, the algorithm uses a second smoothing\nconstant (\u03b2) for modeling the time series trend.\nLt = \u03b1zt + (1 \u2212\u03b1)(Lt\u22121 + Tt\u22121)\n(6)\nTt = \u03b2(Lt \u2212Lt\u22121) + (1 \u2212\u03b2)Tt\u22121\n(7)\nzt+h = Lt + hTt\n(8)\nThe smoothing constants values \u03b1 and \u03b2 lie in the range [0, 1], and Eqs. 6 and 7 estimate the level\nand trend components, respectively. These equations, as well any exponential smoothing method,\nmodify previous estimates when a new observation is computed. Moreover, in Eq. 8, zt+h indicates\nthe prediction value of z at time t + h, where h represents the prediction horizon.\nTo implement the HES algorithm recurrence relation, we need to provide its initial values. A\nwidely accepted rule in the literature is to assume L1 = z1 and T1 = z2 \u2212z1. As the method is based\non the self-learning concept, the initial values do not a\ufb00ect the predictions. However, this fact does\nnot apply to the smoothing constants, which are di\ufb03cult to set and bad choices may degrade the\npredictive performance of the algorithm.\n4.1.4. Holt-Winters\u2019 Seasonal Exponential Smoothing\nThe HW models structure comprises three equations with distinct smoothing constants, which are\nrelated to the time series primary components. Given this design outline, such models are divided\ninto two groups [47]: (i) Multiplicative and (ii) Additive. The choice of a structure depends on the\nseasonal pattern of the investigated series.\nWe can use the Multiplicative model of HW (MHW) to adjust time series in which the amplitude\nof the seasonal variation rises with the increase of the series average level. The algorithm employs the\nfollowing equations [47]:\nLt = \u03b1 zt\nSt\u2212s\n+ (1 \u2212\u03b1)(Lt\u22121 + Tt\u22121)\n(9)\nTt = \u03b2(Lt \u2212Lt\u22121) + (1 \u2212\u03b2)Tt\u22121\n(10)\nSt = \u03b3 zt\nLt\n+ (1 \u2212\u03b3)St\u2212s\n(11)\nzt+h = (Lt + hTt)St\u2212s+h\n(12)\nIn these equations, \u03b1, \u03b2 and \u03b3 are smoothing constants whose values lie in the range [0, 1], s\nindicates the number of observations that make up a seasonal variation, and zt+h represents the\nprediction value z for the period t + h.\nAnalogously to SES and HES methods, the MHW algorithm receives as input a time series and\nrecursively applies the three described equations. Such application should be started at some time\nin the past, where the values of L, T and S was previously estimated. A simple way to obtain this\napproximation is through the initialization of level and trend in the same period s. Thus, the level\ncan be determined from the average of the \ufb01rst station (Eq. 13).\nLs = 1\ns(z1 + z2 + . . . + zs)\n(13)\nThe trend can be initialized using two complete stations, as de\ufb01ned by Eq. 14.\nTs = 1\ns\n\u0012zs+1 \u2212z1\ns\n+ zs+2 \u2212z2\ns\n+ . . . + zs+s \u2212zs\ns\n\u0013\n(14)\nThe initial seasonal indexes can be computed by the ratio between the \ufb01rst observations and the\naverage of the \ufb01rst period, as shown in Eq. 15.\n9\n", []], "Holt's Exponential Smoothing": ["that cannot be explained by the time series past values. The SARIMA generalization also allows to\nmodel temporal data with seasonal variations.\n4.1.1. Moving Averages\nThe MA model of order r, MA(r), is a simple technique that performs an arithmetic average of the\nlast r values of time series to predict the next value. MA considers a constant number of observations\nto exploit the autocorrelation structure of the prediction residues at the current time with those\noccurred in previous periods. Eq. 3 de\ufb01nes the MA model, where r is the number of observations\nincluded in the average zt+1.\nzt+1 = zt + zt\u22121 + zt\u22122 + . . . + zt\u2212r+1\nr\n(3)\nThe higher the value of r, the more uniform (smoothed) will be the predicted data behavior. Thus,\nwhen the series shows small distortions in their patterns or random \ufb02uctuations, it is recommended\nto employ a substantial r value to avoid the in\ufb02uence of noise in the predictions. Otherwise, if the\nseries is nearly devoid of randomness and presents a signi\ufb01cant shift in the curves in\ufb02ection points, it\nis indicated to use a smaller r value to allow the model to react quickly to the data changes.\nThe drawbacks of the MA model are their low accuracy to deal with trend and seasonality. Since\nthe prediction of the next value always involves the addition of new data and the discard of the previous\none. Furthermore, the weights assigned to the r observations are typically all equal. Therefore, there\nis no emphasis on the most recent observations [29].\n4.1.2. Simple Exponential Smoothing\nThe SES method is equivalent to MA with r = m, except by the fact that each series value receives\na di\ufb00erent weight. The weights increase exponentially over time so that the most recent observations\nexert more in\ufb02uence on the calculation of future predictions [23].\nEq. 4 expresses the SES model structure [15], where L denotes the level at time t, \u03b1 (0 < \u03b1 < 1)\nis the weight (or constant smoothing) assigned to historical values, and zt corresponds to the last\nobserved value.\nLt = \u03b1zt + \u03b1(1 \u2212\u03b1)zt\u22121 + . . . + \u03b1(1 \u2212\u03b1)m\u22121z1\n(4)\nTo avoid an expensive computation that involves all observations for each new estimate L, we can\nreduce Eq. 4 in function of the current value of the time series and the level computed in the previous\ntime. Eq. 5 formalizes the result of this simpli\ufb01cation [15].\nLt = \u03b1zt + (1 \u2212\u03b1)Lt\u22121\n(5)\nUsually, at the beginning of the SES prediction process, it is supposed that the \ufb01rst \ufb01tted value\nis equal to the \ufb01rst series value, i.e., L1 = z1. In this case, the adjustment procedure starts from\nthe second observation of the time series.\nThe exponential smoothing of the last observed value\n(zm+1 = Lm) gives the prediction at time m + 1. We call this strategy one-step-ahead. This method\ndoes not support an extension to larger horizons. In this case, the \ufb01tted value Lm gives the prediction\nof all future values for multiple horizons.\nThe \ufb02exibility, mathematical simplicity, and reasonable accuracy explain the popularity conferred\nto the SES method. For making a new prediction, the algorithm needs the most recent observation,\nthe last predicted value, and the parameter \u03b1.\nAmong the drawbacks of the method, stands the\ndi\ufb03culty in \ufb01nding the most appropriate value for the smoothing constant [23].\n4.1.3. Holt\u2019s Exponential Smoothing\nThe SES model when applied to temporal data that present increasing (or decreasing) linear\nbehavior, provides predictions which underestimate (or overestimate) the actual values. To avoid this\nsystematic error, we can make use of methods as Holt\u2019s Exponential Smoothing (HES) [15].\n8\n", []], "Simple Exponential Smoothing": ["that cannot be explained by the time series past values. The SARIMA generalization also allows to\nmodel temporal data with seasonal variations.\n4.1.1. Moving Averages\nThe MA model of order r, MA(r), is a simple technique that performs an arithmetic average of the\nlast r values of time series to predict the next value. MA considers a constant number of observations\nto exploit the autocorrelation structure of the prediction residues at the current time with those\noccurred in previous periods. Eq. 3 de\ufb01nes the MA model, where r is the number of observations\nincluded in the average zt+1.\nzt+1 = zt + zt\u22121 + zt\u22122 + . . . + zt\u2212r+1\nr\n(3)\nThe higher the value of r, the more uniform (smoothed) will be the predicted data behavior. Thus,\nwhen the series shows small distortions in their patterns or random \ufb02uctuations, it is recommended\nto employ a substantial r value to avoid the in\ufb02uence of noise in the predictions. Otherwise, if the\nseries is nearly devoid of randomness and presents a signi\ufb01cant shift in the curves in\ufb02ection points, it\nis indicated to use a smaller r value to allow the model to react quickly to the data changes.\nThe drawbacks of the MA model are their low accuracy to deal with trend and seasonality. Since\nthe prediction of the next value always involves the addition of new data and the discard of the previous\none. Furthermore, the weights assigned to the r observations are typically all equal. Therefore, there\nis no emphasis on the most recent observations [29].\n4.1.2. Simple Exponential Smoothing\nThe SES method is equivalent to MA with r = m, except by the fact that each series value receives\na di\ufb00erent weight. The weights increase exponentially over time so that the most recent observations\nexert more in\ufb02uence on the calculation of future predictions [23].\nEq. 4 expresses the SES model structure [15], where L denotes the level at time t, \u03b1 (0 < \u03b1 < 1)\nis the weight (or constant smoothing) assigned to historical values, and zt corresponds to the last\nobserved value.\nLt = \u03b1zt + \u03b1(1 \u2212\u03b1)zt\u22121 + . . . + \u03b1(1 \u2212\u03b1)m\u22121z1\n(4)\nTo avoid an expensive computation that involves all observations for each new estimate L, we can\nreduce Eq. 4 in function of the current value of the time series and the level computed in the previous\ntime. Eq. 5 formalizes the result of this simpli\ufb01cation [15].\nLt = \u03b1zt + (1 \u2212\u03b1)Lt\u22121\n(5)\nUsually, at the beginning of the SES prediction process, it is supposed that the \ufb01rst \ufb01tted value\nis equal to the \ufb01rst series value, i.e., L1 = z1. In this case, the adjustment procedure starts from\nthe second observation of the time series.\nThe exponential smoothing of the last observed value\n(zm+1 = Lm) gives the prediction at time m + 1. We call this strategy one-step-ahead. This method\ndoes not support an extension to larger horizons. In this case, the \ufb01tted value Lm gives the prediction\nof all future values for multiple horizons.\nThe \ufb02exibility, mathematical simplicity, and reasonable accuracy explain the popularity conferred\nto the SES method. For making a new prediction, the algorithm needs the most recent observation,\nthe last predicted value, and the parameter \u03b1.\nAmong the drawbacks of the method, stands the\ndi\ufb03culty in \ufb01nding the most appropriate value for the smoothing constant [23].\n4.1.3. Holt\u2019s Exponential Smoothing\nThe SES model when applied to temporal data that present increasing (or decreasing) linear\nbehavior, provides predictions which underestimate (or overestimate) the actual values. To avoid this\nsystematic error, we can make use of methods as Holt\u2019s Exponential Smoothing (HES) [15].\n8\n", []], "Moving Averages": ["that cannot be explained by the time series past values. The SARIMA generalization also allows to\nmodel temporal data with seasonal variations.\n4.1.1. Moving Averages\nThe MA model of order r, MA(r), is a simple technique that performs an arithmetic average of the\nlast r values of time series to predict the next value. MA considers a constant number of observations\nto exploit the autocorrelation structure of the prediction residues at the current time with those\noccurred in previous periods. Eq. 3 de\ufb01nes the MA model, where r is the number of observations\nincluded in the average zt+1.\nzt+1 = zt + zt\u22121 + zt\u22122 + . . . + zt\u2212r+1\nr\n(3)\nThe higher the value of r, the more uniform (smoothed) will be the predicted data behavior. Thus,\nwhen the series shows small distortions in their patterns or random \ufb02uctuations, it is recommended\nto employ a substantial r value to avoid the in\ufb02uence of noise in the predictions. Otherwise, if the\nseries is nearly devoid of randomness and presents a signi\ufb01cant shift in the curves in\ufb02ection points, it\nis indicated to use a smaller r value to allow the model to react quickly to the data changes.\nThe drawbacks of the MA model are their low accuracy to deal with trend and seasonality. Since\nthe prediction of the next value always involves the addition of new data and the discard of the previous\none. Furthermore, the weights assigned to the r observations are typically all equal. Therefore, there\nis no emphasis on the most recent observations [29].\n4.1.2. Simple Exponential Smoothing\nThe SES method is equivalent to MA with r = m, except by the fact that each series value receives\na di\ufb00erent weight. The weights increase exponentially over time so that the most recent observations\nexert more in\ufb02uence on the calculation of future predictions [23].\nEq. 4 expresses the SES model structure [15], where L denotes the level at time t, \u03b1 (0 < \u03b1 < 1)\nis the weight (or constant smoothing) assigned to historical values, and zt corresponds to the last\nobserved value.\nLt = \u03b1zt + \u03b1(1 \u2212\u03b1)zt\u22121 + . . . + \u03b1(1 \u2212\u03b1)m\u22121z1\n(4)\nTo avoid an expensive computation that involves all observations for each new estimate L, we can\nreduce Eq. 4 in function of the current value of the time series and the level computed in the previous\ntime. Eq. 5 formalizes the result of this simpli\ufb01cation [15].\nLt = \u03b1zt + (1 \u2212\u03b1)Lt\u22121\n(5)\nUsually, at the beginning of the SES prediction process, it is supposed that the \ufb01rst \ufb01tted value\nis equal to the \ufb01rst series value, i.e., L1 = z1. In this case, the adjustment procedure starts from\nthe second observation of the time series.\nThe exponential smoothing of the last observed value\n(zm+1 = Lm) gives the prediction at time m + 1. We call this strategy one-step-ahead. This method\ndoes not support an extension to larger horizons. In this case, the \ufb01tted value Lm gives the prediction\nof all future values for multiple horizons.\nThe \ufb02exibility, mathematical simplicity, and reasonable accuracy explain the popularity conferred\nto the SES method. For making a new prediction, the algorithm needs the most recent observation,\nthe last predicted value, and the parameter \u03b1.\nAmong the drawbacks of the method, stands the\ndi\ufb03culty in \ufb01nding the most appropriate value for the smoothing constant [23].\n4.1.3. Holt\u2019s Exponential Smoothing\nThe SES model when applied to temporal data that present increasing (or decreasing) linear\nbehavior, provides predictions which underestimate (or overestimate) the actual values. To avoid this\nsystematic error, we can make use of methods as Holt\u2019s Exponential Smoothing (HES) [15].\n8\n", []], "Parametric Methods": ["Test \nSequence\nTraining \nSequence\nTime\nSeries\ny(t)\nt\nt         y(t)\n1        0.05\n2        0.10\n3        0.15\n4        0.20\n\u205e           \u205e\n97        0.15\n98        0.20\n99        0.25\n100        0.30\n\u2460\nParameter\nEstimation\n\u2461\nModel Building\nand Model Fit\n\u2462\nPrediction\nof Values\n\u2463\nPredicted Sequence\nPerformance\nEvaluation\n\u2464\ny(t)\nt\nPrediction of\nFuture Values\n\u2465\nFigure 4: Time series prediction process\nGlobal\nLocal\nExponential\nSmoothing\nARIMA\nPrediction Methods\nNon-parametric\nParametric\nFigure 5: Hierarchy of approaches for time\nseries prediction\nThe third step builds the model with the previously found parameters values and \ufb01ts the training\nsequence data. This model is then extrapolated, in the fourth step, for the periods of the test sequence.\nEvidently, the prediction error of the model re\ufb02ects the chosen values for the parameters. Such error\ncan be ampli\ufb01ed for long time horizons.\nThe fourth step also chooses the strategy to predict the values of a time series several periods\nahead (prediction horizon h > 1). The most intuitive strategy is known as multi-step (or recursive),\nwhere the prediction of h > 1 is conducted h successive times considering a predictive model with\nh = 1 [3]. After the model\u2019s extrapolation, the predicted value or the respective actual value can be\nemployed to calculate the next prediction. In this paper, when the predicted values are used, we called\nthe strategy of multi-step-ahead with approximate iteration. Otherwise, when the actual values are\nadopted, the strategy is called multi-step-ahead with updated iteration.\nThe \ufb01fth step compares the predicted values to the test sequence to measure the model\u2019s accuracy.\nThe performance analysis is essential given that distinct models may have similar adjustments, but\nresult in signi\ufb01cantly di\ufb00erent predictive values.\nThe sixth step makes predictions for future periods of the time series. This step should monitor the\nprediction error as soon as the actual values of the series arrive. This monitoring aims to indicate when\nit is necessary to update the model with new data or readjust its parameters since the distribution of\nmost recent data is distinct from old data.\nThe time series prediction methods have evolved over the years passing from simple regression\ntechniques to robust statistical and arti\ufb01cial intelligence algorithms. We can group the methods into\ntwo approaches according to the prior knowledge about the data distribution (Fig. 5): (i) parametric\n(exponential smoothing or based on autoregression and MA), and (ii) non-parametric (global or local).\nThe following subsections discuss these approaches along with the most renowned algorithms for time\nseries prediction.\n4.1. Parametric Methods\nThe statistical methods require a priori knowledge about the data distribution to build predictive\nmodels. This assumption makes the model depends on a set of parameters, which must be determined\nto optimize the prediction results.\nWe can divide the models into two groups according to their\nmathematical complexity [31]: (i) exponential smoothing models and (ii) ARIMA models.\nThe exponential smoothing models decompose the time series into components whose values are\nsmoothed by weights that exponentially decay over time. In the end, an additive or multiplicative\nstructure recomposes the smoothed components to predict future values [15].\nIn contrast, ARIMA models involve three statistical procedures [5]: (i) autoregression, (ii) integra-\ntion, and (iii) MA. Autoregression expresses the correlation between observations, i.e., how much the\ncurrent values in\ufb02uences the next ones. The integration procedure indicates the number of di\ufb00erences\nrequired to guarantee the stationarity of the series. Lastly, the MA part comprises unknown factors\n7\n", [1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1736]], "Time Series Prediction": ["techniques, beyond allowing the identi\ufb01cation of components that act in a series, enable to obtain\npatterns (via indexes and equations) that may be coupled to a computational model for prediction of\nfuture values. The three major components of a time series are:\nTrend (T) is a long-term increase or decrease in the data which can assume a great variety of patterns\n(e.g., linear, exponential, damped, and polynomial) [31]. Real time series with an increasing\ntrend can be found in phenomena related to the demographic development, gradual change of\nconsumption habits, and demand for technologies in the social sectors. The decreasing trend,\nin turn, can be found in series concerning the mortality rates, epidemics, and unemployment.\nWe can use regression models and the MA method to obtain this component [31, 41]. We use\nthe trend to estimate the level, i.e., the value or the typical range of values that the variable\nassumes if there is no increasing or decreasing trend in the long term. Fig. 3(b) shows the trend,\nestimated using MA with 12 periods (monthly data), of the chocolate production time series;\nSeasonality (S) is the occurrence of cyclic patterns of variation that repeat, at relatively constant\ntime intervals, along with the trend component. Examples of seasonal patterns are the increase in\nsales of air conditioners in summer and warm clothing in winter. Average percentage, percentage\nrelation, relation between MA, and relative links are some of the algorithms that allow computing\nthe seasonality from the extraction of seasonal indexes [41];\nResidue (R) is the short-term \ufb02uctuations that are neither systematic nor predictable. In the real\nworld, unforeseen events cause such instabilities, such as natural disasters, terrorist attacks, and\nstrikes. In practical terms, the residual component is what remains after the estimation of T\nand S components, and their removals from the time series [31]. We can de\ufb01ne the residue\nR of a period t of a series Z, in agreement with the additive and multiplicative models, as\nRt = Zt \u2212(Tt + St) and Rt = Zt \u00f7 (Tt \u00d7 St), respectively.\nFor a better analysis and understanding, we can reformulate the time series Z, according to Eqs. 1\nand 2, by an additive or multiplicative decomposition of its components.\nZt = Tt + St + Rt\n(1)\nZt = Tt \u00d7 St \u00d7 Rt\n(2)\nIn the additive model (Eq. 1), the interest variable value is the sum of the components values,\nwhich contemplate the same unit of the observation Zt. Considering the additive decomposition, Fig. 3\n\u2013 (c) and (e) \u2013 shows the seasonality provided by the relation between MA, and the residue of the\nchocolate production time series.\nDi\ufb00erently, in the multiplicative model (Eq. 2), only the trend has the same unit of the investigated\nvariable. The other components exhibit values that can modify the trend, i.e., they assume values\nlarger, smaller or exactly equal to 1. Adopting the multiplicative decomposition, Fig. 3 \u2013 (d) and\n(f) \u2013 outlines the seasonality, obtained via relation between MA, and the residue of the chocolate\nproduction time series.\nWe note that not every data sequence will have all the three mentioned components, even when\nthe classical decomposition is considered.\n4. Time Series Prediction\nThe time series prediction process covers six steps, as illustrated in Fig. 4.\nThe \ufb01rst step partitions the time series in two sequences: one before the prediction horizon, which\nis intended to the model training (building and \ufb01t); and another after that period, which is used to\ntest (evaluate) the quality of the \ufb01tted model.\nThe second step chooses the predictive model structure based on data characteristics and estimates\nthe parameters using some search technique. Usually, the algorithm that implements this technique\nreceives as input the training sequence, which is subdivided into subsequences (samples) for train-\ning and validation, and a set of prede\ufb01ned parameters. At each iteration, the algorithm seeks for\nparameters values that minimize the predictive error of the model.\n6\n", [1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1736]], "Fundamentals of Time Series": ["Fig. 1 displays a distribution bar chart, by publication year, regarding the number of selected\npapers at the end of the systematic review. In particular, the graph shows that from January 2009 to\nDecember 2018, were published annually about twelve articles, with two peaks in 2010 and 2015.\n9\n18\n10\n9\n13\n9\n16\n11\n14\n8\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\nFigure 1: Number of papers published by year\nThe inspection of the 117 papers discovered by the systematic review allowed us to elaborate\na broad and in-depth meta-analysis, of which Fig. 2 illustrates some results. Fig. 2(a) shows the\npercentage of use of each prediction approach in 68 papers with real applications. Fig. 2(b) portrays the\nfrequency in which the most popular methods appeared in the 117 publications. Fig. 2(c) graphically\nsummarizes the algorithms most used as baselines in 29 empirical studies involving both statistical\nand machine learning methods.\nNon-parametric\n54%\nParametric\n25%\nHybrid\n21%\n(a) Approaches\n62%\n52%\n37%\n27%\n13%\n12%\n11%\n10%\n9%\n8%\n8%\n7%\n3%\nANN ARIMA SVM Hybrid kNN\nFL\nDL\nMA\nBNN\nSES\nWT\nHW\nGP\nkNN\n(b) Methods\nARIMA\n66%\nMA\n21%\nSES\n10%\nHW\n3%\n(c) Baseline methods\nFigure 2: Summary of the meta-analysis results. The acronyms are: Arti\ufb01cial Neural Networks (ANN), ARIMA mod-\nels \u2013 Autoregressive Integrated Moving Average (ARIMA) or Seasonal ARIMA (SARIMA) \u2013, Support Vector Ma-\nchines (SVM), k-Nearest Neighbors (kNN), Fuzzy Logic (FL), Deep Learning (DL), Bayesian Neural Networks (BNN),\nSimple Exponential Smoothing (SES), Wavelet Transform (WT), Holt-Winters (HW) models, and Gaussian Process (GP)\nTable 1 compares our study with other 11 experimental papers previously published. This compar-\nison uses the following criteria: the number of parametric and non-parametric predictors compared;\nthe amount of synthetic and real time series selected; the number of measures chosen to evaluate the\nalgorithms\u2019 performance; and if any statistical signi\ufb01cance test was applied to support the comparison\nof results. Most of these articles assess predictors on datasets from speci\ufb01c domains. Although [1]\nconsidered a signi\ufb01cant number of datasets, the lengths of the series are short with values between 81\nand 126 observations. In general, such studies have concluded that no particular traditional model\ncan provide the best predictions. The supplementary material presents a description of these papers.\nAs summarized in the last row of Table 1, our proposal di\ufb00ers from the literature not only by com-\nparing eleven popular predictive algorithms using two multi-step-ahead strategies on 95 time series,\nbut also includes a more rigorous experimental design supported by a new multi-criteria performance\nmeasure. Besides discussing which algorithms should be used as baseline and topline when investi-\ngating the proposition of novel models, we highlighted the advantages and drawbacks of each method\nfor certain types of datasets. We compare the models\u2019 performances with the support of statistical\nsigni\ufb01cance tests.\n3. Fundamentals of Time Series\nA time series Z of size m can be formulated as an ordered sequence of observations, i.e., Z =\n(z1, z2, . . . , zm) where zt \u2208\u211cis an observation at time t. In this work, we assume the time series are\n4\n", []], "Systematic Review and Meta-analysis of the Literature": ["diction. Using a set of 95 time series from synthetic and real domains, we face eleven predictive\nalgorithms, seven parametric and four non-parametric, employing two multi-step-ahead projec-\ntion strategies and four performance evaluation measures.\nThe \ufb01ndings of our experimental\ncomparison will facilitate further research on this topic since they provide a better insight into\nthe predictive performance of the methods currently available in the literature.\nIn addition\nto discussing which algorithms should be used as baseline and topline when investigating the\nproposition of novel models, we highlight the advantages and drawbacks of each method. We\nperformed this analysis according to characteristics of the data, and we hope that it can guide\npractitioners in statistics and machine learning;\n\u2022 Building an online archive, namely of ICMC-USP Time Series Prediction Repository [32], which\ngrants access to all materials produced in this work. Thereby, other researchers can reuse the\navailable datasets to replicate our results and compare their methods more rigorously against\ndi\ufb00erent predictors.\nWe emphasize that the outlined experimental protocol and our discussion on its usage are a guide-\nline for model selection, parameters setting, and employment of statistical and machine learning\nalgorithms for time series prediction.\nThe remaining of this paper is structured as follows: Section 2 presents the related work and the\nstatistics derived from a meta-analysis of the literature, which was guided by the results of a systematic\nreview. Section 3 reports the main de\ufb01nitions and notations about time series. Section 4 describes\nthe temporal data prediction approaches with their usual methods and some techniques that help in\nits parameters estimation. Section 5 speci\ufb01es the con\ufb01guration of the experiments, which includes\ndatasets, algorithms and performance measures.\nSection 6 presents results and discussion, while\nSection 7 punctuates the limitations, recommendations, and practical implications of the outcomes.\nFinally, Section 8 reports the conclusions and directions for future work.\n2. Systematic Review and Meta-analysis of the Literature\nFor over a half-century, statistical methods based on autoregression and MA have in\ufb02uenced the\ntemporal data processing and analysis \ufb01elds. Although some studies have stated, between the seventies\nand eighties, that the parametric models could not be readily adapted to many real applications, they\nresisted over the years [16]. The preference for these methods made them reached the condition of\nstate-of-art for time series modeling and prediction.\nIn the last two decades, with the rise of the data mining process, there is an increasing interest in\nthe adaptation of machine learning methods, especially those for regression tasks, to support analysis\nwith time dependence. Due to their simplicity and comprehensibility, the non-parametric techniques\nhave established themselves as serious candidates to the classical models, so that scienti\ufb01c competitions\nhave been undertaken to encourage both the improvement of these algorithms as the development of\nnew solutions [1].\nThe researchers from statistics and machine learning communities have contributed to several\naspects of the prediction process, such as the assistance in selecting the most promising model [26],\nthe study of the deseasonalization e\ufb00ects on the projection of future values [3], and the construction\nof hybrid models by a combination of statistical and machine learning methods [2]. In this sense,\nthe quality of parametric and non-parametric models has been explored mainly in annual or biennial\ncompetitions aimed at assessing the performance of prediction algorithms on a considerable amount\nof time series data [3].\nTo position this study in the corresponding state-of-the-art, we performed a meta-analysis of\nthe literature. We conducted this meta-analysis using the results of a systematic review of papers\npublished in the last ten years. Our supplementary material details the systematic review protocol,\nwhich includes ten research questions, a search key, and \ufb01ve search sources as well as the selected\npublications and a detailed results interpretation.\nThroughout the remainder of this section, we\nhighlight the most relevant points found and describe the most related papers to the present work.\n3\n", []], "Introduction": ["Evaluation of statistical and machine learning models for time series\nprediction: Identifying the state-of-the-art and the best conditions for the use\nof each model\nAntonio Rafael Sabino Parmezana,\u2217, Vinicius M. A. Souzaa, Gustavo E. A. P. A. Batistaa\naLaboratory of Computational Intelligence, Instituto de Ci\u02c6encias Matem\u00b4aticas e de Computa\u00b8c\u02dcao, Universidade de S\u02dcao\nPaulo, Av. Trabalhador S\u02dcao-carlense, 400,\n13566-590 S\u02dcao Carlos, SP, Brazil\nAbstract\nThe choice of the most promising algorithm to model and predict a particular phenomenon is one of\nthe most prominent activities of the temporal data forecasting. Forecasting (or prediction), similarly\nto other data mining tasks, uses empirical evidence to select the most suitable model for a problem at\nhand since no modeling method can be considered as the best. However, according to our systematic\nliterature review of the last decade, few scienti\ufb01c publications rigorously expose the bene\ufb01ts and\nlimitations of the most popular algorithms for time series prediction. At the same time, there is a\nlimited performance record of these models when applied to complex and highly nonlinear data. In this\npaper, we present one of the most extensive, impartial and comprehensible experimental evaluations\never done in the time series prediction \ufb01eld. From 95 datasets, we evaluate eleven predictors, seven\nparametric and four non-parametric, employing two multi-step-ahead projection strategies and four\nperformance evaluation measures. We report many lessons learned and recommendations concerning\nthe advantages, drawbacks, and the best conditions for the use of each model.\nThe results show\nthat SARIMA is the only statistical method able to outperform, but without a statistical di\ufb00erence,\nthe following machine learning algorithms: ANN, SVM, and kNN-TSPI. However, such forecasting\naccuracy comes at the expense of a larger number of parameters. The evaluated datasets, as well\ndetailed results achieved by di\ufb00erent indexes as MSE, Theil\u2019s U coe\ufb03cient, POCID, and a recently-\nproposed multi-criteria performance measure are available online in our repository. Such repository\nis another contribution of this paper since other researchers can replicate our results and evaluate\ntheir methods more rigorously. The \ufb01ndings of this study will impact further research on this topic\nsince they provide a broad insight into models selection, parameters setting, evaluation measures, and\nexperimental setup.\nKeywords:\nUnivariate analysis, automatic parameter tuning, multi-step-ahead prediction, time\nseries forecasting, data mining\n1. Introduction\nThe technological advances in computing area, including databases systems, machine learning,\nand cloud computing have leveraged o\ufb00the conversion of data into useful information and knowledge\nto support decision making. These advances contributed to the development of computer systems\ncapable of storing, analyzing and managing an increasing amount of data.\nData can be represented in di\ufb00erent formats, from the most basic, such as numeric and nominal,\nto more complex ones, such as audio and video. However, the storage of temporal information, which\nallows the chronological organization of the collected data, is one of the data representations that has\nattracted most attention of researchers and driven the creation of large databases [14].\n\u2217Corresponding author\nEmail addresses: parmezan@usp.br (Antonio Rafael Sabino Parmezan), vmasouza@icmc.usp.br (Vinicius M. A.\nSouza), gbatista@icmc.usp.br (Gustavo E. A. P. A. Batista)\nPreprint submitted to Information Sciences\nJanuary 28, 2019\n", []]}