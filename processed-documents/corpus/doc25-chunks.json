{"D.4 Does language pretraining improve compute efficiency over random initialization?": ["D.4\nDoes language pretraining improve compute ef\ufb01ciency over random initialization?\nThis section refers to Table 6 in Section 3.4.\nAll tasks\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams). Note: same models as \u201cFPT\u201d in Table 2, Section 3.2.\n2. Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, position, and layernorm params). Note: same models as \u201cRandom\u201d in Table\n2, Section 3.2.\n26\n", []], "D.3 How important is the transformer architecture compared to LSTM architecture?": ["D.2\nWhat is the importance of the pretraining modality?\nThis section refers to Table 2 in Section 3.2.\nAll tasks\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams). This differs from Table 1, Section 3.1 only in the CIFAR-10 model size.\n2. Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, position, and layernorm params).\n3. Bit: 12-layer base size GPT-2 model (\ufb01netuning input, output, position, and layernorm\nparams), after \ufb01rst being fully \ufb01netuned on Bit Memory following default random initial-\nization.\n4. ViT: 12-layer, 768 hidden dimension base size ViT model (\ufb01netuning input, output, posi-\ntion, and layernorm params), pretrained on 224 \u00d7 224 ImageNet-21k with a patch size of\n16. (vit base patch16 224 from the timm Pytorch library (Wightman, 2019)). We\nreinitialize the input layer from scratch to match each task, and do not use a CLS token or\nan MLP as the output network \u2013 instead using a linear layer from the last token \u2013 matching\nthe protocol for the other methods.\nD.3\nHow important is the transformer architecture compared to LSTM architecture?\nThe following refer to Section 3.3. In Table 3:\nAll tasks\n1. Trans: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, and layernorm params). Note: same as \u201cRandom\u201d in Table 2, Section 3.2.\n2. LSTM: 3-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output, and lay-\nernorm params). Does not have residual connections or positional embeddings.\n3. LSTM\u2217: 12-layer, 768 hidden dimension LSTM (training input, output, position, and lay-\nernorm params).\nTable 4:\nAll tasks\n1. 12: 12-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output, and layer-\nnorm params).\n2. 3: 3-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output, and layernorm\nparams).\nTable 5:\nAll tasks\n1. 12-layer LSTM: 12-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output,\nand layernorm params). Note: same as \u201c12\u201d in Table 4, Section 3.3.\n2. + Residual Connections: 12-layer, 768 hidden dimension LSTM with residual connections\n(training input, output, and layernorm params).\n3. + Positional Embeddings: 12-layer, 768 hidden dimension LSTM with residual connec-\ntions and positional embeddings (training input, output, position, and layernorm params).\nNote: same as \u201cLSTM\u2217\u201d in Table 3, Section 3.3.\n25\n", []], "D.2 What is the importance of the pretraining modality?": ["D.2\nWhat is the importance of the pretraining modality?\nThis section refers to Table 2 in Section 3.2.\nAll tasks\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams). This differs from Table 1, Section 3.1 only in the CIFAR-10 model size.\n2. Random: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, position, and layernorm params).\n3. Bit: 12-layer base size GPT-2 model (\ufb01netuning input, output, position, and layernorm\nparams), after \ufb01rst being fully \ufb01netuned on Bit Memory following default random initial-\nization.\n4. ViT: 12-layer, 768 hidden dimension base size ViT model (\ufb01netuning input, output, posi-\ntion, and layernorm params), pretrained on 224 \u00d7 224 ImageNet-21k with a patch size of\n16. (vit base patch16 224 from the timm Pytorch library (Wightman, 2019)). We\nreinitialize the input layer from scratch to match each task, and do not use a CLS token or\nan MLP as the output network \u2013 instead using a linear layer from the last token \u2013 matching\nthe protocol for the other methods.\nD.3\nHow important is the transformer architecture compared to LSTM architecture?\nThe following refer to Section 3.3. In Table 3:\nAll tasks\n1. Trans: 12-layer randomly initialized (default scheme) base size GPT-2 model (training\ninput, output, and layernorm params). Note: same as \u201cRandom\u201d in Table 2, Section 3.2.\n2. LSTM: 3-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output, and lay-\nernorm params). Does not have residual connections or positional embeddings.\n3. LSTM\u2217: 12-layer, 768 hidden dimension LSTM (training input, output, position, and lay-\nernorm params).\nTable 4:\nAll tasks\n1. 12: 12-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output, and layer-\nnorm params).\n2. 3: 3-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output, and layernorm\nparams).\nTable 5:\nAll tasks\n1. 12-layer LSTM: 12-layer, 768 hidden dimension \u201cstandard\u201d LSTM (training input, output,\nand layernorm params). Note: same as \u201c12\u201d in Table 4, Section 3.3.\n2. + Residual Connections: 12-layer, 768 hidden dimension LSTM with residual connections\n(training input, output, and layernorm params).\n3. + Positional Embeddings: 12-layer, 768 hidden dimension LSTM with residual connec-\ntions and positional embeddings (training input, output, position, and layernorm params).\nNote: same as \u201cLSTM\u2217\u201d in Table 3, Section 3.3.\n25\n", []], "D.1 Can pretrained language models transfer to different modalities?": ["D\nDetails by Table\nFor clarity, we explicitly write out \ufb01ner details for some experiment sections where numbers can\nrepresent different model types.\nD.1\nCan pretrained language models transfer to different modalities?\nThis section refers to Table 1 in Section 3.1.\nBit Memory\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: 12-layer base size GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nBit XOR\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: 12-layer base size GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nListOps\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Tay et al. (2020) (3-layer vanilla transformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nCIFAR-10\n1. FPT: 36-layer large size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: 3-layer, 768 hidden dimension GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nCIFAR-10 LRA\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Tay et al. (2020) (3-layer vanilla transformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nRemote Homology\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Rao et al. (2019) (12-layer, 512 hidden dimension vanilla\ntransformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\n24\n", []], "D Details by Table": ["D\nDetails by Table\nFor clarity, we explicitly write out \ufb01ner details for some experiment sections where numbers can\nrepresent different model types.\nD.1\nCan pretrained language models transfer to different modalities?\nThis section refers to Table 1 in Section 3.1.\nBit Memory\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: 12-layer base size GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nBit XOR\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: 12-layer base size GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nListOps\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Tay et al. (2020) (3-layer vanilla transformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nCIFAR-10\n1. FPT: 36-layer large size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: 3-layer, 768 hidden dimension GPT-2 model (training all params).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nCIFAR-10 LRA\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Tay et al. (2020) (3-layer vanilla transformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\nRemote Homology\n1. FPT: 12-layer base size FPT model (\ufb01netuning input, output, position, and layernorm\nparams).\n2. Full: number reported from Rao et al. (2019) (12-layer, 512 hidden dimension vanilla\ntransformer).\n3. LSTM: 3-layer, 768 hidden dimension LSTM model (training all params).\n24\n", []], "C Experimental Details": ["B.3\nLayer Norm\nLayer norm (Ba et al., 2016) is frequently used in recurrent and transformer architectures as a means\nof normalizing the activations. In particular, for the activations of training example x of dimension\nndim, it normalizes by the mean and variance over the features:\n\u02dcyi =\nxi \u2212mean({xj}ndim\nj=1 )\nstd({xj}ndim\nj=1 )\n(2)\nThen, af\ufb01ne scale and shift parameters each of dimension ndim \u2013 \u03b3 and \u03b2, respectively \u2013 are learned\nto generate the outputs y.\nyi = \u03b3i\u02dcyi + \u03b2i\n(3)\nLayer norm is applied twice per self-attention block: once before the attention layer and once before\nthe MLP. As a result, a total of 4 \u00b7 nlayers \u00b7 ndim layer norm parameters are learned.\nB.4\nPretraining Objective\nGPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters\nwhich maximize the log-likelihood of the data: max\u03b8E[log p\u03b8(x)]. GPT-2 models sequences au-\ntoregressively, factorizing the probability distribution p(x) = p(x1, . . . , xl) via chain rule as:\np(x) =\nlY\ni=1\np(xi|xi\u22121, . . . , x1)\n(4)\nFor the language domain, this objective can be interpreted as \u201cgiven the previous i \u22121 words of a\nsentence, predict the next word\u201d.\nB.5\nModel Sizes\nThe model sizes from Section 3.7 are as follows:\nModel Size\nnlayers\nndim\nnheads\n# Parameters\nSmall (Base)\n12\n768\n12\n117M\nMedium\n24\n1024\n16\n345M\nLarge\n36\n1280\n20\n774M\nTable 22: Hyperparameters for architectures for larger model sizes.\nThe hyperparameters for the experiments with other architectures (Vision Transformer, BERT, Long-\nformer, T5) are the same as for the base model size shown above.\nC\nExperimental Details\nWe use implementations of and pretrained models from the Huggingface Transformers library (Wolf\net al., 2020). We train all models using the Adam (Kingma & Ba, 2014) optimizer following Pytorch\n(Paszke et al., 2019) defaults. For all transformer models, we use a learning rate of 10\u22123 without\nlearning rate scheduling. For the remote homology task only, we use a learning rate of 10\u22124 as we\nfound it to give better performance than 10\u22123. We generally use the largest batch size that \ufb01ts on\nan RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation. Note\nthat except for the remote homology task, we did not tune the FPT hyperparameters. For all LSTMs,\nwe use a lower learning rate of 3 \u00d7 10\u22124 and the same batch sizes as transformers of the same size.\nModels are trained to convergence and evaluated on a heldout test set.\n23\n", []], "B.5 Model Sizes": ["B.3\nLayer Norm\nLayer norm (Ba et al., 2016) is frequently used in recurrent and transformer architectures as a means\nof normalizing the activations. In particular, for the activations of training example x of dimension\nndim, it normalizes by the mean and variance over the features:\n\u02dcyi =\nxi \u2212mean({xj}ndim\nj=1 )\nstd({xj}ndim\nj=1 )\n(2)\nThen, af\ufb01ne scale and shift parameters each of dimension ndim \u2013 \u03b3 and \u03b2, respectively \u2013 are learned\nto generate the outputs y.\nyi = \u03b3i\u02dcyi + \u03b2i\n(3)\nLayer norm is applied twice per self-attention block: once before the attention layer and once before\nthe MLP. As a result, a total of 4 \u00b7 nlayers \u00b7 ndim layer norm parameters are learned.\nB.4\nPretraining Objective\nGPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters\nwhich maximize the log-likelihood of the data: max\u03b8E[log p\u03b8(x)]. GPT-2 models sequences au-\ntoregressively, factorizing the probability distribution p(x) = p(x1, . . . , xl) via chain rule as:\np(x) =\nlY\ni=1\np(xi|xi\u22121, . . . , x1)\n(4)\nFor the language domain, this objective can be interpreted as \u201cgiven the previous i \u22121 words of a\nsentence, predict the next word\u201d.\nB.5\nModel Sizes\nThe model sizes from Section 3.7 are as follows:\nModel Size\nnlayers\nndim\nnheads\n# Parameters\nSmall (Base)\n12\n768\n12\n117M\nMedium\n24\n1024\n16\n345M\nLarge\n36\n1280\n20\n774M\nTable 22: Hyperparameters for architectures for larger model sizes.\nThe hyperparameters for the experiments with other architectures (Vision Transformer, BERT, Long-\nformer, T5) are the same as for the base model size shown above.\nC\nExperimental Details\nWe use implementations of and pretrained models from the Huggingface Transformers library (Wolf\net al., 2020). We train all models using the Adam (Kingma & Ba, 2014) optimizer following Pytorch\n(Paszke et al., 2019) defaults. For all transformer models, we use a learning rate of 10\u22123 without\nlearning rate scheduling. For the remote homology task only, we use a learning rate of 10\u22124 as we\nfound it to give better performance than 10\u22123. We generally use the largest batch size that \ufb01ts on\nan RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation. Note\nthat except for the remote homology task, we did not tune the FPT hyperparameters. For all LSTMs,\nwe use a lower learning rate of 3 \u00d7 10\u22124 and the same batch sizes as transformers of the same size.\nModels are trained to convergence and evaluated on a heldout test set.\n23\n", []], "B.4 Pretraining Objective": ["B.3\nLayer Norm\nLayer norm (Ba et al., 2016) is frequently used in recurrent and transformer architectures as a means\nof normalizing the activations. In particular, for the activations of training example x of dimension\nndim, it normalizes by the mean and variance over the features:\n\u02dcyi =\nxi \u2212mean({xj}ndim\nj=1 )\nstd({xj}ndim\nj=1 )\n(2)\nThen, af\ufb01ne scale and shift parameters each of dimension ndim \u2013 \u03b3 and \u03b2, respectively \u2013 are learned\nto generate the outputs y.\nyi = \u03b3i\u02dcyi + \u03b2i\n(3)\nLayer norm is applied twice per self-attention block: once before the attention layer and once before\nthe MLP. As a result, a total of 4 \u00b7 nlayers \u00b7 ndim layer norm parameters are learned.\nB.4\nPretraining Objective\nGPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters\nwhich maximize the log-likelihood of the data: max\u03b8E[log p\u03b8(x)]. GPT-2 models sequences au-\ntoregressively, factorizing the probability distribution p(x) = p(x1, . . . , xl) via chain rule as:\np(x) =\nlY\ni=1\np(xi|xi\u22121, . . . , x1)\n(4)\nFor the language domain, this objective can be interpreted as \u201cgiven the previous i \u22121 words of a\nsentence, predict the next word\u201d.\nB.5\nModel Sizes\nThe model sizes from Section 3.7 are as follows:\nModel Size\nnlayers\nndim\nnheads\n# Parameters\nSmall (Base)\n12\n768\n12\n117M\nMedium\n24\n1024\n16\n345M\nLarge\n36\n1280\n20\n774M\nTable 22: Hyperparameters for architectures for larger model sizes.\nThe hyperparameters for the experiments with other architectures (Vision Transformer, BERT, Long-\nformer, T5) are the same as for the base model size shown above.\nC\nExperimental Details\nWe use implementations of and pretrained models from the Huggingface Transformers library (Wolf\net al., 2020). We train all models using the Adam (Kingma & Ba, 2014) optimizer following Pytorch\n(Paszke et al., 2019) defaults. For all transformer models, we use a learning rate of 10\u22123 without\nlearning rate scheduling. For the remote homology task only, we use a learning rate of 10\u22124 as we\nfound it to give better performance than 10\u22123. We generally use the largest batch size that \ufb01ts on\nan RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation. Note\nthat except for the remote homology task, we did not tune the FPT hyperparameters. For all LSTMs,\nwe use a lower learning rate of 3 \u00d7 10\u22124 and the same batch sizes as transformers of the same size.\nModels are trained to convergence and evaluated on a heldout test set.\n23\n", []], "B.3 Layer Norm": ["B.3\nLayer Norm\nLayer norm (Ba et al., 2016) is frequently used in recurrent and transformer architectures as a means\nof normalizing the activations. In particular, for the activations of training example x of dimension\nndim, it normalizes by the mean and variance over the features:\n\u02dcyi =\nxi \u2212mean({xj}ndim\nj=1 )\nstd({xj}ndim\nj=1 )\n(2)\nThen, af\ufb01ne scale and shift parameters each of dimension ndim \u2013 \u03b3 and \u03b2, respectively \u2013 are learned\nto generate the outputs y.\nyi = \u03b3i\u02dcyi + \u03b2i\n(3)\nLayer norm is applied twice per self-attention block: once before the attention layer and once before\nthe MLP. As a result, a total of 4 \u00b7 nlayers \u00b7 ndim layer norm parameters are learned.\nB.4\nPretraining Objective\nGPT-2 is pretrained on an retrogressive language modeling objective optimizing for parameters\nwhich maximize the log-likelihood of the data: max\u03b8E[log p\u03b8(x)]. GPT-2 models sequences au-\ntoregressively, factorizing the probability distribution p(x) = p(x1, . . . , xl) via chain rule as:\np(x) =\nlY\ni=1\np(xi|xi\u22121, . . . , x1)\n(4)\nFor the language domain, this objective can be interpreted as \u201cgiven the previous i \u22121 words of a\nsentence, predict the next word\u201d.\nB.5\nModel Sizes\nThe model sizes from Section 3.7 are as follows:\nModel Size\nnlayers\nndim\nnheads\n# Parameters\nSmall (Base)\n12\n768\n12\n117M\nMedium\n24\n1024\n16\n345M\nLarge\n36\n1280\n20\n774M\nTable 22: Hyperparameters for architectures for larger model sizes.\nThe hyperparameters for the experiments with other architectures (Vision Transformer, BERT, Long-\nformer, T5) are the same as for the base model size shown above.\nC\nExperimental Details\nWe use implementations of and pretrained models from the Huggingface Transformers library (Wolf\net al., 2020). We train all models using the Adam (Kingma & Ba, 2014) optimizer following Pytorch\n(Paszke et al., 2019) defaults. For all transformer models, we use a learning rate of 10\u22123 without\nlearning rate scheduling. For the remote homology task only, we use a learning rate of 10\u22124 as we\nfound it to give better performance than 10\u22123. We generally use the largest batch size that \ufb01ts on\nan RTX 2080 Ti graphics card, somewhere between 2 and 16, without gradient accumulation. Note\nthat except for the remote homology task, we did not tune the FPT hyperparameters. For all LSTMs,\nwe use a lower learning rate of 3 \u00d7 10\u22124 and the same batch sizes as transformers of the same size.\nModels are trained to convergence and evaluated on a heldout test set.\n23\n", []], "B.2 Positional Embeddings": ["A\nSummary of arXiv Updates\nWe summarize changes made in updated versions:\nv1. (9 Mar 2021) Original release.\nv2. (30 June 2021) Updated Section 3.3 with more analysis of the frozen LSTM architecture\nand additional experimental details. Added new Section 3.10 discussing model depth and\ntoken mixing, new results in Section 3.11 discussing how different freezing strategies can\nimprove performance, and attention mask visualization for random frozen transformer to\nSection 3.5. Included more details about experiments and hyperparameters, and added\nsome new citations (notably Wu et al. (2021) for related LIME work and Frankle et al.\n(2020) for similar frozen analysis for CNNs). Github was also updated to include LSTM\narchitecture, vision pretraining, and remote homology tasks. Minor writing updates.\nB\nBackground on Transformers\nIn this section, we give a description of the transformer architecture used in our experiments, namely\nthe GPT-2 architecture (Radford et al., 2019).\nB.1\nSelf-Attention\nThe main subcomponent of the transformer architecture is the self-attention layer, which takes in l\ninput tokens and outputs l output tokens, both of dimension ndim. Each input token xi is mapped by\nlinear transformations Q, K, and V \u2013 denoting query, key, and values, respectively \u2013 into qi, ki, and\nvi. Both qi and ki have dimension dk, and vi has dimension dv. To generate the output token yi, dot\nproducts are calculated between query qi and keys kj, and fed into a softmax operation to generate\nweights wi \u2208[0, 1] (in practice, a scaling temperature factor of\n1\n\u221adk is used to reduce the sharpness\nof the softmax). Then, the weights are used to generate yi as a weighted sum of all the values, i.e.:\nyi =\nl\nX\nj=1\nexp(q\u22a4\ni kj)\nPl\nk=1 exp(q\u22a4\ni kk)\nvj\n(1)\nThis is extended to multi-head attention over nheads heads by doing the above procedure nheads\ntimes, and then concatenating. To recover the original dimension the concatenated vector (of di-\nmension dvnheads) is multiplied by a projection matrix Wproj \u2208Rdvnheads\u00d7ndim.\nGPT-2 applies a causal mask to its inputs, i.e. the output token i is only allowed to attend to the\ninput tokens j \u2264i, which changes the upper bounds of the sums in Equation 1 to i instead of l. This\nallows for unsupervised pretraining methods like language modeling (see Appendix B.4).\nA residual connection is used to connect the inputs with the outputs of the attention layer. Then, in\nthe rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension\nupwards to 4 \u00b7 ndim for the inner dimension and using the GELU activation function (Hendrycks\n& Gimpel, 2016). Another residual connection is used to connect the outputs of the MLP with the\nprevious outputs of the attention layer.\nThis forms the basis of the transformer block. As it preserves the dimension ndim, multiple blocks\ncan be learned and stacked on top of each other nlayers times, before feeding the \ufb01nal hidden states\nto the output layer. In our work, we only use the output of the last hidden state for classi\ufb01cation,\nalthough in principle other methods are reasonable.\nB.2\nPositional Embeddings\nAs the self-attention blocks are permutation-invariant, in order to capture positional information\nabout sequences, positional embeddings are learned. For each position i \u2208(1, . . . , max len), a\nvector pi is learned. At the front of the transformer, before feeding in the inputs xi into the self-\nattention blocks, the positional embeddings are added to the input embeddings as xi := xi + pi.\n22\n", []], "B.1 Self-Attention": ["A\nSummary of arXiv Updates\nWe summarize changes made in updated versions:\nv1. (9 Mar 2021) Original release.\nv2. (30 June 2021) Updated Section 3.3 with more analysis of the frozen LSTM architecture\nand additional experimental details. Added new Section 3.10 discussing model depth and\ntoken mixing, new results in Section 3.11 discussing how different freezing strategies can\nimprove performance, and attention mask visualization for random frozen transformer to\nSection 3.5. Included more details about experiments and hyperparameters, and added\nsome new citations (notably Wu et al. (2021) for related LIME work and Frankle et al.\n(2020) for similar frozen analysis for CNNs). Github was also updated to include LSTM\narchitecture, vision pretraining, and remote homology tasks. Minor writing updates.\nB\nBackground on Transformers\nIn this section, we give a description of the transformer architecture used in our experiments, namely\nthe GPT-2 architecture (Radford et al., 2019).\nB.1\nSelf-Attention\nThe main subcomponent of the transformer architecture is the self-attention layer, which takes in l\ninput tokens and outputs l output tokens, both of dimension ndim. Each input token xi is mapped by\nlinear transformations Q, K, and V \u2013 denoting query, key, and values, respectively \u2013 into qi, ki, and\nvi. Both qi and ki have dimension dk, and vi has dimension dv. To generate the output token yi, dot\nproducts are calculated between query qi and keys kj, and fed into a softmax operation to generate\nweights wi \u2208[0, 1] (in practice, a scaling temperature factor of\n1\n\u221adk is used to reduce the sharpness\nof the softmax). Then, the weights are used to generate yi as a weighted sum of all the values, i.e.:\nyi =\nl\nX\nj=1\nexp(q\u22a4\ni kj)\nPl\nk=1 exp(q\u22a4\ni kk)\nvj\n(1)\nThis is extended to multi-head attention over nheads heads by doing the above procedure nheads\ntimes, and then concatenating. To recover the original dimension the concatenated vector (of di-\nmension dvnheads) is multiplied by a projection matrix Wproj \u2208Rdvnheads\u00d7ndim.\nGPT-2 applies a causal mask to its inputs, i.e. the output token i is only allowed to attend to the\ninput tokens j \u2264i, which changes the upper bounds of the sums in Equation 1 to i instead of l. This\nallows for unsupervised pretraining methods like language modeling (see Appendix B.4).\nA residual connection is used to connect the inputs with the outputs of the attention layer. Then, in\nthe rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension\nupwards to 4 \u00b7 ndim for the inner dimension and using the GELU activation function (Hendrycks\n& Gimpel, 2016). Another residual connection is used to connect the outputs of the MLP with the\nprevious outputs of the attention layer.\nThis forms the basis of the transformer block. As it preserves the dimension ndim, multiple blocks\ncan be learned and stacked on top of each other nlayers times, before feeding the \ufb01nal hidden states\nto the output layer. In our work, we only use the output of the last hidden state for classi\ufb01cation,\nalthough in principle other methods are reasonable.\nB.2\nPositional Embeddings\nAs the self-attention blocks are permutation-invariant, in order to capture positional information\nabout sequences, positional embeddings are learned. For each position i \u2208(1, . . . , max len), a\nvector pi is learned. At the front of the transformer, before feeding in the inputs xi into the self-\nattention blocks, the positional embeddings are added to the input embeddings as xi := xi + pi.\n22\n", []], "B Background on Transformers": ["A\nSummary of arXiv Updates\nWe summarize changes made in updated versions:\nv1. (9 Mar 2021) Original release.\nv2. (30 June 2021) Updated Section 3.3 with more analysis of the frozen LSTM architecture\nand additional experimental details. Added new Section 3.10 discussing model depth and\ntoken mixing, new results in Section 3.11 discussing how different freezing strategies can\nimprove performance, and attention mask visualization for random frozen transformer to\nSection 3.5. Included more details about experiments and hyperparameters, and added\nsome new citations (notably Wu et al. (2021) for related LIME work and Frankle et al.\n(2020) for similar frozen analysis for CNNs). Github was also updated to include LSTM\narchitecture, vision pretraining, and remote homology tasks. Minor writing updates.\nB\nBackground on Transformers\nIn this section, we give a description of the transformer architecture used in our experiments, namely\nthe GPT-2 architecture (Radford et al., 2019).\nB.1\nSelf-Attention\nThe main subcomponent of the transformer architecture is the self-attention layer, which takes in l\ninput tokens and outputs l output tokens, both of dimension ndim. Each input token xi is mapped by\nlinear transformations Q, K, and V \u2013 denoting query, key, and values, respectively \u2013 into qi, ki, and\nvi. Both qi and ki have dimension dk, and vi has dimension dv. To generate the output token yi, dot\nproducts are calculated between query qi and keys kj, and fed into a softmax operation to generate\nweights wi \u2208[0, 1] (in practice, a scaling temperature factor of\n1\n\u221adk is used to reduce the sharpness\nof the softmax). Then, the weights are used to generate yi as a weighted sum of all the values, i.e.:\nyi =\nl\nX\nj=1\nexp(q\u22a4\ni kj)\nPl\nk=1 exp(q\u22a4\ni kk)\nvj\n(1)\nThis is extended to multi-head attention over nheads heads by doing the above procedure nheads\ntimes, and then concatenating. To recover the original dimension the concatenated vector (of di-\nmension dvnheads) is multiplied by a projection matrix Wproj \u2208Rdvnheads\u00d7ndim.\nGPT-2 applies a causal mask to its inputs, i.e. the output token i is only allowed to attend to the\ninput tokens j \u2264i, which changes the upper bounds of the sums in Equation 1 to i instead of l. This\nallows for unsupervised pretraining methods like language modeling (see Appendix B.4).\nA residual connection is used to connect the inputs with the outputs of the attention layer. Then, in\nthe rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension\nupwards to 4 \u00b7 ndim for the inner dimension and using the GELU activation function (Hendrycks\n& Gimpel, 2016). Another residual connection is used to connect the outputs of the MLP with the\nprevious outputs of the attention layer.\nThis forms the basis of the transformer block. As it preserves the dimension ndim, multiple blocks\ncan be learned and stacked on top of each other nlayers times, before feeding the \ufb01nal hidden states\nto the output layer. In our work, we only use the output of the last hidden state for classi\ufb01cation,\nalthough in principle other methods are reasonable.\nB.2\nPositional Embeddings\nAs the self-attention blocks are permutation-invariant, in order to capture positional information\nabout sequences, positional embeddings are learned. For each position i \u2208(1, . . . , max len), a\nvector pi is learned. At the front of the transformer, before feeding in the inputs xi into the self-\nattention blocks, the positional embeddings are added to the input embeddings as xi := xi + pi.\n22\n", []], "A Summary of arXiv Updates": ["A\nSummary of arXiv Updates\nWe summarize changes made in updated versions:\nv1. (9 Mar 2021) Original release.\nv2. (30 June 2021) Updated Section 3.3 with more analysis of the frozen LSTM architecture\nand additional experimental details. Added new Section 3.10 discussing model depth and\ntoken mixing, new results in Section 3.11 discussing how different freezing strategies can\nimprove performance, and attention mask visualization for random frozen transformer to\nSection 3.5. Included more details about experiments and hyperparameters, and added\nsome new citations (notably Wu et al. (2021) for related LIME work and Frankle et al.\n(2020) for similar frozen analysis for CNNs). Github was also updated to include LSTM\narchitecture, vision pretraining, and remote homology tasks. Minor writing updates.\nB\nBackground on Transformers\nIn this section, we give a description of the transformer architecture used in our experiments, namely\nthe GPT-2 architecture (Radford et al., 2019).\nB.1\nSelf-Attention\nThe main subcomponent of the transformer architecture is the self-attention layer, which takes in l\ninput tokens and outputs l output tokens, both of dimension ndim. Each input token xi is mapped by\nlinear transformations Q, K, and V \u2013 denoting query, key, and values, respectively \u2013 into qi, ki, and\nvi. Both qi and ki have dimension dk, and vi has dimension dv. To generate the output token yi, dot\nproducts are calculated between query qi and keys kj, and fed into a softmax operation to generate\nweights wi \u2208[0, 1] (in practice, a scaling temperature factor of\n1\n\u221adk is used to reduce the sharpness\nof the softmax). Then, the weights are used to generate yi as a weighted sum of all the values, i.e.:\nyi =\nl\nX\nj=1\nexp(q\u22a4\ni kj)\nPl\nk=1 exp(q\u22a4\ni kk)\nvj\n(1)\nThis is extended to multi-head attention over nheads heads by doing the above procedure nheads\ntimes, and then concatenating. To recover the original dimension the concatenated vector (of di-\nmension dvnheads) is multiplied by a projection matrix Wproj \u2208Rdvnheads\u00d7ndim.\nGPT-2 applies a causal mask to its inputs, i.e. the output token i is only allowed to attend to the\ninput tokens j \u2264i, which changes the upper bounds of the sums in Equation 1 to i instead of l. This\nallows for unsupervised pretraining methods like language modeling (see Appendix B.4).\nA residual connection is used to connect the inputs with the outputs of the attention layer. Then, in\nthe rest of the transformer block, a two-layer MLP is used, conventionally projecting the dimension\nupwards to 4 \u00b7 ndim for the inner dimension and using the GELU activation function (Hendrycks\n& Gimpel, 2016). Another residual connection is used to connect the outputs of the MLP with the\nprevious outputs of the attention layer.\nThis forms the basis of the transformer block. As it preserves the dimension ndim, multiple blocks\ncan be learned and stacked on top of each other nlayers times, before feeding the \ufb01nal hidden states\nto the output layer. In our work, we only use the output of the last hidden state for classi\ufb01cation,\nalthough in principle other methods are reasonable.\nB.2\nPositional Embeddings\nAs the self-attention blocks are permutation-invariant, in order to capture positional information\nabout sequences, positional embeddings are learned. For each position i \u2208(1, . . . , max len), a\nvector pi is learned. At the front of the transformer, before feeding in the inputs xi into the self-\nattention blocks, the positional embeddings are added to the input embeddings as xi := xi + pi.\n22\n", []], "Appendix": ["References\nJosh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Stephen Clark, An-\ndrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, et al. Imitating interactive intelligence.\narXiv preprint arXiv:2012.05672, 2020.\nJacob Andreas, Dan Klein, and Sergey Levine.\nLearning with latent language.\narXiv preprint\narXiv:1711.00482, 2017.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of mono-\nlingual representations. arXiv preprint arXiv:1910.11856, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nBernard J Baars. A cognitive theory of consciousness. Cambridge University Press, 1993.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\nDeep equilibrium models.\narXiv preprint\narXiv:1909.01377, 2019.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big. In Proceedings of the 2020 Con-\nference on Fairness, Accountability, and Transparency; Association for Computing Machinery:\nNew York, NY, USA, 2021.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International Conference on Machine Learning, pp. 1691\u2013\n1703. PMLR, 2020a.\nTing Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative adver-\nsarial networks. arXiv preprint arXiv:1810.01365, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597\u20131607. PMLR, 2020b.\nKristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. Fair generative modeling\nvia weak supervision. In International Conference on Machine Learning, pp. 1887\u20131898. PMLR,\n2020.\nAndrew M Dai and Quoc V Le.\nSemi-supervised sequence learning.\narXiv preprint\narXiv:1511.01432, 2015.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJeff Donahue, Philipp Kr\u00a8ahenb\u00a8uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n17\n", []], "References": ["References\nJosh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Stephen Clark, An-\ndrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, et al. Imitating interactive intelligence.\narXiv preprint arXiv:2012.05672, 2020.\nJacob Andreas, Dan Klein, and Sergey Levine.\nLearning with latent language.\narXiv preprint\narXiv:1711.00482, 2017.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of mono-\nlingual representations. arXiv preprint arXiv:1910.11856, 2019.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nBernard J Baars. A cognitive theory of consciousness. Cambridge University Press, 1993.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\nDeep equilibrium models.\narXiv preprint\narXiv:1909.01377, 2019.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big. In Proceedings of the 2020 Con-\nference on Fairness, Accountability, and Transparency; Association for Computing Machinery:\nNew York, NY, USA, 2021.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International Conference on Machine Learning, pp. 1691\u2013\n1703. PMLR, 2020a.\nTing Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative adver-\nsarial networks. arXiv preprint arXiv:1810.01365, 2018.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning,\npp. 1597\u20131607. PMLR, 2020b.\nKristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. Fair generative modeling\nvia weak supervision. In International Conference on Machine Learning, pp. 1887\u20131898. PMLR,\n2020.\nAndrew M Dai and Quoc V Le.\nSemi-supervised sequence learning.\narXiv preprint\narXiv:1511.01432, 2015.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJeff Donahue, Philipp Kr\u00a8ahenb\u00a8uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint\narXiv:1605.09782, 2016.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\n17\n", []], "Acknowledgements": ["4.6\nReservoir computing\nSimilarly to the FPT setup and Global Workspace Theory, in reservoir computing (Tanaka et al.,\n2019) and echo state networks (Jaeger, 2001; Jaeger & Haas, 2004), a random recurrent network\nis frozen and only the output readout layer is trained. These models are very fast to train, using\na similar setup as in Section 3.9, because the activations of the recurrent network can be cached\nand it is unnecessary to backpropagate over time. Somewhat differently to the FPT architecture,\necho state networks are recurrent and thus feed back into themselves, which allows the outputs of\nthe random frozen network to modulate future inputs. Unlike echo state networks, we also notably\n\ufb01netune the input and positional embeddings, which allow the inputs to the frozen network to adapt\nto a particular modality/for a query to the frozen network to be learned. Echo state networks are also\nsimilar to the perspective of self-attention applying a data-dependent \ufb01lter to the inputs, as opposed\nto 1D convolutions, which are \ufb01xed \ufb01lters regardless of the input modality.\n5\nConclusion\nWe proposed transferring a pretrained transformer language model for downstream tasks in non-\nlanguage modalities. Through extensive empirical evaluation, we showed that these models could\nachieve performance competitive with transformers fully trained on the downstream task without\nhaving to \ufb01netune the self-attention and feedforward layers, relying solely on frozen parameters\nfrom the language model to perform the bulk of the computation.\nWe believe this work can serve as the foundation for future work investigating transfer between\nmodalities. In future, we are interested in investigating the use of other data-rich modalities (e.g.,\nvision) or a hybrid of multiple domains being used to provide the necessary substrate for pretraining\na universal computational engine. It would also be interesting to explore frozen pretrained models\nfor tasks beyond predictive modeling, such as reinforcement learning (Abramson et al., 2020).\nWe note that a limitation of our analysis in that we analyze speci\ufb01c models on a restricted set of\ntasks. More investigation can highlight whether or not similar behavior occurs for other models on\nother tasks. For instance, in Section 3.14, we \ufb01nd the architecture can have a signi\ufb01cant impact\non results. As training regimes for these models evolve, performing similar experiments may yield\ndifferent results, and we are excited for more research in this direction.\nFor high stakes applications in the real-world, there are potential concerns with transfer of harmful\nbiases from one modality to one another using pretrained transformer models trained on vast quan-\ntities of unlabeled, uncurated datasets (Sheng et al., 2019; Bender et al., 2021). Mitigating these\nbiases is an active area of research (Grover et al., 2019; Choi et al., 2020). Conversely, there are also\npotential upsides with FPT models being able to better exploit representative datasets from one or\nmore modalities, which merit future investigation as well.\nAcknowledgements\nWe would like to thank Luke Metz, Kimin Lee, Fangchen Liu, Roshan Rao, Aravind Srinivas, Nikita\nKitaev, Daniel Freeman, Marc\u2019Aurelio Ranzato, Jacob Andreas, and Ashish Vaswani for valuable\nfeedback and discussions. We would also like to thank members of the community for providing\nfeedback online on an earlier version of this paper.\n15\n", []], "5 Conclusion": ["4.6\nReservoir computing\nSimilarly to the FPT setup and Global Workspace Theory, in reservoir computing (Tanaka et al.,\n2019) and echo state networks (Jaeger, 2001; Jaeger & Haas, 2004), a random recurrent network\nis frozen and only the output readout layer is trained. These models are very fast to train, using\na similar setup as in Section 3.9, because the activations of the recurrent network can be cached\nand it is unnecessary to backpropagate over time. Somewhat differently to the FPT architecture,\necho state networks are recurrent and thus feed back into themselves, which allows the outputs of\nthe random frozen network to modulate future inputs. Unlike echo state networks, we also notably\n\ufb01netune the input and positional embeddings, which allow the inputs to the frozen network to adapt\nto a particular modality/for a query to the frozen network to be learned. Echo state networks are also\nsimilar to the perspective of self-attention applying a data-dependent \ufb01lter to the inputs, as opposed\nto 1D convolutions, which are \ufb01xed \ufb01lters regardless of the input modality.\n5\nConclusion\nWe proposed transferring a pretrained transformer language model for downstream tasks in non-\nlanguage modalities. Through extensive empirical evaluation, we showed that these models could\nachieve performance competitive with transformers fully trained on the downstream task without\nhaving to \ufb01netune the self-attention and feedforward layers, relying solely on frozen parameters\nfrom the language model to perform the bulk of the computation.\nWe believe this work can serve as the foundation for future work investigating transfer between\nmodalities. In future, we are interested in investigating the use of other data-rich modalities (e.g.,\nvision) or a hybrid of multiple domains being used to provide the necessary substrate for pretraining\na universal computational engine. It would also be interesting to explore frozen pretrained models\nfor tasks beyond predictive modeling, such as reinforcement learning (Abramson et al., 2020).\nWe note that a limitation of our analysis in that we analyze speci\ufb01c models on a restricted set of\ntasks. More investigation can highlight whether or not similar behavior occurs for other models on\nother tasks. For instance, in Section 3.14, we \ufb01nd the architecture can have a signi\ufb01cant impact\non results. As training regimes for these models evolve, performing similar experiments may yield\ndifferent results, and we are excited for more research in this direction.\nFor high stakes applications in the real-world, there are potential concerns with transfer of harmful\nbiases from one modality to one another using pretrained transformer models trained on vast quan-\ntities of unlabeled, uncurated datasets (Sheng et al., 2019; Bender et al., 2021). Mitigating these\nbiases is an active area of research (Grover et al., 2019; Choi et al., 2020). Conversely, there are also\npotential upsides with FPT models being able to better exploit representative datasets from one or\nmore modalities, which merit future investigation as well.\nAcknowledgements\nWe would like to thank Luke Metz, Kimin Lee, Fangchen Liu, Roshan Rao, Aravind Srinivas, Nikita\nKitaev, Daniel Freeman, Marc\u2019Aurelio Ranzato, Jacob Andreas, and Ashish Vaswani for valuable\nfeedback and discussions. We would also like to thank members of the community for providing\nfeedback online on an earlier version of this paper.\n15\n", []], "4.6 Reservoir computing": ["4.6\nReservoir computing\nSimilarly to the FPT setup and Global Workspace Theory, in reservoir computing (Tanaka et al.,\n2019) and echo state networks (Jaeger, 2001; Jaeger & Haas, 2004), a random recurrent network\nis frozen and only the output readout layer is trained. These models are very fast to train, using\na similar setup as in Section 3.9, because the activations of the recurrent network can be cached\nand it is unnecessary to backpropagate over time. Somewhat differently to the FPT architecture,\necho state networks are recurrent and thus feed back into themselves, which allows the outputs of\nthe random frozen network to modulate future inputs. Unlike echo state networks, we also notably\n\ufb01netune the input and positional embeddings, which allow the inputs to the frozen network to adapt\nto a particular modality/for a query to the frozen network to be learned. Echo state networks are also\nsimilar to the perspective of self-attention applying a data-dependent \ufb01lter to the inputs, as opposed\nto 1D convolutions, which are \ufb01xed \ufb01lters regardless of the input modality.\n5\nConclusion\nWe proposed transferring a pretrained transformer language model for downstream tasks in non-\nlanguage modalities. Through extensive empirical evaluation, we showed that these models could\nachieve performance competitive with transformers fully trained on the downstream task without\nhaving to \ufb01netune the self-attention and feedforward layers, relying solely on frozen parameters\nfrom the language model to perform the bulk of the computation.\nWe believe this work can serve as the foundation for future work investigating transfer between\nmodalities. In future, we are interested in investigating the use of other data-rich modalities (e.g.,\nvision) or a hybrid of multiple domains being used to provide the necessary substrate for pretraining\na universal computational engine. It would also be interesting to explore frozen pretrained models\nfor tasks beyond predictive modeling, such as reinforcement learning (Abramson et al., 2020).\nWe note that a limitation of our analysis in that we analyze speci\ufb01c models on a restricted set of\ntasks. More investigation can highlight whether or not similar behavior occurs for other models on\nother tasks. For instance, in Section 3.14, we \ufb01nd the architecture can have a signi\ufb01cant impact\non results. As training regimes for these models evolve, performing similar experiments may yield\ndifferent results, and we are excited for more research in this direction.\nFor high stakes applications in the real-world, there are potential concerns with transfer of harmful\nbiases from one modality to one another using pretrained transformer models trained on vast quan-\ntities of unlabeled, uncurated datasets (Sheng et al., 2019; Bender et al., 2021). Mitigating these\nbiases is an active area of research (Grover et al., 2019; Choi et al., 2020). Conversely, there are also\npotential upsides with FPT models being able to better exploit representative datasets from one or\nmore modalities, which merit future investigation as well.\nAcknowledgements\nWe would like to thank Luke Metz, Kimin Lee, Fangchen Liu, Roshan Rao, Aravind Srinivas, Nikita\nKitaev, Daniel Freeman, Marc\u2019Aurelio Ranzato, Jacob Andreas, and Ashish Vaswani for valuable\nfeedback and discussions. We would also like to thank members of the community for providing\nfeedback online on an earlier version of this paper.\n15\n", []], "4.5 Global workspace theory": ["investigate transfer for LSTMs between modalities including code, different languages, and music,\n\ufb01nding that pretraining on \u201cnon-linguistic data with latent structure\u201d can transfer to language, \ufb01nd-\ning grammatical structure in a modality to be important, although we generally investigate the other\ndirection and explore more distanced modalities. Kiela et al. (2019) make similar observations for\naligning representation spaces of language and vision. Li et al. (2020) pretrain on a referential com-\nmunication game where an emergent learned language is used to transfer to NLP tasks. Wu et al.\n(2021) found explicitly pretraining computational primitives to transfer to mathematics tasks.\n4.3\nPretraining and \ufb01netuning of transformer models\nA common trend in deep learning models is to \ufb01rst train a large model on an unsupervised objective\non a large dataset (Dai & Le, 2015; Radford et al., 2018) and then \ufb01netune on a small downstream\ndataset (e.g., by freezing the model and only \ufb01netuing the output layer). A common method used\nto \ufb01netune transformers are adapter networks (Rebuf\ufb01et al., 2017; Houlsby et al., 2019), which\nadd a fully connected residual block for each unique downstream task and also \ufb01netune the layer\nnorm parameters. For simplicity, we do not add the full adapter block but only train the layer norm\nparameters, reducing the number of parameters we consider. These techniques used are similar\nto prior approaches such as FiLM (Perez et al., 2018) and self-modulation (Chen et al., 2018). A\nrecent direction of research has explored learning prompt templates for large models (Shin et al.,\n2020) that simply require forward passes over the transformer. Unlike these works, we consider\n\ufb01netuning on one modality (language) and \ufb01netuning on others, whereas prior work investigates\n\ufb01netuning on the same modality as the pretraining task. Another interesting related work, although\nnot investigating transformers, by Frankle et al. (2020) \ufb01nd randomly initialized CNNs, which only\ntrain the batchnorm af\ufb01ne parameters, to work well on CIFAR-10. Their numbers are stronger than\nours on CIFAR-10, but include signi\ufb01cantly more inductive bias via a convolutional architecture, so\nthe main takeaway is slightly more relevant towards image tasks rather than arbitrary sequences.\n4.4\nSelf-attention layers as optimization steps\nThe nature of computation performed by self-attention layers has also been explored by other related\nworks. Bai et al. (2019) show that a single transformer self-attention block can be trained to perform\nan optimization step towards \ufb01nding a stationary point, representing the solution to the task. Ram-\nsauer et al. (2020) show that the self-attention layer is a gradient step in a Hop\ufb01eld network with a\nlearning rate of 1, hinting that transformers are capable of storing and retrieving a large amount of\npatterns with an implicit energy function. An interesting discussion from Goyal & Bengio (2020)\npoints out a connection in viewing the key-value queries used in attention as similar to function sig-\nnatures in computer programming: the key maps the input to a type (e.g., \ufb02oat) and the value maps\nthe input to its value (e.g., 3.14), and if the type matches the function signature, the function can be\napplied to the value \u2013 this may be particularly relevant when we consider using a single self-attention\nlayer applied to different modalities, as the modality may be embedded in the type.\n4.5\nGlobal workspace theory\nA common technique for evaluating the embeddings learned by an unsupervised model is to train a\nlinear layer on top of the embeddings for a downstream task (Donahue et al., 2016; Oord et al., 2018;\nChen et al., 2020b), which is reasonable when you \ufb01netune on the same modality as the pretrained\none. However, when \ufb01netuning on a different modality, as in our setting, we have to reframe this\nnotion of generalizable embedding quality \u2013 instead of only \ufb01netuning the output layer, we also\nwant to \ufb01netune the input layer, and instead evaluate the ability of the frozen intermediate model\nto perform generalizable computation. This is reminiscent of Global Workspace Theory (Baars,\n1993), which revolves around the notion that there is a \u201cblackboard\u201d that different parts of the brain\nsend data to; we might consider the frozen language model as being a blackboard in this setting.\nLanguage might also be a natural choice of model for this blackboard, as there are hypotheses that\nlanguage may serve as a good multipurpose high-level representation for cognitive behavior and\nconscious planning (Andreas et al., 2017; Goyal & Bengio, 2020).\n14\n", []], "4.4 Self-attention layers as optimization steps": ["investigate transfer for LSTMs between modalities including code, different languages, and music,\n\ufb01nding that pretraining on \u201cnon-linguistic data with latent structure\u201d can transfer to language, \ufb01nd-\ning grammatical structure in a modality to be important, although we generally investigate the other\ndirection and explore more distanced modalities. Kiela et al. (2019) make similar observations for\naligning representation spaces of language and vision. Li et al. (2020) pretrain on a referential com-\nmunication game where an emergent learned language is used to transfer to NLP tasks. Wu et al.\n(2021) found explicitly pretraining computational primitives to transfer to mathematics tasks.\n4.3\nPretraining and \ufb01netuning of transformer models\nA common trend in deep learning models is to \ufb01rst train a large model on an unsupervised objective\non a large dataset (Dai & Le, 2015; Radford et al., 2018) and then \ufb01netune on a small downstream\ndataset (e.g., by freezing the model and only \ufb01netuing the output layer). A common method used\nto \ufb01netune transformers are adapter networks (Rebuf\ufb01et al., 2017; Houlsby et al., 2019), which\nadd a fully connected residual block for each unique downstream task and also \ufb01netune the layer\nnorm parameters. For simplicity, we do not add the full adapter block but only train the layer norm\nparameters, reducing the number of parameters we consider. These techniques used are similar\nto prior approaches such as FiLM (Perez et al., 2018) and self-modulation (Chen et al., 2018). A\nrecent direction of research has explored learning prompt templates for large models (Shin et al.,\n2020) that simply require forward passes over the transformer. Unlike these works, we consider\n\ufb01netuning on one modality (language) and \ufb01netuning on others, whereas prior work investigates\n\ufb01netuning on the same modality as the pretraining task. Another interesting related work, although\nnot investigating transformers, by Frankle et al. (2020) \ufb01nd randomly initialized CNNs, which only\ntrain the batchnorm af\ufb01ne parameters, to work well on CIFAR-10. Their numbers are stronger than\nours on CIFAR-10, but include signi\ufb01cantly more inductive bias via a convolutional architecture, so\nthe main takeaway is slightly more relevant towards image tasks rather than arbitrary sequences.\n4.4\nSelf-attention layers as optimization steps\nThe nature of computation performed by self-attention layers has also been explored by other related\nworks. Bai et al. (2019) show that a single transformer self-attention block can be trained to perform\nan optimization step towards \ufb01nding a stationary point, representing the solution to the task. Ram-\nsauer et al. (2020) show that the self-attention layer is a gradient step in a Hop\ufb01eld network with a\nlearning rate of 1, hinting that transformers are capable of storing and retrieving a large amount of\npatterns with an implicit energy function. An interesting discussion from Goyal & Bengio (2020)\npoints out a connection in viewing the key-value queries used in attention as similar to function sig-\nnatures in computer programming: the key maps the input to a type (e.g., \ufb02oat) and the value maps\nthe input to its value (e.g., 3.14), and if the type matches the function signature, the function can be\napplied to the value \u2013 this may be particularly relevant when we consider using a single self-attention\nlayer applied to different modalities, as the modality may be embedded in the type.\n4.5\nGlobal workspace theory\nA common technique for evaluating the embeddings learned by an unsupervised model is to train a\nlinear layer on top of the embeddings for a downstream task (Donahue et al., 2016; Oord et al., 2018;\nChen et al., 2020b), which is reasonable when you \ufb01netune on the same modality as the pretrained\none. However, when \ufb01netuning on a different modality, as in our setting, we have to reframe this\nnotion of generalizable embedding quality \u2013 instead of only \ufb01netuning the output layer, we also\nwant to \ufb01netune the input layer, and instead evaluate the ability of the frozen intermediate model\nto perform generalizable computation. This is reminiscent of Global Workspace Theory (Baars,\n1993), which revolves around the notion that there is a \u201cblackboard\u201d that different parts of the brain\nsend data to; we might consider the frozen language model as being a blackboard in this setting.\nLanguage might also be a natural choice of model for this blackboard, as there are hypotheses that\nlanguage may serve as a good multipurpose high-level representation for cognitive behavior and\nconscious planning (Andreas et al., 2017; Goyal & Bengio, 2020).\n14\n", []], "4.3 Pretraining and finetuning of transformer models": ["investigate transfer for LSTMs between modalities including code, different languages, and music,\n\ufb01nding that pretraining on \u201cnon-linguistic data with latent structure\u201d can transfer to language, \ufb01nd-\ning grammatical structure in a modality to be important, although we generally investigate the other\ndirection and explore more distanced modalities. Kiela et al. (2019) make similar observations for\naligning representation spaces of language and vision. Li et al. (2020) pretrain on a referential com-\nmunication game where an emergent learned language is used to transfer to NLP tasks. Wu et al.\n(2021) found explicitly pretraining computational primitives to transfer to mathematics tasks.\n4.3\nPretraining and \ufb01netuning of transformer models\nA common trend in deep learning models is to \ufb01rst train a large model on an unsupervised objective\non a large dataset (Dai & Le, 2015; Radford et al., 2018) and then \ufb01netune on a small downstream\ndataset (e.g., by freezing the model and only \ufb01netuing the output layer). A common method used\nto \ufb01netune transformers are adapter networks (Rebuf\ufb01et al., 2017; Houlsby et al., 2019), which\nadd a fully connected residual block for each unique downstream task and also \ufb01netune the layer\nnorm parameters. For simplicity, we do not add the full adapter block but only train the layer norm\nparameters, reducing the number of parameters we consider. These techniques used are similar\nto prior approaches such as FiLM (Perez et al., 2018) and self-modulation (Chen et al., 2018). A\nrecent direction of research has explored learning prompt templates for large models (Shin et al.,\n2020) that simply require forward passes over the transformer. Unlike these works, we consider\n\ufb01netuning on one modality (language) and \ufb01netuning on others, whereas prior work investigates\n\ufb01netuning on the same modality as the pretraining task. Another interesting related work, although\nnot investigating transformers, by Frankle et al. (2020) \ufb01nd randomly initialized CNNs, which only\ntrain the batchnorm af\ufb01ne parameters, to work well on CIFAR-10. Their numbers are stronger than\nours on CIFAR-10, but include signi\ufb01cantly more inductive bias via a convolutional architecture, so\nthe main takeaway is slightly more relevant towards image tasks rather than arbitrary sequences.\n4.4\nSelf-attention layers as optimization steps\nThe nature of computation performed by self-attention layers has also been explored by other related\nworks. Bai et al. (2019) show that a single transformer self-attention block can be trained to perform\nan optimization step towards \ufb01nding a stationary point, representing the solution to the task. Ram-\nsauer et al. (2020) show that the self-attention layer is a gradient step in a Hop\ufb01eld network with a\nlearning rate of 1, hinting that transformers are capable of storing and retrieving a large amount of\npatterns with an implicit energy function. An interesting discussion from Goyal & Bengio (2020)\npoints out a connection in viewing the key-value queries used in attention as similar to function sig-\nnatures in computer programming: the key maps the input to a type (e.g., \ufb02oat) and the value maps\nthe input to its value (e.g., 3.14), and if the type matches the function signature, the function can be\napplied to the value \u2013 this may be particularly relevant when we consider using a single self-attention\nlayer applied to different modalities, as the modality may be embedded in the type.\n4.5\nGlobal workspace theory\nA common technique for evaluating the embeddings learned by an unsupervised model is to train a\nlinear layer on top of the embeddings for a downstream task (Donahue et al., 2016; Oord et al., 2018;\nChen et al., 2020b), which is reasonable when you \ufb01netune on the same modality as the pretrained\none. However, when \ufb01netuning on a different modality, as in our setting, we have to reframe this\nnotion of generalizable embedding quality \u2013 instead of only \ufb01netuning the output layer, we also\nwant to \ufb01netune the input layer, and instead evaluate the ability of the frozen intermediate model\nto perform generalizable computation. This is reminiscent of Global Workspace Theory (Baars,\n1993), which revolves around the notion that there is a \u201cblackboard\u201d that different parts of the brain\nsend data to; we might consider the frozen language model as being a blackboard in this setting.\nLanguage might also be a natural choice of model for this blackboard, as there are hypotheses that\nlanguage may serve as a good multipurpose high-level representation for cognitive behavior and\nconscious planning (Andreas et al., 2017; Goyal & Bengio, 2020).\n14\n", []], "4.2 Transformers in transfer settings": ["Initialization\nFrozen Layer Norm\nFinetuned Layer Norm\nPretrained\n61.5%\n68.2%\nRandom\n55.0%\n61.7%\nTable 16: Test accuracy on CIFAR-10 when only \ufb01netuning the input and output layer parameters.\n3.14\nHow well do the trends hold across other transformer models?\nWe also investigate how other transformer architectures perform when swapped out with GPT-2:\nBERT (Devlin et al., 2018), T5 (Raffel et al., 2019), and Longformer (Beltagy et al., 2020). For\nT5, we only use the encoder, and not the decoder. Our results are in Table 17. We \ufb01nd results to\nroughly hold across some architectures \u2013 with some differences \u2013 although T5 tends to be slightly\nworse than the other models. An interesting question for future work is whether subtle differences\nin architecture, pretraining objective, or dataset contribute to these differences.\nTask\nGPT-2 (FPT Default)\nBERT\nT5\nLongformer\nListOps\n38.4%\n38.3%\n15.4%\n17.0%\nCIFAR-10\n68.2%\n68.8%\n64.7%\n66.8%\nTable 17: Test accuracy for frozen pretrained transformer variants (base model sizes).\n4\nRelated Work and Discussion\n4.1\nTransformers in multimodal settings\nTransformers (Vaswani et al., 2017) were \ufb01rst used successfully for natural language processing\n(Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Brown et al., 2020). In recent\nyears, they have also been shown to be effective architectures for other modalities. One particular\nmodality of interest is computer vision (Chen et al., 2020a; Touvron et al., 2020); in particular,\nDosovitskiy et al. (2020) showed that transformers can outperform CNNs in the high-data regime on\nstandard object recognition benchmarks such as ImageNet and CIFAR. Furthermore, transformers\nhave also been used for prediction tasks over protein sequences (Jumper et al., 2021; Rao et al.,\n2021), reinforcement learning (Parisotto et al., 2020), and imitation learning (Abramson et al., 2020).\nWork speci\ufb01cally tackling multimodal tasks include Kaiser et al. (2017), who showed a single model\ncould learn a variety of multimodal tasks with an attention architecture. Recent work has utilized\ntransformers for multimodal predictive tasks, such as images and text in ViLBERT (Lu et al., 2019)\nand CLIP (Radford et al., 2021); these approaches generally use two distinct transformers to embed\nimages and text. Lu et al. (2020) applies ViLBERT to train a single model for a variety of combined\nvision and language tasks. Recent work from OpenAI (Goh et al., 2021) \ufb01nds that some neurons\nlearned by CLIP are activated by a particular semantic concept, regardless of if the concept is pre-\nsented in language or picture form. Our work is most similar to DALL-E (Ramesh et al., 2021),\nwhich uses a single transformer to embed both the image and text modalities, which we consider\nto be generating a \u201cuniversal latent space\u201d that projects any type of input into a single latent space.\nSuch a latent space would be useful for a model that could learn from many sources of supervision.\n4.2\nTransformers in transfer settings\nThere are also many works looking at transformers speci\ufb01cally in the context of in-modality trans-\nfer, such as ViT for vision (Dosovitskiy et al., 2020), T5 for language (Raffel et al., 2019), and\nUDSMProt for protein sequences (Strodthoff et al., 2020). CLIP (Radford et al., 2021) showed that\ntraining on text in addition to images could allow for zero-shot classi\ufb01cation via providing down-\nstream labels as text. Hernandez et al. (2021) do a thorough investigation of transfer with language\npretraining, notably showing transfer from English to Python, which they consider to be reasonably\ndistanced from English; many works have also looked at transferring from one langauge to another\n(Artetxe et al., 2019; Ponti et al., 2019). Similar to our work, Papadimitriou & Jurafsky (2020)\n13\n", []], "4.1 Transformers in multimodal settings": ["Initialization\nFrozen Layer Norm\nFinetuned Layer Norm\nPretrained\n61.5%\n68.2%\nRandom\n55.0%\n61.7%\nTable 16: Test accuracy on CIFAR-10 when only \ufb01netuning the input and output layer parameters.\n3.14\nHow well do the trends hold across other transformer models?\nWe also investigate how other transformer architectures perform when swapped out with GPT-2:\nBERT (Devlin et al., 2018), T5 (Raffel et al., 2019), and Longformer (Beltagy et al., 2020). For\nT5, we only use the encoder, and not the decoder. Our results are in Table 17. We \ufb01nd results to\nroughly hold across some architectures \u2013 with some differences \u2013 although T5 tends to be slightly\nworse than the other models. An interesting question for future work is whether subtle differences\nin architecture, pretraining objective, or dataset contribute to these differences.\nTask\nGPT-2 (FPT Default)\nBERT\nT5\nLongformer\nListOps\n38.4%\n38.3%\n15.4%\n17.0%\nCIFAR-10\n68.2%\n68.8%\n64.7%\n66.8%\nTable 17: Test accuracy for frozen pretrained transformer variants (base model sizes).\n4\nRelated Work and Discussion\n4.1\nTransformers in multimodal settings\nTransformers (Vaswani et al., 2017) were \ufb01rst used successfully for natural language processing\n(Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Brown et al., 2020). In recent\nyears, they have also been shown to be effective architectures for other modalities. One particular\nmodality of interest is computer vision (Chen et al., 2020a; Touvron et al., 2020); in particular,\nDosovitskiy et al. (2020) showed that transformers can outperform CNNs in the high-data regime on\nstandard object recognition benchmarks such as ImageNet and CIFAR. Furthermore, transformers\nhave also been used for prediction tasks over protein sequences (Jumper et al., 2021; Rao et al.,\n2021), reinforcement learning (Parisotto et al., 2020), and imitation learning (Abramson et al., 2020).\nWork speci\ufb01cally tackling multimodal tasks include Kaiser et al. (2017), who showed a single model\ncould learn a variety of multimodal tasks with an attention architecture. Recent work has utilized\ntransformers for multimodal predictive tasks, such as images and text in ViLBERT (Lu et al., 2019)\nand CLIP (Radford et al., 2021); these approaches generally use two distinct transformers to embed\nimages and text. Lu et al. (2020) applies ViLBERT to train a single model for a variety of combined\nvision and language tasks. Recent work from OpenAI (Goh et al., 2021) \ufb01nds that some neurons\nlearned by CLIP are activated by a particular semantic concept, regardless of if the concept is pre-\nsented in language or picture form. Our work is most similar to DALL-E (Ramesh et al., 2021),\nwhich uses a single transformer to embed both the image and text modalities, which we consider\nto be generating a \u201cuniversal latent space\u201d that projects any type of input into a single latent space.\nSuch a latent space would be useful for a model that could learn from many sources of supervision.\n4.2\nTransformers in transfer settings\nThere are also many works looking at transformers speci\ufb01cally in the context of in-modality trans-\nfer, such as ViT for vision (Dosovitskiy et al., 2020), T5 for language (Raffel et al., 2019), and\nUDSMProt for protein sequences (Strodthoff et al., 2020). CLIP (Radford et al., 2021) showed that\ntraining on text in addition to images could allow for zero-shot classi\ufb01cation via providing down-\nstream labels as text. Hernandez et al. (2021) do a thorough investigation of transfer with language\npretraining, notably showing transfer from English to Python, which they consider to be reasonably\ndistanced from English; many works have also looked at transferring from one langauge to another\n(Artetxe et al., 2019; Ponti et al., 2019). Similar to our work, Papadimitriou & Jurafsky (2020)\n13\n", []], "4 Related Work and Discussion": ["Initialization\nFrozen Layer Norm\nFinetuned Layer Norm\nPretrained\n61.5%\n68.2%\nRandom\n55.0%\n61.7%\nTable 16: Test accuracy on CIFAR-10 when only \ufb01netuning the input and output layer parameters.\n3.14\nHow well do the trends hold across other transformer models?\nWe also investigate how other transformer architectures perform when swapped out with GPT-2:\nBERT (Devlin et al., 2018), T5 (Raffel et al., 2019), and Longformer (Beltagy et al., 2020). For\nT5, we only use the encoder, and not the decoder. Our results are in Table 17. We \ufb01nd results to\nroughly hold across some architectures \u2013 with some differences \u2013 although T5 tends to be slightly\nworse than the other models. An interesting question for future work is whether subtle differences\nin architecture, pretraining objective, or dataset contribute to these differences.\nTask\nGPT-2 (FPT Default)\nBERT\nT5\nLongformer\nListOps\n38.4%\n38.3%\n15.4%\n17.0%\nCIFAR-10\n68.2%\n68.8%\n64.7%\n66.8%\nTable 17: Test accuracy for frozen pretrained transformer variants (base model sizes).\n4\nRelated Work and Discussion\n4.1\nTransformers in multimodal settings\nTransformers (Vaswani et al., 2017) were \ufb01rst used successfully for natural language processing\n(Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Brown et al., 2020). In recent\nyears, they have also been shown to be effective architectures for other modalities. One particular\nmodality of interest is computer vision (Chen et al., 2020a; Touvron et al., 2020); in particular,\nDosovitskiy et al. (2020) showed that transformers can outperform CNNs in the high-data regime on\nstandard object recognition benchmarks such as ImageNet and CIFAR. Furthermore, transformers\nhave also been used for prediction tasks over protein sequences (Jumper et al., 2021; Rao et al.,\n2021), reinforcement learning (Parisotto et al., 2020), and imitation learning (Abramson et al., 2020).\nWork speci\ufb01cally tackling multimodal tasks include Kaiser et al. (2017), who showed a single model\ncould learn a variety of multimodal tasks with an attention architecture. Recent work has utilized\ntransformers for multimodal predictive tasks, such as images and text in ViLBERT (Lu et al., 2019)\nand CLIP (Radford et al., 2021); these approaches generally use two distinct transformers to embed\nimages and text. Lu et al. (2020) applies ViLBERT to train a single model for a variety of combined\nvision and language tasks. Recent work from OpenAI (Goh et al., 2021) \ufb01nds that some neurons\nlearned by CLIP are activated by a particular semantic concept, regardless of if the concept is pre-\nsented in language or picture form. Our work is most similar to DALL-E (Ramesh et al., 2021),\nwhich uses a single transformer to embed both the image and text modalities, which we consider\nto be generating a \u201cuniversal latent space\u201d that projects any type of input into a single latent space.\nSuch a latent space would be useful for a model that could learn from many sources of supervision.\n4.2\nTransformers in transfer settings\nThere are also many works looking at transformers speci\ufb01cally in the context of in-modality trans-\nfer, such as ViT for vision (Dosovitskiy et al., 2020), T5 for language (Raffel et al., 2019), and\nUDSMProt for protein sequences (Strodthoff et al., 2020). CLIP (Radford et al., 2021) showed that\ntraining on text in addition to images could allow for zero-shot classi\ufb01cation via providing down-\nstream labels as text. Hernandez et al. (2021) do a thorough investigation of transfer with language\npretraining, notably showing transfer from English to Python, which they consider to be reasonably\ndistanced from English; many works have also looked at transferring from one langauge to another\n(Artetxe et al., 2019; Ponti et al., 2019). Similar to our work, Papadimitriou & Jurafsky (2020)\n13\n", []], "3.14 How well do the trends hold across other transformer models?": ["Initialization\nFrozen Layer Norm\nFinetuned Layer Norm\nPretrained\n61.5%\n68.2%\nRandom\n55.0%\n61.7%\nTable 16: Test accuracy on CIFAR-10 when only \ufb01netuning the input and output layer parameters.\n3.14\nHow well do the trends hold across other transformer models?\nWe also investigate how other transformer architectures perform when swapped out with GPT-2:\nBERT (Devlin et al., 2018), T5 (Raffel et al., 2019), and Longformer (Beltagy et al., 2020). For\nT5, we only use the encoder, and not the decoder. Our results are in Table 17. We \ufb01nd results to\nroughly hold across some architectures \u2013 with some differences \u2013 although T5 tends to be slightly\nworse than the other models. An interesting question for future work is whether subtle differences\nin architecture, pretraining objective, or dataset contribute to these differences.\nTask\nGPT-2 (FPT Default)\nBERT\nT5\nLongformer\nListOps\n38.4%\n38.3%\n15.4%\n17.0%\nCIFAR-10\n68.2%\n68.8%\n64.7%\n66.8%\nTable 17: Test accuracy for frozen pretrained transformer variants (base model sizes).\n4\nRelated Work and Discussion\n4.1\nTransformers in multimodal settings\nTransformers (Vaswani et al., 2017) were \ufb01rst used successfully for natural language processing\n(Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Brown et al., 2020). In recent\nyears, they have also been shown to be effective architectures for other modalities. One particular\nmodality of interest is computer vision (Chen et al., 2020a; Touvron et al., 2020); in particular,\nDosovitskiy et al. (2020) showed that transformers can outperform CNNs in the high-data regime on\nstandard object recognition benchmarks such as ImageNet and CIFAR. Furthermore, transformers\nhave also been used for prediction tasks over protein sequences (Jumper et al., 2021; Rao et al.,\n2021), reinforcement learning (Parisotto et al., 2020), and imitation learning (Abramson et al., 2020).\nWork speci\ufb01cally tackling multimodal tasks include Kaiser et al. (2017), who showed a single model\ncould learn a variety of multimodal tasks with an attention architecture. Recent work has utilized\ntransformers for multimodal predictive tasks, such as images and text in ViLBERT (Lu et al., 2019)\nand CLIP (Radford et al., 2021); these approaches generally use two distinct transformers to embed\nimages and text. Lu et al. (2020) applies ViLBERT to train a single model for a variety of combined\nvision and language tasks. Recent work from OpenAI (Goh et al., 2021) \ufb01nds that some neurons\nlearned by CLIP are activated by a particular semantic concept, regardless of if the concept is pre-\nsented in language or picture form. Our work is most similar to DALL-E (Ramesh et al., 2021),\nwhich uses a single transformer to embed both the image and text modalities, which we consider\nto be generating a \u201cuniversal latent space\u201d that projects any type of input into a single latent space.\nSuch a latent space would be useful for a model that could learn from many sources of supervision.\n4.2\nTransformers in transfer settings\nThere are also many works looking at transformers speci\ufb01cally in the context of in-modality trans-\nfer, such as ViT for vision (Dosovitskiy et al., 2020), T5 for language (Raffel et al., 2019), and\nUDSMProt for protein sequences (Strodthoff et al., 2020). CLIP (Radford et al., 2021) showed that\ntraining on text in addition to images could allow for zero-shot classi\ufb01cation via providing down-\nstream labels as text. Hernandez et al. (2021) do a thorough investigation of transfer with language\npretraining, notably showing transfer from English to Python, which they consider to be reasonably\ndistanced from English; many works have also looked at transferring from one langauge to another\n(Artetxe et al., 2019; Ponti et al., 2019). Similar to our work, Papadimitriou & Jurafsky (2020)\n13\n", []], "3.13 Is finetuning layer norm necessary for FPT to perform well?": ["optimization or learning rate scheme, although this is suboptimal. Our results are shown in Table\n13. Note that +Both is fully \ufb01netuning the 12-layer transformer (in other sections, we use full trans-\nformer to denote fully \ufb01netuning a transformer from scratch where the depth was tuned, whereas\nhere the depth is \ufb01xed). We \ufb01nd that \ufb01netuning the feedforward layers can improve performance,\nwhich is similar to techniques used in prior work (Houlsby et al., 2019), but \ufb01netuning the attention\nlayers can lead to divergence.\nModel\nMemory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nFPT\n100%\n100%\n38.4%\n98.0%\n68.2%\n38.6%\n12.7%\n+ Feedforward\n100%\n100%\n36.0%\n98.3%\n76.6%\n38.2%\n13.1%\n+ Attention\n100%\n100%\n36.8%\n89.0%\u2020\n47.7%\u2020\n23.0%\n10.9%\n+ Both\n100%\n100%\n35.8%\n93.1%\u2020\n32.9%\n21.0%\n10.5%\nTable 13: Additionally \ufb01netuning either the feedforward layers, attention layers, or both. We do not\nuse a per-layer learning scheme/etc. \u2020training diverged, number reported before divergence.\nOn CIFAR-10, we experiment with additionally \ufb01netuning the last attention layer, shown in Table\n14. Generally we \ufb01nd smarter pretraining methods can yield better performance, so we are optimistic\nabout the possibility of multimodal training/architectures improving performance in future work.\nTask\nBase (FPT)\n+ Finetuning All FF Layers\n+ Finetuning Last Attn Layer\nCIFAR-10\n68.2%\n76.6%\n80.0%\nTable 14: Test accuracy on CIFAR-10 when \ufb01netuning additional parameters. In addition to FPT, if\nwe \ufb01netune the feedforward layers and the last self-attention layer, we can achieve 80% accuracy.\n3.12\nWhich parameters of the model are important to \ufb01netune?\nWe now run ablations for only \ufb01netuning select parameters to see which parameters are most sensi-\ntive. Note for all experiments (including the previous ones), we initialize the input layers as Gaussian\nif embeddings are used, or use an orthogonal initialization for linear layers; in particular, we \ufb01nd\northogonal initialization to be very important when input parameters are not trained. We highlight\nsome results in Table 19; full results are shown on Page 16. Similar to a study of random CNNs by\nFrankle et al. (2020), we generally \ufb01nd the layer norm parameters to be most important.\nTask\noutput only\n+ layernorm\n+ input\n+ positions\nBit Memory\n76%\n94%\n100%\n100%\nBit XOR\n56%\n98%\n98%\n100%\nListOps\n15%\n36%\n36%\n38%\nMNIST\n23%\n96%\n98%\n98%\nCIFAR-10\n25%\n54%\n60%\n68%\nCIFAR-10 LRA\n17%\n39%\n39%\n39%\nHomology\n2%\n9%\n10%\n13%\nTable 15: Ablation by successively adding certain parameters to the list of \ufb01netuned parameters for\npretrained frozen transformers.\n3.13\nIs \ufb01netuning layer norm necessary for FPT to perform well?\nWhile previously we showed performance gains with \ufb01netuning layer norm, we could instead con-\nsider only \ufb01netuning the input and output layers, treating the entire GPT model as a black box. We\nshow results on CIFAR-10 in Table 16. The model performs worse; note accuracy is similar to not\n\ufb01netuning the positional embeddings (see Section 3.12). This suggests the internal modulation of\nthe af\ufb01ne layer norm parameters help, possibly by about as much as \ufb01ner positional information.\n12\n", []], "3.12 Which parameters of the model are important to finetune?": ["optimization or learning rate scheme, although this is suboptimal. Our results are shown in Table\n13. Note that +Both is fully \ufb01netuning the 12-layer transformer (in other sections, we use full trans-\nformer to denote fully \ufb01netuning a transformer from scratch where the depth was tuned, whereas\nhere the depth is \ufb01xed). We \ufb01nd that \ufb01netuning the feedforward layers can improve performance,\nwhich is similar to techniques used in prior work (Houlsby et al., 2019), but \ufb01netuning the attention\nlayers can lead to divergence.\nModel\nMemory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nFPT\n100%\n100%\n38.4%\n98.0%\n68.2%\n38.6%\n12.7%\n+ Feedforward\n100%\n100%\n36.0%\n98.3%\n76.6%\n38.2%\n13.1%\n+ Attention\n100%\n100%\n36.8%\n89.0%\u2020\n47.7%\u2020\n23.0%\n10.9%\n+ Both\n100%\n100%\n35.8%\n93.1%\u2020\n32.9%\n21.0%\n10.5%\nTable 13: Additionally \ufb01netuning either the feedforward layers, attention layers, or both. We do not\nuse a per-layer learning scheme/etc. \u2020training diverged, number reported before divergence.\nOn CIFAR-10, we experiment with additionally \ufb01netuning the last attention layer, shown in Table\n14. Generally we \ufb01nd smarter pretraining methods can yield better performance, so we are optimistic\nabout the possibility of multimodal training/architectures improving performance in future work.\nTask\nBase (FPT)\n+ Finetuning All FF Layers\n+ Finetuning Last Attn Layer\nCIFAR-10\n68.2%\n76.6%\n80.0%\nTable 14: Test accuracy on CIFAR-10 when \ufb01netuning additional parameters. In addition to FPT, if\nwe \ufb01netune the feedforward layers and the last self-attention layer, we can achieve 80% accuracy.\n3.12\nWhich parameters of the model are important to \ufb01netune?\nWe now run ablations for only \ufb01netuning select parameters to see which parameters are most sensi-\ntive. Note for all experiments (including the previous ones), we initialize the input layers as Gaussian\nif embeddings are used, or use an orthogonal initialization for linear layers; in particular, we \ufb01nd\northogonal initialization to be very important when input parameters are not trained. We highlight\nsome results in Table 19; full results are shown on Page 16. Similar to a study of random CNNs by\nFrankle et al. (2020), we generally \ufb01nd the layer norm parameters to be most important.\nTask\noutput only\n+ layernorm\n+ input\n+ positions\nBit Memory\n76%\n94%\n100%\n100%\nBit XOR\n56%\n98%\n98%\n100%\nListOps\n15%\n36%\n36%\n38%\nMNIST\n23%\n96%\n98%\n98%\nCIFAR-10\n25%\n54%\n60%\n68%\nCIFAR-10 LRA\n17%\n39%\n39%\n39%\nHomology\n2%\n9%\n10%\n13%\nTable 15: Ablation by successively adding certain parameters to the list of \ufb01netuned parameters for\npretrained frozen transformers.\n3.13\nIs \ufb01netuning layer norm necessary for FPT to perform well?\nWhile previously we showed performance gains with \ufb01netuning layer norm, we could instead con-\nsider only \ufb01netuning the input and output layers, treating the entire GPT model as a black box. We\nshow results on CIFAR-10 in Table 16. The model performs worse; note accuracy is similar to not\n\ufb01netuning the positional embeddings (see Section 3.12). This suggests the internal modulation of\nthe af\ufb01ne layer norm parameters help, possibly by about as much as \ufb01ner positional information.\n12\n", []], "3.11 Can training more parameters improve performance?": ["3.10\nWhat is the role of model depth in token mixing?\nOne interesting question is the importance of the depth of the transformer for generating represen-\ntions which \u201cmix\u201d tokens: for instance, if there is only one layer and the parameters are random, it\nis unlikely for the tokens to be mixed well, whereas if there are many layers, there are many chances\nfor the tokens to mix and form interesting representations useful for downstream tasks. We inves-\ntigate this on ListOps by considering pretrained vs random models, where we only take the \ufb01rst X\nlayers of the 12-layer pretrained model (i.e. for X=3, we use the \ufb01rst 3 layers of the pretrained GPT-2\nmodel and perform classi\ufb01cation from those hidden states). Additionally, to maximally highlight the\nimportance of the pretrained parameters, we randomly initialize the input layer, and do not train the\ninput or positional parameters. We \ufb01rst show results are \ufb01netuning the output layer and layernorm\nparameters, and then show only \ufb01netuning the output layer.\nWith \ufb01netuning layernorm. We \ufb01rst investigate this question with \ufb01netuning the layernorm pa-\nrameters (i.e. we \ufb01netune only the output layer and the layernorm parameters). Results are shown in\nTable 11. Both models are unable to do well with only one layer, but the pretrained model performs\nsigni\ufb01cantly better than the random model at 2 layers, indicating that while the difference in per-\nformance at 12 layers is relatively small, there is a great bene\ufb01t to using pretrained layers for when\nconsidering a small number of layers in that the tokens are \u201cmixed\u201d faster.\nNumber of Layers\nPretrained\nRandom\n1\n17%\n17%\n2\n36%\n16%\n6\n38%\n35%\nTable 11: Test accuracy on Listops while varying model depth and \ufb01netuning layernorm parameters.\nPretrained layers \u201cmix\u201d the tokens faster, performing better at low model depths.\nWithout \ufb01netuning layernorm. We now investigate this question without \ufb01netuning the layernorm\nparameters, and only \ufb01netuning the output parameters, as in the reservoir computing setup in Section\n3.9. Note this is equivalent to linear classi\ufb01cation. This setting is the most challenging since all\nprocessing that is able to mix tokens is done by either random or pretrained parameters, and we\nare only able to train a linear layer on top of the output of the last token; as a result, the only token\nmixing that is done is performed entirely by the pretrained self-attention layers. Results are shown in\nTable 12. The random model does not do well even for a large number of layers, while the pretrained\nmodel can still do reasonably well, even though it requires more layers than before.\nNumber of Layers\nPretrained\nRandom\n1\n12%\n-\n3\n18%\n-\n6\n33%\n-\n12\n33%\n17%\n24\n-\n17%\nTable 12: Test accuracy on Listops while varying model depth and only training output parameters.\nEven for a large number of layers, the random model does not learn to perform well.\n3.11\nCan training more parameters improve performance?\nOur focus in this work was primarily to investigate if and how ef\ufb01cient, general-purpose pretraining\ncan transfer across modalities. However, for practical applications, it would naturally be more suited\nto choose a more specialized \ufb01netuning scheme or add more trainable parameters. In this section,\nwe investigate additionally \ufb01netuning parameters with various methods, to see if frozen language\ntransformers can serve as a practical base for future work.\nWe \ufb01rst investigate additionally \ufb01netuning the self-attention and feedforward layers, which were\npreviously frozen. We simply add them to the list of parameters \ufb01netuned, without changing the\n11\n", []], "3.10 What is the role of model depth in token mixing?": ["3.10\nWhat is the role of model depth in token mixing?\nOne interesting question is the importance of the depth of the transformer for generating represen-\ntions which \u201cmix\u201d tokens: for instance, if there is only one layer and the parameters are random, it\nis unlikely for the tokens to be mixed well, whereas if there are many layers, there are many chances\nfor the tokens to mix and form interesting representations useful for downstream tasks. We inves-\ntigate this on ListOps by considering pretrained vs random models, where we only take the \ufb01rst X\nlayers of the 12-layer pretrained model (i.e. for X=3, we use the \ufb01rst 3 layers of the pretrained GPT-2\nmodel and perform classi\ufb01cation from those hidden states). Additionally, to maximally highlight the\nimportance of the pretrained parameters, we randomly initialize the input layer, and do not train the\ninput or positional parameters. We \ufb01rst show results are \ufb01netuning the output layer and layernorm\nparameters, and then show only \ufb01netuning the output layer.\nWith \ufb01netuning layernorm. We \ufb01rst investigate this question with \ufb01netuning the layernorm pa-\nrameters (i.e. we \ufb01netune only the output layer and the layernorm parameters). Results are shown in\nTable 11. Both models are unable to do well with only one layer, but the pretrained model performs\nsigni\ufb01cantly better than the random model at 2 layers, indicating that while the difference in per-\nformance at 12 layers is relatively small, there is a great bene\ufb01t to using pretrained layers for when\nconsidering a small number of layers in that the tokens are \u201cmixed\u201d faster.\nNumber of Layers\nPretrained\nRandom\n1\n17%\n17%\n2\n36%\n16%\n6\n38%\n35%\nTable 11: Test accuracy on Listops while varying model depth and \ufb01netuning layernorm parameters.\nPretrained layers \u201cmix\u201d the tokens faster, performing better at low model depths.\nWithout \ufb01netuning layernorm. We now investigate this question without \ufb01netuning the layernorm\nparameters, and only \ufb01netuning the output parameters, as in the reservoir computing setup in Section\n3.9. Note this is equivalent to linear classi\ufb01cation. This setting is the most challenging since all\nprocessing that is able to mix tokens is done by either random or pretrained parameters, and we\nare only able to train a linear layer on top of the output of the last token; as a result, the only token\nmixing that is done is performed entirely by the pretrained self-attention layers. Results are shown in\nTable 12. The random model does not do well even for a large number of layers, while the pretrained\nmodel can still do reasonably well, even though it requires more layers than before.\nNumber of Layers\nPretrained\nRandom\n1\n12%\n-\n3\n18%\n-\n6\n33%\n-\n12\n33%\n17%\n24\n-\n17%\nTable 12: Test accuracy on Listops while varying model depth and only training output parameters.\nEven for a large number of layers, the random model does not learn to perform well.\n3.11\nCan training more parameters improve performance?\nOur focus in this work was primarily to investigate if and how ef\ufb01cient, general-purpose pretraining\ncan transfer across modalities. However, for practical applications, it would naturally be more suited\nto choose a more specialized \ufb01netuning scheme or add more trainable parameters. In this section,\nwe investigate additionally \ufb01netuning parameters with various methods, to see if frozen language\ntransformers can serve as a practical base for future work.\nWe \ufb01rst investigate additionally \ufb01netuning the self-attention and feedforward layers, which were\npreviously frozen. We simply add them to the list of parameters \ufb01netuned, without changing the\n11\n", []], "3.9 Can we train a transformer by only finetuning the output layer?": ["3.8\nCan performance be attributed simply to better statistics for initialization?\nIn this section, we ablate taking the layer-wise mean and standard deviation from the pretrained\nmodel and using it to initialize a random transformer, in order to ablate if a better initialization\nscheme via an \u201coracle\u201d standard deviation can recover the performance of FPT. Note that the GPT-2\ninitialization scheme initializes parameters as Gaussian; traditionally, the standard deviation is 0.02\nby default. For clarity, we show the standard deviation by layer for the weights and biases of the\nattention and feedforward layers in Figure 6 for the pretrained models.\n0\n5\n10\n0.1\n0.2\nattn.c_attn.weight\n0\n5\n10\n0.05\n0.10\n0.15\nattn.c_proj.weight\n0\n5\n10\n0.05\n0.10\nmlp.c_fc.weight\n0\n5\n10\n0.1\n0.2\nmlp.c_proj.weight\n0\n5\n10\nLayer\n0.00\n0.25\nattn.c_proj.bias\n0\n5\n10\nLayer\n0.0\n0.1\nmlp.c_fc.bias\n0\n5\n10\nLayer\n0.0\n0.1\nmlp.c_proj.bias\nPretrained Statistics\nDefault Random Statistics\nFigure 6: Standard deviation of the parameters by layer for the pretrained GPT-2 model versus\ndefault initialization hyperparameters (0.02 for weights and 0 for biases).\nWe show the results using this initialization scheme in Table 9 (note that all of the weights, biases,\nlayer norm, and positional embeddings are initialized \u2013 both mean and variance \u2013 in this fashion).\nThis yields better results on most tasks, but does poorly on CIFAR-10. As a result, we believe\nthe bene\ufb01ts of language pretraining cannot be recovered with a simple better initialization scheme,\nalthough we believe future work in transformer initialization could yield different results.\nInitialization\nMemory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nPretrained\n100%\n100%\n38.4%\n98.0%\n68.2%\n38.6%\n12.7%\nStatistics Only\n100%\n100%\n37.4%\n97.2%\n56.5%\n33.1%\n11.0%\nDefault\n75.8%\n100%\n34.3%\n91.7%\n61.7%\n36.1%\n9.3%\nTable 9: Test accuracy when initializing parameters with pretrained weights (i.e., FPT) vs randomly\ninitializing parameters according to the mean and variance of the pretrained transformer (Statistics\nOnly) vs random initialization with default parameters (Default).\n3.9\nCan we train a transformer by only \ufb01netuning the output layer?\nWe consider using FPT solely for naive feature extraction for linear classi\ufb01cation, where we \ufb01x a\nrandomly initialized input layer and freeze all parts of the model except for the output. Note that\nthis resembles resevoir computing/echo state networks (see Section 4.5 for discussion). The model\nevaluates on every example in the training set once, caches the features, and then we train a linear\noutput layer. This enables subsequent epochs after the \ufb01rst to run extremely quickly, but does not\neasily handle dropout/data augmentations, and scales well in terms of number of epochs, but not\nin dataset size. Note that this is mathematically equivalent to linear classi\ufb01cation. Our results are\nshown in Table 10. Although we \ufb01nd speedups extremely signi\ufb01cant and they obtain nontrivial\nperformance, performance signi\ufb01cantly degrades and the models also exhibit over\ufb01tting (likely due\nto lack of regularization; unlike the training of FPT, dropout is not applied).\nTask\nSpeedup\nOutput Only\nFPT\nFull Transformer\nListOps\n500 \u22122000\u00d7\n32.8%\n38.4%\n38%\nCIFAR-10 LRA\n500 \u22122000\u00d7\n24.7%\n38.6%\n42%\nTable 10: Training only the output layer as a linear regression problem. Speedup refers to wall clock\ntime per epoch (after the \ufb01rst). Larger models have larger speedups.\n10\n", []], "3.8 Can performance be attributed simply to better statistics for initialization?": ["3.8\nCan performance be attributed simply to better statistics for initialization?\nIn this section, we ablate taking the layer-wise mean and standard deviation from the pretrained\nmodel and using it to initialize a random transformer, in order to ablate if a better initialization\nscheme via an \u201coracle\u201d standard deviation can recover the performance of FPT. Note that the GPT-2\ninitialization scheme initializes parameters as Gaussian; traditionally, the standard deviation is 0.02\nby default. For clarity, we show the standard deviation by layer for the weights and biases of the\nattention and feedforward layers in Figure 6 for the pretrained models.\n0\n5\n10\n0.1\n0.2\nattn.c_attn.weight\n0\n5\n10\n0.05\n0.10\n0.15\nattn.c_proj.weight\n0\n5\n10\n0.05\n0.10\nmlp.c_fc.weight\n0\n5\n10\n0.1\n0.2\nmlp.c_proj.weight\n0\n5\n10\nLayer\n0.00\n0.25\nattn.c_proj.bias\n0\n5\n10\nLayer\n0.0\n0.1\nmlp.c_fc.bias\n0\n5\n10\nLayer\n0.0\n0.1\nmlp.c_proj.bias\nPretrained Statistics\nDefault Random Statistics\nFigure 6: Standard deviation of the parameters by layer for the pretrained GPT-2 model versus\ndefault initialization hyperparameters (0.02 for weights and 0 for biases).\nWe show the results using this initialization scheme in Table 9 (note that all of the weights, biases,\nlayer norm, and positional embeddings are initialized \u2013 both mean and variance \u2013 in this fashion).\nThis yields better results on most tasks, but does poorly on CIFAR-10. As a result, we believe\nthe bene\ufb01ts of language pretraining cannot be recovered with a simple better initialization scheme,\nalthough we believe future work in transformer initialization could yield different results.\nInitialization\nMemory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nPretrained\n100%\n100%\n38.4%\n98.0%\n68.2%\n38.6%\n12.7%\nStatistics Only\n100%\n100%\n37.4%\n97.2%\n56.5%\n33.1%\n11.0%\nDefault\n75.8%\n100%\n34.3%\n91.7%\n61.7%\n36.1%\n9.3%\nTable 9: Test accuracy when initializing parameters with pretrained weights (i.e., FPT) vs randomly\ninitializing parameters according to the mean and variance of the pretrained transformer (Statistics\nOnly) vs random initialization with default parameters (Default).\n3.9\nCan we train a transformer by only \ufb01netuning the output layer?\nWe consider using FPT solely for naive feature extraction for linear classi\ufb01cation, where we \ufb01x a\nrandomly initialized input layer and freeze all parts of the model except for the output. Note that\nthis resembles resevoir computing/echo state networks (see Section 4.5 for discussion). The model\nevaluates on every example in the training set once, caches the features, and then we train a linear\noutput layer. This enables subsequent epochs after the \ufb01rst to run extremely quickly, but does not\neasily handle dropout/data augmentations, and scales well in terms of number of epochs, but not\nin dataset size. Note that this is mathematically equivalent to linear classi\ufb01cation. Our results are\nshown in Table 10. Although we \ufb01nd speedups extremely signi\ufb01cant and they obtain nontrivial\nperformance, performance signi\ufb01cantly degrades and the models also exhibit over\ufb01tting (likely due\nto lack of regularization; unlike the training of FPT, dropout is not applied).\nTask\nSpeedup\nOutput Only\nFPT\nFull Transformer\nListOps\n500 \u22122000\u00d7\n32.8%\n38.4%\n38%\nCIFAR-10 LRA\n500 \u22122000\u00d7\n24.7%\n38.6%\n42%\nTable 10: Training only the output layer as a linear regression problem. Speedup refers to wall clock\ntime per epoch (after the \ufb01rst). Larger models have larger speedups.\n10\n", []], "3.7 Does performance scale with model size?": ["We also include the attention map for Bit XOR using a randomly initialized transformer (which also\nsolves the task) in Figure 5. This model also learns to exploit the diagonal pattern, although the\nstrength is a little weaker. This indicates that while the random transformer still learns to solve the\ntask, it learns a less semantically interpretable/strong attention pattern.\n0\n2\n4\n6\n8\nInput Token\n0\n2\n4\n6\n8\nOutput Token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: A transformer with frozen randomly initialized self-attention layers also learns to correlate\nthe two diagonal elements on Bit XOR, although the magnitude of the diagonals is lower (note the\nextra attention weights distributed in between the diagonals).\n3.6\nDoes freezing the transformer prevent over\ufb01tting or under\ufb01tting?\nOur general \ufb01ndings are that \u2013 in contrast to their fully trained counterparts \u2013 FPT models under\ufb01t\nthe data, which lends them to further improvements by increasing model capacity (see Section 3.7).\nFor example, consider CIFAR-10 LRA, which is maximally dif\ufb01cult due to lack of inductive prior\nover the sequence (each pixel is fed in as an arbitrary token only ordered by a raster scan) and rel-\natively small dataset (50k images). In Table 7, we show the train/test gap between training FPT vs\na 3-layer transformer from Tay et al. (2020), which we \ufb01nd to give stronger results than our experi-\nments. In particular, they are much better than training a 12-layer transformer, which works poorly.\nOur results indicate that FPT is generally providing generalizable task representations without caus-\ning over\ufb01tting, whereas transformers can over\ufb01t arbitrarily poorly in low-data regimes (such as for\nLinformer, which over\ufb01t the most out of the architectures tested by Tay et al. (2020)). More work\ncan investigate how to increase the model expressiveness, which could yield performance bene\ufb01ts.\nModel\n# Layers\nTest Accuracy\nTrain Accuracy\nFPT (GPT-2)\n12\n38.6%\n38.5%\nVanilla Transformer\n3\n42%\n70%\nLinformer\n3\n39%\n97%\nTable 7: Train vs test accuracies on CIFAR-10 LRA task.\n3.7\nDoes performance scale with model size?\nWe evaluate the ef\ufb01cacy of adding more parameters to these models on CIFAR-10. Most of the\nadditional parameters are in the transformer layers and are trained during the natural language pre-\ntraining phase. Our results for pretrained and random models are in Table 8. Unlike fully training\na transformer, which exhibits more over\ufb01tting and divergence during training with larger models,\nincreasing model size stably increases the capacity of the models. This result indicates our observa-\ntions and results are likely to scale as we move towards larger models and higher-data regimes.\nModel Size\n# Layers\nTotal Params\nTrained Params\nFPT\nRandom\nSmall (Base)\n12\n117M\n106K\n68.2%\n61.7%\nMedium\n24\n345M\n190K\n69.8%\n64.0%\nLarge\n36\n774M\n300K\n72.1%\n65.7%\nTable 8: Test accuracy of larger frozen transformer models on CIFAR-10.\n9\n", [591, 592]], "3.6 Does freezing the transformer prevent overfitting or underfitting?": ["We also include the attention map for Bit XOR using a randomly initialized transformer (which also\nsolves the task) in Figure 5. This model also learns to exploit the diagonal pattern, although the\nstrength is a little weaker. This indicates that while the random transformer still learns to solve the\ntask, it learns a less semantically interpretable/strong attention pattern.\n0\n2\n4\n6\n8\nInput Token\n0\n2\n4\n6\n8\nOutput Token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: A transformer with frozen randomly initialized self-attention layers also learns to correlate\nthe two diagonal elements on Bit XOR, although the magnitude of the diagonals is lower (note the\nextra attention weights distributed in between the diagonals).\n3.6\nDoes freezing the transformer prevent over\ufb01tting or under\ufb01tting?\nOur general \ufb01ndings are that \u2013 in contrast to their fully trained counterparts \u2013 FPT models under\ufb01t\nthe data, which lends them to further improvements by increasing model capacity (see Section 3.7).\nFor example, consider CIFAR-10 LRA, which is maximally dif\ufb01cult due to lack of inductive prior\nover the sequence (each pixel is fed in as an arbitrary token only ordered by a raster scan) and rel-\natively small dataset (50k images). In Table 7, we show the train/test gap between training FPT vs\na 3-layer transformer from Tay et al. (2020), which we \ufb01nd to give stronger results than our experi-\nments. In particular, they are much better than training a 12-layer transformer, which works poorly.\nOur results indicate that FPT is generally providing generalizable task representations without caus-\ning over\ufb01tting, whereas transformers can over\ufb01t arbitrarily poorly in low-data regimes (such as for\nLinformer, which over\ufb01t the most out of the architectures tested by Tay et al. (2020)). More work\ncan investigate how to increase the model expressiveness, which could yield performance bene\ufb01ts.\nModel\n# Layers\nTest Accuracy\nTrain Accuracy\nFPT (GPT-2)\n12\n38.6%\n38.5%\nVanilla Transformer\n3\n42%\n70%\nLinformer\n3\n39%\n97%\nTable 7: Train vs test accuracies on CIFAR-10 LRA task.\n3.7\nDoes performance scale with model size?\nWe evaluate the ef\ufb01cacy of adding more parameters to these models on CIFAR-10. Most of the\nadditional parameters are in the transformer layers and are trained during the natural language pre-\ntraining phase. Our results for pretrained and random models are in Table 8. Unlike fully training\na transformer, which exhibits more over\ufb01tting and divergence during training with larger models,\nincreasing model size stably increases the capacity of the models. This result indicates our observa-\ntions and results are likely to scale as we move towards larger models and higher-data regimes.\nModel Size\n# Layers\nTotal Params\nTrained Params\nFPT\nRandom\nSmall (Base)\n12\n117M\n106K\n68.2%\n61.7%\nMedium\n24\n345M\n190K\n69.8%\n64.0%\nLarge\n36\n774M\n300K\n72.1%\n65.7%\nTable 8: Test accuracy of larger frozen transformer models on CIFAR-10.\n9\n", [591, 592]], "3.5 Do the frozen attention layers attend to modality-specific tokens?": ["3.4\nDoes language pretraining improve compute ef\ufb01ciency over random initialization?\nWe investigate compute ef\ufb01ciency by considering the number of gradient steps to converge for FPT\nvs random transformer models, shown in Table 6. We generally \ufb01nd FPT converges faster, which\nindicates language pretrainining can yield compute bene\ufb01ts for non-language tasks. While random\ntransformer models achieve decent test accuracies, in particular when compared to random LSTMs,\nthere is still a considerable gap in the compute ef\ufb01ciency compared to using pretraining. Note that bit\nmemory pretraining introduced in Section 3.2 generally falls between the two models, and notably\nis 6\u00d7 slower than FPT on Bit XOR, which is signi\ufb01cantly better than random.\nModel\nMemory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nFPT\n1 \u00d7 104\n5 \u00d7 102\n2 \u00d7 103\n5 \u00d7 103\n4 \u00d7 105\n3 \u00d7 105\n1 \u00d7 105\nRandom\n4 \u00d7 104\n2 \u00d7 104\n6 \u00d7 103\n2 \u00d7 104\n4 \u00d7 105\n6 \u00d7 105\n1 \u00d7 105\nSpeedup\n4\u00d7\n40\u00d7\n3\u00d7\n4\u00d7\n1\u00d7\n2\u00d7\n1\u00d7\nTable 6: Approximate number of gradient steps until convergence for pretrained (FPT) vs randomly\ninitialized (Random) models. Note that we use the same batch size and learning rate for both models.\n3.5\nDo the frozen attention layers attend to modality-speci\ufb01c tokens?\nWe investigate if FPT attends to semantically meaningful patterns in the data. We plot the attention\nweights (i.e. the values of the softmax of query-key dot product) from the \ufb01rst layer. We show the\nresults in Figures 3 and 4 for the bit tasks. Note GPT-2 is autoregressive, so the upper right corner\nof the attention mask is zeroed out. On these tasks, FPT yields an interpretable attention pattern\ndespite not training the self-attention layers themselves. We did not \ufb01nd easily interpretable patterns\non the other tasks.\n0\n2\n4\n6\n8\nInput Token\n0\n2\n4\n6\n8\nOutput Token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n                       0101111001 => 1\n                       0101111001 => 0\n                       0101111001 => 0\n                       0101111001 => 1\n                       0101111001 => 0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\nString 1 String 2\nFigure 3: On Bit XOR, the model must produce the element-wise XOR of two bitstrings presented\nsequentially (inputs 0-4 are the \ufb01rst bitstring, inputs 5-9 are the second). Each token is one bit. FPT\nlearns to attend positionally to the two bits that are XOR\u2019ed by the output token.\n0\n20 40 60 80 100120\nInput Token\n0\n20\n40\n60\n80\n100\n120\nOutput Token\nMasked String Is 1\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 2\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 3\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 4\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 5\nFigure 4: On Bit Memory, the model must return one of \ufb01ve strings (inputs 0-99) given a masked\nversion of one of the strings (inputs 100-119). Each token is 50 bits. FPT learns to attend to the\ncorrect string based on \ufb01nding similarity to the inputs, not relying solely on position as in Bit XOR.\n8\n", [505, 524, 528, 529, 530, 531, 532, 591, 592]], "3.4 Does language pretraining improve compute efficiency over random initialization?": ["3.4\nDoes language pretraining improve compute ef\ufb01ciency over random initialization?\nWe investigate compute ef\ufb01ciency by considering the number of gradient steps to converge for FPT\nvs random transformer models, shown in Table 6. We generally \ufb01nd FPT converges faster, which\nindicates language pretrainining can yield compute bene\ufb01ts for non-language tasks. While random\ntransformer models achieve decent test accuracies, in particular when compared to random LSTMs,\nthere is still a considerable gap in the compute ef\ufb01ciency compared to using pretraining. Note that bit\nmemory pretraining introduced in Section 3.2 generally falls between the two models, and notably\nis 6\u00d7 slower than FPT on Bit XOR, which is signi\ufb01cantly better than random.\nModel\nMemory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nFPT\n1 \u00d7 104\n5 \u00d7 102\n2 \u00d7 103\n5 \u00d7 103\n4 \u00d7 105\n3 \u00d7 105\n1 \u00d7 105\nRandom\n4 \u00d7 104\n2 \u00d7 104\n6 \u00d7 103\n2 \u00d7 104\n4 \u00d7 105\n6 \u00d7 105\n1 \u00d7 105\nSpeedup\n4\u00d7\n40\u00d7\n3\u00d7\n4\u00d7\n1\u00d7\n2\u00d7\n1\u00d7\nTable 6: Approximate number of gradient steps until convergence for pretrained (FPT) vs randomly\ninitialized (Random) models. Note that we use the same batch size and learning rate for both models.\n3.5\nDo the frozen attention layers attend to modality-speci\ufb01c tokens?\nWe investigate if FPT attends to semantically meaningful patterns in the data. We plot the attention\nweights (i.e. the values of the softmax of query-key dot product) from the \ufb01rst layer. We show the\nresults in Figures 3 and 4 for the bit tasks. Note GPT-2 is autoregressive, so the upper right corner\nof the attention mask is zeroed out. On these tasks, FPT yields an interpretable attention pattern\ndespite not training the self-attention layers themselves. We did not \ufb01nd easily interpretable patterns\non the other tasks.\n0\n2\n4\n6\n8\nInput Token\n0\n2\n4\n6\n8\nOutput Token\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n                       0101111001 => 1\n                       0101111001 => 0\n                       0101111001 => 0\n                       0101111001 => 1\n                       0101111001 => 0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n1\nString 1 String 2\nFigure 3: On Bit XOR, the model must produce the element-wise XOR of two bitstrings presented\nsequentially (inputs 0-4 are the \ufb01rst bitstring, inputs 5-9 are the second). Each token is one bit. FPT\nlearns to attend positionally to the two bits that are XOR\u2019ed by the output token.\n0\n20 40 60 80 100120\nInput Token\n0\n20\n40\n60\n80\n100\n120\nOutput Token\nMasked String Is 1\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 2\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 3\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 4\n0\n20 40 60 80 100120\nInput Token\nMasked String Is 5\nFigure 4: On Bit Memory, the model must return one of \ufb01ve strings (inputs 0-99) given a masked\nversion of one of the strings (inputs 100-119). Each token is 50 bits. FPT learns to attend to the\ncorrect string based on \ufb01nding similarity to the inputs, not relying solely on position as in Bit XOR.\n8\n", [505, 524, 528, 529, 530, 531, 532]], "3.3 How important is the transformer architecture compared to LSTM architecture?": ["3.3\nHow important is the transformer architecture compared to LSTM architecture?\nIn Section 3.2 we found the transformer architecture can already be fairly effective in this regime,\neven with only random parameters. In this section, we consider using a random LSTM architec-\nture instead of the transformer, allowing us to consider the raw effect of architecture and ablating\npretraining. Like FPT, we \ufb01netune the input, output, and layernorm parameters for the LSTMs.\nModel\nBit Memory\nXOR\nListOps\nMNIST\nCIFAR-10\nC10 LRA\nHomology\nTrans.\n75.8%\n100%\n34.3%\n91.7%\n61.7%\n36.1%\n9.3%\nLSTM\n50.9%\n50.0%\n16.8%\n70.9%\n34.4%\n10.4%\n6.6%\nLSTM\u2217\n75.0%\n50.0%\n16.7%\n92.5%\n43.5%\n10.6%\n8.6%\nTable 3: Test accuracy of randomly initialized transformers vs randomly initialized LSTM models.\nNote unlike in Figure 1, the LSTM here is frozen. Frozen LSTMs perform very poorly. LSTM\u2217rep-\nresents an LSTM with additional architecture improvements to match the transformers (see below).\nOur results are shown in Table 3. \u201cLSTM\u201d refers to a 3-layer \u201cstandard\u201d LSTM with a hidden\ndimension of 768, matching standard implementations of LSTMs, without residual connections or\npositional embeddings (see discussion below). This matches the width of the FPT models, but not\nthe depth or total parameter count (note that LSTMs also do not have positional embeddings). We\n\ufb01nd that the self-attention architecture already serves as an effective inductive bias for universal\ncomputation, improving signi\ufb01cantly over the recurrent LSTM model and comprising most of the\nimprovement in test accuracy from random LSTM to FPT.\nHere, we compare the 3-layer \u201cstandard\u201d LSTM to a 12-layer \u201cstandard\u201d LSTM. Note that most\nLSTM implementations, including the one used in Table 3, do not feature residual connections\nand positional embeddings. We include this comparison to represent the traditional method more\nfaithfully, but add these additional architectural components below. In the same style of FPT and\nGPT-2, we do not use a bidirectional LSTM. Under these model choices, we report the performance\nof a frozen random 3-layer vs 12-layer LSTM in Table 4. Naively, the 12-layer model is much worse\nthan the 3-layer model, hinting that there is some loss of information by repeated LSTM layers.\nLayers\nListOps\nMNIST\nCIFAR-10\nC10 LRA\n12\n16.2%\n11.7%\n10.8%\n10.4%\n3\n16.8%\n70.9%\n34.4%\n10.4%\nTable 4: Test accuracy of randomly initialized \u201cstandard\u201d LSTMs varying number of layers with a\nhidden dimension of 768. The simple 12-layer LSTM achieves only near-trivial performance.\nWe also experiment with ablating other architectural improvements included with the transformer\narchitecture in Table 5. Once residual connections (He et al., 2016) are added, the 12-layer LSTM\nmakes up a lot of the performance drops, hinting that residual connections could make up for loss\nof information from the LSTM layers which otherwise linearly combine the features. We also add\npositional embeddings, which \ufb01nishes bridging the gap between standard LSTM implementations\nand the transformer. Even with these additional bene\ufb01ts, the LSTM still performs worse. Note that\nthe \ufb01nal 12-layer LSTM has about the same number of trainable parameters as the transformer.\nModel\nListOps\nMNIST\nCIFAR-10\nC10 LRA\n12-Layer LSTM\n16.2%\n11.7%\n10.8%\n10.4%\n+ Residual Connections\n16.8%\n70.9%\n34.4%\n10.4%\n+ Positional Embeddings\n16.7%\n92.5%\n43.5%\n10.6%\nRandom Transformer\n34.3%\n91.7%\n61.7%\n36.1%\nTable 5: Test accuracy of 12-layer randomly initialized \u201cstandard\u201d LSTMs additional architectures\nmodi\ufb01cations to match transformers: residual connections and positional embeddings. The bottom\nrow, LSTM with residual connections and positional embeddings, is nearly identical to GPT-2.\n7\n", [505, 524, 528, 529, 530, 531, 532]], "3.2 What is the importance of the pretraining modality?": ["analysis. In particular, the numbers from Tay et al. (2020) are generated from \u201cextensive sweeps\nover different hyper-parameters\u201d and use task-speci\ufb01c hyperparameters, while we do not tune the\nhyperparameters for FPT (except for remote homology; see Appendix C). In contrast, we \ufb01nd it is\neasy to improve the performance of FPT by increasing model size (see Section 3.7) \u2013 the CIFAR-10\nnumber for FPT here is for the 36-layer large model.\nFurthermore, unlike some other works utilizing transformers for vision, we use minimal spatial bias\nto emphasize the universal sequential aspect of the problem \u2013 for instance, we do not interleave self-\nattention and convolution layers. Note that we also do not use 2D positional embeddings (or other\ndomain-speci\ufb01c techniques), hence providing very weak inductive prior to the model. Our reasoning\nfor these decisions is to evaluate the ability of transformers to work on arbitrary sequential tasks.\n3.2\nWhat is the importance of the pretraining modality?\nWe now compare pretraining on language to other pretraining methods for base model sizes:\n\u2022 Random initialization (Random): initialization of the frozen transformer parameters randomly\nusing the default initialization choices for GPT-2, i.e. without pretraining.\n\u2022 Bit memory pretraining (Bit): pretraining from scratch on the Bit Memory task and then freezing\nthe parameters before transferring. This allows the transformer to gain supervision working with\narbitrary bit strings and performing memory/denoising on independent inputs.\n\u2022 Image pretraining (ViT): using a pretrained Vision Transformer (Dosovitskiy et al., 2020) pre-\ntrained on ImageNet-21k (Deng et al., 2009). Note that the architecture is a bit different, notably\nnot using the autoregressive masking of GPT-2, since ViT is only pretrained on classi\ufb01cation tasks\n(for other details, see Appendix D.2).\nThese experiments highlight the signi\ufb01cance of pretraining \u2013 as opposed to simply the transformer\narchitecture \u2013 and compare language to other methods of supervision. Our results are shown in Table\n2. Although the random transformers can achieve surprisingly strong accuracies, there is a consid-\nerable gap to using natural language pretraining, such as in MNIST, where random transformers\nachieve similar performance to a linear classi\ufb01er on top of raw features (92%). Thus we believe that\nwhile the transformer architecture might be naturally conducive to these evaluations, the attention\nmechanisms used to transfer may be nontrivial and not fully speci\ufb01ed by the architecture. We also\n\ufb01nd that, in addition to performance bene\ufb01ts, language pretraining improves convergence compared\nto the randomly initialized transformer (see Section 3.4).\nModel\nBit Memory\nXOR\nListOps\nMNIST\nC10\nC10 LRA\nHomology\nFPT\n100%\n100%\n38.4%\n98.0%\n68.2%\n38.6%\n12.7%\nRandom\n75.8%\n100%\n34.3%\n91.7%\n61.7%\n36.1%\n9.3%\nBit\n100%\n100%\n35.4%\n97.8%\n62.6%\n36.7%\n7.8%\nViT\n100%\n100%\n37.4%\n97.8%\n72.5%\n43.0%\n7.5%\nTable 2: Test accuracy of language-pretrained (FPT) vs randomly initialized (Random) vs Bit Mem-\nory pretraining (Bit) vs pretrained Vision Transformer (ViT) models. The transformer is frozen.\nPretraining on bit memory improves performance compared to the random models, but still lags\nbehind training on natural language data. Furthermore, measured by gradient steps, all models\nconverge faster than the randomly initialized transformers (more details in Section 3.4), indicating\nthat all modes of pretraining improve upon random initialization even without considering accuracy.\nAdditionally, while freezing a vision transformer yields better improvements on CIFAR-10, pretrain-\ning on images is not uniformly better; e.g., ViT is worse on protein classi\ufb01cation. One hypothesis is\nthat protein sequences are structured like language, in terms of discrete units of information with a\n\u201cgrammar\u201d, so transfer from language to proteins may be more natural.\n6\n", []], "3.1 Can pretrained language models transfer to different modalities?": ["Given the cheap linear scaling of these parameters, the parameter counts of large transformer models\nare dominated by the quadratic (in ndim and l) self-attention and feedforward layers. For the base\nCIFAR-10 model with 124M parameters, these come out to approximately 0.086% of the network.\nDue to this scaling, this number decreases with larger model sizes, down to 0.029% of the GPT-2\nXL model. We further ablate the importance of each parameter in Section 3.12. For more details\nand a description of the architecture, see Appendix B.\nNote that, crucially, all communication between tokens in the model are frozen. The data in each\ndatapoint is chunked into discrete tokens (bits, image patches, amino acids, etc.), and can only\nreference each other via the frozen attention connections, which are not trained; additionally, neither\nthe output nor the input layers are connected to multiple tokens. Our key investigation is to analyze\nthe computation that is already inherent in the language model, and hence we do a minimal amount\nof computation that is learned on the downstream modality.\n3\nEmpirical Evaluations\nIn this section, we review the results demonstrating transfer from language to other modalities, and\nseek to better understand why this occurs and what enables this transfer. All model sizes are the\nbase model size (12 layers, 768 hidden dimension), unless stated otherwise. See Appendix C for\nmore details on experiments.\n3.1\nCan pretrained language models transfer to different modalities?\nWe investigate if the self-attention and feedforward layers \u2013 the main body \u2013 of a pretrained trans-\nformer can be applied to a classi\ufb01cation problem in a different modality without \ufb01netuning. To\ndo this, we apply our base procedure as described above, where the input embedding layer, output\nreadout layer, and layer norm parameters are \ufb01netuned.\nOur results are shown in Figure 1 and also summarized below in Table 1. We compare to state-of-\nthe-art from literature when available (full transformer on ListOps, CIFAR-10 LRA, and Remote\nHomology; LSTM on Remote Homology). Note the benchmarks from literature do not include\ndecimal points, so for those numbers we report without a decimal.\nWe \ufb01nd that across all seven tasks considered, FPT achieves comparable performance to the fully\ntrained transformer benchmarks. We believe these results support the idea that these models are\nlearning representations and performing computation that is agnostic to the modality. We also note\nthat both transformer variants signi\ufb01cantly outperform LSTMs on some tasks, particularly ListOps\nand CIFAR-10 LRA, which have long sequence lengths of 512 and 1024, respectively.\nOn the two bit tasks (Memory and XOR), the models achieve 100% performance, i.e. they are able\nto recover the exact algorithm. Although our tables show results for n = 5, we actually \ufb01nd FPT can\nstill recover the exact algorithm on sequence lengths greater than n = 256 (the elementwise XOR\nof two bitstrings each of length 256), hinting that FPT has a fairly large working memory.\nModel\nBit Memory\nXOR\nListOps\nMNIST\nCIFAR-10\nC10 LRA\nHomology\nFPT\n100%\n100%\n38.4%\n98.0%\n72.1%\n38.6%\n12.7%\nFull\n100%\n100%\n38%\n99.1%\n70.3%\n42%\n9%\nLSTM\n60.9%\n50.1%\n17.1%\n99.5%\n73.6%\n11.7%\n12%\nTable 1: Test accuracy of FPT vs fully training transformer on downstream task vs fully training\nLSTM on downstream task (results are transcribed from Figure 1).\nWe highlight a few important points for contextualizing these results. We \ufb01nd that it can be dif\ufb01cult\nto fully train a 12-layer transformer on some of these (relatively small) datasets, as training can\neither diverge/over\ufb01t or be unstable. For CIFAR-10, we report the full transformer results for a 3-\nlayer model; for ListOps and CIFAR-10 LRA we report the number given for the 3-layer model from\nTay et al. (2020); for Remote Homology we report the number for a smaller 12-layer model from\nRao et al. (2019). From an engineering perspective, this makes the full transformers harder to tune\nsince we must choose model sizes that are stable and avoid over\ufb01tting \u2013 see Section 3.6 for more\n5\n", []], "3 Empirical Evaluations": ["Given the cheap linear scaling of these parameters, the parameter counts of large transformer models\nare dominated by the quadratic (in ndim and l) self-attention and feedforward layers. For the base\nCIFAR-10 model with 124M parameters, these come out to approximately 0.086% of the network.\nDue to this scaling, this number decreases with larger model sizes, down to 0.029% of the GPT-2\nXL model. We further ablate the importance of each parameter in Section 3.12. For more details\nand a description of the architecture, see Appendix B.\nNote that, crucially, all communication between tokens in the model are frozen. The data in each\ndatapoint is chunked into discrete tokens (bits, image patches, amino acids, etc.), and can only\nreference each other via the frozen attention connections, which are not trained; additionally, neither\nthe output nor the input layers are connected to multiple tokens. Our key investigation is to analyze\nthe computation that is already inherent in the language model, and hence we do a minimal amount\nof computation that is learned on the downstream modality.\n3\nEmpirical Evaluations\nIn this section, we review the results demonstrating transfer from language to other modalities, and\nseek to better understand why this occurs and what enables this transfer. All model sizes are the\nbase model size (12 layers, 768 hidden dimension), unless stated otherwise. See Appendix C for\nmore details on experiments.\n3.1\nCan pretrained language models transfer to different modalities?\nWe investigate if the self-attention and feedforward layers \u2013 the main body \u2013 of a pretrained trans-\nformer can be applied to a classi\ufb01cation problem in a different modality without \ufb01netuning. To\ndo this, we apply our base procedure as described above, where the input embedding layer, output\nreadout layer, and layer norm parameters are \ufb01netuned.\nOur results are shown in Figure 1 and also summarized below in Table 1. We compare to state-of-\nthe-art from literature when available (full transformer on ListOps, CIFAR-10 LRA, and Remote\nHomology; LSTM on Remote Homology). Note the benchmarks from literature do not include\ndecimal points, so for those numbers we report without a decimal.\nWe \ufb01nd that across all seven tasks considered, FPT achieves comparable performance to the fully\ntrained transformer benchmarks. We believe these results support the idea that these models are\nlearning representations and performing computation that is agnostic to the modality. We also note\nthat both transformer variants signi\ufb01cantly outperform LSTMs on some tasks, particularly ListOps\nand CIFAR-10 LRA, which have long sequence lengths of 512 and 1024, respectively.\nOn the two bit tasks (Memory and XOR), the models achieve 100% performance, i.e. they are able\nto recover the exact algorithm. Although our tables show results for n = 5, we actually \ufb01nd FPT can\nstill recover the exact algorithm on sequence lengths greater than n = 256 (the elementwise XOR\nof two bitstrings each of length 256), hinting that FPT has a fairly large working memory.\nModel\nBit Memory\nXOR\nListOps\nMNIST\nCIFAR-10\nC10 LRA\nHomology\nFPT\n100%\n100%\n38.4%\n98.0%\n72.1%\n38.6%\n12.7%\nFull\n100%\n100%\n38%\n99.1%\n70.3%\n42%\n9%\nLSTM\n60.9%\n50.1%\n17.1%\n99.5%\n73.6%\n11.7%\n12%\nTable 1: Test accuracy of FPT vs fully training transformer on downstream task vs fully training\nLSTM on downstream task (results are transcribed from Figure 1).\nWe highlight a few important points for contextualizing these results. We \ufb01nd that it can be dif\ufb01cult\nto fully train a 12-layer transformer on some of these (relatively small) datasets, as training can\neither diverge/over\ufb01t or be unstable. For CIFAR-10, we report the full transformer results for a 3-\nlayer model; for ListOps and CIFAR-10 LRA we report the number given for the 3-layer model from\nTay et al. (2020); for Remote Homology we report the number for a smaller 12-layer model from\nRao et al. (2019). From an engineering perspective, this makes the full transformers harder to tune\nsince we must choose model sizes that are stable and avoid over\ufb01tting \u2013 see Section 3.6 for more\n5\n", []], "2.2 Architecture": ["Input\nEmbedding\nMulti-Head\nAttention\n+\nFeed\nForward\nOutput\nLayer\nPositional\nEmbeddings\nL frozen self-attention blocks\nAdd &\nLayer Norm\nAdd &\nLayer Norm\nx L\nFigure 2: Frozen Pretrained Transformer (FPT). The self-attention & feedforward layers are frozen.\nCIFAR-10. We use the standard CIFAR-10 benchmark (Krizhevsky et al., 2009), where the tokens\ngiven to the model are 4 \u00d7 4 image patches, so the models are fed 64 tokens of dimension 16.\nCIFAR-10 LRA. This is a modi\ufb01ed version of the above task taken from the Long Range Arena\nbenchmark where the images are converted to grayscale and \ufb02attened with a token length of 1 (Tay\net al., 2020). As a result, the input sequence consists of 1024 tokens of dimension 1. This task is\nmuch more challenging than vanilla CIFAR-10 classi\ufb01cation above as the models must learn patterns\nover a signi\ufb01cantly longer sequence length and have minimal spatial inductive bias.\nRemote homology detection. In this task, we are interested in predicting the fold for a protein,\nrepresented as an amino acid sequence. We use the datasets provided by TAPE (Rao et al., 2019;\nFox et al., 2013; Hou et al., 2018), where the train/test split is generated by holding out certain\nevolutionary groups. Note that we do not pretrain on Pfam (El-Gebali et al., 2019), which is common\nin other works. There are 20 common and 5 uncommon amino acids (25 different types of inputs),\nand there are 1195 possible labels to predict. We only consider sequences of length less than 1024\nfor simplicity. The models are thus fed up to 1024 tokens of dimension 25.\n2.2\nArchitecture\nThe architecture we use is summarized in Figure 2. Denote the embedding size/hidden dimension\nof the transformer as ndim, the number of layers as nlayers, (note ndim = 768 and nlayers = 12 for\nthe base size models), the input dimension as din, the output dimension (number of classes) as dout,\nand the maximum length of the sequence as l. We consider \ufb01netuning the following parameters of a\npretrained GPT-2 model (Radford et al., 2019):\n\u2022 Output layer: it is crucial to \ufb01netune the output layer since we are transferring to a completely\nnew task \u2013 we use the simplest possible instantiation of an output network, being a single linear\nlayer applied to the last output token output by the transformer, in order to highlight that almost all\nthe computation is being performed by the frozen transformer. The output layer has ndim \u00d7 dout\nparameters for the weight matrix. For example, for the base models on CIFAR-10, this comes out\nto 768 \u00b7 10 = 7680 parameters.\n\u2022 Input layer: it is important to reinitialize a new input layer since we are reading in a new modality;\nin essence, we are learning how to query the transformer. This contrasts with prior unsupervised\nembedding evaluation techniques, such as linear probing \u2013 due to the change in modality, we\ninstead should train the input layer as well, and evaluate if the frozen intermediate transformer\nmodel performs effective computation. Again, we use a linear layer to minimize the amount of\ncomputation outside the transformer. The input layer has din \u00d7 ndim parameters for the weight\nmatrix/embeddings, and an additional ndim parameters if there is a bias term. For the base models\non CIFAR-10, this comes out to 16 \u00b7 768 = 13056 parameters.\n\u2022 Layer norm parameters: as is standard practice in other \ufb01netuning works (Rebuf\ufb01et al., 2017;\nHoulsby et al., 2019), we also \ufb01netune the af\ufb01ne layer norm parameters (scale and bias), which\nadapt to the statistics of the downstream task in a new domain. In GPT-2, layer norm is applied\ntwice per block, so these are a total of 4 \u00d7 ndim \u00d7 nlayers parameters. For the base models on\nCIFAR-10, these come out to 4 \u00b7 768 \u00b7 12 = 36684 parameters.\n\u2022 Positional embeddings: While we observe that positional embeddings can be surprisingly uni-\nversal between modalities (see Section 3.12), we generally see a small bene\ufb01t to \ufb01netuning the\npositional embeddings which have a cheap parameter cost of l \u00d7 ndim. For the base models on\nCIFAR-10, these come out to 64 \u00b7 768 = 49512 parameters.\n4\n", []], "2.1 Tasks": ["1\nIntroduction\nThe transformer architecture (Vaswani et al., 2017) has shown broad successes in deep learning,\nserving as the backbone of large models for tasks such as modeling natural language (Brown et al.,\n2020), images (Dosovitskiy et al., 2020), proteins (Jumper et al., 2021), behaviors (Abramson et al.,\n2020), and multimodal tasks comprising of both images and text (Lu et al., 2019; Radford et al.,\n2021). Inspired by these successes, we seek to explore the generalization capabilities of a trans-\nformer in transferring from one modality to another.\nClassical approaches to sequence processing used recurrent neural network (RNN) approaches\n(Rumelhart et al., 1985; Hochreiter & Schmidhuber, 1997). In contrast, transformers utilize self-\nattention layers to extract features across tokens of a sequence, such as words (Vaswani et al., 2017)\nor image patches (Dosovitskiy et al., 2020). Furthermore, it has become common practice to train\nlarge models on unsupervised or weakly supervised objectives before \ufb01netuning or evaluating zero-\nshot generalization on a downstream task. However, the downstream tasks that have been studied\nare generally restricted to the same modality as the original training set: for example, train GPT\n(Radford et al., 2018) on a large language corpus, and \ufb01netune on a small task-speci\ufb01c dataset. Our\ngoal in this work is to investigate \ufb01netuning on modalities distinct from the training modality.\nWe hypothesize that transformers, namely the self-attention layers, can be pretrained on a data-rich\nmodality (i.e. where data is plentiful, such as a natural language corpus) and identify feature rep-\nresentations that are useful for arbitrary data sequences, enabling downstream transfer to different\nmodalities. In particular, we seek to investigate what pretrained language models (LMs) are capable\nof in terms of generalizing to other modalities with sequential structure.\nTo investigate this hypothesis, we take a transformer model pretrained on natural language data,\nGPT-2 (Radford et al., 2019), and \ufb01netune only the linear input and output layers, as well as the\npositional embeddings and layer norm parameters. We call this model a Frozen Pretrained Trans-\nformer (FPT). On a range of tasks across a variety of modalities \u2013 including numerical computation,\nimage classi\ufb01cation, and protein fold prediction \u2013 FPT displays comparable performance to training\nthe entire transformer or LSTM models from scratch, matching reported benchmarks for these tasks\n(Figure 1). Additionally, we \ufb01nd FPT models also converge faster during training. Our results sug-\ngest the self-attention layers learned by a language model may have properties amenable to ef\ufb01cient\nuniversal computation. Through a series of experiments, we seek to investigate what contributes to\nthe performance of FPTs by isolating various sub-components of these models.\n2\nMethodology\n2.1\nTasks\nWe evaluate on a diverse set of classi\ufb01cation tasks representative of different modalities. In partic-\nular, we are interested in if language models are inherently capable of universal computation, by\nwhich we mean the ability to learn representations for predictive learning across diverse modalities.\nBit memory. Similar to the task proposed by Miconi et al. (2018), we consider a bit memory\ntask where the model is shown 5 bitstrings each of length 1000. Afterwards, the model is shown\na masked version of one of the bitstrings, where each bit is masked with probability 0.5, and the\nmodel is tasked with producing the original bitstring. The bitstrings are broken up into sequences of\nlength 50, so that the models are fed 120 tokens of dimension 50.\nBit XOR. Similar to the bit memory task, the model is shown 2 bitstrings of length 5, where the\nmodel must predict the element-wise XOR of the two bitstrings. The bitstrings are shown 1 bit at a\ntime, so the models are fed 10 tokens of dimension 1.\nListOps. Taken from Tay et al. (2020), the model is shown a sequence of list operations (ex. [\nMAX 4 3 [ MIN 2 3 ] 1 0 ]) and tasked with predicting the resulting output digit (ex. 4).\nThis task evaluates the ability of a model to parse mathematical expressions and evaluate over a long\ncontext. The model is shown 1 token at a time, so the models are fed 512 tokens of dimension 15.\nMNIST. We use the standard MNIST benchmark, where the model must classify a handwritten digit\nfrom a 32 \u00d7 32 black-and-white image. The tokens given to the model are 4 \u00d7 4 image patches, so\nthe models are fed 64 tokens of dimension 16.\n3\n", []], "2 Methodology": ["1\nIntroduction\nThe transformer architecture (Vaswani et al., 2017) has shown broad successes in deep learning,\nserving as the backbone of large models for tasks such as modeling natural language (Brown et al.,\n2020), images (Dosovitskiy et al., 2020), proteins (Jumper et al., 2021), behaviors (Abramson et al.,\n2020), and multimodal tasks comprising of both images and text (Lu et al., 2019; Radford et al.,\n2021). Inspired by these successes, we seek to explore the generalization capabilities of a trans-\nformer in transferring from one modality to another.\nClassical approaches to sequence processing used recurrent neural network (RNN) approaches\n(Rumelhart et al., 1985; Hochreiter & Schmidhuber, 1997). In contrast, transformers utilize self-\nattention layers to extract features across tokens of a sequence, such as words (Vaswani et al., 2017)\nor image patches (Dosovitskiy et al., 2020). Furthermore, it has become common practice to train\nlarge models on unsupervised or weakly supervised objectives before \ufb01netuning or evaluating zero-\nshot generalization on a downstream task. However, the downstream tasks that have been studied\nare generally restricted to the same modality as the original training set: for example, train GPT\n(Radford et al., 2018) on a large language corpus, and \ufb01netune on a small task-speci\ufb01c dataset. Our\ngoal in this work is to investigate \ufb01netuning on modalities distinct from the training modality.\nWe hypothesize that transformers, namely the self-attention layers, can be pretrained on a data-rich\nmodality (i.e. where data is plentiful, such as a natural language corpus) and identify feature rep-\nresentations that are useful for arbitrary data sequences, enabling downstream transfer to different\nmodalities. In particular, we seek to investigate what pretrained language models (LMs) are capable\nof in terms of generalizing to other modalities with sequential structure.\nTo investigate this hypothesis, we take a transformer model pretrained on natural language data,\nGPT-2 (Radford et al., 2019), and \ufb01netune only the linear input and output layers, as well as the\npositional embeddings and layer norm parameters. We call this model a Frozen Pretrained Trans-\nformer (FPT). On a range of tasks across a variety of modalities \u2013 including numerical computation,\nimage classi\ufb01cation, and protein fold prediction \u2013 FPT displays comparable performance to training\nthe entire transformer or LSTM models from scratch, matching reported benchmarks for these tasks\n(Figure 1). Additionally, we \ufb01nd FPT models also converge faster during training. Our results sug-\ngest the self-attention layers learned by a language model may have properties amenable to ef\ufb01cient\nuniversal computation. Through a series of experiments, we seek to investigate what contributes to\nthe performance of FPTs by isolating various sub-components of these models.\n2\nMethodology\n2.1\nTasks\nWe evaluate on a diverse set of classi\ufb01cation tasks representative of different modalities. In partic-\nular, we are interested in if language models are inherently capable of universal computation, by\nwhich we mean the ability to learn representations for predictive learning across diverse modalities.\nBit memory. Similar to the task proposed by Miconi et al. (2018), we consider a bit memory\ntask where the model is shown 5 bitstrings each of length 1000. Afterwards, the model is shown\na masked version of one of the bitstrings, where each bit is masked with probability 0.5, and the\nmodel is tasked with producing the original bitstring. The bitstrings are broken up into sequences of\nlength 50, so that the models are fed 120 tokens of dimension 50.\nBit XOR. Similar to the bit memory task, the model is shown 2 bitstrings of length 5, where the\nmodel must predict the element-wise XOR of the two bitstrings. The bitstrings are shown 1 bit at a\ntime, so the models are fed 10 tokens of dimension 1.\nListOps. Taken from Tay et al. (2020), the model is shown a sequence of list operations (ex. [\nMAX 4 3 [ MIN 2 3 ] 1 0 ]) and tasked with predicting the resulting output digit (ex. 4).\nThis task evaluates the ability of a model to parse mathematical expressions and evaluate over a long\ncontext. The model is shown 1 token at a time, so the models are fed 512 tokens of dimension 15.\nMNIST. We use the standard MNIST benchmark, where the model must classify a handwritten digit\nfrom a 32 \u00d7 32 black-and-white image. The tokens given to the model are 4 \u00d7 4 image patches, so\nthe models are fed 64 tokens of dimension 16.\n3\n", []], "1 Introduction": ["1\nIntroduction\nThe transformer architecture (Vaswani et al., 2017) has shown broad successes in deep learning,\nserving as the backbone of large models for tasks such as modeling natural language (Brown et al.,\n2020), images (Dosovitskiy et al., 2020), proteins (Jumper et al., 2021), behaviors (Abramson et al.,\n2020), and multimodal tasks comprising of both images and text (Lu et al., 2019; Radford et al.,\n2021). Inspired by these successes, we seek to explore the generalization capabilities of a trans-\nformer in transferring from one modality to another.\nClassical approaches to sequence processing used recurrent neural network (RNN) approaches\n(Rumelhart et al., 1985; Hochreiter & Schmidhuber, 1997). In contrast, transformers utilize self-\nattention layers to extract features across tokens of a sequence, such as words (Vaswani et al., 2017)\nor image patches (Dosovitskiy et al., 2020). Furthermore, it has become common practice to train\nlarge models on unsupervised or weakly supervised objectives before \ufb01netuning or evaluating zero-\nshot generalization on a downstream task. However, the downstream tasks that have been studied\nare generally restricted to the same modality as the original training set: for example, train GPT\n(Radford et al., 2018) on a large language corpus, and \ufb01netune on a small task-speci\ufb01c dataset. Our\ngoal in this work is to investigate \ufb01netuning on modalities distinct from the training modality.\nWe hypothesize that transformers, namely the self-attention layers, can be pretrained on a data-rich\nmodality (i.e. where data is plentiful, such as a natural language corpus) and identify feature rep-\nresentations that are useful for arbitrary data sequences, enabling downstream transfer to different\nmodalities. In particular, we seek to investigate what pretrained language models (LMs) are capable\nof in terms of generalizing to other modalities with sequential structure.\nTo investigate this hypothesis, we take a transformer model pretrained on natural language data,\nGPT-2 (Radford et al., 2019), and \ufb01netune only the linear input and output layers, as well as the\npositional embeddings and layer norm parameters. We call this model a Frozen Pretrained Trans-\nformer (FPT). On a range of tasks across a variety of modalities \u2013 including numerical computation,\nimage classi\ufb01cation, and protein fold prediction \u2013 FPT displays comparable performance to training\nthe entire transformer or LSTM models from scratch, matching reported benchmarks for these tasks\n(Figure 1). Additionally, we \ufb01nd FPT models also converge faster during training. Our results sug-\ngest the self-attention layers learned by a language model may have properties amenable to ef\ufb01cient\nuniversal computation. Through a series of experiments, we seek to investigate what contributes to\nthe performance of FPTs by isolating various sub-components of these models.\n2\nMethodology\n2.1\nTasks\nWe evaluate on a diverse set of classi\ufb01cation tasks representative of different modalities. In partic-\nular, we are interested in if language models are inherently capable of universal computation, by\nwhich we mean the ability to learn representations for predictive learning across diverse modalities.\nBit memory. Similar to the task proposed by Miconi et al. (2018), we consider a bit memory\ntask where the model is shown 5 bitstrings each of length 1000. Afterwards, the model is shown\na masked version of one of the bitstrings, where each bit is masked with probability 0.5, and the\nmodel is tasked with producing the original bitstring. The bitstrings are broken up into sequences of\nlength 50, so that the models are fed 120 tokens of dimension 50.\nBit XOR. Similar to the bit memory task, the model is shown 2 bitstrings of length 5, where the\nmodel must predict the element-wise XOR of the two bitstrings. The bitstrings are shown 1 bit at a\ntime, so the models are fed 10 tokens of dimension 1.\nListOps. Taken from Tay et al. (2020), the model is shown a sequence of list operations (ex. [\nMAX 4 3 [ MIN 2 3 ] 1 0 ]) and tasked with predicting the resulting output digit (ex. 4).\nThis task evaluates the ability of a model to parse mathematical expressions and evaluate over a long\ncontext. The model is shown 1 token at a time, so the models are fed 512 tokens of dimension 15.\nMNIST. We use the standard MNIST benchmark, where the model must classify a handwritten digit\nfrom a 32 \u00d7 32 black-and-white image. The tokens given to the model are 4 \u00d7 4 image patches, so\nthe models are fed 64 tokens of dimension 16.\n3\n", []]}