{"A.1 Hyperparameters used in the experiments": ["[26] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated\nprobabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence,\n2015.\n[27] Lucas Baier, Tim Schl\u00f6r, Jakob Sch\u00f6ffer, and Niklas K\u00fchl. Detecting concept drift with neural\nnetwork model uncertainty. arXiv preprint arXiv:2107.01873, 2021.\nA\nAppendix\nA.1\nHyperparameters used in the experiments\nAll input features are standardized using statistics calculated on the training set. Implementation is\ndone using [19] and [20]. Only non-default values are reported here:\nNeural Network The results are reported for a fully connected neural network with 3 hidden layers\nconsisting of 32, 16 and 8 units respectively. The hidden layers use Recti\ufb01ed Linear Unit (ReLU)\nactivations, while the class scores are calculated using the softmax function in the \ufb01nal layer. Cross-\nentropy loss is used in training with the ADAM optimizer for 50 epochs at a learning rate of 0.001.\nDeep Ensembles (NN-Ens) utilize the same architecture for the ensemble models. M = 10 models\nare used in the ensemble. No dropout layers are used in the default case. For Monte-Carlo Dropout\n(NN-MCD), dropout layers are added after each of the \ufb01rst two hidden layers with p = 0.2. M = 20\nforward passes are performed for each sample. Decision Tree (DT) The decision tree is \ufb01tted with\na maximum depth of 6. Random Forest (RF) The random forest uses 100 single estimators in the\nensemble.\n7\n", []], "A Appendix": ["[26] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated\nprobabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence,\n2015.\n[27] Lucas Baier, Tim Schl\u00f6r, Jakob Sch\u00f6ffer, and Niklas K\u00fchl. Detecting concept drift with neural\nnetwork model uncertainty. arXiv preprint arXiv:2107.01873, 2021.\nA\nAppendix\nA.1\nHyperparameters used in the experiments\nAll input features are standardized using statistics calculated on the training set. Implementation is\ndone using [19] and [20]. Only non-default values are reported here:\nNeural Network The results are reported for a fully connected neural network with 3 hidden layers\nconsisting of 32, 16 and 8 units respectively. The hidden layers use Recti\ufb01ed Linear Unit (ReLU)\nactivations, while the class scores are calculated using the softmax function in the \ufb01nal layer. Cross-\nentropy loss is used in training with the ADAM optimizer for 50 epochs at a learning rate of 0.001.\nDeep Ensembles (NN-Ens) utilize the same architecture for the ensemble models. M = 10 models\nare used in the ensemble. No dropout layers are used in the default case. For Monte-Carlo Dropout\n(NN-MCD), dropout layers are added after each of the \ufb01rst two hidden layers with p = 0.2. M = 20\nforward passes are performed for each sample. Decision Tree (DT) The decision tree is \ufb01tted with\na maximum depth of 6. Random Forest (RF) The random forest uses 100 single estimators in the\nensemble.\n7\n", []], "3 Conclusion and Outlook": ["ensemble slightly over\ufb01ts on different aspects of the training data, which in combination yields lower\ncon\ufb01dences on the drifted data, as the predictions will deviate more strongly than on in-distribution\ndata.\n3\nConclusion and Outlook\nIn this work, we highlighted the general relevance, implications and possible sources of drifts\naffecting the continuous reliability of ML applications in the manufacturing domain. Using an\nindustrial dataset, we exemplarily show, that none of the most commonly used ML algorithms in\nmanufacturing are robust against drifts in the data distribution in\ufb02icted by the environment or the\nsensors used for perception thereof. A consequent analysis regarding the con\ufb01dence calibration of\nthe algorithms showed, that in the majority of cases, the calibration strongly degrades with the drift,\nrendering the con\ufb01dences unexpressive. Positively, the con\ufb01dence calibration of ensemble algorithms\nsuch as random forests degrades less strongly and may be used to estimate the current performance\nand identify drifts. In a continual learning setup, the con\ufb01dence could thus be used as a trigger signal\nfor data collection and retraining of the respective model.\nThere are multiple opportunities for future work on drift detection and adaption through means of\ncontinual learning or domain adaptation speci\ufb01c to ML use cases in manufacturing such as condition\nmonitoring, predictive maintenance or quality prediction to enable further adaption of ML applications\nin this domain. Especially the practical implementation of such systems on the shop \ufb02oor level is still\nan open research issue.\nAcknowledgments and Disclosure of Funding\nThis project has received funding from the European Union\u2019s Horizon 2020 Research and Innovation\nProgramme under grant agreement No. 958357 (InterQ), and it is an initiative of the Factories-of-the-\nFuture (FoF) Public Private Partnership.\nReferences\n[1] Nicolas Jourdan, Lukas Longard, Tobias Biegel, and Joachim Metternich. Machine Learning for\nIntelligent Maintenance and Quality Control: A Review of Existing Datasets and Corresponding\nUse Cases. Hannover: Institutionelles Repositorium der Leibniz Universit\u00e4t Hannover, 2021.\n[2] Thorsten Wuest, Daniel Weimer, Christopher Irgens, and Klaus-Dieter Thoben. Machine learn-\ning in manufacturing: advantages, challenges, and applications. Production & Manufacturing\nResearch, 4(1):23\u201345, 2016.\n[3] Yeounoh Chung, Peter J Haas, Eli Upfal, and Tim Kraska. Unknown examples & machine\nlearning model generalization. arXiv preprint arXiv:1808.08294, 2018.\n[4] Nicolas Jourdan, Eike Rehder, and Uwe Franke. Identi\ufb01cation of uncertainty in arti\ufb01cial neural\nnetworks. In Proceedings of the 13th Uni-DAS eV Workshop Fahrerassistenz und automatisiertes\nFahren, volume 2, 2020.\n[5] Andrew Kusiak. Smart manufacturing must embrace big data. Nature, (544), 2017.\n[6] Xinyang Wu, Mohamed El-Shamouty, and Philipp Wagner. White Paper: Dependable AI.\nUsing AI in Safety-Critical Industrial Applications. Technical report, Fraunhofer Institute For\nManufacturing Engineering and Automation (IPA), 2021.\n[7] Beatriz Bretones Cassoli, Nicolas Jourdan, Phu H Nguyen, Sagar Sen, Enrique Garcia-Ceja,\nand Joachim Metternich. Frameworks for data-driven quality management in cyber-physical\nsystems for manufacturing: A systematic review. CIRP Conference on Intelligent Computation\nin Manufacturing (ICME), 2021.\n[8] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin,\nJoshua V Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncer-\ntainty? evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530,\n2019.\n5\n", []], "2.4 Results": ["(a) Top: Prediction accuracies over time. Bottom:\nModel con\ufb01dences over time. Val indicates the valida-\ntion set containing non-drifted data.\nVal. (non-drifted M. 1-10)\nTest (drifted M. 11-36)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nECE\nDT\nRF\nKNN\nSVM\nNN\nNN-MCD\nNN-Ens\n(b) ECEs of the assessed models for the non-drifted\nvalidation set (months 1-10) as well as averaged over\nthe drifted test set (months 11-36). Error bars show\nthe standard deviation of the experiment runs.\nFigure 2: Results of the experiments on the UCSD Gas Sensor Array [9] dataset regarding model\naccuracy, con\ufb01dence and calibration under drift. All experiments are repeated 10 times with different\nrandom seeds and the results consequently averaged.\n2.3\nExperiment Setting and Metrics\nWe train all the models on a random 50% split of the \ufb01rst 10 months of the available data and use the\nremaining 50% as the validation set for performance evaluation on non-drifted data. To be able to\nassess the robustness to drifts, we test on the remaining 26 months. All available features are used for\ntraining and the experiments are repeated 10 times with varying random seeds.\nFor evaluation, we employ two metrics capturing different aspects of interest:\nAccuracy \u2191is used to assess the performance of the model on the non-drifted test set as well as the\nperformance degradation under drift. The accuracy measures the percentage of correct classi\ufb01cations.\nExpected Calibration Error (ECE) \u2193[26] is used to evaluate the calibration of the con\ufb01dences pro-\nduced by the model. The ECE is closely related to calibration curves and corresponds to the average\ngap between model con\ufb01dence and achieved accuracy. While the ECE has several shortcomings [8],\nwe choose it over other calibration metrics for its simplicity and interpretability to strengthen the\npractical relevance.\n2.4\nResults\nAs visualized in the upper part of Figure 2 (a), the classi\ufb01cation performance of all tested algorithms\nstrongly degrades with an increasing time difference to the non-drifted validation set. This indicates,\nthat none of the tested algorithms is robust against drifts in the environment. Thus, online monitoring\nand eventual model updates would be required to guarantee a reliable and safe application in real\nmanufacturing environments. In parallel, the reduction in accuracy is re\ufb02ected by the lowering in\ncon\ufb01dence of a subset of the algorithms, most pronounced with RF. These con\ufb01dences may be used to\nidentify drift in this scenario using frameworks such as [27]. The calibration of the model con\ufb01dences\nis further analyzed in Figure 2 (b). Notably, all tested algorithms show well-calibrated con\ufb01dences\non the validation set, re\ufb02ected by the low ECE, while the calibration strongly degrades for the drifted\ndata. In addition, the error bars in Figure 2 (b) show, that the standard deviation with respect to the\nECEs of the 10 experiment runs on the drifted data increased for all algorithms when compared to\nthe standard deviation on the validation set. This further highlights, that the calibration of a model\non drifted data, can usually not be inferred from the calibration on in-distribution data as it highly\ndepends on the type and magnitude of the drift. Lastly, it can be observed that the calibration of the\nRF degrades least for the drifted data, followed by SVM and NN-Ens, supporting the visualization in\nFigure 2 (a). Depending on the application requirements, the con\ufb01dence of the RF may be used as a\nmeasure of the performance that can be expected of the algorithm as well as an indicator for drift. The\nobserved comparably high calibration robustness of ensemble methods in the presence of drift aligns\nwith previous work on deep ensembles [8]. A possible reason may be, that each of the models in the\n4\n", []], "2.3 Experiment Setting and Metrics": ["(a) Top: Prediction accuracies over time. Bottom:\nModel con\ufb01dences over time. Val indicates the valida-\ntion set containing non-drifted data.\nVal. (non-drifted M. 1-10)\nTest (drifted M. 11-36)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nECE\nDT\nRF\nKNN\nSVM\nNN\nNN-MCD\nNN-Ens\n(b) ECEs of the assessed models for the non-drifted\nvalidation set (months 1-10) as well as averaged over\nthe drifted test set (months 11-36). Error bars show\nthe standard deviation of the experiment runs.\nFigure 2: Results of the experiments on the UCSD Gas Sensor Array [9] dataset regarding model\naccuracy, con\ufb01dence and calibration under drift. All experiments are repeated 10 times with different\nrandom seeds and the results consequently averaged.\n2.3\nExperiment Setting and Metrics\nWe train all the models on a random 50% split of the \ufb01rst 10 months of the available data and use the\nremaining 50% as the validation set for performance evaluation on non-drifted data. To be able to\nassess the robustness to drifts, we test on the remaining 26 months. All available features are used for\ntraining and the experiments are repeated 10 times with varying random seeds.\nFor evaluation, we employ two metrics capturing different aspects of interest:\nAccuracy \u2191is used to assess the performance of the model on the non-drifted test set as well as the\nperformance degradation under drift. The accuracy measures the percentage of correct classi\ufb01cations.\nExpected Calibration Error (ECE) \u2193[26] is used to evaluate the calibration of the con\ufb01dences pro-\nduced by the model. The ECE is closely related to calibration curves and corresponds to the average\ngap between model con\ufb01dence and achieved accuracy. While the ECE has several shortcomings [8],\nwe choose it over other calibration metrics for its simplicity and interpretability to strengthen the\npractical relevance.\n2.4\nResults\nAs visualized in the upper part of Figure 2 (a), the classi\ufb01cation performance of all tested algorithms\nstrongly degrades with an increasing time difference to the non-drifted validation set. This indicates,\nthat none of the tested algorithms is robust against drifts in the environment. Thus, online monitoring\nand eventual model updates would be required to guarantee a reliable and safe application in real\nmanufacturing environments. In parallel, the reduction in accuracy is re\ufb02ected by the lowering in\ncon\ufb01dence of a subset of the algorithms, most pronounced with RF. These con\ufb01dences may be used to\nidentify drift in this scenario using frameworks such as [27]. The calibration of the model con\ufb01dences\nis further analyzed in Figure 2 (b). Notably, all tested algorithms show well-calibrated con\ufb01dences\non the validation set, re\ufb02ected by the low ECE, while the calibration strongly degrades for the drifted\ndata. In addition, the error bars in Figure 2 (b) show, that the standard deviation with respect to the\nECEs of the 10 experiment runs on the drifted data increased for all algorithms when compared to\nthe standard deviation on the validation set. This further highlights, that the calibration of a model\non drifted data, can usually not be inferred from the calibration on in-distribution data as it highly\ndepends on the type and magnitude of the drift. Lastly, it can be observed that the calibration of the\nRF degrades least for the drifted data, followed by SVM and NN-Ens, supporting the visualization in\nFigure 2 (a). Depending on the application requirements, the con\ufb01dence of the RF may be used as a\nmeasure of the performance that can be expected of the algorithm as well as an indicator for drift. The\nobserved comparably high calibration robustness of ensemble methods in the presence of drift aligns\nwith previous work on deep ensembles [8]. A possible reason may be, that each of the models in the\n4\n", []], "2.2 Dataset": ["seasons and time of day [6], environmental conditions such as temperature or humidity [6, 12, 13],\nsensor failure/drift/recalibration [5, 6] and data transmission problems [5].\nReliable machine learning applications in dynamic environments may be established in two ways:\nEither the model and data acquisition setup employed are robust against the relevant sources of drift,\ne.g. [15], or the model is continually assessed and, if required, adapted to the current environment in\na continual learning setup [16].\nCommonly, ML models with trained parameters \u03b8 in classi\ufb01cation tasks produce probability estimates\np (\u02c6yc | x, \u03b8) for all classes c \u2208{1, . . . , C}, given a sample of data x. The probability may be used\nto assess the models\u2019 con\ufb01dence/uncertainty in its own prediction. The con\ufb01dence is referred to as\nwell-calibrated, when empirically, it is equal to the probability of the corresponding sample being\ncorrectly classi\ufb01ed [17]. Thus, con\ufb01dence estimates that are well-calibrated even in the presence of\nconcept drift, may be used to reason about the reliability of a ML model and determine if it should be\nadapted, i.e., retrained with new data. Additionally, well-calibrated con\ufb01dence estimates may be used\nto identify product con\ufb01gurations or situations that are dif\ufb01cult for the ML model to handle. In the\nexample of quality estimation, this could, e.g., trigger an additional human quality control for a given\npart or the manual inspection of a machine.\n2.1\nML Algorithms and Uncertainty Estimation Methods\nTo maximize the value for practical applications, we assess ML algorithms that have been identi\ufb01ed\nas most commonly used in the manufacturing domain by a recent review study [18]. We explicitly\ninclude non-deep learning algorithms in the analysis, as these are highly relevant in the manufacturing\ndomain, where labeled datasets are often small. In the scope of this work, we focus on classi\ufb01cation\ntasks. Implementation is done using [19] and [20]. The hyperparameters are selected by performing\na grid-search over the parameter space of the models, optimizing for accuracy. For each algorithm,\nwe employ a con\ufb01dence/uncertainty estimation method as described below:\nSupport Vector Machine (SVM) con\ufb01dence estimates are obtained using Platt scaling [21] of the\nsample distances to the separating hyperplane. The parameters for con\ufb01dence estimation are \ufb01tted\nvia 5-fold cross validation.\nDecision Tree (DT) con\ufb01dence estimates are computed as the fraction of training samples of the\nsame class in the leaf node [19].\nK-Nearest Neighbors (KNN) con\ufb01dence estimates are calculated similar to DTs. The probability of\na class is computed as the fraction of training samples of the same class in the set of nearest neighbors,\nweighted by their distance.\nRandom Forest (RF) con\ufb01dence estimates are computed as the mean predicted class probabilities\nof the trees in the forest. The individual tree con\ufb01dences are computed as described above (DT).\nThis method of con\ufb01dence estimation for RFs has been shown to be superior to more complicated\nextensions [22].\nNeural Network For neural networks, we assess multiple recently proposed uncertainty estimation\nmethods: Max. Softmax Probability (NN) [23]; Deep Ensembles (NN-Ens) [24] with M = 10\nensemble members. Randomness is introduced by reshuf\ufb02ing of the training set as well as different\nrandom initialization for each NN in the ensemble; Monte-Carlo Dropout (NN-MCD) [25] with\nM = 20 forward passes for each sample. Dropout rate p is set to 0.2.\n2.2\nDataset\nFor our experiments, we use the Gas Sensor Array Drift dataset [9] that was recorded at the University\nof California San Diego (UCSD). The dataset was recorded over 36 months at an industrial test rig.\nDue to the long recording time, the dataset contains both sensor drift due to aging sensors and concept\ndrift due to external in\ufb02uences which resembles the expected environment conditions of a real-world\nML application in manufacturing, cf. Figure 1 (b). The dataset represents a classi\ufb01cation task, in\nwhich the target variable is the type of gas (one of six) that is currently present in the apparatus.\nThe experiments are perceived using 16 sensors and each row of the dataset contains 128 extracted\nstatistical features (8 per sensor) of the corresponding experiment run with a total of 13,910 runs.\nThe dataset is split into 10 consecutive batches, each capturing a varying amount of months.\n3\n", []], "2.1 ML Algorithms and Uncertainty Estimation Methods": ["seasons and time of day [6], environmental conditions such as temperature or humidity [6, 12, 13],\nsensor failure/drift/recalibration [5, 6] and data transmission problems [5].\nReliable machine learning applications in dynamic environments may be established in two ways:\nEither the model and data acquisition setup employed are robust against the relevant sources of drift,\ne.g. [15], or the model is continually assessed and, if required, adapted to the current environment in\na continual learning setup [16].\nCommonly, ML models with trained parameters \u03b8 in classi\ufb01cation tasks produce probability estimates\np (\u02c6yc | x, \u03b8) for all classes c \u2208{1, . . . , C}, given a sample of data x. The probability may be used\nto assess the models\u2019 con\ufb01dence/uncertainty in its own prediction. The con\ufb01dence is referred to as\nwell-calibrated, when empirically, it is equal to the probability of the corresponding sample being\ncorrectly classi\ufb01ed [17]. Thus, con\ufb01dence estimates that are well-calibrated even in the presence of\nconcept drift, may be used to reason about the reliability of a ML model and determine if it should be\nadapted, i.e., retrained with new data. Additionally, well-calibrated con\ufb01dence estimates may be used\nto identify product con\ufb01gurations or situations that are dif\ufb01cult for the ML model to handle. In the\nexample of quality estimation, this could, e.g., trigger an additional human quality control for a given\npart or the manual inspection of a machine.\n2.1\nML Algorithms and Uncertainty Estimation Methods\nTo maximize the value for practical applications, we assess ML algorithms that have been identi\ufb01ed\nas most commonly used in the manufacturing domain by a recent review study [18]. We explicitly\ninclude non-deep learning algorithms in the analysis, as these are highly relevant in the manufacturing\ndomain, where labeled datasets are often small. In the scope of this work, we focus on classi\ufb01cation\ntasks. Implementation is done using [19] and [20]. The hyperparameters are selected by performing\na grid-search over the parameter space of the models, optimizing for accuracy. For each algorithm,\nwe employ a con\ufb01dence/uncertainty estimation method as described below:\nSupport Vector Machine (SVM) con\ufb01dence estimates are obtained using Platt scaling [21] of the\nsample distances to the separating hyperplane. The parameters for con\ufb01dence estimation are \ufb01tted\nvia 5-fold cross validation.\nDecision Tree (DT) con\ufb01dence estimates are computed as the fraction of training samples of the\nsame class in the leaf node [19].\nK-Nearest Neighbors (KNN) con\ufb01dence estimates are calculated similar to DTs. The probability of\na class is computed as the fraction of training samples of the same class in the set of nearest neighbors,\nweighted by their distance.\nRandom Forest (RF) con\ufb01dence estimates are computed as the mean predicted class probabilities\nof the trees in the forest. The individual tree con\ufb01dences are computed as described above (DT).\nThis method of con\ufb01dence estimation for RFs has been shown to be superior to more complicated\nextensions [22].\nNeural Network For neural networks, we assess multiple recently proposed uncertainty estimation\nmethods: Max. Softmax Probability (NN) [23]; Deep Ensembles (NN-Ens) [24] with M = 10\nensemble members. Randomness is introduced by reshuf\ufb02ing of the training set as well as different\nrandom initialization for each NN in the ensemble; Monte-Carlo Dropout (NN-MCD) [25] with\nM = 20 forward passes for each sample. Dropout rate p is set to 0.2.\n2.2\nDataset\nFor our experiments, we use the Gas Sensor Array Drift dataset [9] that was recorded at the University\nof California San Diego (UCSD). The dataset was recorded over 36 months at an industrial test rig.\nDue to the long recording time, the dataset contains both sensor drift due to aging sensors and concept\ndrift due to external in\ufb02uences which resembles the expected environment conditions of a real-world\nML application in manufacturing, cf. Figure 1 (b). The dataset represents a classi\ufb01cation task, in\nwhich the target variable is the type of gas (one of six) that is currently present in the apparatus.\nThe experiments are perceived using 16 sensors and each row of the dataset contains 128 extracted\nstatistical features (8 per sensor) of the corresponding experiment run with a total of 13,910 runs.\nThe dataset is split into 10 consecutive batches, each capturing a varying amount of months.\n3\n", []], "2 Methodology and Experiments": ["(Est.) Labels\nLabels\nTrain & Test \nData\nOnline Data\nMachine Learning\nModel\nSensors\nIndustrial \nEnvironment\nTrain & Test Model\nDeployed\nUsage\n(a) Stationary environment\nSensors \n[t=0]\nIndustrial\nEnvironment\n[t = 0]\nSensors\n[t=1]\nIndustrial \nEnvironment\n[t = 1]\nSensors \n[t=?]\nIndustrial \nEnvironment\n[t = ?]\n\u2026\nLabels\nTrain & Test\nData\nMachine Learning\nModel\nTrain & Test Model\nDeployed\nUsage\nOnline Data\n(Est.) Labels\n(b) Environment under concept and sensor drift\nFigure 1: Supervised machine learning in stationary conditions with static sensors (a) in contrast\nto a dynamic system where the environment as well as the sensors used for perception are non-\nstationary, which applies to the majority of manufacturing use cases of ML (b). In the latter case,\nstatic training/testing sets do not provide continuous performance guarantees. Figure based on [10],\nadapted and extended with permission of the original authors.\nthat the generalization ability of a model mainly depends on the con\ufb01guration and variety of the\navailable training data and is far from guaranteed [3, 4]. Long-term reliability and the handling of\nuncertainties caused by degrading equipment or faulty sensors are seen as key factors and major\nhurdles when deploying ML systems in manufacturing environments [5, 6]. With respect to safety\ncerti\ufb01cation and risk assessment, online performance monitoring and uncertainty estimation are seen\nas critical for detecting drifts in the data distribution as well as estimating error magnitudes [6]. In\nthe context of quality management, [7] showed, that the majority of the analyzed frameworks still\nlack any form of uncertainty estimation or online monitoring.\nIn this work, we analyze the long-term reliability of ML applications in the manufacturing industry,\nhighlighting the domain-speci\ufb01c issues and potential sources of drift. We benchmark a set of ML\nalgorithms that are most relevant in this domain for robustness to time-dependent drift on an industrial\ndataset. Further, we assess uncertainty estimation techniques and highlight their potential utility for\nonline performance estimation and drift detection in the context of continual learning to overcome\nthe issue of silently failing ML applications.\nSimilar experiments have been conducted in [8] but did not consider non-deep learning algorithms,\nwhich are of high relevance in the manufacturing domain. Additionally, the introduced drifts were\nsynthetic, while we evaluate on real-world time-dependend drifts. The performance degradation of a\nclassi\ufb01er (support vector machine) on the utilized dataset has been observed in [9]. In contrast to\nthe existing work, we extend the analysis to a broad spectrum of commonly used ML algorithms\nand additionally analyze the expressiveness of uncertainty estimation techniques suitable for the\nrespective algorithms.\n2\nMethodology and Experiments\nConcept drift refers to a change in the underlying data distributions of machine learning applications.\nEspecially in the context of pattern recognition, the terms covariate shift or dataset shift are used\ninterchangeably [11]. Concept drift in the context of this publication, cf. Figure 1 (b), can be de\ufb01ned\nas Ptrain(X, Y ) \u0338= Ponline,t(X, Y ), where Ptrain and Ponline,t denote the joint distributions of\ninput samples X and target labels Y during training and deployed usage of the model at time t,\nrespectively.\nRelevant short- and long-term sources of changes in manufacturing environments which may in\ufb02uence\nthe reliability of ML models include: Tool and machine wear [5, 6, 12, 13], changes in product\ncon\ufb01gurations and material properties [12, 13], changes in upstream processes [13], changes in\nfactory layout and machine placement [14], differences in operator preferences and training [12, 13],\n2\n", []], "1 Introduction and Motivation": ["On The Reliability Of Machine Learning\nApplications In Manufacturing Environments\nNicolas Jourdan\nTU Darmstadt, Germany\nn.jourdan@ptw.tu-darmstadt.de\nSagar Sen\nSINTEF, Norway\nsagar.sen@sintef.no\nErik Johannes Husom\nSINTEF, Norway\nerik.husom@sintef.no\nEnrique Garcia-Ceja\nSINTEF, Norway\ne.g.mx@ieee.org\nTobias Biegel\nTU Darmstadt, Germany\nt.biegel@ptw.tu-darmstadt.de\nJoachim Metternich\nTU Darmstadt, Germany\nj.metternich@ptw.tu-darmstadt.de\nAbstract\nThe increasing deployment of advanced digital technologies such as Internet of\nThings (IoT) devices and Cyber-Physical Systems (CPS) in industrial environments\nis enabling the productive use of machine learning (ML) algorithms in the manu-\nfacturing domain. As ML applications transcend from research to productive use\nin real-world industrial environments, the question of reliability arises. Since the\nmajority of ML models are trained and evaluated on static datasets, continuous\nonline monitoring of their performance is required to build reliable systems. Fur-\nthermore, concept and sensor drift can lead to degrading accuracy of the algorithm\nover time, thus compromising safety, acceptance and economics if undetected and\nnot properly addressed. In this work, we exemplarily highlight the severity of the\nissue on a publicly available industrial dataset which was recorded over the course\nof 36 months and explain possible sources of drift. We assess the robustness of ML\nalgorithms commonly used in manufacturing and show, that the accuracy strongly\ndeclines with increasing drift for all tested algorithms. We further investigate how\nuncertainty estimation may be leveraged for online performance estimation as well\nas drift detection as a \ufb01rst step towards continually learning applications. The\nresults indicate, that ensemble algorithms like random forests show the least decay\nof con\ufb01dence calibration under drift.\n1\nIntroduction and Motivation\nIncreasing digitization and the deployment of advanced technologies in the context of Internet of\nThings (IoT) and Industry 4.0 are transforming manufacturing lines into Cyber-Physical Systems\n(CPS) that generate large amounts of data. The availability of this data enables a multitude of\napplications, including the development and deployment of ML algorithms for use cases such as\ncondition monitoring, predictive maintenance and quality prediction [1]. As ML applications are\ndeployed to productive usage, their continuous reliability has to be guaranteed to protect human\noperators as well as the \ufb01nancial investments involved. Manufacturing environments are fast changing,\nhighly dynamic and inherently uncertain which poses the requirement for ML applications to be able\nto adapt to changing environments with reasonable effort and cost [2]. While the ability of adapting\nto a changing environment is often seen as a default property of machine learning [2], studies show,\nWorkshop on Distribution Shifts, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2112.06986v2  [cs.LG]  19 Dec 2021\n", []]}