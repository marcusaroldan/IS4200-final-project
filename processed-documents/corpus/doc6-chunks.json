{"References": ["A.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\n \n1213\nTable\n 8\nError\n rate\n (%)\n of\n BMOANN\n for\n modeling\n of\n the\n Ballard\n MK5-E\n PEMFC\n system.\n The\nresults\n have\n been\n averaged\n over\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nTest\n set\nAvg.\nStd.\nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\nBMOANN\n0.11\n6.18e\u22122\n \n0.02\n0.29\n0.11\n \n7.52e\u22122\n \n0.01\n \n0.35\nTable\n 9\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n the\n best\n testing\n error\n rate\n(%)\n on\n the\n Ballard\n MK5-E\n PEMFC\n system.\nAlgorithm\n \nBMOANN\n \nPSOANN\n \nBPANN\n \nANN-new\nTest\n error\n rate\n (%)\n \n0.01\n \n0.0247\n \n0.14\n \n0.024\n0 \n100 \n200 \n300 \n400 \n500 \n600 \n700\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nCell voltage (V)\nCurrent density (mA/cm2)\nANN model - 56 \u00baC \ndata - 56 \u00baC \nFig.\n 5.\n Comparison\n between\n experimental\n data\n and\n ANN\n model\n for\n the\n test\n set.\nWe\n have\n tabulated\n the\n results\n generated\n by\n BMOANN\n over\n50\n independent\n runs\n in\n Table\n 8.\n Table\n 9\n shows\n the\n compari-\nson\n between\n the\n BMO\n result\n and\n the\n best\n performance\n of\n an\nANN\n trained\n by\n BP\n (BPANN),\n an\n ANN\n trained\n with\n PSO\n algorithm\n(PSOANN),\n and\n an\n ANN\n trained\n by\n a\n new\n learning\n algorithm\n (ANN-\nnew)\n [27].\n It\n is\n obvious\n that\n BMOANN\n yields\n better\n result\n than\n the\nother\n ANNs.\n To\n observe\n the\n similarity\n between\n the\n experimental\ndata\n and\n those\n predicted\n by\n the\n ANN,\n Fig.\n 5\n illustrates\n the\n cell\n volt-\nage\n vs.\n current\n density\n for\n the\n test\n data.\n From\n the\n \ufb01gure,\n we\n can\nsee\n that\n the\n BMOANN\n has\n successfully\n modeled\n the\n PEMFC\n system\nso\n that\n the\n experimental\n data\n and\n model\n results\n are\n close\n to\n each\nother.\n4.\n Conclusion\nThe\n main\n goal\n of\n this\n paper\n is\n to\n develop\n a\n BMO-based\n learning\nalgorithm\n to\n train\n ANNs.\n BMO\n is\n a\n recently\n devised\n population-\nbased\n optimization\n algorithm\n which\n imitates\n the\n mating\n behavior\nof\n bird\n species\n for\n breeding\n superior\n broods\n and\n provides\n differ-\nent\n strategies\n to\n effectively\n seek\n the\n search\n space.\n The\n main\n merit\nof\n the\n algorithm\n is\n to\n employ\n distinct\n patterns\n to\n move\n through\nthe\n search\n space\n which\n leads\n to\n avoiding\n premature\n convergence\nand\n maintaining\n population\n diversity.\n BMOANN\n is\n \ufb01rstly\n used\n to\nclassify\n three\n real-world\n problems.\n The\n results\n of\n the\n BMOANN\n are\npromising\n when\n we\n compare\n its\n performance\n with\n those\n of\n the\nother\n classi\ufb01ers.\n Although\n BMO\n algorithm\n is\n not\n the\n best\n classi\ufb01er,\nhas\n a\n satisfying\n performance\n and\n can\n compete\n the\n already\n known\nclassi\ufb01ers.\n BMO\n is\n then\n applied\n to\n build\n an\n ANN\n model\n for\n polariza-\ntion\n curve\n of\n the\n Ballard\n MK5-E\n PEMFC\n system.\n Simulation\n results\ndisclose\n that\n the\n model\n results\n are\n in\n good\n agreement\n with\n the\nexperimental\n data\n and\n BMOANN\n yields\n better\n result\n than\n the\n other\nANNs.\n Therefore,\n BMO\n algorithm\n can\n be\n an\n ef\ufb01cient\n candidate\n for\ntraining\n ANNs.\nReferences\n[1] R.J.\n Schalkoff,\n Arti\ufb01cial\n Neural\n Networks,\n McGraw-Hill,\n New\n York,\n 1997.\n[2]  D.E.\n Rumelhart,\n G.E.\n Hinton,\n R.J.\n Williams,\n Learning\n representations\n by\n back-\npropagating\n errors,\n Nature\n 323\n (1986)\n 533\u2013536.\n[3] D.J.\n Montana,\n L.\n Davis,\n Training\n feedforward\n neural\n networks\n using\n genetic\nalgorithms,\n Machine\n Learning\n (1989)\n 762\u2013767.\n[4] V.G.\n Gudise,\n G.K.\n Venayagamoorthy,\n Comparison\n of\n particle\n swarm\n optimiza-\ntion\n and\n backpropagation\n as\n training\n algorithms\n for\n neural\n networks,\n in:\nProceedings\n of\n the\n 2003\n IEEE,\n Swarm\n Intelligence\n Symposium,\n 2003.\n SIS\n \u201903,\n2003,\n pp.\n 110\u2013117.\n[5] K.\n Socha,\n C.\n Blum,\n An\n ant\n colony\n optimization\n algorithm\n for\n continuous\noptimization:\n application\n to\n feed-forward\n neural\n network\n training,\n Neural\nComputing\n &\n Applications\n 16\n (2007)\n 235\u2013247.\n[6]\n C.\n Blum,\n K.\n Socha,\n Training\n feed-forward\n neural\n networks\n with\n ant\n colony\noptimization:\n an\n application\n to\n pattern\n classi\ufb01cation,\n in:\n Hybrid\n Intelligent\nSystems,\n 2005,\n HIS\n \u201905,\n Fifth\n International\n Conference\n on,\n 2005.\n[7] Y.\n Xin,\n Evolving\n arti\ufb01cial\n neural\n networks,\n Proceedings\n of\n the\n IEEE\n 87\n (1999)\n1423\u20131447.\n[8] X.\n Yao,\n A\n review\n of\n evolutionary\n arti\ufb01cial\n neural\n networks,\n International\n Journal\nof Intelligent\n Systems\n 8\n (1993)\n 539\u2013567.\n[9] E.\n Cant\u00fa-Paz,\n C.\n Kamath,\n An\n empirical\n comparison\n of\n combinations\n of\nevolutionary\n algorithms\n and\n neural\n networks\n for\n classi\ufb01cation\n problems,\nIEEE\n Transactions\n on\n Systems,\n Man,\n and\n Cybernetics,\n Part-B\n 35\n (2005)\n915\u2013927.\n[10]\n A.\n Askarzadeh,\n A.\n Rezazadeh,\n A\n new\n heuristic\n optimization\n algorithm\n for\nmodeling\n of\n proton\n exchange\n membrane\n fuel\n cell:\n bird\n mating\n optimizer,\n Inter-\nnational\n Journal\n of\n Energy\n Research,\n in\n press\n[11]\n http://en.wikipedia.org/wiki/Bird.\n in\n press.\n[12]\n http://www.stanford.edu/group/stanfordbirds/text/essays/Monogamy.html.\nin\n press.\n[13] http://www.paulnoll.com/Oregon/Birds/courtship-systems.html.\n in\n press.\n[14] T.\n Van\n Gestel,\n Benchmarking\n least\n squares\n support\n vector\n machine\n classi\ufb01ers,\nMachine\n Learning\n 54\n (2004)\n 5\u201332.\n[15]\n S.\n D\u02c7zeroski,\n B. \u02c7Zenko,\n Is\n combining\n classi\ufb01ers\n with\n stacking\n better\n than\n selecting\nthe\n best\n one?\n Machine\n Learning\n 54\n (2004)\n 255\u2013273.\n[16] T.G.\n Dietterich,\n An\n experimental\n comparison\n of\n three\n methods\n for\n constructing\nensembles\n of\n decision\n trees:\n bagging,\n boosting,\n and\n randomization,\n Machine\nLearning\n 40\n (2000)\n 139\u2013157.\n[17] E.\n Cantu-Paz,\n C.\n Kamath,\n Inducing\n oblique\n decision\n trees\n with\n evolutionary\nalgorithms,\n IEEE\n Abbreviations\n for\n Transactions,\n Journals,\n Letters,\n and\n Maga-\nzines\n 7\n (2003)\n 54\u201368.\n[18]\n S.K.\n Murthy,\n On\n Growing\n Better\n Decision\n Trees\n from\n Data,\n Doctoral\n Disserta-\ntion,\n University\n of\n Maryland,1997.\n[19]\n N.\n Garc\u00eda-Pedrajas,\n C.\n Hervas-Martinez,\n D.\n Ortiz-Boyer,\n Cooperative\n coevo-\nlution\n of\n arti\ufb01cial\n neural\n network\n ensembles\n for\n pattern\n classi\ufb01cation,\n IEEE\nAbbreviations\n for\n Transactions,\n Journals,\n Letters,\n and\n Magazines\n 9\n (2005)\n271\u2013302.\n[20]\n M.\n Islam,\n X.\n Yao,\n K.\n Murase,\n A\n constructive\n algorithm\n for\n training\n cooperative\nneural\n network\n ensembles,\n IEEE\n Transactions\n on\n Neural\n Networks\n 14\n (2003)\n820\u2013834.\n[21] P.P.\n Palmes,\n T.\n Hayasaka,\n S.\n Usui,\n Mutation-based\n genetic\n neural\n network,\n IEEE\nTransactions\n on\n Neural\n Networks\n 16\n (2005)\n 587\u2013600.\n[22]\n A.\n Askarzadeh,\n A.\n Rezazadeh,\n A\n grouping-based\n global\n harmony\n search\n algo-\nrithm\n for\n modeling\n of\n proton\n exchange\n membrane\n fuel\n cell,\n International\nJournal\n of\n Hydrogen\n Energy\n 36\n (2011)\n 5047\u20135053.\n[23]\n A.\n Askarzadeh,\n A.\n Rezazadeh,\n Arti\ufb01cial\n immune\n system-based\n parameter\nextraction\n of\n proton\n exchange\n membrane\n fuel\n cell,\n International\n Journal\n of\nElectrical\n Power\n &\n Energy\n Systems\n 33\n (2011)\n 933\u2013938.\n[24]\n M.\n Ye,\n X.\n Wang,\n Y.\n Xu,\n Parameter\n identi\ufb01cation\n for\n proton\n exchange\n mem-\nbrane\n fuel\n cell\n model\n using\n particle\n swarm\n optimization,\n International\n Journal\nof Hydrogen\n Energy\n 34\n (2009)\n 981\u2013989.\n[25]\n X.D.\n Xue,\n et\n al.,\n Uni\ufb01ed\n mathematical\n modelling\n of\n steady-state\n and\n dynamic\nvoltage\u2013current\n characteristics\n for\n PEM\n fuel\n cells,\n Electrochimica\n Acta\n 52\n(2006)\n 1135\u20131144.\n[26]\n Z.-D.\n Zhong,\n X.-J.\n Zhu,\n G.-Y.\n Cao,\n Modeling\n a PEMFC\n by\n a\n support\n vector\n machine,\nJournal\n of\n Power\n Sources\n 160\n (2006)\n 293\u2013298.\n[27]\n A.\n Rezazadeh,\n A.\n Askarzadeh,\n M.\n sedighizadeh,\n ANN-based\n PEMFC\n modeling\n by\na\n new\n learning\n algorithm,\n The\n International\n Review\n on\n Modelling\n and\n Simula-\ntions 3\n (2010)\n 187\u2013193.\n", []], "4 Conclusion": ["A.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\n \n1213\nTable\n 8\nError\n rate\n (%)\n of\n BMOANN\n for\n modeling\n of\n the\n Ballard\n MK5-E\n PEMFC\n system.\n The\nresults\n have\n been\n averaged\n over\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nTest\n set\nAvg.\nStd.\nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\nBMOANN\n0.11\n6.18e\u22122\n \n0.02\n0.29\n0.11\n \n7.52e\u22122\n \n0.01\n \n0.35\nTable\n 9\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n the\n best\n testing\n error\n rate\n(%)\n on\n the\n Ballard\n MK5-E\n PEMFC\n system.\nAlgorithm\n \nBMOANN\n \nPSOANN\n \nBPANN\n \nANN-new\nTest\n error\n rate\n (%)\n \n0.01\n \n0.0247\n \n0.14\n \n0.024\n0 \n100 \n200 \n300 \n400 \n500 \n600 \n700\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nCell voltage (V)\nCurrent density (mA/cm2)\nANN model - 56 \u00baC \ndata - 56 \u00baC \nFig.\n 5.\n Comparison\n between\n experimental\n data\n and\n ANN\n model\n for\n the\n test\n set.\nWe\n have\n tabulated\n the\n results\n generated\n by\n BMOANN\n over\n50\n independent\n runs\n in\n Table\n 8.\n Table\n 9\n shows\n the\n compari-\nson\n between\n the\n BMO\n result\n and\n the\n best\n performance\n of\n an\nANN\n trained\n by\n BP\n (BPANN),\n an\n ANN\n trained\n with\n PSO\n algorithm\n(PSOANN),\n and\n an\n ANN\n trained\n by\n a\n new\n learning\n algorithm\n (ANN-\nnew)\n [27].\n It\n is\n obvious\n that\n BMOANN\n yields\n better\n result\n than\n the\nother\n ANNs.\n To\n observe\n the\n similarity\n between\n the\n experimental\ndata\n and\n those\n predicted\n by\n the\n ANN,\n Fig.\n 5\n illustrates\n the\n cell\n volt-\nage\n vs.\n current\n density\n for\n the\n test\n data.\n From\n the\n \ufb01gure,\n we\n can\nsee\n that\n the\n BMOANN\n has\n successfully\n modeled\n the\n PEMFC\n system\nso\n that\n the\n experimental\n data\n and\n model\n results\n are\n close\n to\n each\nother.\n4.\n Conclusion\nThe\n main\n goal\n of\n this\n paper\n is\n to\n develop\n a\n BMO-based\n learning\nalgorithm\n to\n train\n ANNs.\n BMO\n is\n a\n recently\n devised\n population-\nbased\n optimization\n algorithm\n which\n imitates\n the\n mating\n behavior\nof\n bird\n species\n for\n breeding\n superior\n broods\n and\n provides\n differ-\nent\n strategies\n to\n effectively\n seek\n the\n search\n space.\n The\n main\n merit\nof\n the\n algorithm\n is\n to\n employ\n distinct\n patterns\n to\n move\n through\nthe\n search\n space\n which\n leads\n to\n avoiding\n premature\n convergence\nand\n maintaining\n population\n diversity.\n BMOANN\n is\n \ufb01rstly\n used\n to\nclassify\n three\n real-world\n problems.\n The\n results\n of\n the\n BMOANN\n are\npromising\n when\n we\n compare\n its\n performance\n with\n those\n of\n the\nother\n classi\ufb01ers.\n Although\n BMO\n algorithm\n is\n not\n the\n best\n classi\ufb01er,\nhas\n a\n satisfying\n performance\n and\n can\n compete\n the\n already\n known\nclassi\ufb01ers.\n BMO\n is\n then\n applied\n to\n build\n an\n ANN\n model\n for\n polariza-\ntion\n curve\n of\n the\n Ballard\n MK5-E\n PEMFC\n system.\n Simulation\n results\ndisclose\n that\n the\n model\n results\n are\n in\n good\n agreement\n with\n the\nexperimental\n data\n and\n BMOANN\n yields\n better\n result\n than\n the\n other\nANNs.\n Therefore,\n BMO\n algorithm\n can\n be\n an\n ef\ufb01cient\n candidate\n for\ntraining\n ANNs.\nReferences\n[1] R.J.\n Schalkoff,\n Arti\ufb01cial\n Neural\n Networks,\n McGraw-Hill,\n New\n York,\n 1997.\n[2]  D.E.\n Rumelhart,\n G.E.\n Hinton,\n R.J.\n Williams,\n Learning\n representations\n by\n back-\npropagating\n errors,\n Nature\n 323\n (1986)\n 533\u2013536.\n[3] D.J.\n Montana,\n L.\n Davis,\n Training\n feedforward\n neural\n networks\n using\n genetic\nalgorithms,\n Machine\n Learning\n (1989)\n 762\u2013767.\n[4] V.G.\n Gudise,\n G.K.\n Venayagamoorthy,\n Comparison\n of\n particle\n swarm\n optimiza-\ntion\n and\n backpropagation\n as\n training\n algorithms\n for\n neural\n networks,\n in:\nProceedings\n of\n the\n 2003\n IEEE,\n Swarm\n Intelligence\n Symposium,\n 2003.\n SIS\n \u201903,\n2003,\n pp.\n 110\u2013117.\n[5] K.\n Socha,\n C.\n Blum,\n An\n ant\n colony\n optimization\n algorithm\n for\n continuous\noptimization:\n application\n to\n feed-forward\n neural\n network\n training,\n Neural\nComputing\n &\n Applications\n 16\n (2007)\n 235\u2013247.\n[6]\n C.\n Blum,\n K.\n Socha,\n Training\n feed-forward\n neural\n networks\n with\n ant\n colony\noptimization:\n an\n application\n to\n pattern\n classi\ufb01cation,\n in:\n Hybrid\n Intelligent\nSystems,\n 2005,\n HIS\n \u201905,\n Fifth\n International\n Conference\n on,\n 2005.\n[7] Y.\n Xin,\n Evolving\n arti\ufb01cial\n neural\n networks,\n Proceedings\n of\n the\n IEEE\n 87\n (1999)\n1423\u20131447.\n[8] X.\n Yao,\n A\n review\n of\n evolutionary\n arti\ufb01cial\n neural\n networks,\n International\n Journal\nof Intelligent\n Systems\n 8\n (1993)\n 539\u2013567.\n[9] E.\n Cant\u00fa-Paz,\n C.\n Kamath,\n An\n empirical\n comparison\n of\n combinations\n of\nevolutionary\n algorithms\n and\n neural\n networks\n for\n classi\ufb01cation\n problems,\nIEEE\n Transactions\n on\n Systems,\n Man,\n and\n Cybernetics,\n Part-B\n 35\n (2005)\n915\u2013927.\n[10]\n A.\n Askarzadeh,\n A.\n Rezazadeh,\n A\n new\n heuristic\n optimization\n algorithm\n for\nmodeling\n of\n proton\n exchange\n membrane\n fuel\n cell:\n bird\n mating\n optimizer,\n Inter-\nnational\n Journal\n of\n Energy\n Research,\n in\n press\n[11]\n http://en.wikipedia.org/wiki/Bird.\n in\n press.\n[12]\n http://www.stanford.edu/group/stanfordbirds/text/essays/Monogamy.html.\nin\n press.\n[13] http://www.paulnoll.com/Oregon/Birds/courtship-systems.html.\n in\n press.\n[14] T.\n Van\n Gestel,\n Benchmarking\n least\n squares\n support\n vector\n machine\n classi\ufb01ers,\nMachine\n Learning\n 54\n (2004)\n 5\u201332.\n[15]\n S.\n D\u02c7zeroski,\n B. \u02c7Zenko,\n Is\n combining\n classi\ufb01ers\n with\n stacking\n better\n than\n selecting\nthe\n best\n one?\n Machine\n Learning\n 54\n (2004)\n 255\u2013273.\n[16] T.G.\n Dietterich,\n An\n experimental\n comparison\n of\n three\n methods\n for\n constructing\nensembles\n of\n decision\n trees:\n bagging,\n boosting,\n and\n randomization,\n Machine\nLearning\n 40\n (2000)\n 139\u2013157.\n[17] E.\n Cantu-Paz,\n C.\n Kamath,\n Inducing\n oblique\n decision\n trees\n with\n evolutionary\nalgorithms,\n IEEE\n Abbreviations\n for\n Transactions,\n Journals,\n Letters,\n and\n Maga-\nzines\n 7\n (2003)\n 54\u201368.\n[18]\n S.K.\n Murthy,\n On\n Growing\n Better\n Decision\n Trees\n from\n Data,\n Doctoral\n Disserta-\ntion,\n University\n of\n Maryland,1997.\n[19]\n N.\n Garc\u00eda-Pedrajas,\n C.\n Hervas-Martinez,\n D.\n Ortiz-Boyer,\n Cooperative\n coevo-\nlution\n of\n arti\ufb01cial\n neural\n network\n ensembles\n for\n pattern\n classi\ufb01cation,\n IEEE\nAbbreviations\n for\n Transactions,\n Journals,\n Letters,\n and\n Magazines\n 9\n (2005)\n271\u2013302.\n[20]\n M.\n Islam,\n X.\n Yao,\n K.\n Murase,\n A\n constructive\n algorithm\n for\n training\n cooperative\nneural\n network\n ensembles,\n IEEE\n Transactions\n on\n Neural\n Networks\n 14\n (2003)\n820\u2013834.\n[21] P.P.\n Palmes,\n T.\n Hayasaka,\n S.\n Usui,\n Mutation-based\n genetic\n neural\n network,\n IEEE\nTransactions\n on\n Neural\n Networks\n 16\n (2005)\n 587\u2013600.\n[22]\n A.\n Askarzadeh,\n A.\n Rezazadeh,\n A\n grouping-based\n global\n harmony\n search\n algo-\nrithm\n for\n modeling\n of\n proton\n exchange\n membrane\n fuel\n cell,\n International\nJournal\n of\n Hydrogen\n Energy\n 36\n (2011)\n 5047\u20135053.\n[23]\n A.\n Askarzadeh,\n A.\n Rezazadeh,\n Arti\ufb01cial\n immune\n system-based\n parameter\nextraction\n of\n proton\n exchange\n membrane\n fuel\n cell,\n International\n Journal\n of\nElectrical\n Power\n &\n Energy\n Systems\n 33\n (2011)\n 933\u2013938.\n[24]\n M.\n Ye,\n X.\n Wang,\n Y.\n Xu,\n Parameter\n identi\ufb01cation\n for\n proton\n exchange\n mem-\nbrane\n fuel\n cell\n model\n using\n particle\n swarm\n optimization,\n International\n Journal\nof Hydrogen\n Energy\n 34\n (2009)\n 981\u2013989.\n[25]\n X.D.\n Xue,\n et\n al.,\n Uni\ufb01ed\n mathematical\n modelling\n of\n steady-state\n and\n dynamic\nvoltage\u2013current\n characteristics\n for\n PEM\n fuel\n cells,\n Electrochimica\n Acta\n 52\n(2006)\n 1135\u20131144.\n[26]\n Z.-D.\n Zhong,\n X.-J.\n Zhu,\n G.-Y.\n Cao,\n Modeling\n a PEMFC\n by\n a\n support\n vector\n machine,\nJournal\n of\n Power\n Sources\n 160\n (2006)\n 293\u2013298.\n[27]\n A.\n Rezazadeh,\n A.\n Askarzadeh,\n M.\n sedighizadeh,\n ANN-based\n PEMFC\n modeling\n by\na\n new\n learning\n algorithm,\n The\n International\n Review\n on\n Modelling\n and\n Simula-\ntions 3\n (2010)\n 187\u2013193.\n", []], "3.2 Using BMO for modeling of PEMFC": ["1212\n \nA.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\nTable\n 5\nError\n rate\n (%)\n of\n BMOANN\n for\n Pima\n Indian\n diabetes.\n The\n results\n have\n been\n averaged\n over\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nValidation\n set\n \nTest\n set\nAvg.\n \nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\nBMOANN\n \n23.57\n \n1.44\n \n20.83\n \n28.13\n \n19.45\n \n1.25\n \n16.67\n \n22.92\n \n22.55\n \n1.89\n \n18.23\n \n27.08\nTable\n 6\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Pima\n diabetes\n disease.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nCOOP\n \nCNNE\n \nSVM-best\nTest\n error\n rate\n (%)\n22.55\n \n24.70\n \n19.69\n \n19.60\n \n22.7\nAlgorithm\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n \n24.02\n \n26.0\nThe\n statistical\n performance\n of\n BMOANN\n over\n 50\n runs\n in\n solving\nPima\n Indian\n diabetes\n problem\n has\n been\n summarized\n in\n Table\n 5.\nThe\n results\n obtained\n by\n the\n other\n classi\ufb01ers\n are\n shown\n in\n Table\n 6.\nFrom\n the\n table,\n the\n best\n performance\n belongs\n to\n CNNE\n classi\ufb01er.\nThe\n results\n found\n by\n COOP\n and\n BMOANN\n classi\ufb01ers\n are\n in\n the\n next\norders.\n In\n this\n case,\n BMOANN\n outperforms\n GANet-best,\n SVM-best,\nCCSS,\n and\n OC1-best\n classi\ufb01ers.\nIn\n order\n to\n make\n a\n conclusion,\n the\n rank\n of\n each\n classi\ufb01er\n to\nsolve\n the\n benchmark\n problems\n has\n been\n indicated\n in\n Table\n 7.\nRank\n 1,\n Rank\n 2,\n and\n Rank\n 3\n denote\n the\n rank\n of\n each\n clas-\nsi\ufb01er\n to\n solve\n Iris\n \ufb02ower,\n Wisconsin\n breast\n cancer,\n and\n Pima\nIndian\n diabetes\n problems,\n respectively.\n In\n summary,\n as\n the\n last\ncolumn\n of\n Table\n 7\n indicates,\n the\n capability\n of\n the\n classi\ufb01ers\ntested\n here\n can\n be\n ordered\n as\n CNNE\n >\n BMOANN\n >\n COOP\n >\n GANet-\nbest\n >\n SVM-best\n >\n CCSS\n =\n EDTs\n >\n OC1-best\n >\n MGNN.\n It\n can\n be\n found\nthat\n BMOANN\n has\n been\n ranked\n the\n second\n and\n outperformed\n by\nCNNE\n classi\ufb01er.\n It\n should\n be\n noted\n that\n the\n most\n of\n the\n classi\ufb01ers\nare\n sophisticated\n ones\n using\n different\n techniques\n to\n improve\n their\nperformance,\n while\n BMOANN\n has\n simple\n structure\n which\n only\n uses\nthe\n search\n power\n of\n BMO\n to\n tune\n the\n ANNs\n parameters.\n3.2.\n Using\n BMO\n for\n modeling\n of\n PEMFC\nAs\n one\n of\n the\n most\n promising\n renewable\n energy\n resources,\nPEMFC\n has\n signi\ufb01cantly\n attracted\n the\n attention\n of\n industrial\n own-\ners.\n PEMFC\n is\n an\n electrochemical\n device\n which\n converts\n the\n stored\nchemical\n energy\n of\n hydrogen\n and\n oxygen\n into\n electricity.\n Polar-\nization\n curve,\n representing\n the\n fuel\n cell\n voltage\n vs.\n current\n (V\u2013I),\nis\n one\n of\n the\n most\n important\n characteristics\n of\n fuel\n cells.\n It\n is\n an\nimportant\n tool\n for\n researchers,\n because\n optimization\n of\n fuel\n cell\noperating\n points\n and\n design\n of\n the\n power\n conditioning\n units,\n sim-\nulators\n for\n fuel\n cell\n stack\n systems,\n as\n well\n as\n the\n system\n controllers\ndepend\n on\n such\n characteristic\n [25].\n Therefore,\n accurate\n modeling\nof\n V\u2013I\n characteristics\n is\n necessary.\nTable\n 7\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n their\n rank\n in\n solving\n the\nclassi\ufb01cation\n problems.\nAlgorithm\n \nRank\n 1\n \nRank\n 2\n \nRank\n 3\n \nAverage\n rank\n \nFinal\n rank\nBMOANN\n \n3\n \n2\n \n3\n \n2.67\n \n2\nGANet-best\n \n5\n \n1\n \n6\n \n4\n \n4\nSVM-best\n \n1\n \n8\n \n4\n \n4.33\n \n5\nCCSS\n \n4\n \n6\n \n5\n \n5\n \n6\nOC1-best\n2\n9\n7\n \n6\n \n8\nCOOP\n\u2013 \n4\n \n2\n \n3\n \n3\nCNNE\n \n\u2013\n \n3\n \n1\n \n2\n \n1\nEDTs\n\u2013\n \n5\n \n\u2013\n \n5\n \n6\nMGNN\n \n\u2013\n \n7\n \n\u2013\n \n7\n \n9\nTwo\n main\n modeling\n can\n be\n found\n in\n the\n literature\n for\n PEMFC\nsystem:\n mathematical\n modeling\n and\n ANN-based\n models.\n Mathe-\nmatical\n models\n describe\n fuel\n cell\n behavior\n by\n a\n variety\n of\n equations.\nThe\n nonlinearity\n and\n complexity\n of\n PEMFC\n result\n in\n considering\nmany\n assumptions\n and\n approximations\n during\n modeling.\n Besides,\nPEMFC\n is\n a\n time-varying\n system\n whose\n parameters\n are\n extremely\nrelated\n to\n the\n operating\n conditions,\n and\n a\n given\n set\n of\n operat-\ning\n conditions\n requires\n a\n corresponding\n set\n of\n parameters.\n To\nobtain\n accurate\n results\n with\n mathematical\n models,\n parameter\n iden-\nti\ufb01cation\n must\n be\n performed\n in\n each\n operating\n condition.\n These\ndrawbacks\n greatly\n limit\n mathematical\n model\u2019s\n application.\n To\n pro-\nvide\n a\n better\n approach,\n ANN\n can\n be\n an\n ef\ufb01cient\n candidate\n to\n model\nPEMFC\n behavior.\nThe\n relation\n of\n fuel\n cell\n voltage,\n V,\n and\n current\n density,\n I, is\n in\ufb02u-\nenced\n by\n many\n operating\n parameters,\n such\n as\n cell\n temperature,\n Tc,\nhumidity,\n \u0007,\n hydrogen\n pressure,\n ph2, oxygen\n pressure,\n po2, hydro-\ngen\n \ufb02ow\n rate,\n qh2, oxygen\n \ufb02ow\n rate,\n qo2, etc.\n It\n is\n de\ufb01ned\n by\n the\nfollowing\n equation.\nV\n =\n f\n (I,\n Tc, \u0007,\n ph2,\n po2,\n qh2, qo2, . . .)  \n(9)\nA\n model\n considering\n all\n the\n operating\n parameters\n has\n not\n been\ndeveloped,\n so\n far.\n Our\n ANN-based\n model\n is\n not\n exception.\n Here,\ncurrent\n density\n and\n cell\n temperature\n are\n taken\n as\n variable\n param-\neters\n while\n the\n others\n are\n constant.\n So,\n Eq.\n (9)\n is\n simpli\ufb01ed\n and\nexpressed\n by\n the\n following\n equation.\nV\n =\n f\n (I,\n Tc) \n(10)\nA\n three-layer\n feed-forward\n ANN\n with\n two\n inputs\n and\n one\n output\nis\n constructed\n to\n model\n polarization\n curve\n of\n the\n Ballard\n MK5-E\nPEMFC\n system\n [26].\n In\n this\n case,\n activation\n function\n used\n in\n the\noutput\n layer\n is\n linear\n and\n genmax is\n set\n to\n 200.\n The\n other\n parameters\nare\n same\n as\n those\n of\n the\n previous\n investigations.\n Five\n sets\n of\n data\n are\nused\n to\n train\n and\n test\n the\n ANN\n model\n as\n Fig.\n 4.\n In\n order\n to\n train\n the\nANN,\n the\n experimental\n data\n obtained\n at\n 24,\n 31,\n 39,\n and\n 72 \u25e6C\n is\n used,\nand\n the\n data\n obtained\n at\n 56 \u25e6C\n is\n employed\n to\n test\n the\n ANN-based\nPEMFC\n model.\n0 \n100 \n200 \n300\n400\n500\n600\n700\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nCell voltage (V)\nCurrent density (mA/cm2)\n24 \u00baC \n31 \u00baC \n39 \u00baC  \n72 \u00baC \n56 \u00baC \nFig.\n 4.\n Experimental\n data\n obtained\n from\n the\n Ballard\n MK5-E\n PEMFC\n system.\n", []], "3.1.3 Pima Indian diabetes": ["A.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\n \n1211\nTable\n 1\nError\n rate\n (%)\n of\n BMOANN\n for\n Iris\n \ufb02ower\n problem.\n The\n results\n have\n been\n averaged\nover\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nTest\n set\nAvg.\n \nStd.\n \nMin.\n \nMax.\n \nAvg.\n \nStd.\n \nMin.\n \nMax.\nBMOANN\n \n1.50\n \n0.78\n \n1.00\n \n4.00\n \n3.88\n \n1.72\n \n0.00\n \n8.00\nTable\n 2\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Iris\n \ufb02ower\n problem.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nSVM-best\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n3.88\n \n6.40\n \n1.40\n \n4.40\n \n3.70\npolyandrous,\n and\n promiscuous\n birds\n make\n 50%,\n 40%,\n 5%,\n and\n 5%\n of\nthe\n society,\n respectively;\n T,\n w,\n and\n mw are\n de\ufb01ned\n as\n decreasing\nlinear\n functions\n where\n Tmax =\n 300,\n Tmin =\n 50,\n wmax =\n 2.5,\n wmin =\n 0.5,\nmw,max =\n 0.01,\n and\n mw,min =\n 0.0001;\n mcf\n is\n selected\n 0.9\n and\n number\nof\n monogamous\n birds\n participating\n in\n rituals\n of\n the\n polyandrous\nbirds\n is\n set\n to\n 5;\n Maximum\n number\n of\n generations\n (epochs)\n is\n set\nto\n 100\n for\n Iris\n \ufb02ower\n and\n Pima\n Indian\n diabetes,\n and\n is\n set\n to\n 50\n for\nWisconsin\n breast\n cancer;\n To\n limit\n the\n search\n rage\n l\n and\n u\n are\n set\nto\n \u221215\n and\n 15,\n respectively.\n It\n should\n be\n noted\n that\n the\n parameter\nsetting\n is\n based\n on\n trial\n and\n no\n attempt\n has\n made\n to\n optimize\n it.\nMatlab\n environment\n is\n implemented\n to\n code\n our\n BMOANN.\n Due\nto\n the\n fact\n that\n the\n nature\n of\n metaheuristic\n algorithms\n is\n stochas-\ntic,\n the\n results\n obtained\n in\n one\n attempt\n will\n differ\n from\n the\n results\nobtained\n in\n another\n attempt.\n Therefore,\n the\n performance\n analysis\nmust\n be\n statistically\n based.\n As\n a\n result,\n in\n each\n case,\n 50\n indepen-\ndent\n runs\n are\n performed\n to\n get\n an\n average\n result\n of\n the\n algorithm.\nTo\n compare\n the\n training\n capability\n of\n our\n BMO,\n the\n results\n will\nbe\n compared\n with\n those\n of\n the\n other\n classi\ufb01ers\n from\n the\n litera-\nture.\n The\n results\n have\n been\n adopted\n from\n the\n literature\n directly\n for\ncomparison.\n3.1.1.\n Iris\n \ufb02ower\nThis\n dataset\n consists\n of\n 150\n samples\n from\n three\n species\n of\n Iris\n\ufb02owers\n (Setosa,\n Virginica,\n and\n Versicolor).\n From\n each\n species,\n four\nfeatures\n including\n the\n length\n and\n the\n width\n of\n sepal\n and\n petal\n have\nbeen\n measured.\n In\n order\n to\n develop\n an\n ANN\n to\n classify\n this\n mul-\nticlass\n problem,\n the\n dataset\n is\n randomly\n partitioned\n as\n a\n training\nset\n including\n 100\n instances\n and\n a\n test\n set\n including\n the\n rest\n 50\ninstances.\nThe\n architecture\n of\n BMOANN\n is\n experimentally\n selected\n 4-5-2,\nmeaning\n that\n 5\n hyperbolic\n tangent\n and\n 2\n step\n functions\n are\n located\nin\n the\n hidden\n and\n output\n layers,\n respectively.\n Due\n to\n the\n face\n that\nIris\n \ufb02ower\n is\n a\n multiclass\n problem\n with\n three\n classes,\n in\n the\n output\nlayer\n two\n nodes\n are\n used.\n The\n computer\n program\n has\n been\n written\nso\n that\n the\n outputs\n of\n 00,\n 01,\n and\n 11\n denote\n the\n \ufb01rst,\n second,\n and\nthird\n classes,\n respectively.\nTable\n 1\n reports\n the\n performance\n of\n BMOANN\n statistically\n in\n solv-\ning\n Iris\n \ufb02ower\n problem\n over\n 50\n runs.\n As\n can\n be\n seen,\n BMOANN\n even\nsolves\n this\n problem\n with\n 0%\n of\n error\n rate\n for\n the\n test\n set.\n To\n evaluate\nthe\n performance\n of\n BMOANN\n in\n solving\n the\n Iris\n \ufb02ower\n problem,\n the\nresults\n of\n the\n other\n methods\n are\n tabulated\n in\n Table\n 2\n in\n comparison\nwith\n the\n BMOANN\n result.\n As\n Table\n 2\n indicates,\n BMOANN\n produces\nbetter\n results\n than\n GANet-best\n and\n CCSS\n and\n is\n outperformed\n by\nTable\n 4\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Wisconsin\n breast\n cancer\n problem.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nCOOP\n \nCNNE\n \nEDTs\nTest\n error\n rate\n (%)\n1.16\n1.06\n1.23\n1.20\n \n2.63\nAlgorithm\n \nMGNN\n \nSVM-best\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n3.05\n \n3.1\n \n2.72\n \n3.9\nSVM-best\n and\n OC1-best\n methods.\n However,\n the\n error\n rate\n is\n slightly\nworse\n than\n that\n of\n the\n OC1-best.\n The\n performance\n of\n BMOANN\n in\ncomparison\n with\n CCSS\n and\n OC1-best\n is\n promising;\n because\n they\nare\n state-of-the-art\n decision\n trees\n classi\ufb01ers.\n It\n is\n worth\n to\n mention\nthat\n GANet-best,\n SVM-best,\n and\n CCSS\n classi\ufb01ers\n use\n k-fold\n cross-\nvalidation,\n which\n leads\n to\n generating\n more\n optimistic\n results.\n3.1.2.\n Wisconsin\n breast\n cancer\nThe\n purpose\n of\n this\n dataset\n is\n to\n classify\n a\n tumor\n as\n either\nbenign\n or\n malignant.\n This\n dataset\n has\n 9\n integer\n attributes\n with\n 699\ninstances\n of\n which\n 458\n instances\n are\n benign\n and\n 241\n instances\n are\nmalignant.\n In\n order\n to\n train\n the\n ANN\n for\n classi\ufb01cation,\n the\n data\n set\nis\n divided\n into\n three\n sets:\n a\n training\n set\n with\n the\n \ufb01rst\n 349\n instances\nis\n used\n to\n adjust\n the\n ANN\n weights,\n a\n validation\n set\n with\n the\n fol-\nlowing\n 175\n instances\n is\n used\n to\n minimize\n over\ufb01tting,\n and\n a\n test\n set\nwith\n the\n \ufb01nal\n 175\n instances\n is\n used\n only\n for\n \ufb01nal\n solution\n in\n order\nto\n verify\n the\n ANN\n predictive\n power.\n As\n previously\n mentioned,\n the\nvalidation\n set\n is\n used\n to\n minimize\n over\ufb01tting.\n The\n network\n weights\nare\n not\n adjusted\n with\n this\n data\n set.\n This\n set\n is\n only\n used\n to\n verify\n that\nany\n increase\n in\n the\n accuracy\n over\n the\n training\n set\n actually\n yields\n an\nincrease\n over\n the\n accuracy\n of\n a\n data\n set\n that\n has\n not\n been\n shown\nto\n the\n network\n before,\n or\n at\n least\n the\n network\n has\n not\n been\n trained\nby\n it\n (i.e.\n validation\n data\n set).\n If\n the\n accuracy\n over\n the\n training\n data\nset\n increases\n while\n the\n accuracy\n over\n the\n validation\n set\n decreases\nor\n stays\n the\n same,\n to\n avoid\n over\ufb01tting\n the\n training\n process\n must\n be\nstopped.\n The\n architecture\n of\n BMOANN\n is\n selected\n 9-5-1.\nWe  have\n tabulated\n the\n performance\n of\n BMOANN\n over\n 50\n runs\nin\n solving\n Wisconsin\n breast\n cancer\n problem\n in\n Table\n 3.\n It\n is\n obvious\nthat\n BMOANN\n at\n its\n best\n performance\n reaches\n to\n 0%\n of\n error\n rate\n for\nthe\n test\n set.\n The\n performance\n of\n the\n other\n eight\n techniques\n has\n been\nsummarized\n in\n Table\n 4\n in\n comparison\n with\n the\n BMOANN\n result.\n As\nresults\n reveal,\n BMOANN\n produces\n better\n results\n than\n COOP,\n CNNE,\nEDTs,\n MGNN,\n SVM-best,\n CCSS,\n and\n OC1-best.\n In\n this\n case,\n it\n is\n only\noutperformed\n by\n GANet-best\n technique.\n From\n the\n table,\n we\n can\nsee\n that\n BMOANN\n markedly\n outperforms\n SVM-best\n and\n OC1-best\nalgorithms.\n3.1.3.\n Pima\n Indian\n diabetes\nThis\n problem\n is\n one\n of\n the\n most\n dif\ufb01cult\n ones\n because\n the\n dataset\nis\n relatively\n small\n and\n is\n heavily\n polluted\n by\n noise.\n This\n dataset\n has\n768\n instances\n of\n patients\n of\n which\n 500\n patients\n have\n signs\n of\n dia-\nbetes\n and\n there\n are\n no\n signs\n of\n diabetes\n for\n the\n other\n 268\n patients.\nFrom\n each\n patient,\n eight\n features\n have\n been\n measured.\n The\n data\n set\nis\n divided\n into\n three\n sets:\n a\n training\n set\n with\n the\n \ufb01rst\n 384\n instances,\na\n validation\n set\n with\n the\n following\n 192\n instances,\n and\n a test\n set\n with\nthe\n \ufb01nal\n 192\n instances.\n The\n architecture\n of\n BMOANN\n is\n experimen-\ntally\n selected\n 8-5-1.\nTable\n 3\nError\n rate\n (%)\n of\n BMOANN\n for\n Wisconsin\n breast\n cancer.\n The\n results\n have\n been\n averaged\n over\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nValidation\n set\n \nTest\n set\nAvg.\nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\nBMOANN\n \n4.07\n \n0.94\n \n3.15\n \n6.88\n \n2.89\n \n0.82\n \n1.71\n \n4.57\n \n1.16\n \n0.59\n \n0.00\n \n2.29\n", []], "3.1.2 Wisconsin breast cancer": ["A.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\n \n1211\nTable\n 1\nError\n rate\n (%)\n of\n BMOANN\n for\n Iris\n \ufb02ower\n problem.\n The\n results\n have\n been\n averaged\nover\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nTest\n set\nAvg.\n \nStd.\n \nMin.\n \nMax.\n \nAvg.\n \nStd.\n \nMin.\n \nMax.\nBMOANN\n \n1.50\n \n0.78\n \n1.00\n \n4.00\n \n3.88\n \n1.72\n \n0.00\n \n8.00\nTable\n 2\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Iris\n \ufb02ower\n problem.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nSVM-best\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n3.88\n \n6.40\n \n1.40\n \n4.40\n \n3.70\npolyandrous,\n and\n promiscuous\n birds\n make\n 50%,\n 40%,\n 5%,\n and\n 5%\n of\nthe\n society,\n respectively;\n T,\n w,\n and\n mw are\n de\ufb01ned\n as\n decreasing\nlinear\n functions\n where\n Tmax =\n 300,\n Tmin =\n 50,\n wmax =\n 2.5,\n wmin =\n 0.5,\nmw,max =\n 0.01,\n and\n mw,min =\n 0.0001;\n mcf\n is\n selected\n 0.9\n and\n number\nof\n monogamous\n birds\n participating\n in\n rituals\n of\n the\n polyandrous\nbirds\n is\n set\n to\n 5;\n Maximum\n number\n of\n generations\n (epochs)\n is\n set\nto\n 100\n for\n Iris\n \ufb02ower\n and\n Pima\n Indian\n diabetes,\n and\n is\n set\n to\n 50\n for\nWisconsin\n breast\n cancer;\n To\n limit\n the\n search\n rage\n l\n and\n u\n are\n set\nto\n \u221215\n and\n 15,\n respectively.\n It\n should\n be\n noted\n that\n the\n parameter\nsetting\n is\n based\n on\n trial\n and\n no\n attempt\n has\n made\n to\n optimize\n it.\nMatlab\n environment\n is\n implemented\n to\n code\n our\n BMOANN.\n Due\nto\n the\n fact\n that\n the\n nature\n of\n metaheuristic\n algorithms\n is\n stochas-\ntic,\n the\n results\n obtained\n in\n one\n attempt\n will\n differ\n from\n the\n results\nobtained\n in\n another\n attempt.\n Therefore,\n the\n performance\n analysis\nmust\n be\n statistically\n based.\n As\n a\n result,\n in\n each\n case,\n 50\n indepen-\ndent\n runs\n are\n performed\n to\n get\n an\n average\n result\n of\n the\n algorithm.\nTo\n compare\n the\n training\n capability\n of\n our\n BMO,\n the\n results\n will\nbe\n compared\n with\n those\n of\n the\n other\n classi\ufb01ers\n from\n the\n litera-\nture.\n The\n results\n have\n been\n adopted\n from\n the\n literature\n directly\n for\ncomparison.\n3.1.1.\n Iris\n \ufb02ower\nThis\n dataset\n consists\n of\n 150\n samples\n from\n three\n species\n of\n Iris\n\ufb02owers\n (Setosa,\n Virginica,\n and\n Versicolor).\n From\n each\n species,\n four\nfeatures\n including\n the\n length\n and\n the\n width\n of\n sepal\n and\n petal\n have\nbeen\n measured.\n In\n order\n to\n develop\n an\n ANN\n to\n classify\n this\n mul-\nticlass\n problem,\n the\n dataset\n is\n randomly\n partitioned\n as\n a\n training\nset\n including\n 100\n instances\n and\n a\n test\n set\n including\n the\n rest\n 50\ninstances.\nThe\n architecture\n of\n BMOANN\n is\n experimentally\n selected\n 4-5-2,\nmeaning\n that\n 5\n hyperbolic\n tangent\n and\n 2\n step\n functions\n are\n located\nin\n the\n hidden\n and\n output\n layers,\n respectively.\n Due\n to\n the\n face\n that\nIris\n \ufb02ower\n is\n a\n multiclass\n problem\n with\n three\n classes,\n in\n the\n output\nlayer\n two\n nodes\n are\n used.\n The\n computer\n program\n has\n been\n written\nso\n that\n the\n outputs\n of\n 00,\n 01,\n and\n 11\n denote\n the\n \ufb01rst,\n second,\n and\nthird\n classes,\n respectively.\nTable\n 1\n reports\n the\n performance\n of\n BMOANN\n statistically\n in\n solv-\ning\n Iris\n \ufb02ower\n problem\n over\n 50\n runs.\n As\n can\n be\n seen,\n BMOANN\n even\nsolves\n this\n problem\n with\n 0%\n of\n error\n rate\n for\n the\n test\n set.\n To\n evaluate\nthe\n performance\n of\n BMOANN\n in\n solving\n the\n Iris\n \ufb02ower\n problem,\n the\nresults\n of\n the\n other\n methods\n are\n tabulated\n in\n Table\n 2\n in\n comparison\nwith\n the\n BMOANN\n result.\n As\n Table\n 2\n indicates,\n BMOANN\n produces\nbetter\n results\n than\n GANet-best\n and\n CCSS\n and\n is\n outperformed\n by\nTable\n 4\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Wisconsin\n breast\n cancer\n problem.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nCOOP\n \nCNNE\n \nEDTs\nTest\n error\n rate\n (%)\n1.16\n1.06\n1.23\n1.20\n \n2.63\nAlgorithm\n \nMGNN\n \nSVM-best\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n3.05\n \n3.1\n \n2.72\n \n3.9\nSVM-best\n and\n OC1-best\n methods.\n However,\n the\n error\n rate\n is\n slightly\nworse\n than\n that\n of\n the\n OC1-best.\n The\n performance\n of\n BMOANN\n in\ncomparison\n with\n CCSS\n and\n OC1-best\n is\n promising;\n because\n they\nare\n state-of-the-art\n decision\n trees\n classi\ufb01ers.\n It\n is\n worth\n to\n mention\nthat\n GANet-best,\n SVM-best,\n and\n CCSS\n classi\ufb01ers\n use\n k-fold\n cross-\nvalidation,\n which\n leads\n to\n generating\n more\n optimistic\n results.\n3.1.2.\n Wisconsin\n breast\n cancer\nThe\n purpose\n of\n this\n dataset\n is\n to\n classify\n a\n tumor\n as\n either\nbenign\n or\n malignant.\n This\n dataset\n has\n 9\n integer\n attributes\n with\n 699\ninstances\n of\n which\n 458\n instances\n are\n benign\n and\n 241\n instances\n are\nmalignant.\n In\n order\n to\n train\n the\n ANN\n for\n classi\ufb01cation,\n the\n data\n set\nis\n divided\n into\n three\n sets:\n a\n training\n set\n with\n the\n \ufb01rst\n 349\n instances\nis\n used\n to\n adjust\n the\n ANN\n weights,\n a\n validation\n set\n with\n the\n fol-\nlowing\n 175\n instances\n is\n used\n to\n minimize\n over\ufb01tting,\n and\n a\n test\n set\nwith\n the\n \ufb01nal\n 175\n instances\n is\n used\n only\n for\n \ufb01nal\n solution\n in\n order\nto\n verify\n the\n ANN\n predictive\n power.\n As\n previously\n mentioned,\n the\nvalidation\n set\n is\n used\n to\n minimize\n over\ufb01tting.\n The\n network\n weights\nare\n not\n adjusted\n with\n this\n data\n set.\n This\n set\n is\n only\n used\n to\n verify\n that\nany\n increase\n in\n the\n accuracy\n over\n the\n training\n set\n actually\n yields\n an\nincrease\n over\n the\n accuracy\n of\n a\n data\n set\n that\n has\n not\n been\n shown\nto\n the\n network\n before,\n or\n at\n least\n the\n network\n has\n not\n been\n trained\nby\n it\n (i.e.\n validation\n data\n set).\n If\n the\n accuracy\n over\n the\n training\n data\nset\n increases\n while\n the\n accuracy\n over\n the\n validation\n set\n decreases\nor\n stays\n the\n same,\n to\n avoid\n over\ufb01tting\n the\n training\n process\n must\n be\nstopped.\n The\n architecture\n of\n BMOANN\n is\n selected\n 9-5-1.\nWe  have\n tabulated\n the\n performance\n of\n BMOANN\n over\n 50\n runs\nin\n solving\n Wisconsin\n breast\n cancer\n problem\n in\n Table\n 3.\n It\n is\n obvious\nthat\n BMOANN\n at\n its\n best\n performance\n reaches\n to\n 0%\n of\n error\n rate\n for\nthe\n test\n set.\n The\n performance\n of\n the\n other\n eight\n techniques\n has\n been\nsummarized\n in\n Table\n 4\n in\n comparison\n with\n the\n BMOANN\n result.\n As\nresults\n reveal,\n BMOANN\n produces\n better\n results\n than\n COOP,\n CNNE,\nEDTs,\n MGNN,\n SVM-best,\n CCSS,\n and\n OC1-best.\n In\n this\n case,\n it\n is\n only\noutperformed\n by\n GANet-best\n technique.\n From\n the\n table,\n we\n can\nsee\n that\n BMOANN\n markedly\n outperforms\n SVM-best\n and\n OC1-best\nalgorithms.\n3.1.3.\n Pima\n Indian\n diabetes\nThis\n problem\n is\n one\n of\n the\n most\n dif\ufb01cult\n ones\n because\n the\n dataset\nis\n relatively\n small\n and\n is\n heavily\n polluted\n by\n noise.\n This\n dataset\n has\n768\n instances\n of\n patients\n of\n which\n 500\n patients\n have\n signs\n of\n dia-\nbetes\n and\n there\n are\n no\n signs\n of\n diabetes\n for\n the\n other\n 268\n patients.\nFrom\n each\n patient,\n eight\n features\n have\n been\n measured.\n The\n data\n set\nis\n divided\n into\n three\n sets:\n a\n training\n set\n with\n the\n \ufb01rst\n 384\n instances,\na\n validation\n set\n with\n the\n following\n 192\n instances,\n and\n a test\n set\n with\nthe\n \ufb01nal\n 192\n instances.\n The\n architecture\n of\n BMOANN\n is\n experimen-\ntally\n selected\n 8-5-1.\nTable\n 3\nError\n rate\n (%)\n of\n BMOANN\n for\n Wisconsin\n breast\n cancer.\n The\n results\n have\n been\n averaged\n over\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nValidation\n set\n \nTest\n set\nAvg.\nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\nBMOANN\n \n4.07\n \n0.94\n \n3.15\n \n6.88\n \n2.89\n \n0.82\n \n1.71\n \n4.57\n \n1.16\n \n0.59\n \n0.00\n \n2.29\n", []], "3.1.1 Iris flower": ["A.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\n \n1211\nTable\n 1\nError\n rate\n (%)\n of\n BMOANN\n for\n Iris\n \ufb02ower\n problem.\n The\n results\n have\n been\n averaged\nover\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nTest\n set\nAvg.\n \nStd.\n \nMin.\n \nMax.\n \nAvg.\n \nStd.\n \nMin.\n \nMax.\nBMOANN\n \n1.50\n \n0.78\n \n1.00\n \n4.00\n \n3.88\n \n1.72\n \n0.00\n \n8.00\nTable\n 2\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Iris\n \ufb02ower\n problem.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nSVM-best\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n3.88\n \n6.40\n \n1.40\n \n4.40\n \n3.70\npolyandrous,\n and\n promiscuous\n birds\n make\n 50%,\n 40%,\n 5%,\n and\n 5%\n of\nthe\n society,\n respectively;\n T,\n w,\n and\n mw are\n de\ufb01ned\n as\n decreasing\nlinear\n functions\n where\n Tmax =\n 300,\n Tmin =\n 50,\n wmax =\n 2.5,\n wmin =\n 0.5,\nmw,max =\n 0.01,\n and\n mw,min =\n 0.0001;\n mcf\n is\n selected\n 0.9\n and\n number\nof\n monogamous\n birds\n participating\n in\n rituals\n of\n the\n polyandrous\nbirds\n is\n set\n to\n 5;\n Maximum\n number\n of\n generations\n (epochs)\n is\n set\nto\n 100\n for\n Iris\n \ufb02ower\n and\n Pima\n Indian\n diabetes,\n and\n is\n set\n to\n 50\n for\nWisconsin\n breast\n cancer;\n To\n limit\n the\n search\n rage\n l\n and\n u\n are\n set\nto\n \u221215\n and\n 15,\n respectively.\n It\n should\n be\n noted\n that\n the\n parameter\nsetting\n is\n based\n on\n trial\n and\n no\n attempt\n has\n made\n to\n optimize\n it.\nMatlab\n environment\n is\n implemented\n to\n code\n our\n BMOANN.\n Due\nto\n the\n fact\n that\n the\n nature\n of\n metaheuristic\n algorithms\n is\n stochas-\ntic,\n the\n results\n obtained\n in\n one\n attempt\n will\n differ\n from\n the\n results\nobtained\n in\n another\n attempt.\n Therefore,\n the\n performance\n analysis\nmust\n be\n statistically\n based.\n As\n a\n result,\n in\n each\n case,\n 50\n indepen-\ndent\n runs\n are\n performed\n to\n get\n an\n average\n result\n of\n the\n algorithm.\nTo\n compare\n the\n training\n capability\n of\n our\n BMO,\n the\n results\n will\nbe\n compared\n with\n those\n of\n the\n other\n classi\ufb01ers\n from\n the\n litera-\nture.\n The\n results\n have\n been\n adopted\n from\n the\n literature\n directly\n for\ncomparison.\n3.1.1.\n Iris\n \ufb02ower\nThis\n dataset\n consists\n of\n 150\n samples\n from\n three\n species\n of\n Iris\n\ufb02owers\n (Setosa,\n Virginica,\n and\n Versicolor).\n From\n each\n species,\n four\nfeatures\n including\n the\n length\n and\n the\n width\n of\n sepal\n and\n petal\n have\nbeen\n measured.\n In\n order\n to\n develop\n an\n ANN\n to\n classify\n this\n mul-\nticlass\n problem,\n the\n dataset\n is\n randomly\n partitioned\n as\n a\n training\nset\n including\n 100\n instances\n and\n a\n test\n set\n including\n the\n rest\n 50\ninstances.\nThe\n architecture\n of\n BMOANN\n is\n experimentally\n selected\n 4-5-2,\nmeaning\n that\n 5\n hyperbolic\n tangent\n and\n 2\n step\n functions\n are\n located\nin\n the\n hidden\n and\n output\n layers,\n respectively.\n Due\n to\n the\n face\n that\nIris\n \ufb02ower\n is\n a\n multiclass\n problem\n with\n three\n classes,\n in\n the\n output\nlayer\n two\n nodes\n are\n used.\n The\n computer\n program\n has\n been\n written\nso\n that\n the\n outputs\n of\n 00,\n 01,\n and\n 11\n denote\n the\n \ufb01rst,\n second,\n and\nthird\n classes,\n respectively.\nTable\n 1\n reports\n the\n performance\n of\n BMOANN\n statistically\n in\n solv-\ning\n Iris\n \ufb02ower\n problem\n over\n 50\n runs.\n As\n can\n be\n seen,\n BMOANN\n even\nsolves\n this\n problem\n with\n 0%\n of\n error\n rate\n for\n the\n test\n set.\n To\n evaluate\nthe\n performance\n of\n BMOANN\n in\n solving\n the\n Iris\n \ufb02ower\n problem,\n the\nresults\n of\n the\n other\n methods\n are\n tabulated\n in\n Table\n 2\n in\n comparison\nwith\n the\n BMOANN\n result.\n As\n Table\n 2\n indicates,\n BMOANN\n produces\nbetter\n results\n than\n GANet-best\n and\n CCSS\n and\n is\n outperformed\n by\nTable\n 4\nComparison\n of\n BMOANN\n and\n the\n other\n ANNs\n in\n terms\n of\n average\n testing\n error\n rate\n(%)\n on\n the\n Wisconsin\n breast\n cancer\n problem.\nAlgorithm\n \nBMOANN\n \nGANet-best\n \nCOOP\n \nCNNE\n \nEDTs\nTest\n error\n rate\n (%)\n1.16\n1.06\n1.23\n1.20\n \n2.63\nAlgorithm\n \nMGNN\n \nSVM-best\n \nCCSS\n \nOC1-best\nTest\n error\n rate\n (%)\n3.05\n \n3.1\n \n2.72\n \n3.9\nSVM-best\n and\n OC1-best\n methods.\n However,\n the\n error\n rate\n is\n slightly\nworse\n than\n that\n of\n the\n OC1-best.\n The\n performance\n of\n BMOANN\n in\ncomparison\n with\n CCSS\n and\n OC1-best\n is\n promising;\n because\n they\nare\n state-of-the-art\n decision\n trees\n classi\ufb01ers.\n It\n is\n worth\n to\n mention\nthat\n GANet-best,\n SVM-best,\n and\n CCSS\n classi\ufb01ers\n use\n k-fold\n cross-\nvalidation,\n which\n leads\n to\n generating\n more\n optimistic\n results.\n3.1.2.\n Wisconsin\n breast\n cancer\nThe\n purpose\n of\n this\n dataset\n is\n to\n classify\n a\n tumor\n as\n either\nbenign\n or\n malignant.\n This\n dataset\n has\n 9\n integer\n attributes\n with\n 699\ninstances\n of\n which\n 458\n instances\n are\n benign\n and\n 241\n instances\n are\nmalignant.\n In\n order\n to\n train\n the\n ANN\n for\n classi\ufb01cation,\n the\n data\n set\nis\n divided\n into\n three\n sets:\n a\n training\n set\n with\n the\n \ufb01rst\n 349\n instances\nis\n used\n to\n adjust\n the\n ANN\n weights,\n a\n validation\n set\n with\n the\n fol-\nlowing\n 175\n instances\n is\n used\n to\n minimize\n over\ufb01tting,\n and\n a\n test\n set\nwith\n the\n \ufb01nal\n 175\n instances\n is\n used\n only\n for\n \ufb01nal\n solution\n in\n order\nto\n verify\n the\n ANN\n predictive\n power.\n As\n previously\n mentioned,\n the\nvalidation\n set\n is\n used\n to\n minimize\n over\ufb01tting.\n The\n network\n weights\nare\n not\n adjusted\n with\n this\n data\n set.\n This\n set\n is\n only\n used\n to\n verify\n that\nany\n increase\n in\n the\n accuracy\n over\n the\n training\n set\n actually\n yields\n an\nincrease\n over\n the\n accuracy\n of\n a\n data\n set\n that\n has\n not\n been\n shown\nto\n the\n network\n before,\n or\n at\n least\n the\n network\n has\n not\n been\n trained\nby\n it\n (i.e.\n validation\n data\n set).\n If\n the\n accuracy\n over\n the\n training\n data\nset\n increases\n while\n the\n accuracy\n over\n the\n validation\n set\n decreases\nor\n stays\n the\n same,\n to\n avoid\n over\ufb01tting\n the\n training\n process\n must\n be\nstopped.\n The\n architecture\n of\n BMOANN\n is\n selected\n 9-5-1.\nWe  have\n tabulated\n the\n performance\n of\n BMOANN\n over\n 50\n runs\nin\n solving\n Wisconsin\n breast\n cancer\n problem\n in\n Table\n 3.\n It\n is\n obvious\nthat\n BMOANN\n at\n its\n best\n performance\n reaches\n to\n 0%\n of\n error\n rate\n for\nthe\n test\n set.\n The\n performance\n of\n the\n other\n eight\n techniques\n has\n been\nsummarized\n in\n Table\n 4\n in\n comparison\n with\n the\n BMOANN\n result.\n As\nresults\n reveal,\n BMOANN\n produces\n better\n results\n than\n COOP,\n CNNE,\nEDTs,\n MGNN,\n SVM-best,\n CCSS,\n and\n OC1-best.\n In\n this\n case,\n it\n is\n only\noutperformed\n by\n GANet-best\n technique.\n From\n the\n table,\n we\n can\nsee\n that\n BMOANN\n markedly\n outperforms\n SVM-best\n and\n OC1-best\nalgorithms.\n3.1.3.\n Pima\n Indian\n diabetes\nThis\n problem\n is\n one\n of\n the\n most\n dif\ufb01cult\n ones\n because\n the\n dataset\nis\n relatively\n small\n and\n is\n heavily\n polluted\n by\n noise.\n This\n dataset\n has\n768\n instances\n of\n patients\n of\n which\n 500\n patients\n have\n signs\n of\n dia-\nbetes\n and\n there\n are\n no\n signs\n of\n diabetes\n for\n the\n other\n 268\n patients.\nFrom\n each\n patient,\n eight\n features\n have\n been\n measured.\n The\n data\n set\nis\n divided\n into\n three\n sets:\n a\n training\n set\n with\n the\n \ufb01rst\n 384\n instances,\na\n validation\n set\n with\n the\n following\n 192\n instances,\n and\n a test\n set\n with\nthe\n \ufb01nal\n 192\n instances.\n The\n architecture\n of\n BMOANN\n is\n experimen-\ntally\n selected\n 8-5-1.\nTable\n 3\nError\n rate\n (%)\n of\n BMOANN\n for\n Wisconsin\n breast\n cancer.\n The\n results\n have\n been\n averaged\n over\n 50\n runs.\nAlgorithm\n \nTraining\n set\n \nValidation\n set\n \nTest\n set\nAvg.\nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\n \nAvg.\n \nStd.\n \nMin\n \nMax\nBMOANN\n \n4.07\n \n0.94\n \n3.15\n \n6.88\n \n2.89\n \n0.82\n \n1.71\n \n4.57\n \n1.16\n \n0.59\n \n0.00\n \n2.29\n", []], "3.1 Using BMO in classification problems": ["1210\n \nA.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\nInitial\n \nizatio n:\nDetermine th\n \ne s ociety size, percentage of monoga\n \nmous,  polygynous\n \n, pro\n \nmis cuous,  \n \nand \npolyandr ous birds,  max imum numbe r of generat\n ion\n s,  and t\n \nhe  oth er para\n \nmeters\nDo\nCompute ob\n \njective  function of the birds\nSort birds\n \n based on their o bjective fu\n \nnction\nPartition the \n \nsociety  int o males\n \n and females\nSpecify monogamous, polygynous, and polyand\n \nrous birds\nRemove the\n \n worst  bi rds \nand\n  ge nerate promisc uous birds bas\n \ned  on th e c haotic se\n \nquence\nCompute ob\n \njective fu\n \nnction of the promisc\n \nuous bi\n \nrds\nFor i = 1 to nu\n \nmbe r of monogamous birds\nSelect inte\n \nres ting\n \nelite bird\nProduce the brood\n  based on Eq. (1)\nNext i\nFor i = 1 to nu\n \nmbe r of  poly\n \ngynous bi rds\nSelect inte\n \nres ting\n \nelite birds\nProduce the brood\n  based on Eq. (3)\nNext i\nFor i = 1 to nu\n \nmbe r of  poly\n \nandr ous bi\n \nrds\nSelect inte\n \nres tin g elite  birds\nProduce the brood\n  based on Eq. (5)\nNext i\nFor i = 1 to nu\n \nmbe r of  promisc uous birds\nSelect inte\n \nres ting elit\n \ne bird\nProduce the brood\n  based on Eq. (6)\nNext i\nComput e ob\n \njec tive  functio n of the  br oods\nPerform repl acement stage\nUpdate th\n \ne parameters\nUntil termination criterion is met\nFig.\n 2.\n Pseudocode\n of\n BMO\n algorithm.\nin\n the\n society.\n The\n \ufb02owchart\n and\n pseudocode\n of\n BMO\n algorithm\nhave\n been\n represented\n in\n Figs.\n 1\n and\n 2,\n respectively.\n3.\n Application\n of\n BMO\n to\n ANN\n training\n3.1.\n Using\n BMO\n in\n classi\ufb01cation\n problems\nThe\n ANN\n tuned\n by\n our\n BMO\n algorithm\n is\n a\n three-layer\n feed-\nforward\n network.\n The\n nodes\n of\n the\n input\n layer\n are\n passive,\n meaning\nthat\n they\n do\n not\n modify\n the\n features,\n they\n only\n receive\n them.\n The\ninputs\n are\n connected\n to\n all\n the\n hidden\n units,\n which\n in\n turn\n all\n con-\nnected\n to\n all\n the\n outputs.\n All\n neurons\n are\n connected\n to\n a\n bias\n unit,\nwith\n constant\n output\n of\n 1.\n The\n units\n calculate\n their\n net\n activation\n as\nnet\n = \u0002p\ni=1uivi +\n v0, where\n p\n is\n the\n number\n of\n inputs\n to\n the\n neuron,\nui denotes\n an\n input,\n vi is\n the\n corresponding\n weight,\n and\n v0 is\n the\nweight\n corresponding\n to\n the\n bias\n unit.\n Hidden\n units\n employ\n hyper-\nbolic\n tangent\n as\n their\n activation\n function,\n while\n output\n units\n make\nuse\n of\n step\n function.\n Each\n hidden\n unit\n emits\n an\n output\n according\n to\nf (net)\n =\n tan\n h\n (\u02c7net),\n we\n set\n \u02c7\n =\n 1\n in\n all\n the\n experiments.\n Connection\nweights\n are\n adjusted\n by\n our\n BMO\n algorithm\n as\n represented\n in\nFig.\n 3.\n In\n order\n to\n evaluate\n the\n performance\n of\n BMO-trained\n ANN\n(BMOANN),\n several\n well-studied\n machine\n learning\n benchmark\nproblems\n from\n the\n UCI\n machine\n learning\n repository\n which\n are\ninvestigated\n by\n human\n experts\n in\n practice,\n have\n been\n considered:\nIris\n \ufb02ower,\n Wisconsin\n breast\n cancer,\n and\n Pima\n Indian\n diabetes.\nIn\n the\n experiments,\n parameter\n setting\n of\n BMO\n algorithm\n is\n as\nfollows:\n The\n society\n size\n is\n set\n to\n 100;\n Monogamous,\n polygynous,\nFig.\n 3.\n Schematic\n diagram\n of\n BMO-based\n ANN.\n", [45]], "3 Application of BMO to ANN training": ["1210\n \nA.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\nInitial\n \nizatio n:\nDetermine th\n \ne s ociety size, percentage of monoga\n \nmous,  polygynous\n \n, pro\n \nmis cuous,  \n \nand \npolyandr ous birds,  max imum numbe r of generat\n ion\n s,  and t\n \nhe  oth er para\n \nmeters\nDo\nCompute ob\n \njective  function of the birds\nSort birds\n \n based on their o bjective fu\n \nnction\nPartition the \n \nsociety  int o males\n \n and females\nSpecify monogamous, polygynous, and polyand\n \nrous birds\nRemove the\n \n worst  bi rds \nand\n  ge nerate promisc uous birds bas\n \ned  on th e c haotic se\n \nquence\nCompute ob\n \njective fu\n \nnction of the promisc\n \nuous bi\n \nrds\nFor i = 1 to nu\n \nmbe r of monogamous birds\nSelect inte\n \nres ting\n \nelite bird\nProduce the brood\n  based on Eq. (1)\nNext i\nFor i = 1 to nu\n \nmbe r of  poly\n \ngynous bi rds\nSelect inte\n \nres ting\n \nelite birds\nProduce the brood\n  based on Eq. (3)\nNext i\nFor i = 1 to nu\n \nmbe r of  poly\n \nandr ous bi\n \nrds\nSelect inte\n \nres tin g elite  birds\nProduce the brood\n  based on Eq. (5)\nNext i\nFor i = 1 to nu\n \nmbe r of  promisc uous birds\nSelect inte\n \nres ting elit\n \ne bird\nProduce the brood\n  based on Eq. (6)\nNext i\nComput e ob\n \njec tive  functio n of the  br oods\nPerform repl acement stage\nUpdate th\n \ne parameters\nUntil termination criterion is met\nFig.\n 2.\n Pseudocode\n of\n BMO\n algorithm.\nin\n the\n society.\n The\n \ufb02owchart\n and\n pseudocode\n of\n BMO\n algorithm\nhave\n been\n represented\n in\n Figs.\n 1\n and\n 2,\n respectively.\n3.\n Application\n of\n BMO\n to\n ANN\n training\n3.1.\n Using\n BMO\n in\n classi\ufb01cation\n problems\nThe\n ANN\n tuned\n by\n our\n BMO\n algorithm\n is\n a\n three-layer\n feed-\nforward\n network.\n The\n nodes\n of\n the\n input\n layer\n are\n passive,\n meaning\nthat\n they\n do\n not\n modify\n the\n features,\n they\n only\n receive\n them.\n The\ninputs\n are\n connected\n to\n all\n the\n hidden\n units,\n which\n in\n turn\n all\n con-\nnected\n to\n all\n the\n outputs.\n All\n neurons\n are\n connected\n to\n a\n bias\n unit,\nwith\n constant\n output\n of\n 1.\n The\n units\n calculate\n their\n net\n activation\n as\nnet\n = \u0002p\ni=1uivi +\n v0, where\n p\n is\n the\n number\n of\n inputs\n to\n the\n neuron,\nui denotes\n an\n input,\n vi is\n the\n corresponding\n weight,\n and\n v0 is\n the\nweight\n corresponding\n to\n the\n bias\n unit.\n Hidden\n units\n employ\n hyper-\nbolic\n tangent\n as\n their\n activation\n function,\n while\n output\n units\n make\nuse\n of\n step\n function.\n Each\n hidden\n unit\n emits\n an\n output\n according\n to\nf (net)\n =\n tan\n h\n (\u02c7net),\n we\n set\n \u02c7\n =\n 1\n in\n all\n the\n experiments.\n Connection\nweights\n are\n adjusted\n by\n our\n BMO\n algorithm\n as\n represented\n in\nFig.\n 3.\n In\n order\n to\n evaluate\n the\n performance\n of\n BMO-trained\n ANN\n(BMOANN),\n several\n well-studied\n machine\n learning\n benchmark\nproblems\n from\n the\n UCI\n machine\n learning\n repository\n which\n are\ninvestigated\n by\n human\n experts\n in\n practice,\n have\n been\n considered:\nIris\n \ufb02ower,\n Wisconsin\n breast\n cancer,\n and\n Pima\n Indian\n diabetes.\nIn\n the\n experiments,\n parameter\n setting\n of\n BMO\n algorithm\n is\n as\nfollows:\n The\n society\n size\n is\n set\n to\n 100;\n Monogamous,\n polygynous,\nFig.\n 3.\n Schematic\n diagram\n of\n BMO-based\n ANN.\n", [45]], "2 Bird mating optimizer": ["A.\n Askarzadeh,\n A.\n Rezazadeh\n / Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\n \n1207\nis\n memorized\n and\n the\n possibility\n of\n making\n a\n better\n one\n increases\nat\n the\n next\n time.\nDuring\n mating\n season,\n birds\n employ\n a\n variety\n of\n intelligent\nbehaviors\n such\n as\n singing,\n tail\n drumming\n or\n dancing\n to\n attract\npotential\n mates.\n Some\n courtship\n rituals\n are\n quite\n elaborate\n and\nserve\n to\n form\n a\n bond\n between\n the\n potential\n mates.\n The\n quality\nof\n each\n bird\n is\n speci\ufb01ed\n by\n its\n features\n such\n as\n beak,\n tail,\n wing,\netc.\n The\n related\n gene\n of\n each\n feature\n determines\n the\n quality\n of\n that\nfeature,\n together\n making\n the\n overall\n quality\n of\n the\n bird.\n A\n gene\n is\na\n hereditary\n unit\n that\n can\n be\n passed\n on\n through\n breeding\n to\n the\nnext\n generations.\n Imagine\n a\n bird\n which\n has\n good\n genes\n among\n the\nspecies.\n This\n bird\n can\n \ufb02y\n adeptly\n and\n get\n more\n food.\n Hence,\n it\n is\nhealthier\n than\n the\n other\n birds,\n lives\n longer\n and\n breeds\n more.\n The\nbird\n passes\n these\n genes\n for\n better\n ones\n on\n to\n its\n broods\n by\n select-\ning\n a\n superior\n mate.\n They\n also\n live\n longer\n and\n have\n more\n broods\nand\n the\n gene\n continues\n to\n be\n inherited\n generation\n after\n genera-\ntion.\nThe\n ultimate\n success\n of\n a\n bird\n to\n raise\n a\n brood\n with\n supe-\nrior\n features\n depends\n on\n the\n used\n strategy.\n Different\n ways\n result\nin\n broods\n with\n diverse\n features.\n Study\n of\n bird\u2019s\n society\n reveals\nthat\n they\n employ\n different\n strategies\n to\n perform\n mating\n pro-\ncess.\n In\n general,\n there\n are\n four\n strategies:\n monogamy,\n polygyny,\npolyandry,\n and\n promiscuity\n [11\u201313].  According\n to\n its\n species\n each\nbird\n makes\n use\n of\n one\n of\n these\n ways\n to\n breed.\n Most\n birds\n are\nmonogamous,\n meaning\n that\n a\n male\n bird\n only\n mates\n with\n a\n female\none.\n In\n polygynous\n species\n a\n male\n tends\n to\n mate\n with\n several\nfemales\n while\n in\n polyandrous\n a\n female\n tends\n to\n mate\n with\n sev-\neral\n males.\n In\n the\n bird\u2019s\n society,\n polygyny\n is\n much\n more\n common\nthan\n polyandry.\n Promiscuity\n is\n another\n mating\n strategy\n employed\nby\n a\n few\n bird\n species,\n meaning\n mating\n systems\n with\n no\n stable\nrelationships\n in\n which\n mating\n between\n two\n birds\n is\n a\n one-time\nevent.\n This\n type\n of\n mating\n indicates\n a\n rather\n chaotic\n social\n struc-\nture\n in\n which\n the\n male\n will\n almost\n certainly\n never\n see\n his\n brood\n or\nthe\n nest,\n and\n most\n likely\n will\n not\n see\n the\n female\n for\n another\n brief\nvisit.\nIn\n order\n to\n study\n the\n usefulness\n of\n BMO\n algorithm\n three\n real-\nworld\n classi\ufb01cation\n problems,\n namely,\n Iris\n \ufb02ower,\n Wisconsin\n breast\ncancer,\n and\n Pima\n Indian\n diabetes,\n have\n been\n considered\n here.\n Iris\n\ufb02ower\n has\n become\n a\n typical\n test\n case\n for\n many\n classi\ufb01cation\n tech-\nniques\n in\n machine\n learning.\n Wisconsin\n breast\n cancer\n and\n Pima\nIndian\n diabetes\n contain\n missing\n attribute\n values\n and\n are\n usually\npolluted\n by\n noises.\n So,\n they\n are\n some\n of\n the\n most\n challenging\nproblems\n in\n machine\n learning\n \ufb01eld.\n In\n theses\n cases,\n the\n BMO\n perfor-\nmance\n will\n be\n compared\n with\n the\n results\n from\n some\n sophisticated\nclassi\ufb01cation\n methods.\n These\n classi\ufb01ers\n are\n the\n best\n result\n found\nby\n an\n ANN\n trained\n by\n a\n subset\n of\n features\n selected\n by\n a\n binary\nencoded\n GA\n (GANet-best)\n [9],  the\n best\n result\n of\n eight\n least\n squares\nSVM\n classi\ufb01ers\n (SVM-best)\n [14], decision\n tree\n ensembles\n of\n CCSS\n[15]\n and\n EDTs\n [16]\n and\n hybrid\n evolutionary\n decision\n tree\n (OC1-\nbest)\n [17].\n Decision\n trees\n (DTs)\n are\n popular\n classi\ufb01ers,\n and\n there\nare\n many\n algorithms\n induce\n a\n tree\n classi\ufb01er\n from\n a\n data\n set\n [18].\nThe\n results\n are\n also\n compared\n with\n an\n evolutionary\n ANN\n ensem-\nble\n evolved\n by\n cooperative\n coevolution\n (COOP)\n [19], a\n constructive\nalgorithm\n for\n training\n cooperative\n ANN\n ensembles\n (CNNE)\n [20], and\nan\n algorithm\n which\n evolves\n ANN\n structure\n and\n connection\n weights\n(MGNN)\n [21].\nAs\n a\n further\n test,\n BMO\n algorithm\n is\n used\n to\n build\n an\n ANN-based\nmodel\n for\n proton\n exchange\n membrane\n fuel\n cell\n (PEMFC)\n system.\nAs\n an\n open\n and\n demanding\n problem,\n accurate\n modeling\n of\n PEMFC\nis\n one\n of\n the\n most\n challenging\n issues\n in\n electrical\n engineering\nwhich\n has\n been\n the\n main\n focus\n of\n various\n researches\n [22\u201324].\nPEMFC\n is\n a\n nonlinear,\n complex,\n time\n varying,\n and\n strongly\n cou-\npled\n system\n that\n is\n hard\n to\n model\n by\n conventional\n methods.\nAccompanied\n \nwith\n \nmany\n \nassumptions\n \nand\n \napproximations,\nPEMFC\n mathematical\n models\n are\n based\n on\n the\n knowledge\n of\nelectrochemistry,\n thermodynamics,\n and\n \ufb02uid\n mechanics.\n As\n a\nresult,\n they\n cannot\n re\ufb02ect\n the\n accurate\n performance\n of\n the\n system\nand\n are\n dif\ufb01cult\n to\n be\n understood\n by\n an\n electrical\n engineering\nperson.\n As\n an\n alternative,\n ANN-based\n models\n have\n received\n signif-\nicant\n attention\n to\n represent\n the\n PEMFC\n behavior.\n An\n ANN\n can\n learn\nthe\n behavior\n of\n PEMFC\n from\n a\n set\n of\n input\u2013output\n data\n which\n are\nexperimentally\n obtained\n from\n the\n system.\nThis\n paper\n is\n arranged\n in\n four\n sections;\n Section\n 2\n describes\n BMO\nalgorithm\n in\n detail;\n in\n Section\n 3,\n the\n proposed\n algorithm\n is\n \ufb01rst\napplied\n to\n classify\n three\n well\n known\n problems,\n and\n then\n is\n used\nto\n model\n a\n PEMFC\n system,\n \ufb01nally,\n conclusion\n is\n stated\n in\n Section\n4.\n2.\n Bird\n mating\n optimizer\nThe\n population\n of\n BMO\n algorithm\n is\n called\n a\n society\n and\n each\nindividual\n in\n the\n society\n is\n called\n a\n bird.\n The\n society\n contains\nfour\n types\n of\n birds:\n monogamous,\n polygynous,\n polyandrous,\n and\npromiscuous,\n breeding\n in\n a\n d-dimensional\n search\n space,\n S\n \u2282\n Rd, to\n\ufb01nd\n the\n optimum\n solution.\n Assume\n that\n we\n have\n a\n set\n of\n birds\n in\na\n society\n indicated\n by\n \u0002.\n The\n birds\n of\n the\n society\n are\n categorized\nbased\n on\n their\n \ufb01tness\n values\n so\n that\n \u0002\n =\n \u0003\n \u222a\n \u0004\n \u222a\n  \n \u222a\n \u0005,\n where\n \u0003,\n \u0004,\n  ,\nand\n \u0005\n represent\n the\n set\n of\n monogamous,\n polygynous,\n polyandrous,\nand\n promiscuous\n birds,\n respectively.\n Each\n bird\n is\n associated\n with\n a\nprede\ufb01ned\n number\n of\n genes\n and\n shown\n by\n a\n vector \u20d7x\n (\u0002)\n = (x(\u0002,1),\nx(\u0002,2),\n . . .,\n x(\u0002,d)).\n In\n the\n society,\n any\n bird\n is\n a\n feasible\n solution\n of\nthe\n problem\n under\n consideration\n with\n a\n quality\n represented\n by\n\ufb01t(\u20d7x\u0002).\n The\n birds\n attempt\n to\n pass\n on\n better\n genes\n to\n their\n broods.\nConsequently,\n as\n the\n algorithm\n progresses,\n the\n quality\n of\n the\n bird\u2019s\nsociety\n improves.\n For\n convenience\n of\n computation,\n we\n assume\n that\nthere\n is\n only\n one\n brood\n when\n a\n bird\n mates\n with\n other\n one(s).\n The\nsociety\n is\n then\n updated\n with\n the\n better\n birds.\n The\n breeding\n among\nthe\n society\n continues\n until\n a\n criterion\n named\n maximum\n number\n of\ngenerations,\n genmax, is\n met.\nIn\n the\n proposed\n algorithm,\n it\n is\n also\n assumed\n that\n the\n birds\n of\nthe\n society\n can\n switch\n their\n types\n during\n generations.\n At\n each\n gen-\neration,\n society\n birds\n which\n have\n the\n most\n promising\n genes,\n are\nchosen\n as\n polyandrous\n birds\n (females).\n They\n have\n the\n best\n \ufb01tness\nvalues\n among\n the\n society.\n A\n prede\ufb01ned\n percentage\n of\n the\n other\nbirds\n which\n have\n the\n worst\n \ufb01tness\n are\n abandoned\n from\n the\n society\nand\n replaced\n by\n new\n ones\n produced\n by\n using\n a\n chaotic\n sequence.\nThe\n new\n birds\n are\n considered\n as\n promiscuous.\n The\n remaining\nbirds\n of\n the\n society\n are\n regarded\n as\n monogamous\n and\n polygynous\nbirds.\n Monogamous\n birds\n have\n better\n \ufb01tness\n than\n polygynous\n ones.\nMonogamous,\n polygynous\n and\n promiscuous\n birds\n make\n the\n males\nof\n the\n society.\n In\n BMO,\n the\n percentage\n of\n each\n type\n is\n determined\nmanually.\n Monogamous\n and\n polygynous\n types\n have\n a great\n portion\nand\n polyandrous\n and\n promiscuous\n types\n have\n a\n low\n percentage\n of\nthe\n society.\nMonogamous\n birds\n are\n those\n males\n that\n tend\n to\n mate\n with\n one\nfemale.\n During\n mating\n season,\n a\n monogamous\n bird\n starts\n to\n sing\nand\n tries\n to\n attract\n female\n birds.\n Polyandrous\n birds\n receive\n his\n song\nand\n gather\n at\n the\n vicinity\n of\n him.\n They\n employ\n intelligent\n behaviors\nsuch\n as\n dancing\n or\n tail\n drumming\n to\n catch\n the\n attention\n of\n the\nmale\n bird.\n The\n ultimate\n aim\n of\n the\n male\n bird\n is\n to\n pass\n on\n better\ngenes\n to\n his\n brood\n by\n combining\n his\n genes\n with\n the\n genes\n of\n his\ninteresting\n elite\n female.\n Therefore,\n he\n evaluates\n the\n quality\n of\n the\nfemales,\n employs\n a\n probabilistic\n approach\n to\n select\n one\n of\n them\n as\nhis\n interesting\n elite\n female,\n and\n mates\n with\n her.\n Female\n birds\n with\nmore\n promising\n genes\n have\n more\n chance\n of\n being\n selected.\n Besides,\neach\n gene\n of\n the\n brood\n may\n be\n produced\n by\n mutation\n in\n the\n bird\ngene.\n The\n probability\n of\n mutation\n is\n controlled\n by\n a\n factor\n named\nmutation\n control\n factor,\n mcf,\n which\n varies\n between\n 0\n and\n 1.\n This\nfactor\n helps\n the\n algorithm\n maintains\n the\n diversity\n and\n avoids\n pre-\nmature\n convergence.\n As\n a\n result,\n the\n resultant\n brood\n is\n produced\n by\nEq.\n (1):\n", [33, 45]], "1 Introduction": ["Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\nContents\n lists\n available\n at\n SciVerse\n ScienceDirect\nApplied\n Soft\n Computing\nj ourna\n l ho\n me\n p\n age:\n www.elsevier.com/l\n ocate/asoc\nArti\ufb01cial\n neural\n network\n training\n using\n a\n new\n ef\ufb01cient\n optimization\n algorithm\nAlireza\n Askarzadeh \u2217, Alireza\n Rezazadeh\nFaculty\n of\n Electrical\n and\n Computer\n Engineering,\n Shahid\n Beheshti\n University,\n G.C.,\n Evin\n 1983963113,\n Tehran,\n Iran\na\n r\n t\n i  c  l  e  \ni  n\n f  o\nArticle\n history:\nReceived\n 12\n March\n 2012\nReceived\n in\n revised\n form\n 9\n September\n 2012\nAccepted\n 23\n October\n 2012\nAvailable\n online\n 12\n November\n 2012\nKeywords:\nArti\ufb01cial\n neural\n network\nWeight\n training\nBird\n mating\n optimizer\nFuel\n cell\na\n b\n s\n t\n r\n a  c  t\nBecause\n search  space  in  arti\ufb01cial\n neural  networks\n (ANNs)\n is  high\n dimensional\n and  multimodal\n which\n is\nusually\n polluted\n by  noises\n and  missing\n data,  the  process\n of  weight\n training  is a complex\n continuous\n opti-\nmization\n problem.\n This  paper\n deals  with  the  application\n of a  recently\n invented\n metaheuristic\n optimization\nalgorithm,\n bird  mating\n optimizer\n (BMO),  for  training\n feed-forward\n ANNs.\n BMO  is  a population-based\nsearch\n method\n which\n tries  to imitate  the  mating\n ways\n of bird  species  for  designing\n optimum\n searching\ntechniques.\n In  order  to  study  the  usefulness\n of  the proposed\n algorithm,\n BMO  is  applied  to  weight\n training\nof\n ANNs  for  solving\n three\n real-world\n classi\ufb01cation\n problems,\n namely,  Iris  \ufb02ower,\n Wisconsin\n breast\n cancer,\nand\n Pima\n Indian\n diabetes.\n The  performance\n of  BMO  is  compared\n with  those\n of  the  other  classi\ufb01ers.\n Simu-\nlation\n results  indicate\n the  superior\n capability\n of  BMO  to tackle  the  problem\n of  ANN  weight  training.\n BMO\nis\n also  applied\n to model  fuel  cell  system\n which  has  been\n addressed\n as  an  open\n and  demanding\n problem\nin\n electrical\n engineering.\n The\n promising\n results  verify\n the  potential\n of  BMO  algorithm.\n\u00a9\n 2012  Elsevier\n B.V.  All  rights  reserved.\n1.\n Introduction\nArti\ufb01cial\n neural\n networks\n (ANNs)\n are\n computational\n modeling\ntools\n that\n are\n de\ufb01ned\n as\n structures\n comprised\n of\n densely\n inter-\nconnected\n adaptive\n simple\n processing\n elements.\n They\n are\n able\n to\nperform\n massive\n parallel\n computations\n for\n data\n processing\n and\nknowledge\n representation\n [1].  ANN\n training\n process\n is\n an\n optimiza-\ntion\n task\n with\n the\n aim\n of\n \ufb01nding\n a\n set\n of\n weights\n to\n minimize\nan\n error\n measure.\n Owing\n to\n this\n fact\n that\n search\n space\n is\n high\ndimensional\n and\n multimodal\n which\n is\n usually\n polluted\n by\n noises\nand\n missing\n data,\n the\n problem\n of\n ANN\n training\n needs\n powerful\noptimization\n techniques.\n Most\n often,\n some\n conventional\n gradient\ndescent\n algorithms,\n such\n as\n backpropagation\n (BP)\n [2],  are\n consid-\nered\n for\n solving\n the\n problem.\n The\n gradient-based\n algorithms\n are\nsusceptible\n to\n be\n converged\n at\n local\n optima,\n because\n they\n are\n local\nsearch\n methods\n that\n the\n \ufb01nal\n result\n depends\n strongly\n on\n the\n ini-\ntial\n weights.\n If\n the\n initial\n weights\n are\n located\n near\n local\n optima,\n the\nalgorithm\n would\n be\n stuck\n at\n them.\nTo\n tackle\n the\n complexity\n of\n AAN\n training\n problem,\n metaheuristic\noptimization\n algorithms\n such\n as\n genetic\n algorithm\n (GA)\n [3],  par-\nticle\n swarm\n optimization\n (PSO)\n [4]\n and\n ant\n colony\n optimization\n(ACO)\n [5,6]\n have\n been\n highly\n proposed\n to\n search\n for\n the\n optimal\nweights\n of\n the\n network.\n Study\n of\n the\n literature\n indicates\n that\n these\nalgorithms\n have\n been\n used\n to\n train\n the\n networks,\n design\n their\n archi-\ntecture,\n and\n feature\n subsets\n [7\u20139].\n In\n contrast\n with\n conventional\n\u2217Corresponding\n author.\n Tel.:\n +98\n 21\n 29904178;\n fax:\n +98\n 21\n 22431804.\nE-mail\n addresses:\n a\n askarzadeh@sbu.ac.ir, askarzadeh\n a@yahoo.com\n(A.\n Askarzadeh),\n a-rezazade@sbu.ac.ir (A.\n Rezazadeh).\nmethods,\n metaheuristic\n algorithms\n do\n not\n use\n any\n gradient\n infor-\nmation,\n and\n have\n more\n chance\n to\n avoid\n local\n optima\n by\n sampling\nsimultaneously\n multiple\n regions\n of\n search\n space.\nRecently,\n a\n novel\n metaheuristic\n optimization\n algorithm,\n trying\nto\n simulate\n the\n evolution\n process\n of\n bird\n species,\n has\n been\n devised\nby\n the\n authors\n [10]. This\n algorithm,\n named\n bird\n mating\n optimizer\n(BMO),\n has\n been\n applied\n to\n an\n engineering\n optimization\n problem\nand\n superior\n results\n have\n been\n obtained\n in\n comparison\n with\n the\nother\n algorithms.\n Simple\n concept\n and\n good\n ef\ufb01ciency\n are\n the\n major\nadvantages\n of\n BMO\n algorithm.\n The\n adequate\n ef\ufb01ciency\n of\n BMO\n orig-\ninates\n from\n using\n distinct\n moving\n patterns\n to\n explore\n the\n search\nspace.\n Using\n distinct\n moving\n patterns\n increases\n the\n \ufb02exibility\n of\nthe\n algorithm\n to\n provide\n good\n balance\n between\n exploration\n and\nexploitation.\n The\n main\n goal\n of\n this\n paper\n is\n to\n deal\n with\n the\n appli-\ncation\n of\n BMO\n algorithm\n for\n \ufb01nding\n ANN\n weights.\nBirds\n are\n the\n most\n speciose1 class\n of\n tetrapod2 vertebrates\n hav-\ning\n around\n 10,000\n living\n species\n [11].\n Mating\n process\n in\n bird\u2019s\nsociety\n has\n many\n similarities\n with\n an\n optimization\n process\n in\n which\neach\n bird\n breeds\n or\n attempts\n to\n breed\n a\n brood\n with\n high\n quality\ngenes,\n because\n a\n bird\n with\n better\n genes\n has\n more\n chance\n to\n live.\nSimilarly,\n an\n optimization\n process\n searches\n to\n discover\n the\n global\nsolution\n in\n which\n the\n quality\n of\n each\n solution\n is\n determined\n by\n a\ncriterion\n named\n objective\n (\ufb01tness)\n function.\n In\n engineering\n opti-\nmization,\n decision\n variables\n are\n given\n values\n in\n the\n search\n space\n and\na\n solution\n vector\n is\n made.\n If\n a\n good\n solution\n is\n made,\n that\n experience\n1 In\n biology\n means\n rich\n in\n species.\n2 Animals\n with\n backbones\n and\n spinal\n columns.\n1568-4946/$\n \u2013\n see\n front\n matter\n \u00a9\n 2012\n Elsevier\n B.V.\n All\n rights\n reserved.\nhttp://dx.doi.org/10.1016/j.asoc.2012.10.023\n", [321, 322]], "Artificial neural network training using a new efficient optimization algorithm": ["Applied\n Soft\n Computing\n 13\n (2013)\n 1206\u20131213\nContents\n lists\n available\n at\n SciVerse\n ScienceDirect\nApplied\n Soft\n Computing\nj ourna\n l ho\n me\n p\n age:\n www.elsevier.com/l\n ocate/asoc\nArti\ufb01cial\n neural\n network\n training\n using\n a\n new\n ef\ufb01cient\n optimization\n algorithm\nAlireza\n Askarzadeh \u2217, Alireza\n Rezazadeh\nFaculty\n of\n Electrical\n and\n Computer\n Engineering,\n Shahid\n Beheshti\n University,\n G.C.,\n Evin\n 1983963113,\n Tehran,\n Iran\na\n r\n t\n i  c  l  e  \ni  n\n f  o\nArticle\n history:\nReceived\n 12\n March\n 2012\nReceived\n in\n revised\n form\n 9\n September\n 2012\nAccepted\n 23\n October\n 2012\nAvailable\n online\n 12\n November\n 2012\nKeywords:\nArti\ufb01cial\n neural\n network\nWeight\n training\nBird\n mating\n optimizer\nFuel\n cell\na\n b\n s\n t\n r\n a  c  t\nBecause\n search  space  in  arti\ufb01cial\n neural  networks\n (ANNs)\n is  high\n dimensional\n and  multimodal\n which\n is\nusually\n polluted\n by  noises\n and  missing\n data,  the  process\n of  weight\n training  is a complex\n continuous\n opti-\nmization\n problem.\n This  paper\n deals  with  the  application\n of a  recently\n invented\n metaheuristic\n optimization\nalgorithm,\n bird  mating\n optimizer\n (BMO),  for  training\n feed-forward\n ANNs.\n BMO  is  a population-based\nsearch\n method\n which\n tries  to imitate  the  mating\n ways\n of bird  species  for  designing\n optimum\n searching\ntechniques.\n In  order  to  study  the  usefulness\n of  the proposed\n algorithm,\n BMO  is  applied  to  weight\n training\nof\n ANNs  for  solving\n three\n real-world\n classi\ufb01cation\n problems,\n namely,  Iris  \ufb02ower,\n Wisconsin\n breast\n cancer,\nand\n Pima\n Indian\n diabetes.\n The  performance\n of  BMO  is  compared\n with  those\n of  the  other  classi\ufb01ers.\n Simu-\nlation\n results  indicate\n the  superior\n capability\n of  BMO  to tackle  the  problem\n of  ANN  weight  training.\n BMO\nis\n also  applied\n to model  fuel  cell  system\n which  has  been\n addressed\n as  an  open\n and  demanding\n problem\nin\n electrical\n engineering.\n The\n promising\n results  verify\n the  potential\n of  BMO  algorithm.\n\u00a9\n 2012  Elsevier\n B.V.  All  rights  reserved.\n1.\n Introduction\nArti\ufb01cial\n neural\n networks\n (ANNs)\n are\n computational\n modeling\ntools\n that\n are\n de\ufb01ned\n as\n structures\n comprised\n of\n densely\n inter-\nconnected\n adaptive\n simple\n processing\n elements.\n They\n are\n able\n to\nperform\n massive\n parallel\n computations\n for\n data\n processing\n and\nknowledge\n representation\n [1].  ANN\n training\n process\n is\n an\n optimiza-\ntion\n task\n with\n the\n aim\n of\n \ufb01nding\n a\n set\n of\n weights\n to\n minimize\nan\n error\n measure.\n Owing\n to\n this\n fact\n that\n search\n space\n is\n high\ndimensional\n and\n multimodal\n which\n is\n usually\n polluted\n by\n noises\nand\n missing\n data,\n the\n problem\n of\n ANN\n training\n needs\n powerful\noptimization\n techniques.\n Most\n often,\n some\n conventional\n gradient\ndescent\n algorithms,\n such\n as\n backpropagation\n (BP)\n [2],  are\n consid-\nered\n for\n solving\n the\n problem.\n The\n gradient-based\n algorithms\n are\nsusceptible\n to\n be\n converged\n at\n local\n optima,\n because\n they\n are\n local\nsearch\n methods\n that\n the\n \ufb01nal\n result\n depends\n strongly\n on\n the\n ini-\ntial\n weights.\n If\n the\n initial\n weights\n are\n located\n near\n local\n optima,\n the\nalgorithm\n would\n be\n stuck\n at\n them.\nTo\n tackle\n the\n complexity\n of\n AAN\n training\n problem,\n metaheuristic\noptimization\n algorithms\n such\n as\n genetic\n algorithm\n (GA)\n [3],  par-\nticle\n swarm\n optimization\n (PSO)\n [4]\n and\n ant\n colony\n optimization\n(ACO)\n [5,6]\n have\n been\n highly\n proposed\n to\n search\n for\n the\n optimal\nweights\n of\n the\n network.\n Study\n of\n the\n literature\n indicates\n that\n these\nalgorithms\n have\n been\n used\n to\n train\n the\n networks,\n design\n their\n archi-\ntecture,\n and\n feature\n subsets\n [7\u20139].\n In\n contrast\n with\n conventional\n\u2217Corresponding\n author.\n Tel.:\n +98\n 21\n 29904178;\n fax:\n +98\n 21\n 22431804.\nE-mail\n addresses:\n a\n askarzadeh@sbu.ac.ir, askarzadeh\n a@yahoo.com\n(A.\n Askarzadeh),\n a-rezazade@sbu.ac.ir (A.\n Rezazadeh).\nmethods,\n metaheuristic\n algorithms\n do\n not\n use\n any\n gradient\n infor-\nmation,\n and\n have\n more\n chance\n to\n avoid\n local\n optima\n by\n sampling\nsimultaneously\n multiple\n regions\n of\n search\n space.\nRecently,\n a\n novel\n metaheuristic\n optimization\n algorithm,\n trying\nto\n simulate\n the\n evolution\n process\n of\n bird\n species,\n has\n been\n devised\nby\n the\n authors\n [10]. This\n algorithm,\n named\n bird\n mating\n optimizer\n(BMO),\n has\n been\n applied\n to\n an\n engineering\n optimization\n problem\nand\n superior\n results\n have\n been\n obtained\n in\n comparison\n with\n the\nother\n algorithms.\n Simple\n concept\n and\n good\n ef\ufb01ciency\n are\n the\n major\nadvantages\n of\n BMO\n algorithm.\n The\n adequate\n ef\ufb01ciency\n of\n BMO\n orig-\ninates\n from\n using\n distinct\n moving\n patterns\n to\n explore\n the\n search\nspace.\n Using\n distinct\n moving\n patterns\n increases\n the\n \ufb02exibility\n of\nthe\n algorithm\n to\n provide\n good\n balance\n between\n exploration\n and\nexploitation.\n The\n main\n goal\n of\n this\n paper\n is\n to\n deal\n with\n the\n appli-\ncation\n of\n BMO\n algorithm\n for\n \ufb01nding\n ANN\n weights.\nBirds\n are\n the\n most\n speciose1 class\n of\n tetrapod2 vertebrates\n hav-\ning\n around\n 10,000\n living\n species\n [11].\n Mating\n process\n in\n bird\u2019s\nsociety\n has\n many\n similarities\n with\n an\n optimization\n process\n in\n which\neach\n bird\n breeds\n or\n attempts\n to\n breed\n a\n brood\n with\n high\n quality\ngenes,\n because\n a\n bird\n with\n better\n genes\n has\n more\n chance\n to\n live.\nSimilarly,\n an\n optimization\n process\n searches\n to\n discover\n the\n global\nsolution\n in\n which\n the\n quality\n of\n each\n solution\n is\n determined\n by\n a\ncriterion\n named\n objective\n (\ufb01tness)\n function.\n In\n engineering\n opti-\nmization,\n decision\n variables\n are\n given\n values\n in\n the\n search\n space\n and\na\n solution\n vector\n is\n made.\n If\n a\n good\n solution\n is\n made,\n that\n experience\n1 In\n biology\n means\n rich\n in\n species.\n2 Animals\n with\n backbones\n and\n spinal\n columns.\n1568-4946/$\n \u2013\n see\n front\n matter\n \u00a9\n 2012\n Elsevier\n B.V.\n All\n rights\n reserved.\nhttp://dx.doi.org/10.1016/j.asoc.2012.10.023\n", [321, 322]]}