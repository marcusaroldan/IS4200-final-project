{"A.7 Visualizing Memory and Permutation": ["x\"\nx#\nx$\nx%\nh\"\n(#)\nh#\n(#)\nh$\n(#)\nh\"\n($)\nh#\n($)\nh$\n($)\nFactorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1\nx\"\nx#\nx$\nx%\nh#\n(#)\nh\"\n($)\nh#\n($)\nh$\n($)\nh%\n($)\nFactorization order: 1 \u00e0 4 \u00e0 2 \u00e0 3\nh\"\n(#)\nh$\n(#)\nh%\n(#)\nh%\n(#)\nh%\n($)\nmem(+)\nmem(+)\nx\"\nx#\nx$\nx%\nh\"\n(#)\nh#\n(#)\nh\"\n($)\nh#\n($)\nh%\n($)\nFactorization order: 2 \u00e0 4 \u00e0 3 \u00e0 1\nh$\n(#)\nh%\n(#)\nh$\n($)\nx\"\nx#\nx$\nx%\nh\"\n(#)\nh#\n(#)\nh$\n(#)\nh%\n(#)\nh\"\n($)\nh#\n($)\nh$\n($)\nh%\n($)\nFactorization order: 4 \u00e0 3 \u00e0 1 \u00e0 2\nmem(+)\nmem(+)\nmem(#)\nmem(#)\nmem(#)\nmem(+)\nx%\nx%\nx%\nx%\nFigure 4: Illustration of the permutation language modeling objective for predicting x3 given the\nsame input sequence x but with different factorization orders.\nA.7\nVisualizing Memory and Permutation\nIn this section, we provide a detailed visualization of the proposed permutation language modeling\nobjective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use\nattention masks to permute the factorization order, and the difference of the two attention streams.\nAs shown in Figure 5 and 6, given the current position zt, the attention mask is decided by the\npermutation (or factorization order) z such that only tokens the occur before zt in the permutation can\nbe attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the\nquery stream and the content stream work differently with a speci\ufb01c permutation through attention\nmasks. The main difference is that the query stream cannot do self-attention and does not have access\nto the token at the position, while the content stream performs normal self-attention.\n4The problem of language modeling is essentially density estimation for text data.\n5https://openreview.net/forum?id=HJePno0cYm\n16\n", []], "A.6 Qualitative Analysis of Attention Patterns": ["\u2022 In comparison, XLNet is able to cover all dependencies in expectation.\nApproaches like ELMo [27] concatenate forward and backward language models in a shallow manner,\nwhich is not suf\ufb01cient for modeling deep interactions between the two directions.\nA.5.3\nBridging the Gap Between Language Modeling and Pretraining\nWith a deep root in density estimation4 [4, 32, 24], language modeling has been a rapidly-developing\nresearch area [9, 1, 3]. However, there has been a gap between language modeling and pretraining\ndue to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It\nhas even been challenged by some machine learning practitioners whether language modeling is a\nmeaningful pursuit if it does not directly improve downstream tasks 5. XLNet generalizes language\nmodeling and bridges such a gap. As a result, it further \u201cjusti\ufb01es\u201d language modeling research.\nMoreover, it becomes possible to leverage the rapid progress of language modeling research for\npretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness\nof the latest language modeling progress.\nA.6\nQualitative Analysis of Attention Patterns\nWe compare the attention pattern of BERT and XLNet without \ufb01netuning. Firstly, we found 4 typical\npatterns shared by both, as shown in Fig. 2.\n(a) Content stripes\n(b) Local/Self focus\n(c) Two segments\n(d) Content-based symme-\ntry\nFigure 2: Attention patterns shared by XLNet and BERT. Rows and columns represent query and key\nrespectively.\nMore interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The\nself-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather\nglobal information; (b) The relative-stride pattern attends to positions every a few stride apart relative\nto the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig.\n1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the\nrelative right half. Note that all these three unique patterns involve the relative positions rather than\nabsolute ones, and hence are likely enabled by the \u201crelative attention\u201d mechanism in XLNet. We\nconjecture these unique patterns contribute to the performance advantage of XLNet. On the other\nhand, the proposed permutation LM objective mostly contributes to a better data ef\ufb01ciency, whose\neffects may not be obvious from qualitative visualization.\n(a) Self exclusion\n(b) Relative stride\n(c) One-side masked\nFigure 3: Attention patterns that appear only in XLNet. Rows and columns represent query and key respec-\ntively.\n15\n", [439, 441, 442, 443, 445, 446, 447]], "A.5.3 Bridging the Gap Between Language Modeling and Pretraining": ["\u2022 In comparison, XLNet is able to cover all dependencies in expectation.\nApproaches like ELMo [27] concatenate forward and backward language models in a shallow manner,\nwhich is not suf\ufb01cient for modeling deep interactions between the two directions.\nA.5.3\nBridging the Gap Between Language Modeling and Pretraining\nWith a deep root in density estimation4 [4, 32, 24], language modeling has been a rapidly-developing\nresearch area [9, 1, 3]. However, there has been a gap between language modeling and pretraining\ndue to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It\nhas even been challenged by some machine learning practitioners whether language modeling is a\nmeaningful pursuit if it does not directly improve downstream tasks 5. XLNet generalizes language\nmodeling and bridges such a gap. As a result, it further \u201cjusti\ufb01es\u201d language modeling research.\nMoreover, it becomes possible to leverage the rapid progress of language modeling research for\npretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness\nof the latest language modeling progress.\nA.6\nQualitative Analysis of Attention Patterns\nWe compare the attention pattern of BERT and XLNet without \ufb01netuning. Firstly, we found 4 typical\npatterns shared by both, as shown in Fig. 2.\n(a) Content stripes\n(b) Local/Self focus\n(c) Two segments\n(d) Content-based symme-\ntry\nFigure 2: Attention patterns shared by XLNet and BERT. Rows and columns represent query and key\nrespectively.\nMore interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The\nself-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather\nglobal information; (b) The relative-stride pattern attends to positions every a few stride apart relative\nto the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig.\n1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the\nrelative right half. Note that all these three unique patterns involve the relative positions rather than\nabsolute ones, and hence are likely enabled by the \u201crelative attention\u201d mechanism in XLNet. We\nconjecture these unique patterns contribute to the performance advantage of XLNet. On the other\nhand, the proposed permutation LM objective mostly contributes to a better data ef\ufb01ciency, whose\neffects may not be obvious from qualitative visualization.\n(a) Self exclusion\n(b) Relative stride\n(c) One-side masked\nFigure 3: Attention patterns that appear only in XLNet. Rows and columns represent query and key respec-\ntively.\n15\n", [439, 441, 442, 443, 445, 446, 447]], "A.5.2 Comparison with Language Modeling": ["Hparam\nRACE\nSQuAD\nMNLI\nYelp-5\nDropout\n0.1\nAttention dropout\n0.1\nMax sequence length\n512\n512\n128\n512\nBatch size\n32\n48\n128\n128\nLearning rate\n2e-5\n3e-5\n2e-5\n1e-5\nNumber of steps\n12K\n8K\n10K\n10K\nLearning rate decay\nlinear\nWeight decay\n0.01\nAdam epsilon\n1e-6\n1e-6\n1e-6\n1e-6\nLayer-wise lr decay\n1.0\n0.75\n1.0\n1.0\nTable 8: Hyperparameters for \ufb01netuning.\nA.5\nDiscussion and Analysis\nA.5.1\nComparison with BERT\nTo prove a general point beyond one example, we now turn to more formal expressions. Inspired\nby previous work [37], given a sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], we de\ufb01ne a set of target-context pairs\nof interest, I = {(x, U)}, where U is a set of tokens in x that form a context of x. Intuitively, we\nwant the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For\nexample, given the above sentence, the pairs of interest I could be instantiated as:\nI =\nn\u0000x = York, U = {New}\n\u0001\n,\n\u0000x = York, U = {city}\n\u0001\n,\n\u0000x = York, U = {New, city}\n\u0001\n, \u00b7 \u00b7 \u00b7\no\n.\nNote that I is merely a virtual notion without unique ground truth, and our analysis will hold\nregardless of how I is instantiated.\nGiven a set of target tokens T and a set of non-target tokens N = x\\T , BERT and XLNet both\nmaximize log p(T | N) but with different formulations:\nJBERT =\nX\nx\u2208T\nlog p(x | N);\nJXLNet =\nX\nx\u2208T\nlog p(x | N \u222aT<x)\nwhere T<x denote tokens in T that have a factorization order prior to x. Both objectives consist\nof multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair\n(x, U) \u2208I such that U \u2286Vx, then the loss term log p(x | Vx) provides a training signal to the\ndependency between x and U. For convenience, we say a target-context pair (x, U) \u2208I is covered\nby a model (objective) if U \u2286Vx.\nGiven the de\ufb01nition, let\u2019s consider two cases:\n\u2022 If U \u2286N, the dependency (x, U) is covered by both BERT and XLNet.\n\u2022 If U \u2286N \u222aT<x and U \u2229T<x \u0338= \u2205, the dependency can only be covered by XLNet but not BERT.\nAs a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet\nobjective contains more effective training signals, which empirically leads to better performance in\nSection 3.\nA.5.2\nComparison with Language Modeling\nBorrowing examples and notations from Section A.5.1, a standard AR language model like GPT\n[28] is only able to cover the dependency (x = York, U = {New}) but not (x = New, U = {York}).\nXLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a\nlimitation of AR language modeling can be critical in real-world applications. For example, consider\na span extraction question answering task with the context \u201cThom Yorke is the singer of Radiohead\u201d\nand the question \u201cWho is the singer of Radiohead\u201d. The representations of \u201cThom Yorke\u201d are not\ndependent on \u201cRadiohead\u201d with AR language modeling and thus they will not be chosen as the\nanswer by the standard approach that employs softmax over all token representations. More formally,\nconsider a context-target pair (x, U):\n\u2022 If U \u0338\u2286T<x, where T<x denotes the tokens prior to x in the original sequence, AR language\nmodeling is not able to cover the dependency.\n14\n", [439, 441, 442, 443, 445, 446, 447]], "A.5.1 Comparison with BERT": ["Hparam\nRACE\nSQuAD\nMNLI\nYelp-5\nDropout\n0.1\nAttention dropout\n0.1\nMax sequence length\n512\n512\n128\n512\nBatch size\n32\n48\n128\n128\nLearning rate\n2e-5\n3e-5\n2e-5\n1e-5\nNumber of steps\n12K\n8K\n10K\n10K\nLearning rate decay\nlinear\nWeight decay\n0.01\nAdam epsilon\n1e-6\n1e-6\n1e-6\n1e-6\nLayer-wise lr decay\n1.0\n0.75\n1.0\n1.0\nTable 8: Hyperparameters for \ufb01netuning.\nA.5\nDiscussion and Analysis\nA.5.1\nComparison with BERT\nTo prove a general point beyond one example, we now turn to more formal expressions. Inspired\nby previous work [37], given a sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], we de\ufb01ne a set of target-context pairs\nof interest, I = {(x, U)}, where U is a set of tokens in x that form a context of x. Intuitively, we\nwant the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For\nexample, given the above sentence, the pairs of interest I could be instantiated as:\nI =\nn\u0000x = York, U = {New}\n\u0001\n,\n\u0000x = York, U = {city}\n\u0001\n,\n\u0000x = York, U = {New, city}\n\u0001\n, \u00b7 \u00b7 \u00b7\no\n.\nNote that I is merely a virtual notion without unique ground truth, and our analysis will hold\nregardless of how I is instantiated.\nGiven a set of target tokens T and a set of non-target tokens N = x\\T , BERT and XLNet both\nmaximize log p(T | N) but with different formulations:\nJBERT =\nX\nx\u2208T\nlog p(x | N);\nJXLNet =\nX\nx\u2208T\nlog p(x | N \u222aT<x)\nwhere T<x denote tokens in T that have a factorization order prior to x. Both objectives consist\nof multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair\n(x, U) \u2208I such that U \u2286Vx, then the loss term log p(x | Vx) provides a training signal to the\ndependency between x and U. For convenience, we say a target-context pair (x, U) \u2208I is covered\nby a model (objective) if U \u2286Vx.\nGiven the de\ufb01nition, let\u2019s consider two cases:\n\u2022 If U \u2286N, the dependency (x, U) is covered by both BERT and XLNet.\n\u2022 If U \u2286N \u222aT<x and U \u2229T<x \u0338= \u2205, the dependency can only be covered by XLNet but not BERT.\nAs a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet\nobjective contains more effective training signals, which empirically leads to better performance in\nSection 3.\nA.5.2\nComparison with Language Modeling\nBorrowing examples and notations from Section A.5.1, a standard AR language model like GPT\n[28] is only able to cover the dependency (x = York, U = {New}) but not (x = New, U = {York}).\nXLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a\nlimitation of AR language modeling can be critical in real-world applications. For example, consider\na span extraction question answering task with the context \u201cThom Yorke is the singer of Radiohead\u201d\nand the question \u201cWho is the singer of Radiohead\u201d. The representations of \u201cThom Yorke\u201d are not\ndependent on \u201cRadiohead\u201d with AR language modeling and thus they will not be chosen as the\nanswer by the standard approach that employs softmax over all token representations. More formally,\nconsider a context-target pair (x, U):\n\u2022 If U \u0338\u2286T<x, where T<x denotes the tokens prior to x in the original sequence, AR language\nmodeling is not able to cover the dependency.\n14\n", []], "A.5 Discussion and Analysis": ["Hparam\nRACE\nSQuAD\nMNLI\nYelp-5\nDropout\n0.1\nAttention dropout\n0.1\nMax sequence length\n512\n512\n128\n512\nBatch size\n32\n48\n128\n128\nLearning rate\n2e-5\n3e-5\n2e-5\n1e-5\nNumber of steps\n12K\n8K\n10K\n10K\nLearning rate decay\nlinear\nWeight decay\n0.01\nAdam epsilon\n1e-6\n1e-6\n1e-6\n1e-6\nLayer-wise lr decay\n1.0\n0.75\n1.0\n1.0\nTable 8: Hyperparameters for \ufb01netuning.\nA.5\nDiscussion and Analysis\nA.5.1\nComparison with BERT\nTo prove a general point beyond one example, we now turn to more formal expressions. Inspired\nby previous work [37], given a sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], we de\ufb01ne a set of target-context pairs\nof interest, I = {(x, U)}, where U is a set of tokens in x that form a context of x. Intuitively, we\nwant the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For\nexample, given the above sentence, the pairs of interest I could be instantiated as:\nI =\nn\u0000x = York, U = {New}\n\u0001\n,\n\u0000x = York, U = {city}\n\u0001\n,\n\u0000x = York, U = {New, city}\n\u0001\n, \u00b7 \u00b7 \u00b7\no\n.\nNote that I is merely a virtual notion without unique ground truth, and our analysis will hold\nregardless of how I is instantiated.\nGiven a set of target tokens T and a set of non-target tokens N = x\\T , BERT and XLNet both\nmaximize log p(T | N) but with different formulations:\nJBERT =\nX\nx\u2208T\nlog p(x | N);\nJXLNet =\nX\nx\u2208T\nlog p(x | N \u222aT<x)\nwhere T<x denote tokens in T that have a factorization order prior to x. Both objectives consist\nof multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair\n(x, U) \u2208I such that U \u2286Vx, then the loss term log p(x | Vx) provides a training signal to the\ndependency between x and U. For convenience, we say a target-context pair (x, U) \u2208I is covered\nby a model (objective) if U \u2286Vx.\nGiven the de\ufb01nition, let\u2019s consider two cases:\n\u2022 If U \u2286N, the dependency (x, U) is covered by both BERT and XLNet.\n\u2022 If U \u2286N \u222aT<x and U \u2229T<x \u0338= \u2205, the dependency can only be covered by XLNet but not BERT.\nAs a result, XLNet is able to cover more dependencies than BERT. In other words, the XLNet\nobjective contains more effective training signals, which empirically leads to better performance in\nSection 3.\nA.5.2\nComparison with Language Modeling\nBorrowing examples and notations from Section A.5.1, a standard AR language model like GPT\n[28] is only able to cover the dependency (x = York, U = {New}) but not (x = New, U = {York}).\nXLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a\nlimitation of AR language modeling can be critical in real-world applications. For example, consider\na span extraction question answering task with the context \u201cThom Yorke is the singer of Radiohead\u201d\nand the question \u201cWho is the singer of Radiohead\u201d. The representations of \u201cThom Yorke\u201d are not\ndependent on \u201cRadiohead\u201d with AR language modeling and thus they will not be chosen as the\nanswer by the standard approach that employs softmax over all token representations. More formally,\nconsider a context-target pair (x, U):\n\u2022 If U \u0338\u2286T<x, where T<x denotes the tokens prior to x in the original sequence, AR language\nmodeling is not able to cover the dependency.\n14\n", []], "A.4.2 Hyperparameters for Finetuning": ["A.3.3\nText classi\ufb01cation Datasets\nFollowing previous work on text classi\ufb01cation [39, 23], we evaluate XLNet on the following bench-\nmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\nA.3.4\nGLUE Dataset\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels\nare removed from the publicly released version, and all the practitioners must submit their predictions\non the evaluation server to obtain test set results. In Table 5, we present results of multiple settings,\nincluding single-task and multi-task, as well as single models and ensembles. In the multi-task setting,\nwe jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and \ufb01netune\nthe network on the other datasets. Only single-task training is employed for the four large datasets.\nFor QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission.\nHowever, for fair comparison with BERT, our result on the QNLI dev set is based on a standard\nclassi\ufb01cation paradigm. For WNLI, we use the loss described in [16].\nA.3.5\nClueWeb09-B Dataset\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-\nmance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on\n50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval\nmethod. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations\ninstead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word\nembeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries\nwithout \ufb01netuning, and employ a kernel pooling network [36] to rank the documents.\nA.4\nHyperparameters\nA.4.1\nPretraining Hyperparameters\nHparam\nValue\nNumber of layers\n24\nHidden size\n1024\nNumber of attention heads\n16\nAttention head size\n64\nFFN inner hidden size\n4096\nHidden Dropout\n0.1\nGeLU Dropout\n0.0\nAttention dropout\n0.1\nPartial prediction K\n6\nMax sequence length\n512\nBatch size\n8192\nLearning rate\n4e-4\nNumber of steps\n500K\nWarmup steps\n40,000\nLearning rate decay\nlinear\nAdam epsilon\n1e-6\nWeight decay\n0.01\nTable 7: Hyperparameters for pretraining.\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2\nHyperparameters for Finetuning\nThe hyperparameters used for \ufb01netuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise\ndecay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.\nFor example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then\nthe learning rate of layer m is l\u03b124\u2212m.\n13\n", []], "A.4.1 Pretraining Hyperparameters": ["A.3.3\nText classi\ufb01cation Datasets\nFollowing previous work on text classi\ufb01cation [39, 23], we evaluate XLNet on the following bench-\nmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\nA.3.4\nGLUE Dataset\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels\nare removed from the publicly released version, and all the practitioners must submit their predictions\non the evaluation server to obtain test set results. In Table 5, we present results of multiple settings,\nincluding single-task and multi-task, as well as single models and ensembles. In the multi-task setting,\nwe jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and \ufb01netune\nthe network on the other datasets. Only single-task training is employed for the four large datasets.\nFor QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission.\nHowever, for fair comparison with BERT, our result on the QNLI dev set is based on a standard\nclassi\ufb01cation paradigm. For WNLI, we use the loss described in [16].\nA.3.5\nClueWeb09-B Dataset\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-\nmance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on\n50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval\nmethod. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations\ninstead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word\nembeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries\nwithout \ufb01netuning, and employ a kernel pooling network [36] to rank the documents.\nA.4\nHyperparameters\nA.4.1\nPretraining Hyperparameters\nHparam\nValue\nNumber of layers\n24\nHidden size\n1024\nNumber of attention heads\n16\nAttention head size\n64\nFFN inner hidden size\n4096\nHidden Dropout\n0.1\nGeLU Dropout\n0.0\nAttention dropout\n0.1\nPartial prediction K\n6\nMax sequence length\n512\nBatch size\n8192\nLearning rate\n4e-4\nNumber of steps\n500K\nWarmup steps\n40,000\nLearning rate decay\nlinear\nAdam epsilon\n1e-6\nWeight decay\n0.01\nTable 7: Hyperparameters for pretraining.\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2\nHyperparameters for Finetuning\nThe hyperparameters used for \ufb01netuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise\ndecay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.\nFor example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then\nthe learning rate of layer m is l\u03b124\u2212m.\n13\n", []], "A.4 Hyperparameters": ["A.3.3\nText classi\ufb01cation Datasets\nFollowing previous work on text classi\ufb01cation [39, 23], we evaluate XLNet on the following bench-\nmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\nA.3.4\nGLUE Dataset\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels\nare removed from the publicly released version, and all the practitioners must submit their predictions\non the evaluation server to obtain test set results. In Table 5, we present results of multiple settings,\nincluding single-task and multi-task, as well as single models and ensembles. In the multi-task setting,\nwe jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and \ufb01netune\nthe network on the other datasets. Only single-task training is employed for the four large datasets.\nFor QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission.\nHowever, for fair comparison with BERT, our result on the QNLI dev set is based on a standard\nclassi\ufb01cation paradigm. For WNLI, we use the loss described in [16].\nA.3.5\nClueWeb09-B Dataset\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-\nmance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on\n50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval\nmethod. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations\ninstead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word\nembeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries\nwithout \ufb01netuning, and employ a kernel pooling network [36] to rank the documents.\nA.4\nHyperparameters\nA.4.1\nPretraining Hyperparameters\nHparam\nValue\nNumber of layers\n24\nHidden size\n1024\nNumber of attention heads\n16\nAttention head size\n64\nFFN inner hidden size\n4096\nHidden Dropout\n0.1\nGeLU Dropout\n0.0\nAttention dropout\n0.1\nPartial prediction K\n6\nMax sequence length\n512\nBatch size\n8192\nLearning rate\n4e-4\nNumber of steps\n500K\nWarmup steps\n40,000\nLearning rate decay\nlinear\nAdam epsilon\n1e-6\nWeight decay\n0.01\nTable 7: Hyperparameters for pretraining.\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2\nHyperparameters for Finetuning\nThe hyperparameters used for \ufb01netuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise\ndecay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.\nFor example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then\nthe learning rate of layer m is l\u03b124\u2212m.\n13\n", []], "A.3.5 ClueWeb09-B Dataset": ["A.3.3\nText classi\ufb01cation Datasets\nFollowing previous work on text classi\ufb01cation [39, 23], we evaluate XLNet on the following bench-\nmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\nA.3.4\nGLUE Dataset\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels\nare removed from the publicly released version, and all the practitioners must submit their predictions\non the evaluation server to obtain test set results. In Table 5, we present results of multiple settings,\nincluding single-task and multi-task, as well as single models and ensembles. In the multi-task setting,\nwe jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and \ufb01netune\nthe network on the other datasets. Only single-task training is employed for the four large datasets.\nFor QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission.\nHowever, for fair comparison with BERT, our result on the QNLI dev set is based on a standard\nclassi\ufb01cation paradigm. For WNLI, we use the loss described in [16].\nA.3.5\nClueWeb09-B Dataset\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-\nmance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on\n50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval\nmethod. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations\ninstead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word\nembeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries\nwithout \ufb01netuning, and employ a kernel pooling network [36] to rank the documents.\nA.4\nHyperparameters\nA.4.1\nPretraining Hyperparameters\nHparam\nValue\nNumber of layers\n24\nHidden size\n1024\nNumber of attention heads\n16\nAttention head size\n64\nFFN inner hidden size\n4096\nHidden Dropout\n0.1\nGeLU Dropout\n0.0\nAttention dropout\n0.1\nPartial prediction K\n6\nMax sequence length\n512\nBatch size\n8192\nLearning rate\n4e-4\nNumber of steps\n500K\nWarmup steps\n40,000\nLearning rate decay\nlinear\nAdam epsilon\n1e-6\nWeight decay\n0.01\nTable 7: Hyperparameters for pretraining.\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2\nHyperparameters for Finetuning\nThe hyperparameters used for \ufb01netuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise\ndecay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.\nFor example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then\nthe learning rate of layer m is l\u03b124\u2212m.\n13\n", []], "A.3.4 GLUE Dataset": ["A.3.3\nText classi\ufb01cation Datasets\nFollowing previous work on text classi\ufb01cation [39, 23], we evaluate XLNet on the following bench-\nmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\nA.3.4\nGLUE Dataset\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels\nare removed from the publicly released version, and all the practitioners must submit their predictions\non the evaluation server to obtain test set results. In Table 5, we present results of multiple settings,\nincluding single-task and multi-task, as well as single models and ensembles. In the multi-task setting,\nwe jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and \ufb01netune\nthe network on the other datasets. Only single-task training is employed for the four large datasets.\nFor QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission.\nHowever, for fair comparison with BERT, our result on the QNLI dev set is based on a standard\nclassi\ufb01cation paradigm. For WNLI, we use the loss described in [16].\nA.3.5\nClueWeb09-B Dataset\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-\nmance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on\n50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval\nmethod. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations\ninstead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word\nembeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries\nwithout \ufb01netuning, and employ a kernel pooling network [36] to rank the documents.\nA.4\nHyperparameters\nA.4.1\nPretraining Hyperparameters\nHparam\nValue\nNumber of layers\n24\nHidden size\n1024\nNumber of attention heads\n16\nAttention head size\n64\nFFN inner hidden size\n4096\nHidden Dropout\n0.1\nGeLU Dropout\n0.0\nAttention dropout\n0.1\nPartial prediction K\n6\nMax sequence length\n512\nBatch size\n8192\nLearning rate\n4e-4\nNumber of steps\n500K\nWarmup steps\n40,000\nLearning rate decay\nlinear\nAdam epsilon\n1e-6\nWeight decay\n0.01\nTable 7: Hyperparameters for pretraining.\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2\nHyperparameters for Finetuning\nThe hyperparameters used for \ufb01netuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise\ndecay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.\nFor example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then\nthe learning rate of layer m is l\u03b124\u2212m.\n13\n", []], "A.3.3 Text classification Datasets": ["A.3.3\nText classi\ufb01cation Datasets\nFollowing previous work on text classi\ufb01cation [39, 23], we evaluate XLNet on the following bench-\nmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5.\nA.3.4\nGLUE Dataset\nThe GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels\nare removed from the publicly released version, and all the practitioners must submit their predictions\non the evaluation server to obtain test set results. In Table 5, we present results of multiple settings,\nincluding single-task and multi-task, as well as single models and ensembles. In the multi-task setting,\nwe jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and \ufb01netune\nthe network on the other datasets. Only single-task training is employed for the four large datasets.\nFor QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission.\nHowever, for fair comparison with BERT, our result on the QNLI dev set is based on a standard\nclassi\ufb01cation paradigm. For WNLI, we use the loss described in [16].\nA.3.5\nClueWeb09-B Dataset\nFollowing the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the perfor-\nmance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on\n50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval\nmethod. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations\ninstead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word\nembeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries\nwithout \ufb01netuning, and employ a kernel pooling network [36] to rank the documents.\nA.4\nHyperparameters\nA.4.1\nPretraining Hyperparameters\nHparam\nValue\nNumber of layers\n24\nHidden size\n1024\nNumber of attention heads\n16\nAttention head size\n64\nFFN inner hidden size\n4096\nHidden Dropout\n0.1\nGeLU Dropout\n0.0\nAttention dropout\n0.1\nPartial prediction K\n6\nMax sequence length\n512\nBatch size\n8192\nLearning rate\n4e-4\nNumber of steps\n500K\nWarmup steps\n40,000\nLearning rate decay\nlinear\nAdam epsilon\n1e-6\nWeight decay\n0.01\nTable 7: Hyperparameters for pretraining.\nThe hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2\nHyperparameters for Finetuning\nThe hyperparameters used for \ufb01netuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise\ndecay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.\nFor example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then\nthe learning rate of layer m is l\u03b124\u2212m.\n13\n", []], "A.3.2 SQuAD": ["A\nTarget-Aware Representation via Two-Stream Self-Attention\nA.1\nA Concrete Example of How Standard LM Parameterization Fails\nIn this section, we provide a concrete example to show how the standard language model parameteri-\nzation fails under the permutation objective, as discussed in Section 2.3. Speci\ufb01cally, let\u2019s consider\ntwo different permutations z(1) and z(2) satisfying the following relationship\nz(1)\n<t = z(2)\n<t = z<t\nbut\nz(1)\nt\n= i \u0338= j = z(2)\nt\n.\nThen, substituting the two permutations respectively into the naive parameterization, we have\np\u03b8(Xi = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=i, z(1)\n<t =z<t\n= p\u03b8(Xj = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=j, z(2)\n<t =z<t\n=\nexp\n\u0000e(x)\u22a4h(xz<t)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4h(xz<t)).\nEffectively, two different target positions i and j share exactly the same model prediction. However,\nthe ground-truth distribution of two positions should certainly be different.\nA.2\nTwo-Stream Attention\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL\nbackbone.\nInitial represetation:\n\u2200t = 1, . . . , T :\nht = e(xt)\nand\ngt = w\nCached layer-m content represetation (memory) from previous segment: \u02dch(m)\nFor the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 , M, attention with relative positional encoding and\nposition-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T :\n\u02c6h(m)\nzt\n= LayerNorm\n\u0010\nh(m\u22121)\nzt\n+ RelAttn\n\u0010\nh(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\u0011\u0011\nh(m)\nzt\n= LayerNorm\n\u0010\n\u02c6h(m)\nzt\n+ PosFF\n\u0010\n\u02c6h(m)\nzt\n\u0011\u0011\n\u02c6g(m)\nzt\n= LayerNorm\n\u0010\ng(m\u22121)\nzt\n+ RelAttn\n\u0010\ng(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz<t\ni\u0011\u0011\ng(m)\nzt\n= LayerNorm\n\u0010\n\u02c6g(m)\nzt\n+ PosFF\n\u0010\n\u02c6g(m)\nzt\n\u0011\u0011\nTarget-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0010\ne(x)\u22a4g(M)\nzt\n\u0011\nP\nx\u2032 exp\n\u0010\ne(x\u2032)\u22a4g(M)\nzt\n\u0011,\nA.3\nDatasets\nA.3.1\nRACE Dataset\nThe RACE dataset [18] contains near 100K questions taken from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, with the answers generated by human\nexperts. This is one of the most dif\ufb01cult reading comprehension datasets that involve challenging\nreasoning questions. Moreover, the average length of the passages in RACE are longer than 300,\nwhich is signi\ufb01cantly longer than other popular reading comprehension datasets such as SQuAD [29].\nAs a result, this dataset serves as a challenging benchmark for long text understanding. We use a\nsequence length of 512 during \ufb01netuning.\nA.3.2\nSQuAD\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains\nquestions that always have a corresponding answer in the given passages, while SQuAD2.0 [29]\nintroduces unanswerable questions. To \ufb01netune an XLNet on SQuAD2.0, we jointly apply a logis-\ntic regression loss for answerability prediction similar to classi\ufb01cation tasks and a standard span\nextraction loss for question answering [10].\n12\n", []], "A.3.1 RACE Dataset": ["A\nTarget-Aware Representation via Two-Stream Self-Attention\nA.1\nA Concrete Example of How Standard LM Parameterization Fails\nIn this section, we provide a concrete example to show how the standard language model parameteri-\nzation fails under the permutation objective, as discussed in Section 2.3. Speci\ufb01cally, let\u2019s consider\ntwo different permutations z(1) and z(2) satisfying the following relationship\nz(1)\n<t = z(2)\n<t = z<t\nbut\nz(1)\nt\n= i \u0338= j = z(2)\nt\n.\nThen, substituting the two permutations respectively into the naive parameterization, we have\np\u03b8(Xi = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=i, z(1)\n<t =z<t\n= p\u03b8(Xj = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=j, z(2)\n<t =z<t\n=\nexp\n\u0000e(x)\u22a4h(xz<t)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4h(xz<t)).\nEffectively, two different target positions i and j share exactly the same model prediction. However,\nthe ground-truth distribution of two positions should certainly be different.\nA.2\nTwo-Stream Attention\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL\nbackbone.\nInitial represetation:\n\u2200t = 1, . . . , T :\nht = e(xt)\nand\ngt = w\nCached layer-m content represetation (memory) from previous segment: \u02dch(m)\nFor the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 , M, attention with relative positional encoding and\nposition-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T :\n\u02c6h(m)\nzt\n= LayerNorm\n\u0010\nh(m\u22121)\nzt\n+ RelAttn\n\u0010\nh(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\u0011\u0011\nh(m)\nzt\n= LayerNorm\n\u0010\n\u02c6h(m)\nzt\n+ PosFF\n\u0010\n\u02c6h(m)\nzt\n\u0011\u0011\n\u02c6g(m)\nzt\n= LayerNorm\n\u0010\ng(m\u22121)\nzt\n+ RelAttn\n\u0010\ng(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz<t\ni\u0011\u0011\ng(m)\nzt\n= LayerNorm\n\u0010\n\u02c6g(m)\nzt\n+ PosFF\n\u0010\n\u02c6g(m)\nzt\n\u0011\u0011\nTarget-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0010\ne(x)\u22a4g(M)\nzt\n\u0011\nP\nx\u2032 exp\n\u0010\ne(x\u2032)\u22a4g(M)\nzt\n\u0011,\nA.3\nDatasets\nA.3.1\nRACE Dataset\nThe RACE dataset [18] contains near 100K questions taken from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, with the answers generated by human\nexperts. This is one of the most dif\ufb01cult reading comprehension datasets that involve challenging\nreasoning questions. Moreover, the average length of the passages in RACE are longer than 300,\nwhich is signi\ufb01cantly longer than other popular reading comprehension datasets such as SQuAD [29].\nAs a result, this dataset serves as a challenging benchmark for long text understanding. We use a\nsequence length of 512 during \ufb01netuning.\nA.3.2\nSQuAD\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains\nquestions that always have a corresponding answer in the given passages, while SQuAD2.0 [29]\nintroduces unanswerable questions. To \ufb01netune an XLNet on SQuAD2.0, we jointly apply a logis-\ntic regression loss for answerability prediction similar to classi\ufb01cation tasks and a standard span\nextraction loss for question answering [10].\n12\n", []], "A.3 Datasets": ["A\nTarget-Aware Representation via Two-Stream Self-Attention\nA.1\nA Concrete Example of How Standard LM Parameterization Fails\nIn this section, we provide a concrete example to show how the standard language model parameteri-\nzation fails under the permutation objective, as discussed in Section 2.3. Speci\ufb01cally, let\u2019s consider\ntwo different permutations z(1) and z(2) satisfying the following relationship\nz(1)\n<t = z(2)\n<t = z<t\nbut\nz(1)\nt\n= i \u0338= j = z(2)\nt\n.\nThen, substituting the two permutations respectively into the naive parameterization, we have\np\u03b8(Xi = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=i, z(1)\n<t =z<t\n= p\u03b8(Xj = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=j, z(2)\n<t =z<t\n=\nexp\n\u0000e(x)\u22a4h(xz<t)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4h(xz<t)).\nEffectively, two different target positions i and j share exactly the same model prediction. However,\nthe ground-truth distribution of two positions should certainly be different.\nA.2\nTwo-Stream Attention\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL\nbackbone.\nInitial represetation:\n\u2200t = 1, . . . , T :\nht = e(xt)\nand\ngt = w\nCached layer-m content represetation (memory) from previous segment: \u02dch(m)\nFor the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 , M, attention with relative positional encoding and\nposition-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T :\n\u02c6h(m)\nzt\n= LayerNorm\n\u0010\nh(m\u22121)\nzt\n+ RelAttn\n\u0010\nh(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\u0011\u0011\nh(m)\nzt\n= LayerNorm\n\u0010\n\u02c6h(m)\nzt\n+ PosFF\n\u0010\n\u02c6h(m)\nzt\n\u0011\u0011\n\u02c6g(m)\nzt\n= LayerNorm\n\u0010\ng(m\u22121)\nzt\n+ RelAttn\n\u0010\ng(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz<t\ni\u0011\u0011\ng(m)\nzt\n= LayerNorm\n\u0010\n\u02c6g(m)\nzt\n+ PosFF\n\u0010\n\u02c6g(m)\nzt\n\u0011\u0011\nTarget-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0010\ne(x)\u22a4g(M)\nzt\n\u0011\nP\nx\u2032 exp\n\u0010\ne(x\u2032)\u22a4g(M)\nzt\n\u0011,\nA.3\nDatasets\nA.3.1\nRACE Dataset\nThe RACE dataset [18] contains near 100K questions taken from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, with the answers generated by human\nexperts. This is one of the most dif\ufb01cult reading comprehension datasets that involve challenging\nreasoning questions. Moreover, the average length of the passages in RACE are longer than 300,\nwhich is signi\ufb01cantly longer than other popular reading comprehension datasets such as SQuAD [29].\nAs a result, this dataset serves as a challenging benchmark for long text understanding. We use a\nsequence length of 512 during \ufb01netuning.\nA.3.2\nSQuAD\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains\nquestions that always have a corresponding answer in the given passages, while SQuAD2.0 [29]\nintroduces unanswerable questions. To \ufb01netune an XLNet on SQuAD2.0, we jointly apply a logis-\ntic regression loss for answerability prediction similar to classi\ufb01cation tasks and a standard span\nextraction loss for question answering [10].\n12\n", []], "A.2 Two-Stream Attention": ["A\nTarget-Aware Representation via Two-Stream Self-Attention\nA.1\nA Concrete Example of How Standard LM Parameterization Fails\nIn this section, we provide a concrete example to show how the standard language model parameteri-\nzation fails under the permutation objective, as discussed in Section 2.3. Speci\ufb01cally, let\u2019s consider\ntwo different permutations z(1) and z(2) satisfying the following relationship\nz(1)\n<t = z(2)\n<t = z<t\nbut\nz(1)\nt\n= i \u0338= j = z(2)\nt\n.\nThen, substituting the two permutations respectively into the naive parameterization, we have\np\u03b8(Xi = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=i, z(1)\n<t =z<t\n= p\u03b8(Xj = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=j, z(2)\n<t =z<t\n=\nexp\n\u0000e(x)\u22a4h(xz<t)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4h(xz<t)).\nEffectively, two different target positions i and j share exactly the same model prediction. However,\nthe ground-truth distribution of two positions should certainly be different.\nA.2\nTwo-Stream Attention\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL\nbackbone.\nInitial represetation:\n\u2200t = 1, . . . , T :\nht = e(xt)\nand\ngt = w\nCached layer-m content represetation (memory) from previous segment: \u02dch(m)\nFor the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 , M, attention with relative positional encoding and\nposition-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T :\n\u02c6h(m)\nzt\n= LayerNorm\n\u0010\nh(m\u22121)\nzt\n+ RelAttn\n\u0010\nh(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\u0011\u0011\nh(m)\nzt\n= LayerNorm\n\u0010\n\u02c6h(m)\nzt\n+ PosFF\n\u0010\n\u02c6h(m)\nzt\n\u0011\u0011\n\u02c6g(m)\nzt\n= LayerNorm\n\u0010\ng(m\u22121)\nzt\n+ RelAttn\n\u0010\ng(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz<t\ni\u0011\u0011\ng(m)\nzt\n= LayerNorm\n\u0010\n\u02c6g(m)\nzt\n+ PosFF\n\u0010\n\u02c6g(m)\nzt\n\u0011\u0011\nTarget-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0010\ne(x)\u22a4g(M)\nzt\n\u0011\nP\nx\u2032 exp\n\u0010\ne(x\u2032)\u22a4g(M)\nzt\n\u0011,\nA.3\nDatasets\nA.3.1\nRACE Dataset\nThe RACE dataset [18] contains near 100K questions taken from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, with the answers generated by human\nexperts. This is one of the most dif\ufb01cult reading comprehension datasets that involve challenging\nreasoning questions. Moreover, the average length of the passages in RACE are longer than 300,\nwhich is signi\ufb01cantly longer than other popular reading comprehension datasets such as SQuAD [29].\nAs a result, this dataset serves as a challenging benchmark for long text understanding. We use a\nsequence length of 512 during \ufb01netuning.\nA.3.2\nSQuAD\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains\nquestions that always have a corresponding answer in the given passages, while SQuAD2.0 [29]\nintroduces unanswerable questions. To \ufb01netune an XLNet on SQuAD2.0, we jointly apply a logis-\ntic regression loss for answerability prediction similar to classi\ufb01cation tasks and a standard span\nextraction loss for question answering [10].\n12\n", []], "A.1 A Concrete Example of How Standard LM Parameterization Fails": ["A\nTarget-Aware Representation via Two-Stream Self-Attention\nA.1\nA Concrete Example of How Standard LM Parameterization Fails\nIn this section, we provide a concrete example to show how the standard language model parameteri-\nzation fails under the permutation objective, as discussed in Section 2.3. Speci\ufb01cally, let\u2019s consider\ntwo different permutations z(1) and z(2) satisfying the following relationship\nz(1)\n<t = z(2)\n<t = z<t\nbut\nz(1)\nt\n= i \u0338= j = z(2)\nt\n.\nThen, substituting the two permutations respectively into the naive parameterization, we have\np\u03b8(Xi = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=i, z(1)\n<t =z<t\n= p\u03b8(Xj = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=j, z(2)\n<t =z<t\n=\nexp\n\u0000e(x)\u22a4h(xz<t)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4h(xz<t)).\nEffectively, two different target positions i and j share exactly the same model prediction. However,\nthe ground-truth distribution of two positions should certainly be different.\nA.2\nTwo-Stream Attention\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL\nbackbone.\nInitial represetation:\n\u2200t = 1, . . . , T :\nht = e(xt)\nand\ngt = w\nCached layer-m content represetation (memory) from previous segment: \u02dch(m)\nFor the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 , M, attention with relative positional encoding and\nposition-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T :\n\u02c6h(m)\nzt\n= LayerNorm\n\u0010\nh(m\u22121)\nzt\n+ RelAttn\n\u0010\nh(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\u0011\u0011\nh(m)\nzt\n= LayerNorm\n\u0010\n\u02c6h(m)\nzt\n+ PosFF\n\u0010\n\u02c6h(m)\nzt\n\u0011\u0011\n\u02c6g(m)\nzt\n= LayerNorm\n\u0010\ng(m\u22121)\nzt\n+ RelAttn\n\u0010\ng(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz<t\ni\u0011\u0011\ng(m)\nzt\n= LayerNorm\n\u0010\n\u02c6g(m)\nzt\n+ PosFF\n\u0010\n\u02c6g(m)\nzt\n\u0011\u0011\nTarget-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0010\ne(x)\u22a4g(M)\nzt\n\u0011\nP\nx\u2032 exp\n\u0010\ne(x\u2032)\u22a4g(M)\nzt\n\u0011,\nA.3\nDatasets\nA.3.1\nRACE Dataset\nThe RACE dataset [18] contains near 100K questions taken from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, with the answers generated by human\nexperts. This is one of the most dif\ufb01cult reading comprehension datasets that involve challenging\nreasoning questions. Moreover, the average length of the passages in RACE are longer than 300,\nwhich is signi\ufb01cantly longer than other popular reading comprehension datasets such as SQuAD [29].\nAs a result, this dataset serves as a challenging benchmark for long text understanding. We use a\nsequence length of 512 during \ufb01netuning.\nA.3.2\nSQuAD\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains\nquestions that always have a corresponding answer in the given passages, while SQuAD2.0 [29]\nintroduces unanswerable questions. To \ufb01netune an XLNet on SQuAD2.0, we jointly apply a logis-\ntic regression loss for answerability prediction similar to classi\ufb01cation tasks and a standard span\nextraction loss for question answering [10].\n12\n", []], "A Target-Aware Representation via Two-Stream Self-Attention": ["A\nTarget-Aware Representation via Two-Stream Self-Attention\nA.1\nA Concrete Example of How Standard LM Parameterization Fails\nIn this section, we provide a concrete example to show how the standard language model parameteri-\nzation fails under the permutation objective, as discussed in Section 2.3. Speci\ufb01cally, let\u2019s consider\ntwo different permutations z(1) and z(2) satisfying the following relationship\nz(1)\n<t = z(2)\n<t = z<t\nbut\nz(1)\nt\n= i \u0338= j = z(2)\nt\n.\nThen, substituting the two permutations respectively into the naive parameterization, we have\np\u03b8(Xi = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=i, z(1)\n<t =z<t\n= p\u03b8(Xj = x | xz<t)\n|\n{z\n}\nz(1)\nt\n=j, z(2)\n<t =z<t\n=\nexp\n\u0000e(x)\u22a4h(xz<t)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4h(xz<t)).\nEffectively, two different target positions i and j share exactly the same model prediction. However,\nthe ground-truth distribution of two positions should certainly be different.\nA.2\nTwo-Stream Attention\nHere, we provide the implementation details of the two-stream attention with a Transformer-XL\nbackbone.\nInitial represetation:\n\u2200t = 1, . . . , T :\nht = e(xt)\nand\ngt = w\nCached layer-m content represetation (memory) from previous segment: \u02dch(m)\nFor the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 , M, attention with relative positional encoding and\nposition-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T :\n\u02c6h(m)\nzt\n= LayerNorm\n\u0010\nh(m\u22121)\nzt\n+ RelAttn\n\u0010\nh(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\u0011\u0011\nh(m)\nzt\n= LayerNorm\n\u0010\n\u02c6h(m)\nzt\n+ PosFF\n\u0010\n\u02c6h(m)\nzt\n\u0011\u0011\n\u02c6g(m)\nzt\n= LayerNorm\n\u0010\ng(m\u22121)\nzt\n+ RelAttn\n\u0010\ng(m\u22121)\nzt\n,\nh\n\u02dch(m\u22121), h(m\u22121)\nz<t\ni\u0011\u0011\ng(m)\nzt\n= LayerNorm\n\u0010\n\u02c6g(m)\nzt\n+ PosFF\n\u0010\n\u02c6g(m)\nzt\n\u0011\u0011\nTarget-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0010\ne(x)\u22a4g(M)\nzt\n\u0011\nP\nx\u2032 exp\n\u0010\ne(x\u2032)\u22a4g(M)\nzt\n\u0011,\nA.3\nDatasets\nA.3.1\nRACE Dataset\nThe RACE dataset [18] contains near 100K questions taken from the English exams for middle and\nhigh school Chinese students in the age range between 12 to 18, with the answers generated by human\nexperts. This is one of the most dif\ufb01cult reading comprehension datasets that involve challenging\nreasoning questions. Moreover, the average length of the passages in RACE are longer than 300,\nwhich is signi\ufb01cantly longer than other popular reading comprehension datasets such as SQuAD [29].\nAs a result, this dataset serves as a challenging benchmark for long text understanding. We use a\nsequence length of 512 during \ufb01netuning.\nA.3.2\nSQuAD\nSQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains\nquestions that always have a corresponding answer in the given passages, while SQuAD2.0 [29]\nintroduces unanswerable questions. To \ufb01netune an XLNet on SQuAD2.0, we jointly apply a logis-\ntic regression loss for answerability prediction similar to classi\ufb01cation tasks and a standard span\nextraction loss for question answering [10].\n12\n", []], "4 Conclusions": ["With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implemen-\ntation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL\nbaseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidi-\nrectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture\nwith the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the\nBooksCorpus. All results reported are the median of 5 runs.\n#\nModel\nRACE\nSQuAD2.0\nMNLI\nSST-2\nF1\nEM\nm/mm\n1\nBERT-Base\n64.3\n76.30\n73.66\n84.34/84.65\n92.78\n2\nDAE + Transformer-XL\n65.03\n79.56\n76.80\n84.88/84.45\n92.60\n3\nXLNet-Base (K = 7)\n66.05\n81.33\n78.46\n85.84/85.43\n92.66\n4\nXLNet-Base (K = 6)\n66.66\n80.98\n78.18\n85.63/85.12\n93.35\n5\n- memory\n65.55\n80.15\n77.27\n85.32/85.05\n92.78\n6\n- span-based pred\n65.95\n80.61\n77.91\n85.49/85.02\n93.12\n7\n- bidirectional data\n66.34\n80.65\n77.87\n85.31/84.99\n92.66\n8\n+ next-sent pred\n66.76\n79.83\n76.94\n85.32/85.09\n92.89\nTable 6: The results of BERT on RACE are taken from [38]. We run BERT on the other datasets using the\nof\ufb01cial implementation and the same hyperparameter search space as XLNet. K is a hyperparameter to control\nthe optimization dif\ufb01culty (see Section 2.3).\nExamining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly\ncontribute the superior performance of XLNet over BERT. Moreover, if we remove the memory\ncaching mechanism (row 5), the performance clearly drops, especially for RACE which involves the\nlongest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and\nthe bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly \ufb01nd the the\nnext-sentence prediction objective proposed in the original BERT does not necessarily lead to an\nimprovement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet.\nFinally, we also perform a qualitative study of the attention patterns, which is included in Appendix\nA.6 due to page limit.\n4\nConclusions\nXLNet is a generalized AR pretraining method that uses a permutation language modeling objective\nto combine the advantages of AR and AE methods. The neural architecture of XLNet is developed to\nwork seamlessly with the AR objective, including integrating Transformer-XL and the careful design\nof the two-stream attention mechanism. XLNet achieves substantial improvement over previous\npretraining objectives on various tasks.\nAcknowledgments\nThe authors would like to thank Qizhe Xie and Adams Wei Yu for providing useful feedback on the\nproject, Jamie Callan for providing the ClueWeb dataset, Youlong Cheng, Yanping Huang and Shibo\nWang for providing ideas to improve our TPU implementation, Chenyan Xiong and Zhuyun Dai\nfor clarifying the setting of the document ranking task. ZY and RS were supported by the Of\ufb01ce of\nNaval Research grant N000141812861, the National Science Foundation (NSF) grant IIS1763562,\nthe Nvidia fellowship, and the Siebel scholarship. ZD and YY were supported in part by NSF under\nthe grant IIS-1546329 and by the DOE-Of\ufb01ce of Science under the grant ASCR #KJ040201.\nReferences\n[1] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\n[2] Anonymous. Bam! born-again multi-task networks for natural language understanding. anony-\nmous preprint under review, 2018.\n[3] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv preprint arXiv:1809.10853, 2018.\n9\n", []], "3.4 Ablation Study": ["SQuAD2.0\nEM\nF1\nSQuAD1.1\nEM\nF1\nDev set results (single model)\nBERT [10]\n78.98\n81.77\nBERT\u2020 [10]\n84.1\n90.9\nRoBERTa [21]\n86.5\n89.4\nRoBERTa [21]\n88.9\n94.6\nXLNet\n87.9\n90.6\nXLNet\n89.7\n95.1\nTest set results on leaderboard (single model, as of Dec 14, 2019)\nBERT [10]\n80.005\n83.061\nBERT [10]\n85.083\n91.835\nRoBERTa [21]\n86.820\n89.795\nBERT\u2217[10]\n87.433\n93.294\nXLNet\n87.926\n90.689\nXLNet\n89.898\u2021\n95.080\u2021\nTable 3: Results on SQuAD, a reading comprehension dataset. \u2020 marks our runs with the of\ufb01cial code. \u2217\nindicates ensembles. \u2021: We are not able to obtain the test results of our latest model on SQuAD1.1 from the\norganizers after submitting our result for more than one month, and thus report the results of an older version for\nthe SQuAD1.1 test set.\nModel\nIMDB\nYelp-2\nYelp-5\nDBpedia\nAG\nAmazon-2\nAmazon-5\nCNN [15]\n-\n2.90\n32.39\n0.84\n6.57\n3.79\n36.24\nDPCNN [15]\n-\n2.64\n30.58\n0.88\n6.87\n3.32\n34.81\nMixed VAT [31, 23]\n4.32\n-\n-\n0.70\n4.95\n-\n-\nULMFiT [14]\n4.6\n2.16\n29.98\n0.80\n5.01\n-\n-\nBERT [35]\n4.51\n1.89\n29.32\n0.64\n-\n2.63\n34.17\nXLNet\n3.20\n1.37\n27.05\n0.60\n4.45\n2.11\n31.67\nTable 4: Comparison with state-of-the-art error rates on the test sets of several text classi\ufb01cation datasets. All\nBERT and XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\nModel\nMNLI\nQNLI\nQQP\nRTE\nSST-2\nMRPC\nCoLA\nSTS-B\nWNLI\nSingle-task single models on dev\nBERT [2]\n86.6/-\n92.3\n91.3\n70.4\n93.2\n88.0\n60.6\n90.0\n-\nRoBERTa [21]\n90.2/90.2\n94.7\n92.2\n86.6\n96.4\n90.9\n68.0\n92.4\n-\nXLNet\n90.8/90.8\n94.9\n92.3\n85.9\n97.0\n90.8\n69.0\n92.5\n-\nMulti-task ensembles on test (from leaderboard as of Oct 28, 2019)\nMT-DNN\u2217[20]\n87.9/87.4\n96.0\n89.9\n86.3\n96.5\n92.7\n68.4\n91.1\n89.0\nRoBERTa\u2217[21]\n90.8/90.2\n98.9\n90.2\n88.2\n96.7\n92.3\n67.8\n92.2\n89.0\nXLNet\u2217\n90.9/90.9\u2020\n99.0\u2020\n90.4\u2020\n88.5\n97.1\u2020\n92.9\n70.2\n93.0\n92.5\nTable 5: Results on GLUE. \u2217indicates using ensembles, and \u2020 denotes single-task results in a multi-task row.\nAll dev results are the median of 10 runs. The upper section shows direct comparison on dev data and the lower\nsection shows comparison with state-of-the-art results on the public leaderboard.\n\u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance\ngain of XLNet is usually larger. This superiority at dealing with longer context could come from\nthe Transformer-XL backbone in XLNet.\n\u2022 For classi\ufb01cation tasks that already have abundant supervised examples such as MNLI (>390K),\nYelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains.\n3.4\nAblation Study\nWe perform an ablation study to understand the importance of each design choice based on four\ndatasets with diverse characteristics. Speci\ufb01cally, there are three main aspects we hope to study:\n\u2022 The effectiveness of the permutation language modeling objective alone, especially compared to\nthe denoising auto-encoding objective used by BERT.\n\u2022 The importance of using Transformer-XL as the backbone neural architecture.\n\u2022 The necessity of some implementation details including span-based prediction, the bidirectional\ninput pipeline, and next-sentence prediction.\n8\n", []], "3.3 Comparison with RoBERTa: Scaling Up": ["observed that the model still under\ufb01ts the data at the end of training. Finally, we perform ablation\nstudy (section 3.4) based on the XLNet-Base-wikibooks.\nSince the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each\nof the forward and backward directions takes half of the batch size. For training XLNet-Large, we set\nthe partial prediction constant K as 6 (see Section 2.3). Our \ufb01netuning procedure follows BERT [10]\nexcept otherwise speci\ufb01ed3. We employ an idea of span-based prediction, where we \ufb01rst sample a\nlength L \u2208[1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets\nwithin a context of (KL) tokens.\nWe use a variety of natural language understanding datasets to evaluate the performance of our\nmethod. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2\nFair Comparison with BERT\nModel\nSQuAD1.1 SQuAD2.0 RACE MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B\nBERT-Large\n(Best of 3)\n86.7/92.8\n82.8/85.5\n75.1\n87.3\n93.0\n91.4\n74.0\n94.0\n88.7\n63.7\n90.2\nXLNet-Large-\nwikibooks\n88.2/94.0\n85.1/87.8\n77.4\n88.4\n93.9\n91.8\n81.2\n94.4\n90.0\n65.2\n91.1\nTable 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in\nBERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word\nmasking, and BERT without next sentence prediction.\nHere, we \ufb01rst compare the performance of BERT and XLNet in a fair setting to decouple the effects\nof using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best\nperformance of three different variants of BERT and (2) XLNet trained with the same data and\nhyperparameters. As we can see, trained on the same data with an almost identical training recipe,\nXLNet outperforms BERT by a sizable margin on all the considered datasets.\n3.3\nComparison with RoBERTa: Scaling Up\nRACE\nAccuracy\nMiddle\nHigh\nModel\nNDCG@20\nERR@20\nGPT [28]\n59.0\n62.9\n57.4\nDRMM [13]\n24.3\n13.8\nBERT [25]\n72.0\n76.6\n70.1\nKNRM [8]\n26.9\n14.9\nBERT+DCMN\u2217[38]\n74.1\n79.5\n71.8\nConv [8]\n28.7\n18.1\nRoBERTa [21]\n83.2\n86.5\n81.8\nBERT\u2020\n30.53\n18.67\nXLNet\n85.4\n88.6\n84.0\nXLNet\n31.10\n20.28\nTable 2: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on\nClueWeb09-B, a document ranking task. \u2217indicates using ensembles. \u2020 indicates our implementations. \u201cMiddle\u201d\nand \u201cHigh\u201d in RACE are two subsets representing middle and high school dif\ufb01culty levels. All BERT, RoBERTa,\nand XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\nAfter the initial publication of our manuscript, a few other pretrained models were released such as\nRoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from\n1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we\nexclude ALBERT from the following results as it is hard to lead to scienti\ufb01c conclusions. To obtain\nrelatively fair comparison with RoBERTa, the experiment in this section is based on full data and\nreuses the hyper-parameters of RoBERTa, as described in section 3.1.\nThe results are presented in Tables 2 (reading comprehension & document ranking), 3 (question\nanswering), 4 (text classi\ufb01cation) and 5 (natural language understanding), where XLNet generally\noutperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and \ufb01netuning are in Appendix A.4.\n7\n", []], "3.2 Fair Comparison with BERT": ["observed that the model still under\ufb01ts the data at the end of training. Finally, we perform ablation\nstudy (section 3.4) based on the XLNet-Base-wikibooks.\nSince the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each\nof the forward and backward directions takes half of the batch size. For training XLNet-Large, we set\nthe partial prediction constant K as 6 (see Section 2.3). Our \ufb01netuning procedure follows BERT [10]\nexcept otherwise speci\ufb01ed3. We employ an idea of span-based prediction, where we \ufb01rst sample a\nlength L \u2208[1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets\nwithin a context of (KL) tokens.\nWe use a variety of natural language understanding datasets to evaluate the performance of our\nmethod. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2\nFair Comparison with BERT\nModel\nSQuAD1.1 SQuAD2.0 RACE MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B\nBERT-Large\n(Best of 3)\n86.7/92.8\n82.8/85.5\n75.1\n87.3\n93.0\n91.4\n74.0\n94.0\n88.7\n63.7\n90.2\nXLNet-Large-\nwikibooks\n88.2/94.0\n85.1/87.8\n77.4\n88.4\n93.9\n91.8\n81.2\n94.4\n90.0\n65.2\n91.1\nTable 1: Fair comparison with BERT. All models are trained using the same data and hyperparameters as in\nBERT. We use the best of 3 BERT variants for comparison; i.e., the original BERT, BERT with whole word\nmasking, and BERT without next sentence prediction.\nHere, we \ufb01rst compare the performance of BERT and XLNet in a fair setting to decouple the effects\nof using more data and the improvement from BERT to XLNet. In Table 1, we compare (1) best\nperformance of three different variants of BERT and (2) XLNet trained with the same data and\nhyperparameters. As we can see, trained on the same data with an almost identical training recipe,\nXLNet outperforms BERT by a sizable margin on all the considered datasets.\n3.3\nComparison with RoBERTa: Scaling Up\nRACE\nAccuracy\nMiddle\nHigh\nModel\nNDCG@20\nERR@20\nGPT [28]\n59.0\n62.9\n57.4\nDRMM [13]\n24.3\n13.8\nBERT [25]\n72.0\n76.6\n70.1\nKNRM [8]\n26.9\n14.9\nBERT+DCMN\u2217[38]\n74.1\n79.5\n71.8\nConv [8]\n28.7\n18.1\nRoBERTa [21]\n83.2\n86.5\n81.8\nBERT\u2020\n30.53\n18.67\nXLNet\n85.4\n88.6\n84.0\nXLNet\n31.10\n20.28\nTable 2: Comparison with state-of-the-art results on the test set of RACE, a reading comprehension task, and on\nClueWeb09-B, a document ranking task. \u2217indicates using ensembles. \u2020 indicates our implementations. \u201cMiddle\u201d\nand \u201cHigh\u201d in RACE are two subsets representing middle and high school dif\ufb01culty levels. All BERT, RoBERTa,\nand XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\nAfter the initial publication of our manuscript, a few other pretrained models were released such as\nRoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from\n1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we\nexclude ALBERT from the following results as it is hard to lead to scienti\ufb01c conclusions. To obtain\nrelatively fair comparison with RoBERTa, the experiment in this section is based on full data and\nreuses the hyper-parameters of RoBERTa, as described in section 3.1.\nThe results are presented in Tables 2 (reading comprehension & document ranking), 3 (question\nanswering), 4 (text classi\ufb01cation) and 5 (natural language understanding), where XLNet generally\noutperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and \ufb01netuning are in Appendix A.4.\n7\n", []], "3.1 Pretraining and Implementation": ["we follow the two-segment data format, XLNet-Large does not use the objective of next sentence\nprediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).\nRelative Segment Encodings Architecturally, different from BERT that adds an absolute segment\nembedding to the word embedding at each position, we extend the idea of relative encodings from\nTransformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if\ni and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212,\nwhere s+ and s\u2212are learnable model parameters for each attention head. In other words, we only\nconsider whether the two positions are within the same segment, as opposed to considering which\nspeci\ufb01c segments they are from. This is consistent with the core idea of relative encodings; i.e., only\nmodeling the relationships between positions. When i attends to j, the segment encoding sij is used\nto compute an attention weight aij = (qi + b)\u22a4sij, where qi is the query vector as in a standard\nattention operation and b is a learnable head-speci\ufb01c bias vector. Finally, the value aij is added to\nthe normal attention weight. There are two bene\ufb01ts of using relative segment encodings. First, the\ninductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of\n\ufb01netuning on tasks that have more than two input segments, which is not possible using absolute\nsegment encodings.\n2.6\nDiscussion\nComparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,\nonly predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all\ntokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT\nand XLNet, partial prediction plays a role of reducing optimization dif\ufb01culty by only predicting\ntokens with suf\ufb01cient context. However, the independence assumption discussed in Section 2.1\ndisables BERT to model dependency between targets.\nTo better understand the difference, let\u2019s consider a concrete example [New, York, is, a, city]. Suppose\nboth BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize\nlog p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,\nNew, York]. In this case, BERT and XLNet respectively reduce to the following objectives:\nJBERT = log p(New | is a city) + log p(York | is a city),\nJXLNet = log p(New | is a city) + log p(York | New, is a city).\nNotice that XLNet is able to capture the dependency between the pair (New, York), which is omitted\nby BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and\n(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and\ncontains \u201cdenser\u201d effective training signals.\nFor more formal analysis and further discussion, please refer to Appendix A.5.\n3\nExperiments\n3.1\nPretraining and Implementation\nFollowing BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining\ndata, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],\nClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics\nto aggressively \ufb01lter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,\nwhich results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we\nobtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,\nClueWeb, and Common Crawl respectively, which are 32.89B in total.\nOur largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which\nresults in a similar model size. During pretraining, we always use a full sequence length of 512.\nFirstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks\non BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the\noriginal BERT. Then, we scale up the training of XLNet-Large by using all the datasets described\nabove. Speci\ufb01cally, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay\noptimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\n6\n", []], "3 Experiments": ["we follow the two-segment data format, XLNet-Large does not use the objective of next sentence\nprediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).\nRelative Segment Encodings Architecturally, different from BERT that adds an absolute segment\nembedding to the word embedding at each position, we extend the idea of relative encodings from\nTransformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if\ni and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212,\nwhere s+ and s\u2212are learnable model parameters for each attention head. In other words, we only\nconsider whether the two positions are within the same segment, as opposed to considering which\nspeci\ufb01c segments they are from. This is consistent with the core idea of relative encodings; i.e., only\nmodeling the relationships between positions. When i attends to j, the segment encoding sij is used\nto compute an attention weight aij = (qi + b)\u22a4sij, where qi is the query vector as in a standard\nattention operation and b is a learnable head-speci\ufb01c bias vector. Finally, the value aij is added to\nthe normal attention weight. There are two bene\ufb01ts of using relative segment encodings. First, the\ninductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of\n\ufb01netuning on tasks that have more than two input segments, which is not possible using absolute\nsegment encodings.\n2.6\nDiscussion\nComparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,\nonly predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all\ntokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT\nand XLNet, partial prediction plays a role of reducing optimization dif\ufb01culty by only predicting\ntokens with suf\ufb01cient context. However, the independence assumption discussed in Section 2.1\ndisables BERT to model dependency between targets.\nTo better understand the difference, let\u2019s consider a concrete example [New, York, is, a, city]. Suppose\nboth BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize\nlog p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,\nNew, York]. In this case, BERT and XLNet respectively reduce to the following objectives:\nJBERT = log p(New | is a city) + log p(York | is a city),\nJXLNet = log p(New | is a city) + log p(York | New, is a city).\nNotice that XLNet is able to capture the dependency between the pair (New, York), which is omitted\nby BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and\n(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and\ncontains \u201cdenser\u201d effective training signals.\nFor more formal analysis and further discussion, please refer to Appendix A.5.\n3\nExperiments\n3.1\nPretraining and Implementation\nFollowing BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining\ndata, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],\nClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics\nto aggressively \ufb01lter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,\nwhich results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we\nobtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,\nClueWeb, and Common Crawl respectively, which are 32.89B in total.\nOur largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which\nresults in a similar model size. During pretraining, we always use a full sequence length of 512.\nFirstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks\non BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the\noriginal BERT. Then, we scale up the training of XLNet-Large by using all the datasets described\nabove. Speci\ufb01cally, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay\noptimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\n6\n", []], "2.6 Discussion": ["we follow the two-segment data format, XLNet-Large does not use the objective of next sentence\nprediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4).\nRelative Segment Encodings Architecturally, different from BERT that adds an absolute segment\nembedding to the word embedding at each position, we extend the idea of relative encodings from\nTransformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if\ni and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212,\nwhere s+ and s\u2212are learnable model parameters for each attention head. In other words, we only\nconsider whether the two positions are within the same segment, as opposed to considering which\nspeci\ufb01c segments they are from. This is consistent with the core idea of relative encodings; i.e., only\nmodeling the relationships between positions. When i attends to j, the segment encoding sij is used\nto compute an attention weight aij = (qi + b)\u22a4sij, where qi is the query vector as in a standard\nattention operation and b is a learnable head-speci\ufb01c bias vector. Finally, the value aij is added to\nthe normal attention weight. There are two bene\ufb01ts of using relative segment encodings. First, the\ninductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of\n\ufb01netuning on tasks that have more than two input segments, which is not possible using absolute\nsegment encodings.\n2.6\nDiscussion\nComparing Eq. (2) and (5), we observe that both BERT and XLNet perform partial prediction, i.e.,\nonly predicting a subset of tokens in the sequence. This is a necessary choice for BERT because if all\ntokens are masked, it is impossible to make any meaningful predictions. In addition, for both BERT\nand XLNet, partial prediction plays a role of reducing optimization dif\ufb01culty by only predicting\ntokens with suf\ufb01cient context. However, the independence assumption discussed in Section 2.1\ndisables BERT to model dependency between targets.\nTo better understand the difference, let\u2019s consider a concrete example [New, York, is, a, city]. Suppose\nboth BERT and XLNet select the two tokens [New, York] as the prediction targets and maximize\nlog p(New York | is a city). Also suppose that XLNet samples the factorization order [is, a, city,\nNew, York]. In this case, BERT and XLNet respectively reduce to the following objectives:\nJBERT = log p(New | is a city) + log p(York | is a city),\nJXLNet = log p(New | is a city) + log p(York | New, is a city).\nNotice that XLNet is able to capture the dependency between the pair (New, York), which is omitted\nby BERT. Although in this example, BERT learns some dependency pairs such as (New, city) and\n(York, city), it is obvious that XLNet always learns more dependency pairs given the same target and\ncontains \u201cdenser\u201d effective training signals.\nFor more formal analysis and further discussion, please refer to Appendix A.5.\n3\nExperiments\n3.1\nPretraining and Implementation\nFollowing BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining\ndata, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],\nClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics\nto aggressively \ufb01lter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,\nwhich results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we\nobtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,\nClueWeb, and Common Crawl respectively, which are 32.89B in total.\nOur largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which\nresults in a similar model size. During pretraining, we always use a full sequence length of 512.\nFirstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks\non BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the\noriginal BERT. Then, we scale up the training of XLNet-Large by using all the datasets described\nabove. Speci\ufb01cally, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay\noptimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\n6\n", []], "2.5 Modeling Multiple Segments": ["with a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):\ng(m)\nzt\n\u2190Attention(Q = g(m\u22121)\nzt\n, KV = h(m\u22121)\nz<t\n; \u03b8),\n(query stream: use zt but cannot see xzt)\nh(m)\nzt\n\u2190Attention(Q = h(m\u22121)\nzt\n, KV = h(m\u22121)\nz\u2264t\n; \u03b8),\n(content stream: use both zt and xzt).\nwhere Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the\ncontent representations is exactly the same as the standard self-attention, so during \ufb01netuning, we\ncan simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,\nwe can use the last-layer query representation g(M)\nzt\nto compute Eq. (4).\nPartial Prediction While the permutation language modeling objective (3) has several bene\ufb01ts, it is\na much more challenging optimization problem due to the permutation and causes slow convergence\nin preliminary experiments. To reduce the optimization dif\ufb01culty, we choose to only predict the last\ntokens in a factorization order. Formally, we split z into a non-target subsequence z\u2264c and a target\nsubsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the\ntarget subsequence conditioned on the non-target subsequence, i.e.,\nmax\n\u03b8\nEz\u223cZT\nh\nlog p\u03b8(xz>c | xz\u2264c)\ni\n= Ez\u223cZT\n\uf8ee\n\uf8f0\n|z|\nX\nt=c+1\nlog p\u03b8(xzt | xz<t)\n\uf8f9\n\uf8fb.\n(5)\nNote that z>c is chosen as the target because it possesses the longest context in the sequence given the\ncurrent factorization order z. A hyperparameter K is used such that about 1/K tokens are selected\nfor predictions; i.e., |z| /(|z| \u2212c) \u2248K. For unselected tokens, their query representations need not\nbe computed, which saves speed and memory.\n2.4\nIncorporating Ideas from Transformer-XL\nSince our objective function \ufb01ts in the AR framework, we incorporate the state-of-the-art AR\nlanguage model, Transformer-XL [9], into our pretraining framework, and name our method after it.\nWe integrate two important techniques in Transformer-XL, namely the relative positional encoding\nscheme and the segment recurrence mechanism. We apply relative positional encodings based on the\noriginal sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the\nrecurrence mechanism into the proposed permutation setting and enable the model to reuse hidden\nstates from previous segments. Without loss of generality, suppose we have two segments taken from\na long sequence s; i.e., \u02dcx = s1:T and x = sT +1:2T . Let \u02dcz and z be permutations of [1 \u00b7 \u00b7 \u00b7 T] and\n[T + 1 \u00b7 \u00b7 \u00b7 2T] respectively. Then, based on the permutation \u02dcz, we process the \ufb01rst segment, and then\ncache the obtained content representations \u02dch(m) for each layer m. Then, for the next segment x, the\nattention update with memory can be written as\nh(m)\nzt\n\u2190Attention(Q = h(m\u22121)\nzt\n, KV =\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\n; \u03b8)\nwhere [., .] denotes concatenation along the sequence dimension. Notice that positional encodings\nonly depend on the actual positions in the original sequence. Thus, the above attention update is\nindependent of \u02dcz once the representations \u02dch(m) are obtained. This allows caching and reusing the\nmemory without knowing the factorization order of the previous segment. In expectation, the model\nlearns to utilize the memory over all factorization orders of the last segment. The query stream can\nbe computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation\nlanguage modeling with two-stream attention (see Appendix A.7 for more detailed illustration).\n2.5\nModeling Multiple Segments\nMany downstream tasks have multiple input segments, e.g., a question and a context paragraph in\nquestion answering. We now discuss how we pretrain XLNet to model multiple segments in the\nautoregressive framework. During the pretraining phase, following BERT, we randomly sample two\nsegments (either from the same context or not) and treat the concatenation of two segments as one\nsequence to perform permutation language modeling. We only reuse the memory that belongs to\nthe same context. Speci\ufb01cally, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP],\nwhere \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments. Although\n5\n", []], "2.4 Incorporating Ideas from Transformer-XL": ["with a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):\ng(m)\nzt\n\u2190Attention(Q = g(m\u22121)\nzt\n, KV = h(m\u22121)\nz<t\n; \u03b8),\n(query stream: use zt but cannot see xzt)\nh(m)\nzt\n\u2190Attention(Q = h(m\u22121)\nzt\n, KV = h(m\u22121)\nz\u2264t\n; \u03b8),\n(content stream: use both zt and xzt).\nwhere Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the\ncontent representations is exactly the same as the standard self-attention, so during \ufb01netuning, we\ncan simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally,\nwe can use the last-layer query representation g(M)\nzt\nto compute Eq. (4).\nPartial Prediction While the permutation language modeling objective (3) has several bene\ufb01ts, it is\na much more challenging optimization problem due to the permutation and causes slow convergence\nin preliminary experiments. To reduce the optimization dif\ufb01culty, we choose to only predict the last\ntokens in a factorization order. Formally, we split z into a non-target subsequence z\u2264c and a target\nsubsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the\ntarget subsequence conditioned on the non-target subsequence, i.e.,\nmax\n\u03b8\nEz\u223cZT\nh\nlog p\u03b8(xz>c | xz\u2264c)\ni\n= Ez\u223cZT\n\uf8ee\n\uf8f0\n|z|\nX\nt=c+1\nlog p\u03b8(xzt | xz<t)\n\uf8f9\n\uf8fb.\n(5)\nNote that z>c is chosen as the target because it possesses the longest context in the sequence given the\ncurrent factorization order z. A hyperparameter K is used such that about 1/K tokens are selected\nfor predictions; i.e., |z| /(|z| \u2212c) \u2248K. For unselected tokens, their query representations need not\nbe computed, which saves speed and memory.\n2.4\nIncorporating Ideas from Transformer-XL\nSince our objective function \ufb01ts in the AR framework, we incorporate the state-of-the-art AR\nlanguage model, Transformer-XL [9], into our pretraining framework, and name our method after it.\nWe integrate two important techniques in Transformer-XL, namely the relative positional encoding\nscheme and the segment recurrence mechanism. We apply relative positional encodings based on the\noriginal sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the\nrecurrence mechanism into the proposed permutation setting and enable the model to reuse hidden\nstates from previous segments. Without loss of generality, suppose we have two segments taken from\na long sequence s; i.e., \u02dcx = s1:T and x = sT +1:2T . Let \u02dcz and z be permutations of [1 \u00b7 \u00b7 \u00b7 T] and\n[T + 1 \u00b7 \u00b7 \u00b7 2T] respectively. Then, based on the permutation \u02dcz, we process the \ufb01rst segment, and then\ncache the obtained content representations \u02dch(m) for each layer m. Then, for the next segment x, the\nattention update with memory can be written as\nh(m)\nzt\n\u2190Attention(Q = h(m\u22121)\nzt\n, KV =\nh\n\u02dch(m\u22121), h(m\u22121)\nz\u2264t\ni\n; \u03b8)\nwhere [., .] denotes concatenation along the sequence dimension. Notice that positional encodings\nonly depend on the actual positions in the original sequence. Thus, the above attention update is\nindependent of \u02dcz once the representations \u02dch(m) are obtained. This allows caching and reusing the\nmemory without knowing the factorization order of the previous segment. In expectation, the model\nlearns to utilize the memory over all factorization orders of the last segment. The query stream can\nbe computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation\nlanguage modeling with two-stream attention (see Appendix A.7 for more detailed illustration).\n2.5\nModeling Multiple Segments\nMany downstream tasks have multiple input segments, e.g., a question and a context paragraph in\nquestion answering. We now discuss how we pretrain XLNet to model multiple segments in the\nautoregressive framework. During the pretraining phase, following BERT, we randomly sample two\nsegments (either from the same context or not) and treat the concatenation of two segments as one\nsequence to perform permutation language modeling. We only reuse the memory that belongs to\nthe same context. Speci\ufb01cally, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP],\nwhere \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments. Although\n5\n", []], "2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations": ["2.3\nArchitecture: Two-Stream Self-Attention for Target-Aware Representations\nSample a factorization order:\n3 \u00e0 2 \u00e0 4 \u00e0 1\nAttention Masks\ne(x$)\nw\ne(x')\nw\ne(x()\nw\ne(x))\nw\nh$\n($)\ng$\n($)\nh'\n($)\ng'\n($)\nh(\n($)\ng(\n($)\nh)\n($)\ng)\n($)\nh$\n(')\ng$\n(')\nh'\n(')\ng'\n(')\nh(\n(')\ng(\n(')\nh)\n(')\ng)\n(')\nContent stream:\ncan see self\nQuery stream:\ncannot see self\nx$\nx'\nx(\nx)\nMasked Two-stream Attention\nMasked Two-stream Attention\n(c)\nh$\n(,)\ng$\n(,)\nh'\n(,)\ng'\n(,)\nh(\n(,)\ng(\n(,)\nh)\n(,)\ng)\n(,)\nh$\n($)\ng$\n($)\nAttention\nQ\nK, V\nh$\n($)\ng$\n($)\nAttention\nQ\nK, V\n(b)\n(a)\nh$\n(,)\ng$\n(,)\nh'\n(,)\ng'\n(,)\nh(\n(,)\ng(\n(,)\nh)\n(,)\ng)\n(,)\nFigure 1: (a): Content stream attention, which is the same as the standard self-attention. (b): Query\nstream attention, which does not have access information about the content xzt. (c): Overview of the\npermutation language modeling training with two-stream attention.\nWhile the permutation language modeling objective has desired properties, naive implementation with\nstandard Transformer parameterization may not work. To see the problem, assume we parameterize\nthe next-token distribution p\u03b8(Xzt | xz<t) using the standard Softmax formulation, i.e., p\u03b8(Xzt =\nx | xz<t) =\nexp(e(x)\u22a4h\u03b8(xz<t))\nP\nx\u2032 exp(e(x\u2032)\u22a4h\u03b8(xz<t)), where h\u03b8(xz<t) denotes the hidden representation of xz<t\nproduced by the shared Transformer network after proper masking. Now notice that the representation\nh\u03b8(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the\nsame distribution is predicted regardless of the target position, which is not able to learn useful\nrepresentations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to\nre-parameterize the next-token distribution to be target position aware:\np\u03b8(Xzt = x | xz<t) =\nexp\n\u0000e(x)\u22a4g\u03b8(xz<t, zt)\n\u0001\nP\nx\u2032 exp (e(x\u2032)\u22a4g\u03b8(xz<t, zt)),\n(4)\nwhere g\u03b8(xz<t, zt) denotes a new type of representations which additionally take the target position\nzt as input.\nTwo-Stream Self-Attention While the idea of target-aware representations removes the ambiguity\nin target prediction, how to formulate g\u03b8(xz<t, zt) remains a non-trivial problem. Among other\npossibilities, we propose to \u201cstand\u201d at the target position zt and rely on the position zt to gather\ninformation from the context xz<t through attention. For this parameterization to work, there are two\nrequirements that are contradictory in a standard Transformer architecture: (1) to predict the token\nxzt, g\u03b8(xz<t, zt) should only use the position zt and not the content xzt, otherwise the objective\nbecomes trivial; (2) to predict the other tokens xzj with j > t, g\u03b8(xz<t, zt) should also encode the\ncontent xzt to provide full contextual information. To resolve such a contradiction, we propose to use\ntwo sets of hidden representations instead of one:\n\u2022 The content representation h\u03b8(xz\u2264t), or abbreviated as hzt, which serves a similar role to the\nstandard hidden states in Transformer. This representation encodes both the context and xzt itself.\n\u2022 The query representation g\u03b8(xz<t, zt), or abbreviated as gzt, which only has access to the contex-\ntual information xz<t and the position zt, but not the content xzt, as discussed above.\nComputationally, the \ufb01rst layer query stream is initialized with a trainable vector, i.e. g(0)\ni\n= w,\nwhile the content stream is set to the corresponding word embedding, i.e. h(0)\ni\n= e(xi). For each\nself-attention layer m = 1, . . . , M, the two streams of representations are schematically2 updated\n2To avoid clutter, we omit the implementation details including multi-head attention, residual connection,\nlayer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in\nAppendix A.2 for reference.\n4\n", []], "2.2 Objective: Permutation Language Modeling": ["where h\u03b8(x1:t\u22121) is a context representation produced by neural models, such as RNNs or Transform-\ners, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding.\nSpeci\ufb01cally, for a text sequence x, BERT \ufb01rst constructs a corrupted version \u02c6x by randomly setting\na portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be \u00afx. The\ntraining objective is to reconstruct \u00afx from \u02c6x:\nmax\n\u03b8\nlog p\u03b8(\u00afx | \u02c6x) \u2248\nT\nX\nt=1\nmt log p\u03b8(xt | \u02c6x) =\nT\nX\nt=1\nmt log\nexp\n\u0000H\u03b8(\u02c6x)\u22a4\nt e(xt)\n\u0001\nP\nx\u2032 exp\n\u0000H\u03b8(\u02c6x)\u22a4\nt e(x\u2032)\n\u0001,\n(2)\nwhere mt = 1 indicates xt is masked, and H\u03b8 is a Transformer that maps a length-T text sequence x\ninto a sequence of hidden vectors H\u03b8(x) = [H\u03b8(x)1, H\u03b8(x)2, \u00b7 \u00b7 \u00b7 , H\u03b8(x)T ]. The pros and cons of\nthe two pretraining objectives are compared in the following aspects:\n\u2022 Independence Assumption: As emphasized by the \u2248sign in Eq. (2), BERT factorizes the joint\nconditional probability p(\u00afx | \u02c6x) based on an independence assumption that all masked tokens \u00afx\nare separately reconstructed. In comparison, the AR language modeling objective (1) factorizes\np\u03b8(x) using the product rule that holds universally without such an independence assumption.\n\u2022 Input noise: The input to BERT contains arti\ufb01cial symbols like [MASK] that never occur in\ndownstream tasks, which creates a pretrain-\ufb01netune discrepancy. Replacing [MASK] with original\ntokens as in [10] does not solve the problem because original tokens can be only used with a small\nprobability \u2014 otherwise Eq. (2) will be trivial to optimize. In comparison, AR language modeling\ndoes not rely on any input corruption and does not suffer from this issue.\n\u2022 Context dependency: The AR representation h\u03b8(x1:t\u22121) is only conditioned on the tokens up\nto position t (i.e. tokens to the left), while the BERT representation H\u03b8(x)t has access to the\ncontextual information on both sides. As a result, the BERT objective allows the model to be\npretrained to better capture bidirectional context.\n2.2\nObjective: Permutation Language Modeling\nAccording to the comparison above, AR language modeling and BERT possess their unique advan-\ntages over the other. A natural question to ask is whether there exists a pretraining objective that\nbrings the advantages of both while avoiding their weaknesses.\nBorrowing ideas from orderless NADE [32], we propose the permutation language modeling objective\nthat not only retains the bene\ufb01ts of AR models but also allows models to capture bidirectional\ncontexts. Speci\ufb01cally, for a sequence x of length T, there are T! different orders to perform a valid\nautoregressive factorization. Intuitively, if model parameters are shared across all factorization orders,\nin expectation, the model will learn to gather information from all positions on both sides.\nTo formalize the idea, let ZT be the set of all possible permutations of the length-T index sequence\n[1, 2, . . . , T]. We use zt and z<t to denote the t-th element and the \ufb01rst t\u22121 elements of a permutation\nz \u2208ZT . Then, our proposed permutation language modeling objective can be expressed as follows:\nmax\n\u03b8\nEz\u223cZT\n\" T\nX\nt=1\nlog p\u03b8(xzt | xz<t)\n#\n.\n(3)\nEssentially, for a text sequence x, we sample a factorization order z at a time and decompose the\nlikelihood p\u03b8(x) according to factorization order. Since the same model parameter \u03b8 is shared across\nall factorization orders during training, in expectation, xt has seen every possible element xi \u0338= xt in\nthe sequence, hence being able to capture the bidirectional context. Moreover, as this objective \ufb01ts\ninto the AR framework, it naturally avoids the independence assumption and the pretrain-\ufb01netune\ndiscrepancy discussed in Section 2.1.\nRemark on Permutation The proposed objective only permutes the factorization order, not the\nsequence order. In other words, we keep the original sequence order, use the positional encodings\ncorresponding to the original sequence, and rely on a proper attention mask in Transformers to\nachieve permutation of the factorization order. Note that this choice is necessary, since the model\nwill only encounter text sequences with the natural order during \ufb01netuning.\nTo provide an overall picture, we show an example of predicting the token x3 given the same input\nsequence x but under different factorization orders in the Appendix A.7 with Figure 4.\n3\n", []], "2.1 Background": ["bidirectional contexts for reconstruction. As an immediate bene\ufb01t, this closes the aforementioned\nbidirectional information gap in AR language modeling, leading to improved performance. However,\nthe arti\ufb01cial symbols like [MASK] used by BERT during pretraining are absent from real data at\n\ufb01netuning time, resulting in a pretrain-\ufb01netune discrepancy. Moreover, since the predicted tokens are\nmasked in the input, BERT is not able to model the joint probability using the product rule as in AR\nlanguage modeling. In other words, BERT assumes the predicted tokens are independent of each\nother given the unmasked tokens, which is oversimpli\ufb01ed as high-order, long-range dependency is\nprevalent in natural language [9].\nFaced with the pros and cons of existing language pretraining objectives, in this work, we propose\nXLNet, a generalized autoregressive method that leverages the best of both AR language modeling\nand AE while avoiding their limitations.\n\u2022 Firstly, instead of using a \ufb01xed forward or backward factorization order as in conventional AR mod-\nels, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations\nof the factorization order. Thanks to the permutation operation, the context for each position can\nconsist of tokens from both left and right. In expectation, each position learns to utilize contextual\ninformation from all positions, i.e., capturing bidirectional context.\n\u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,\nXLNet does not suffer from the pretrain-\ufb01netune discrepancy that BERT is subject to. Meanwhile,\nthe autoregressive objective also provides a natural way to use the product rule for factorizing the\njoint probability of the predicted tokens, eliminating the independence assumption made in BERT.\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\n\u2022 Inspired by the latest advancements in AR language modeling, XLNet integrates the segment\nrecurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which\nempirically improves the performance especially for tasks involving a longer text sequence.\n\u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does\nnot work because the factorization order is arbitrary and the target is ambiguous. As a solution, we\npropose to reparameterize the Transformer(-XL) network to remove the ambiguity.\nEmpirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a\nwide spectrum of problems including GLUE language understanding tasks, reading comprehension\ntasks like SQuAD and RACE, text classi\ufb01cation tasks such as Yelp and IMDB, and the ClueWeb09-B\ndocument ranking task.\nRelated Work The idea of permutation-based AR modeling has been explored in [32, 12], but there\nare several key differences. Firstly, previous models aim to improve density estimation by baking\nan \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language\nmodels to learn bidirectional contexts. Technically, to construct a valid target-aware prediction\ndistribution, XLNet incorporates the target position into the hidden state via two-stream attention\nwhile previous permutation-based AR models relied on implicit position awareness inherent to their\nMLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that\n\u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model\nallows for different factorization orders of the distribution.\nAnother related idea is to perform autoregressive denoising in the context of text generation [11],\nwhich only considers a \ufb01xed order though.\n2\nProposed Method\n2.1\nBackground\nIn this section, we \ufb01rst review and compare the conventional AR language modeling and BERT for\nlanguage pretraining. Given a text sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], AR language modeling performs\npretraining by maximizing the likelihood under the forward autoregressive factorization:\nmax\n\u03b8\nlog p\u03b8(x) =\nT\nX\nt=1\nlog p\u03b8(xt | x<t) =\nT\nX\nt=1\nlog\nexp\n\u0000h\u03b8(x1:t\u22121)\u22a4e(xt)\n\u0001\nP\nx\u2032 exp (h\u03b8(x1:t\u22121)\u22a4e(x\u2032)),\n(1)\n2\n", []], "2 Proposed Method": ["bidirectional contexts for reconstruction. As an immediate bene\ufb01t, this closes the aforementioned\nbidirectional information gap in AR language modeling, leading to improved performance. However,\nthe arti\ufb01cial symbols like [MASK] used by BERT during pretraining are absent from real data at\n\ufb01netuning time, resulting in a pretrain-\ufb01netune discrepancy. Moreover, since the predicted tokens are\nmasked in the input, BERT is not able to model the joint probability using the product rule as in AR\nlanguage modeling. In other words, BERT assumes the predicted tokens are independent of each\nother given the unmasked tokens, which is oversimpli\ufb01ed as high-order, long-range dependency is\nprevalent in natural language [9].\nFaced with the pros and cons of existing language pretraining objectives, in this work, we propose\nXLNet, a generalized autoregressive method that leverages the best of both AR language modeling\nand AE while avoiding their limitations.\n\u2022 Firstly, instead of using a \ufb01xed forward or backward factorization order as in conventional AR mod-\nels, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations\nof the factorization order. Thanks to the permutation operation, the context for each position can\nconsist of tokens from both left and right. In expectation, each position learns to utilize contextual\ninformation from all positions, i.e., capturing bidirectional context.\n\u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence,\nXLNet does not suffer from the pretrain-\ufb01netune discrepancy that BERT is subject to. Meanwhile,\nthe autoregressive objective also provides a natural way to use the product rule for factorizing the\njoint probability of the predicted tokens, eliminating the independence assumption made in BERT.\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\n\u2022 Inspired by the latest advancements in AR language modeling, XLNet integrates the segment\nrecurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which\nempirically improves the performance especially for tasks involving a longer text sequence.\n\u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does\nnot work because the factorization order is arbitrary and the target is ambiguous. As a solution, we\npropose to reparameterize the Transformer(-XL) network to remove the ambiguity.\nEmpirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a\nwide spectrum of problems including GLUE language understanding tasks, reading comprehension\ntasks like SQuAD and RACE, text classi\ufb01cation tasks such as Yelp and IMDB, and the ClueWeb09-B\ndocument ranking task.\nRelated Work The idea of permutation-based AR modeling has been explored in [32, 12], but there\nare several key differences. Firstly, previous models aim to improve density estimation by baking\nan \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language\nmodels to learn bidirectional contexts. Technically, to construct a valid target-aware prediction\ndistribution, XLNet incorporates the target position into the hidden state via two-stream attention\nwhile previous permutation-based AR models relied on implicit position awareness inherent to their\nMLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that\n\u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model\nallows for different factorization orders of the distribution.\nAnother related idea is to perform autoregressive denoising in the context of text generation [11],\nwhich only considers a \ufb01xed order though.\n2\nProposed Method\n2.1\nBackground\nIn this section, we \ufb01rst review and compare the conventional AR language modeling and BERT for\nlanguage pretraining. Given a text sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], AR language modeling performs\npretraining by maximizing the likelihood under the forward autoregressive factorization:\nmax\n\u03b8\nlog p\u03b8(x) =\nT\nX\nt=1\nlog p\u03b8(xt | x<t) =\nT\nX\nt=1\nlog\nexp\n\u0000h\u03b8(x1:t\u22121)\u22a4e(xt)\n\u0001\nP\nx\u2032 exp (h\u03b8(x1:t\u22121)\u22a4e(x\u2032)),\n(1)\n2\n", []], "1 Introduction": ["XLNet: Generalized Autoregressive Pretraining\nfor Language Understanding\nZhilin Yang\u22171, Zihang Dai\u221712, Yiming Yang1, Jaime Carbonell1,\nRuslan Salakhutdinov1, Quoc V. Le2\n1Carnegie Mellon University, 2Google AI Brain Team\n{zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu, qvl@google.com\nAbstract\nWith the capability of modeling bidirectional contexts, denoising autoencoding\nbased pretraining like BERT achieves better performance than pretraining ap-\nproaches based on autoregressive language modeling. However, relying on corrupt-\ning the input with masks, BERT neglects dependency between the masked positions\nand suffers from a pretrain-\ufb01netune discrepancy. In light of these pros and cons, we\npropose XLNet, a generalized autoregressive pretraining method that (1) enables\nlearning bidirectional contexts by maximizing the expected likelihood over all\npermutations of the factorization order and (2) overcomes the limitations of BERT\nthanks to its autoregressive formulation. Furthermore, XLNet integrates ideas\nfrom Transformer-XL, the state-of-the-art autoregressive model, into pretraining.\nEmpirically, under comparable experiment settings, XLNet outperforms BERT on\n20 tasks, often by a large margin, including question answering, natural language\ninference, sentiment analysis, and document ranking.1.\n1\nIntroduction\nUnsupervised representation learning has been highly successful in the domain of natural language\nprocessing [7, 22, 27, 28, 10]. Typically, these methods \ufb01rst pretrain neural networks on large-scale\nunlabeled text corpora, and then \ufb01netune the models or representations on downstream tasks. Under\nthis shared high-level idea, different unsupervised pretraining objectives have been explored in\nliterature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been\nthe two most successful pretraining objectives.\nAR language modeling seeks to estimate the probability distribution of a text corpus with an au-\ntoregressive model [7, 27, 28]. Speci\ufb01cally, given a text sequence x = (x1, \u00b7 \u00b7 \u00b7 , xT ), AR language\nmodeling factorizes the likelihood into a forward product p(x) = QT\nt=1 p(xt | x<t) or a backward\none p(x) = Q1\nt=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each\nconditional distribution. Since an AR language model is only trained to encode a uni-directional con-\ntext (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the\ncontrary, downstream language understanding tasks often require bidirectional context information.\nThis results in a gap between AR language modeling and effective pretraining.\nIn comparison, AE based pretraining does not perform explicit density estimation but instead aims to\nreconstruct the original data from corrupted input. A notable example is BERT [10], which has been\nthe state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens\nare replaced by a special symbol [MASK], and the model is trained to recover the original tokens from\nthe corrupted version. Since density estimation is not part of the objective, BERT is allowed to utilize\n\u2217Equal contribution. Order determined by swapping the one in [9].\n1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\narXiv:1906.08237v2  [cs.CL]  2 Jan 2020\n", []]}