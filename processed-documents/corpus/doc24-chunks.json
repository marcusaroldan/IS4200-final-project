{"A.11 Further improve the model efficiency": ["Published as a conference paper at ICLR 2021\n(a)\n(b)\n(c)\nFigure 5: Comparison on attention patterns of last layer between DeBERTa and its variants (i.e.\nDeBERTa without EMD, C2P and P2C respectively).\nA.10\nACCOUNT FOR THE VARIANCE IN FINE-TUNING\nAccounting for the variance of different runs of \ufb01ne-tuning, in our experiments, we always follow\n(Liu et al., 2019c) to report the results on downstream tasks by averaging over \ufb01ve runs with different\nrandom initialization seeds, and perform signi\ufb01cance test when comparing results. As the examples\nshown in Table 14, DeBERTabase signi\ufb01cantly outperforms RoBERTabase (p-value < 0.05).\nModel\nMNLI-matched (Min/Max/Avg)\nSQuAD v1.1 (Min/Max/Avg)\np-value\nRoBERTabase\n84.7/85.0/84.9\n90.8/91.3/91.1\n0.02\nDeBERTabase\n86.1/86.5/86.3\n91.8/92.2/92.1\n0.01\nTable 14: Comparison of DeBERTa and RoBERTa on MNLI-matched and SQuAD v1.1.\nA.11\nFURTHER IMPROVE THE MODEL EFFICIENCY\nIn addition to scaling up transformer models with billions or trillions of parameters (Raffel et al.,\n2020; Brown et al., 2020; Fedus et al., 2021), it is important to improve model\u2019s parameter ef\ufb01ciency\n(Kanakarajan et al., 2021). In A.3.1 we have shown that DeBERTa is more parameter ef\ufb01cient than\nBERT and RoBERTa. In this section, we show further improvements in terms of parameter ef\ufb01ciency.\n22\n", [739, 740, 738]], "A.10 Account for the Variance in Fine-Tuning": ["Published as a conference paper at ICLR 2021\n(a)\n(b)\n(c)\nFigure 5: Comparison on attention patterns of last layer between DeBERTa and its variants (i.e.\nDeBERTa without EMD, C2P and P2C respectively).\nA.10\nACCOUNT FOR THE VARIANCE IN FINE-TUNING\nAccounting for the variance of different runs of \ufb01ne-tuning, in our experiments, we always follow\n(Liu et al., 2019c) to report the results on downstream tasks by averaging over \ufb01ve runs with different\nrandom initialization seeds, and perform signi\ufb01cance test when comparing results. As the examples\nshown in Table 14, DeBERTabase signi\ufb01cantly outperforms RoBERTabase (p-value < 0.05).\nModel\nMNLI-matched (Min/Max/Avg)\nSQuAD v1.1 (Min/Max/Avg)\np-value\nRoBERTabase\n84.7/85.0/84.9\n90.8/91.3/91.1\n0.02\nDeBERTabase\n86.1/86.5/86.3\n91.8/92.2/92.1\n0.01\nTable 14: Comparison of DeBERTa and RoBERTa on MNLI-matched and SQuAD v1.1.\nA.11\nFURTHER IMPROVE THE MODEL EFFICIENCY\nIn addition to scaling up transformer models with billions or trillions of parameters (Raffel et al.,\n2020; Brown et al., 2020; Fedus et al., 2021), it is important to improve model\u2019s parameter ef\ufb01ciency\n(Kanakarajan et al., 2021). In A.3.1 we have shown that DeBERTa is more parameter ef\ufb01cient than\nBERT and RoBERTa. In this section, we show further improvements in terms of parameter ef\ufb01ciency.\n22\n", [739, 740, 738]], "A.9 Attention Patterns": ["Published as a conference paper at ICLR 2021\nModel\nCoLA QQP\nMNLI-m/mm\nSST-2\nSTS-B\nQNLI\nRTE\nMRPC\nAvg.\nMcc\nAcc\nAcc\nAcc\nCorr\nAcc\nAcc\nAcc\nDeBERTalarge\n70.5\n92.3\n91.1/91.1\n96.8\n92.8\n95.3\n88.3\n91.9\n90.00\nDeBERTa900M\n71.1\n92.3\n91.7/91.6\n97.5\n92.0\n95.8\n93.5\n93.1\n90.86\nDeBERTa1.5B\n72.0\n92.7\n91.7/91.9\n97.2\n92.9\n96.0\n93.9\n92.0\n91.17\nDeBERTa1.5B+SiFT\n73.5\n93.0\n92.0/92.1\n97.5\n93.2\n96.5\n96.5\n93.2\n91.93\nTable 12: Comparison results of DeBERTa models with different sizes on the GLUE development set.\nModel\nParameters\nMNLI-m/mm\nSQuAD v1.1\nSQuAD v2.0\nAcc\nF1/EM\nF1/EM\nRoBERTa-ReImpbase\n120M\n84.9/85.1\n91.1/84.8\n79.5/76.0\nDeBERTabase\n134M\n86.3/86.2\n92.1/86.1\n82.5/79.3\n+ ShareProjection\n120M\n86.3/86.3\n92.2/86.2\n82.3/79.5\n+ Conv\n122M\n86.3/86.5\n92.5/86.4\n82.5/79.7\n+ 128k Vocab\n190M\n86.7/86.9\n93.1/86.8\n83.0/80.1\nTable 13: Ablation study of the additional modi\ufb01cations in DeBERTa1.5B and DeBERTa900M models.\nNote that we progressively add each component on the top of DeBERTabase.\nA.7\nMODEL COMPLEXITY\nWith the disentangled attention mechanism, we introduce three additional sets of parameters\nWq,r, Wk,r P Rd\u02c6d and P P R2k\u02c6d. The total increase in model parameters is 2L \u02c6 d2 ` 2k \u02c6 d.\nFor the large model pd \u201c 1024, L \u201c 24, k \u201c 512q, this amounts to about 49M additional parameters,\nan increase of 13%. For the base modelpd \u201c 768, L \u201c 12, k \u201c 512q, this amounts to 14M additional\nparameters, an increase of 12%. However, by sharing the projection matrix between content and\nposition embedding, i.e. Wq,r \u201c Wq,c, Wk,r \u201c Wk,c, the number of parameters of DeBERTa is\nthe same as RoBERTa. Our experiment on base model shows that the results are almost the same, as\nin Table 13.\nThe additional computational complexity is OpNkdq due to the calculation of the additional position-\nto-content and content-to-position attention scores. Compared with BERT or RoBERTa, this increases\nthe computational cost by 30%. Compared with XLNet which also uses relative position embedding,\nthe increase of computational cost is about 15%. A further optimization by fusing the attention\ncomputation kernel can signi\ufb01cantly reduce this additional cost. For EMD, since the decoder in\npre-training only reconstructs the masked tokens, it does not introduce additional computational cost\nfor unmasked tokens. In the situation where 15% tokens are masked and we use only two decoder\nlayers, the additional cost is 0.15 \u02c6 2{L which results in an additional computational cost of only 3%\nfor base model(L \u201c 12) and 2% for large model(L \u201c 24) in EMD.\nA.8\nADDITIONAL DETAILS OF ENHANCED MASK DECODER\nThe structure of EMD is shown in Figure 2b. There are two inputs for EMD, (i.e., I, H). H denotes\nthe hidden states from the previous Transformer layer, and I can be any necessary information for\ndecoding, e.g., H, absolute position embedding or output from previous EMD layer. n denotes n\nstacked layers of EMD where the output of each EMD layer will be the input I for next EMD layer\nand the output of last EMD layer will be fed to the language model head directly. The n layers can\nshare the same weight. In our experiment we share the same weight for n \u201c 2 layers to reduce the\nnumber of parameters and use absolute position embedding as I of the \ufb01rst EMD layer. When I \u201c H\nand n \u201c 1, EMD is the same as the BERT decoder layer. However, EMD is more general and \ufb02exible\nas it can take various types of input information for decoding.\nA.9\nATTENTION PATTERNS\nTo visualize how DeBERTa operates differently from RoBERTa, we present in Figure 3 the attention\npatterns (taken in the last self-attention layers) of RoBERTa, DeBERTa and three DeBERTa variants.\n19\n", [680, 691, 692, 693, 739, 740, 738]], "A.8 Additional Details of Enhanced Mask Decoder": ["Published as a conference paper at ICLR 2021\nModel\nCoLA QQP\nMNLI-m/mm\nSST-2\nSTS-B\nQNLI\nRTE\nMRPC\nAvg.\nMcc\nAcc\nAcc\nAcc\nCorr\nAcc\nAcc\nAcc\nDeBERTalarge\n70.5\n92.3\n91.1/91.1\n96.8\n92.8\n95.3\n88.3\n91.9\n90.00\nDeBERTa900M\n71.1\n92.3\n91.7/91.6\n97.5\n92.0\n95.8\n93.5\n93.1\n90.86\nDeBERTa1.5B\n72.0\n92.7\n91.7/91.9\n97.2\n92.9\n96.0\n93.9\n92.0\n91.17\nDeBERTa1.5B+SiFT\n73.5\n93.0\n92.0/92.1\n97.5\n93.2\n96.5\n96.5\n93.2\n91.93\nTable 12: Comparison results of DeBERTa models with different sizes on the GLUE development set.\nModel\nParameters\nMNLI-m/mm\nSQuAD v1.1\nSQuAD v2.0\nAcc\nF1/EM\nF1/EM\nRoBERTa-ReImpbase\n120M\n84.9/85.1\n91.1/84.8\n79.5/76.0\nDeBERTabase\n134M\n86.3/86.2\n92.1/86.1\n82.5/79.3\n+ ShareProjection\n120M\n86.3/86.3\n92.2/86.2\n82.3/79.5\n+ Conv\n122M\n86.3/86.5\n92.5/86.4\n82.5/79.7\n+ 128k Vocab\n190M\n86.7/86.9\n93.1/86.8\n83.0/80.1\nTable 13: Ablation study of the additional modi\ufb01cations in DeBERTa1.5B and DeBERTa900M models.\nNote that we progressively add each component on the top of DeBERTabase.\nA.7\nMODEL COMPLEXITY\nWith the disentangled attention mechanism, we introduce three additional sets of parameters\nWq,r, Wk,r P Rd\u02c6d and P P R2k\u02c6d. The total increase in model parameters is 2L \u02c6 d2 ` 2k \u02c6 d.\nFor the large model pd \u201c 1024, L \u201c 24, k \u201c 512q, this amounts to about 49M additional parameters,\nan increase of 13%. For the base modelpd \u201c 768, L \u201c 12, k \u201c 512q, this amounts to 14M additional\nparameters, an increase of 12%. However, by sharing the projection matrix between content and\nposition embedding, i.e. Wq,r \u201c Wq,c, Wk,r \u201c Wk,c, the number of parameters of DeBERTa is\nthe same as RoBERTa. Our experiment on base model shows that the results are almost the same, as\nin Table 13.\nThe additional computational complexity is OpNkdq due to the calculation of the additional position-\nto-content and content-to-position attention scores. Compared with BERT or RoBERTa, this increases\nthe computational cost by 30%. Compared with XLNet which also uses relative position embedding,\nthe increase of computational cost is about 15%. A further optimization by fusing the attention\ncomputation kernel can signi\ufb01cantly reduce this additional cost. For EMD, since the decoder in\npre-training only reconstructs the masked tokens, it does not introduce additional computational cost\nfor unmasked tokens. In the situation where 15% tokens are masked and we use only two decoder\nlayers, the additional cost is 0.15 \u02c6 2{L which results in an additional computational cost of only 3%\nfor base model(L \u201c 12) and 2% for large model(L \u201c 24) in EMD.\nA.8\nADDITIONAL DETAILS OF ENHANCED MASK DECODER\nThe structure of EMD is shown in Figure 2b. There are two inputs for EMD, (i.e., I, H). H denotes\nthe hidden states from the previous Transformer layer, and I can be any necessary information for\ndecoding, e.g., H, absolute position embedding or output from previous EMD layer. n denotes n\nstacked layers of EMD where the output of each EMD layer will be the input I for next EMD layer\nand the output of last EMD layer will be fed to the language model head directly. The n layers can\nshare the same weight. In our experiment we share the same weight for n \u201c 2 layers to reduce the\nnumber of parameters and use absolute position embedding as I of the \ufb01rst EMD layer. When I \u201c H\nand n \u201c 1, EMD is the same as the BERT decoder layer. However, EMD is more general and \ufb02exible\nas it can take various types of input information for decoding.\nA.9\nATTENTION PATTERNS\nTo visualize how DeBERTa operates differently from RoBERTa, we present in Figure 3 the attention\npatterns (taken in the last self-attention layers) of RoBERTa, DeBERTa and three DeBERTa variants.\n19\n", []], "A.7 Model complexity": ["Published as a conference paper at ICLR 2021\nModel\nCoLA QQP\nMNLI-m/mm\nSST-2\nSTS-B\nQNLI\nRTE\nMRPC\nAvg.\nMcc\nAcc\nAcc\nAcc\nCorr\nAcc\nAcc\nAcc\nDeBERTalarge\n70.5\n92.3\n91.1/91.1\n96.8\n92.8\n95.3\n88.3\n91.9\n90.00\nDeBERTa900M\n71.1\n92.3\n91.7/91.6\n97.5\n92.0\n95.8\n93.5\n93.1\n90.86\nDeBERTa1.5B\n72.0\n92.7\n91.7/91.9\n97.2\n92.9\n96.0\n93.9\n92.0\n91.17\nDeBERTa1.5B+SiFT\n73.5\n93.0\n92.0/92.1\n97.5\n93.2\n96.5\n96.5\n93.2\n91.93\nTable 12: Comparison results of DeBERTa models with different sizes on the GLUE development set.\nModel\nParameters\nMNLI-m/mm\nSQuAD v1.1\nSQuAD v2.0\nAcc\nF1/EM\nF1/EM\nRoBERTa-ReImpbase\n120M\n84.9/85.1\n91.1/84.8\n79.5/76.0\nDeBERTabase\n134M\n86.3/86.2\n92.1/86.1\n82.5/79.3\n+ ShareProjection\n120M\n86.3/86.3\n92.2/86.2\n82.3/79.5\n+ Conv\n122M\n86.3/86.5\n92.5/86.4\n82.5/79.7\n+ 128k Vocab\n190M\n86.7/86.9\n93.1/86.8\n83.0/80.1\nTable 13: Ablation study of the additional modi\ufb01cations in DeBERTa1.5B and DeBERTa900M models.\nNote that we progressively add each component on the top of DeBERTabase.\nA.7\nMODEL COMPLEXITY\nWith the disentangled attention mechanism, we introduce three additional sets of parameters\nWq,r, Wk,r P Rd\u02c6d and P P R2k\u02c6d. The total increase in model parameters is 2L \u02c6 d2 ` 2k \u02c6 d.\nFor the large model pd \u201c 1024, L \u201c 24, k \u201c 512q, this amounts to about 49M additional parameters,\nan increase of 13%. For the base modelpd \u201c 768, L \u201c 12, k \u201c 512q, this amounts to 14M additional\nparameters, an increase of 12%. However, by sharing the projection matrix between content and\nposition embedding, i.e. Wq,r \u201c Wq,c, Wk,r \u201c Wk,c, the number of parameters of DeBERTa is\nthe same as RoBERTa. Our experiment on base model shows that the results are almost the same, as\nin Table 13.\nThe additional computational complexity is OpNkdq due to the calculation of the additional position-\nto-content and content-to-position attention scores. Compared with BERT or RoBERTa, this increases\nthe computational cost by 30%. Compared with XLNet which also uses relative position embedding,\nthe increase of computational cost is about 15%. A further optimization by fusing the attention\ncomputation kernel can signi\ufb01cantly reduce this additional cost. For EMD, since the decoder in\npre-training only reconstructs the masked tokens, it does not introduce additional computational cost\nfor unmasked tokens. In the situation where 15% tokens are masked and we use only two decoder\nlayers, the additional cost is 0.15 \u02c6 2{L which results in an additional computational cost of only 3%\nfor base model(L \u201c 12) and 2% for large model(L \u201c 24) in EMD.\nA.8\nADDITIONAL DETAILS OF ENHANCED MASK DECODER\nThe structure of EMD is shown in Figure 2b. There are two inputs for EMD, (i.e., I, H). H denotes\nthe hidden states from the previous Transformer layer, and I can be any necessary information for\ndecoding, e.g., H, absolute position embedding or output from previous EMD layer. n denotes n\nstacked layers of EMD where the output of each EMD layer will be the input I for next EMD layer\nand the output of last EMD layer will be fed to the language model head directly. The n layers can\nshare the same weight. In our experiment we share the same weight for n \u201c 2 layers to reduce the\nnumber of parameters and use absolute position embedding as I of the \ufb01rst EMD layer. When I \u201c H\nand n \u201c 1, EMD is the same as the BERT decoder layer. However, EMD is more general and \ufb02exible\nas it can take various types of input information for decoding.\nA.9\nATTENTION PATTERNS\nTo visualize how DeBERTa operates differently from RoBERTa, we present in Figure 3 the attention\npatterns (taken in the last self-attention layers) of RoBERTa, DeBERTa and three DeBERTa variants.\n19\n", []], "A.6 Performance improvements of different model scales": ["Published as a conference paper at ICLR 2021\nA.4\nMAIN RESULTS ON GENERATION TASKS\nIn addition to NLU tasks, DeBERTa can also be extended to handle NLG tasks. To allow DeBERTa\noperating like an auto-regressive model for text generation, we use a triangular matrix for self-\nattention and set the upper triangular part of the self-attention mask to \u00b48, following Dong et al.\n(2019).\nWe evaluate DeBERTa on the task of auto-regressive language model (ARLM) using Wikitext-\n103 (Merity et al., 2016). To do so, we train a new version of DeBERTa, denoted as DeBERTa-MT.\nIt is jointly pre-trained using the MLM and ARLM tasks as in UniLM (Dong et al., 2019). The\npre-training hyper-parameters follows that of DeBERTabase except that we use fewer training steps\n(200k). For comparison, we use RoBERTa as baseline, and include GPT-2 and Transformer-XL as\nadditional references. DeBERTa-AP is a variant of DeBERTa where absolute position embeddings\nare incorporated in the input layer as RoBERTa. For a fair comparison, all these models are base\nmodels pre-trained in a similar setting.\nModel\nRoBERTa DeBERTa-AP\nDeBERTa DeBERTa-MT GPT-2 Transformer-XL\nDev PPL\n21.6\n20.7\n20.5\n19.5\n-\n23.1\nTest PPL\n21.6\n20.0\n19.9\n19.5\n37.50\n24\nTable 10: Language model results in perplexity (lower is better) on Wikitext-103 .\nTable 10 summarizes the results on Wikitext-103. We see that DeBERTabase obtains lower perplexities\non both dev and test data, and joint training using MLM and ARLM reduces perplexity further.\nThat DeBERTa-AP is inferior to DeBERTa indicates that it is more effective to incorporate absolute\nposition embeddings of words in the decoding layer as the EMD in DeBERTa than in the input layer\nas RoBERTa.\nA.5\nHANDLING LONG SEQUENCE INPUT\nWith relative position bias, we choose to truncate the maximum relative distance to k as in equation 3.\nThus in each layer, each token can attend directly to at most 2pk \u00b4 1q tokens and itself. By stacking\nTransformer layers, each token in the l\u00b4th layer can attend to at most p2k \u00b4 1ql tokens implicitly.\nTaking DeBERTalarge as an example, where k \u201c 512, L \u201c 24, in theory, the maximum sequence\nlength that can be handled is 24,528. This is a byproduct bene\ufb01t of our design choice and we \ufb01nd it\nbene\ufb01cial for the RACE task. A comparison of long sequence effect on the RACE task is shown in\nTable 11.\nSequence length Middle High Accuracy\n512\n88.8\n85.0\n86.3\n768\n88.7\n86.3\n86.8\nTable 11: The effect of handling long sequence input for RACE task with DeBERTa\nLong sequence handling is an active research area. There have been a lot of studies where the\nTransformer architecture is extended for long sequence handling(Beltagy et al., 2020; Kitaev et al.,\n2019; Child et al., 2019; Dai et al., 2019). One of our future research directions is to extend DeBERTa\nto deal with extremely long sequences.\nA.6\nPERFORMANCE IMPROVEMENTS OF DIFFERENT MODEL SCALES\nIn this subsection, we study the effect of different model sizes applied to large models on GLUE.\nTable 12 summarizes the results, showing that larger models can obtain a better result and SiFT also\nimproves the model performance consistently.\n18\n", []], "A.5 Handling long sequence input": ["Published as a conference paper at ICLR 2021\nA.4\nMAIN RESULTS ON GENERATION TASKS\nIn addition to NLU tasks, DeBERTa can also be extended to handle NLG tasks. To allow DeBERTa\noperating like an auto-regressive model for text generation, we use a triangular matrix for self-\nattention and set the upper triangular part of the self-attention mask to \u00b48, following Dong et al.\n(2019).\nWe evaluate DeBERTa on the task of auto-regressive language model (ARLM) using Wikitext-\n103 (Merity et al., 2016). To do so, we train a new version of DeBERTa, denoted as DeBERTa-MT.\nIt is jointly pre-trained using the MLM and ARLM tasks as in UniLM (Dong et al., 2019). The\npre-training hyper-parameters follows that of DeBERTabase except that we use fewer training steps\n(200k). For comparison, we use RoBERTa as baseline, and include GPT-2 and Transformer-XL as\nadditional references. DeBERTa-AP is a variant of DeBERTa where absolute position embeddings\nare incorporated in the input layer as RoBERTa. For a fair comparison, all these models are base\nmodels pre-trained in a similar setting.\nModel\nRoBERTa DeBERTa-AP\nDeBERTa DeBERTa-MT GPT-2 Transformer-XL\nDev PPL\n21.6\n20.7\n20.5\n19.5\n-\n23.1\nTest PPL\n21.6\n20.0\n19.9\n19.5\n37.50\n24\nTable 10: Language model results in perplexity (lower is better) on Wikitext-103 .\nTable 10 summarizes the results on Wikitext-103. We see that DeBERTabase obtains lower perplexities\non both dev and test data, and joint training using MLM and ARLM reduces perplexity further.\nThat DeBERTa-AP is inferior to DeBERTa indicates that it is more effective to incorporate absolute\nposition embeddings of words in the decoding layer as the EMD in DeBERTa than in the input layer\nas RoBERTa.\nA.5\nHANDLING LONG SEQUENCE INPUT\nWith relative position bias, we choose to truncate the maximum relative distance to k as in equation 3.\nThus in each layer, each token can attend directly to at most 2pk \u00b4 1q tokens and itself. By stacking\nTransformer layers, each token in the l\u00b4th layer can attend to at most p2k \u00b4 1ql tokens implicitly.\nTaking DeBERTalarge as an example, where k \u201c 512, L \u201c 24, in theory, the maximum sequence\nlength that can be handled is 24,528. This is a byproduct bene\ufb01t of our design choice and we \ufb01nd it\nbene\ufb01cial for the RACE task. A comparison of long sequence effect on the RACE task is shown in\nTable 11.\nSequence length Middle High Accuracy\n512\n88.8\n85.0\n86.3\n768\n88.7\n86.3\n86.8\nTable 11: The effect of handling long sequence input for RACE task with DeBERTa\nLong sequence handling is an active research area. There have been a lot of studies where the\nTransformer architecture is extended for long sequence handling(Beltagy et al., 2020; Kitaev et al.,\n2019; Child et al., 2019; Dai et al., 2019). One of our future research directions is to extend DeBERTa\nto deal with extremely long sequences.\nA.6\nPERFORMANCE IMPROVEMENTS OF DIFFERENT MODEL SCALES\nIn this subsection, we study the effect of different model sizes applied to large models on GLUE.\nTable 12 summarizes the results, showing that larger models can obtain a better result and SiFT also\nimproves the model performance consistently.\n18\n", []], "A.4 Main Results on Generation Tasks": ["Published as a conference paper at ICLR 2021\nHyper-parameter\nDeBERTa1.5B DeBERTalarge DeBERTabase DeBERTabase\u00b4ablation\nNumber of Layers\n48\n24\n12\n12\nHidden size\n1536\n1024\n768\n768\nFNN inner hidden size\n6144\n4096\n3072\n3072\nAttention Heads\n24\n16\n12\n12\nAttention Head size\n64\n64\n64\n64\nDropout\n0.1\n0.1\n0.1\n0.1\nWarmup Steps\n10k\n10k\n10k\n10k\nLearning Rates\n1.5e-4\n2e-4\n2e-4\n1e-4\nBatch Size\n2k\n2k\n2k\n256\nWeight Decay\n0.01\n0.01\n0.01\n0.01\nMax Steps\n1M\n1M\n1M\n1M\nLearning Rate Decay\nLinear\nLinear\nLinear\nLinear\nAdam \u03f5\n1e-6\n1e-6\n1e-6\n1e-6\nAdam \u03b21\n0.9\n0.9\n0.9\n0.9\nAdam \u03b22\n0.999\n0.999\n0.999\n0.999\nGradient Clipping\n1.0\n1.0\n1.0\n1.0\nNumber of DGX-2 nodes\n16\n6\n4\n1\nTraining Time\n30 days\n20 days\n10 days\n7 days\nTable 8: Hyper-parameters for pre-training DeBERTa.\nHyper-parameter\nDeBERTa1.5B\nDeBERTalarge\nDeBERTabase\nDropout of task layer\n{0,0.15,0.3}\n{0,0.1,0.15}\n{0,0.1,0.15}\nWarmup Steps\n{50,100,500,1000}\n{50,100,500,1000}\n{50,100,500,1000}\nLearning Rates\n{1e-6, 3e-6, 5e-6} {5e-6, 8e-6, 9e-6, 1e-5} {1.5e-5,2e-5, 3e-5, 4e-5}\nBatch Size\n{16,32,64}\n{16,32,48,64}\n{16,32,48,64}\nWeight Decay\n0.01\n0.01\nMaximun Training Epochs\n10\n10\n10\nLearning Rate Decay\nLinear\nLinear\nLinear\nAdam \u03f5\n1e-6\n1e-6\n1e-6\nAdam \u03b21\n0.9\n0.9\n0.9\nAdam \u03b22\n0.999\n0.999\n0.999\nGradient Clipping\n1.0\n1.0\n1.0\nTable 9: Hyper-parameters for \ufb01ne-tuning DeBERTa on down-streaming tasks.\n(a) Results on MNLI development\n(b) Results on SQuAD v2.0 development\nFigure 1: Pre-training performance curve between DeBERTa and its counterparts on the MNLI and\nSQuAD v2.0 development set.\n17\n", [621, 623]], "A.3.1 Pre-training Efficiency": ["Published as a conference paper at ICLR 2021\n\u201a SWAG is a large-scale adversarial dataset for the task of grounded commonsense inference, which\nuni\ufb01es natural language inference and physically grounded reasoning (Zellers et al., 2018). SWAG\nconsists of 113k multiple choice questions about grounded situations.\n\u201a CoNLL 2003 is an English dataset consisting of text from a wide variety of sources. It has 4 types\nof named entity.\nA.2\nPRE-TRAINING DATASET\nFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump8; 12GB), BookCorpus (Zhu\net al., 2015) 9 (6GB), OPENWEBTEXT (public Reddit content (Gokaslan & Cohen, 2019); 38GB)\nand STORIES10 (a subset of CommonCrawl (Trinh & Le, 2018); 31GB). The total data size after\ndata deduplication(Shoeybi et al., 2019) is about 78GB. For pre-training, we also sample 5% training\ndata as the validation set to monitor the training process. Table 7 compares datasets used in different\npre-trained models.\nModel\nWiki+Book OpenWebText Stories CC-News Giga5 ClueWeb Common Crawl\n16GB\n38GB\n31GB\n76GB\n16GB\n19GB\n110GB\nBERT\n\u2713\nXLNet\n\u2713\n\u2713\n\u2713\n\u2713\nRoBERTa\n\u2713\n\u2713\n\u2713\n\u2713\nDeBERTa\n\u2713\n\u2713\n\u2713\nDeBERTa1.5B\n\u2713\n\u2713\n\u2713\n\u2713\nTable 7: Comparison of the pre-training data.\nA.3\nIMPLEMENTATION DETAILS\nFollowing RoBERTa (Liu et al., 2019c), we adopt dynamic data batching. We also include span\nmasking (Joshi et al., 2020) as an additional masking strategy with the span size up to three. We list\nthe detailed hyperparameters of pre-training in Table 8. For pre-training, we use Adam (Kingma &\nBa, 2014) as the optimizer with weight decay (Loshchilov & Hutter, 2018). For \ufb01ne-tuning, even\nthough we can get better and robust results with RAdam(Liu et al., 2019a) on some tasks, e.g. CoLA,\nRTE and RACE, we use Adam(Kingma & Ba, 2014) as the optimizer for a fair comparison. For\n\ufb01ne-tuning, we train each task with a hyper-parameter search procedure, each run takes about 1-2\nhours on a DGX-2 node. All the hyper-parameters are presented in Table 9. The model selection is\nbased on the performance on the task-speci\ufb01c development sets.\nOur code is implemented based on Huggingface Transformers11, FairSeq12 and Megatron (Shoeybi\net al., 2019)13.\nA.3.1\nPRE-TRAINING EFFICIENCY\nTo investigate the ef\ufb01ciency of model pre-training, we plot the performance of the \ufb01ne-tuned model\non downstream tasks as a function of the number of pre-training steps. As shown in Figure 1, for\nRoBERTa-ReImpbase and DeBERTabase, we dump a checkpoint every 150K pre-training steps, and\nthen \ufb01ne-tune the checkpoint on two representative downstream tasks, MNLI and SQuAD v2.0, and\nthen report the accuracy and F1 score, respectively. As a reference, we also report the \ufb01nal model\nperformance of both the original RoBERTabase (Liu et al., 2019c) and XLNetbase (Yang et al., 2019).\nThe results show that DeBERTabase consistently outperforms RoBERTa-ReImpbase during the course\nof pre-training.\n8https://dumps.wikimedia.org/enwiki/\n9https://github.com/butsugiri/homemade_bookcorpus\n10https://github.com/tensor\ufb02ow/models/tree/master/research/lm_commonsense\n11https://github.com/huggingface/transformers\n12https://github.com/pytorch/fairseq\n13https://github.com/NVIDIA/Megatron-LM\n16\n", [621, 623]], "A.3 Implementation Details": ["Published as a conference paper at ICLR 2021\n\u201a SWAG is a large-scale adversarial dataset for the task of grounded commonsense inference, which\nuni\ufb01es natural language inference and physically grounded reasoning (Zellers et al., 2018). SWAG\nconsists of 113k multiple choice questions about grounded situations.\n\u201a CoNLL 2003 is an English dataset consisting of text from a wide variety of sources. It has 4 types\nof named entity.\nA.2\nPRE-TRAINING DATASET\nFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump8; 12GB), BookCorpus (Zhu\net al., 2015) 9 (6GB), OPENWEBTEXT (public Reddit content (Gokaslan & Cohen, 2019); 38GB)\nand STORIES10 (a subset of CommonCrawl (Trinh & Le, 2018); 31GB). The total data size after\ndata deduplication(Shoeybi et al., 2019) is about 78GB. For pre-training, we also sample 5% training\ndata as the validation set to monitor the training process. Table 7 compares datasets used in different\npre-trained models.\nModel\nWiki+Book OpenWebText Stories CC-News Giga5 ClueWeb Common Crawl\n16GB\n38GB\n31GB\n76GB\n16GB\n19GB\n110GB\nBERT\n\u2713\nXLNet\n\u2713\n\u2713\n\u2713\n\u2713\nRoBERTa\n\u2713\n\u2713\n\u2713\n\u2713\nDeBERTa\n\u2713\n\u2713\n\u2713\nDeBERTa1.5B\n\u2713\n\u2713\n\u2713\n\u2713\nTable 7: Comparison of the pre-training data.\nA.3\nIMPLEMENTATION DETAILS\nFollowing RoBERTa (Liu et al., 2019c), we adopt dynamic data batching. We also include span\nmasking (Joshi et al., 2020) as an additional masking strategy with the span size up to three. We list\nthe detailed hyperparameters of pre-training in Table 8. For pre-training, we use Adam (Kingma &\nBa, 2014) as the optimizer with weight decay (Loshchilov & Hutter, 2018). For \ufb01ne-tuning, even\nthough we can get better and robust results with RAdam(Liu et al., 2019a) on some tasks, e.g. CoLA,\nRTE and RACE, we use Adam(Kingma & Ba, 2014) as the optimizer for a fair comparison. For\n\ufb01ne-tuning, we train each task with a hyper-parameter search procedure, each run takes about 1-2\nhours on a DGX-2 node. All the hyper-parameters are presented in Table 9. The model selection is\nbased on the performance on the task-speci\ufb01c development sets.\nOur code is implemented based on Huggingface Transformers11, FairSeq12 and Megatron (Shoeybi\net al., 2019)13.\nA.3.1\nPRE-TRAINING EFFICIENCY\nTo investigate the ef\ufb01ciency of model pre-training, we plot the performance of the \ufb01ne-tuned model\non downstream tasks as a function of the number of pre-training steps. As shown in Figure 1, for\nRoBERTa-ReImpbase and DeBERTabase, we dump a checkpoint every 150K pre-training steps, and\nthen \ufb01ne-tune the checkpoint on two representative downstream tasks, MNLI and SQuAD v2.0, and\nthen report the accuracy and F1 score, respectively. As a reference, we also report the \ufb01nal model\nperformance of both the original RoBERTabase (Liu et al., 2019c) and XLNetbase (Yang et al., 2019).\nThe results show that DeBERTabase consistently outperforms RoBERTa-ReImpbase during the course\nof pre-training.\n8https://dumps.wikimedia.org/enwiki/\n9https://github.com/butsugiri/homemade_bookcorpus\n10https://github.com/tensor\ufb02ow/models/tree/master/research/lm_commonsense\n11https://github.com/huggingface/transformers\n12https://github.com/pytorch/fairseq\n13https://github.com/NVIDIA/Megatron-LM\n16\n", []], "A.2 Pre-training Dataset": ["Published as a conference paper at ICLR 2021\n\u201a SWAG is a large-scale adversarial dataset for the task of grounded commonsense inference, which\nuni\ufb01es natural language inference and physically grounded reasoning (Zellers et al., 2018). SWAG\nconsists of 113k multiple choice questions about grounded situations.\n\u201a CoNLL 2003 is an English dataset consisting of text from a wide variety of sources. It has 4 types\nof named entity.\nA.2\nPRE-TRAINING DATASET\nFor DeBERTa pre-training, we use Wikipedia (English Wikipedia dump8; 12GB), BookCorpus (Zhu\net al., 2015) 9 (6GB), OPENWEBTEXT (public Reddit content (Gokaslan & Cohen, 2019); 38GB)\nand STORIES10 (a subset of CommonCrawl (Trinh & Le, 2018); 31GB). The total data size after\ndata deduplication(Shoeybi et al., 2019) is about 78GB. For pre-training, we also sample 5% training\ndata as the validation set to monitor the training process. Table 7 compares datasets used in different\npre-trained models.\nModel\nWiki+Book OpenWebText Stories CC-News Giga5 ClueWeb Common Crawl\n16GB\n38GB\n31GB\n76GB\n16GB\n19GB\n110GB\nBERT\n\u2713\nXLNet\n\u2713\n\u2713\n\u2713\n\u2713\nRoBERTa\n\u2713\n\u2713\n\u2713\n\u2713\nDeBERTa\n\u2713\n\u2713\n\u2713\nDeBERTa1.5B\n\u2713\n\u2713\n\u2713\n\u2713\nTable 7: Comparison of the pre-training data.\nA.3\nIMPLEMENTATION DETAILS\nFollowing RoBERTa (Liu et al., 2019c), we adopt dynamic data batching. We also include span\nmasking (Joshi et al., 2020) as an additional masking strategy with the span size up to three. We list\nthe detailed hyperparameters of pre-training in Table 8. For pre-training, we use Adam (Kingma &\nBa, 2014) as the optimizer with weight decay (Loshchilov & Hutter, 2018). For \ufb01ne-tuning, even\nthough we can get better and robust results with RAdam(Liu et al., 2019a) on some tasks, e.g. CoLA,\nRTE and RACE, we use Adam(Kingma & Ba, 2014) as the optimizer for a fair comparison. For\n\ufb01ne-tuning, we train each task with a hyper-parameter search procedure, each run takes about 1-2\nhours on a DGX-2 node. All the hyper-parameters are presented in Table 9. The model selection is\nbased on the performance on the task-speci\ufb01c development sets.\nOur code is implemented based on Huggingface Transformers11, FairSeq12 and Megatron (Shoeybi\net al., 2019)13.\nA.3.1\nPRE-TRAINING EFFICIENCY\nTo investigate the ef\ufb01ciency of model pre-training, we plot the performance of the \ufb01ne-tuned model\non downstream tasks as a function of the number of pre-training steps. As shown in Figure 1, for\nRoBERTa-ReImpbase and DeBERTabase, we dump a checkpoint every 150K pre-training steps, and\nthen \ufb01ne-tune the checkpoint on two representative downstream tasks, MNLI and SQuAD v2.0, and\nthen report the accuracy and F1 score, respectively. As a reference, we also report the \ufb01nal model\nperformance of both the original RoBERTabase (Liu et al., 2019c) and XLNetbase (Yang et al., 2019).\nThe results show that DeBERTabase consistently outperforms RoBERTa-ReImpbase during the course\nof pre-training.\n8https://dumps.wikimedia.org/enwiki/\n9https://github.com/butsugiri/homemade_bookcorpus\n10https://github.com/tensor\ufb02ow/models/tree/master/research/lm_commonsense\n11https://github.com/huggingface/transformers\n12https://github.com/pytorch/fairseq\n13https://github.com/NVIDIA/Megatron-LM\n16\n", []], "A.1 Dataset": ["Published as a conference paper at ICLR 2021\nA\nAPPENDIX\nA.1\nDATASET\nCorpus\nTask\n#Train\n#Dev\n#Test\n#Label\nMetrics\nGeneral Language Understanding Evaluation (GLUE)\nCoLA\nAcceptability\n8.5k\n1k\n1k\n2\nMatthews corr\nSST\nSentiment\n67k\n872\n1.8k\n2\nAccuracy\nMNLI\nNLI\n393k\n20k\n20k\n3\nAccuracy\nRTE\nNLI\n2.5k\n276\n3k\n2\nAccuracy\nWNLI\nNLI\n634\n71\n146\n2\nAccuracy\nQQP\nParaphrase\n364k\n40k\n391k\n2\nAccuracy/F1\nMRPC\nParaphrase\n3.7k\n408\n1.7k\n2\nAccuracy/F1\nQNLI\nQA/NLI\n108k\n5.7k\n5.7k\n2\nAccuracy\nSTS-B\nSimilarity\n7k\n1.5k\n1.4k\n1\nPearson/Spearman corr\nSuperGLUE\nWSC\nCoreference\n554k\n104\n146\n2\nAccuracy\nBoolQ\nQA\n9,427\n3,270\n3,245\n2\nAccuracy\nCOPA\nQA\n400k\n100\n500\n2\nAccuracy\nCB\nNLI\n250\n57\n250\n3\nAccuracy/F1\nRTE\nNLI\n2.5k\n276\n3k\n2\nAccuracy\nWiC\nWSD\n2.5k\n276\n3k\n2\nAccuracy\nReCoRD\nMRC\n101k\n10k\n10k\n-\nExact Match (EM)/F1\nMultiRC\nMultiple choice\n5,100\n953\n1,800\n-\nExact Match (EM)/F1\nQuestion Answering\nSQuAD v1.1\nMRC\n87.6k\n10.5k\n9.5k\n-\nExact Match (EM)/F1\nSQuAD v2.0\nMRC\n130.3k\n11.9k\n8.9k\n-\nExact Match (EM)/F1\nRACE\nMRC\n87,866\n4,887\n4,934\n4\nAccuracy\nSWAG\nMultiple choice\n73.5k\n20k\n20k\n4\nAccuracy\nToken Classi\ufb01cation\nCoNLL 2003\nNER\n14,987\n3,466\n3,684\n8\nF1\nTable 6: Summary information of the NLP application benchmarks.\n\u201a GLUE. The General Language Understanding Evaluation (GLUE) benchmark is a collection of\nnine natural language understanding (NLU) tasks. As shown in Table 6, it includes question answer-\ning (Rajpurkar et al., 2016), linguistic acceptability (Warstadt et al., 2018), sentiment analysis (Socher\net al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan & Brockett, 2005), and\nnatural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al.,\n2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks\nmakes GLUE very suitable for evaluating the generalization and robustness of NLU models.\n\u201a SuperGLUE. SuperGLUE is an extension of the GLUE benchmark, but more dif\ufb01cult, which is\na collection of eight NLU tasks. It covers a various of tasks including question answering (Zhang\net al., 2018; Clark et al., 2019; Khashabi et al., 2018), natural language inference (Dagan et al.,\n2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; De Marneffe et al.,\n2019), coreference resolution (Levesque et al., 2012) and word sense disambiguation (Pilehvar &\nCamacho-Collados, 2019).\n\u201a RACE is a large-scale machine reading comprehension dataset, collected from English examinations\nin China, which are designed for middle school and high school students (Lai et al., 2017).\n\u201a SQuAD v1.1/v2.0 is the Stanford Question Answering Dataset (SQuAD) v1.1 and v2.0 (Rajpurkar\net al., 2016; 2018) are popular machine reading comprehension benchmarks. Their passages come\nfrom approximately 500 Wikipedia articles and the questions and answers are obtained by crowd-\nsourcing. The SQuAD v2.0 dataset includes unanswerable questions about the same paragraphs.\n15\n", []], "A Appendix": ["Published as a conference paper at ICLR 2021\nA\nAPPENDIX\nA.1\nDATASET\nCorpus\nTask\n#Train\n#Dev\n#Test\n#Label\nMetrics\nGeneral Language Understanding Evaluation (GLUE)\nCoLA\nAcceptability\n8.5k\n1k\n1k\n2\nMatthews corr\nSST\nSentiment\n67k\n872\n1.8k\n2\nAccuracy\nMNLI\nNLI\n393k\n20k\n20k\n3\nAccuracy\nRTE\nNLI\n2.5k\n276\n3k\n2\nAccuracy\nWNLI\nNLI\n634\n71\n146\n2\nAccuracy\nQQP\nParaphrase\n364k\n40k\n391k\n2\nAccuracy/F1\nMRPC\nParaphrase\n3.7k\n408\n1.7k\n2\nAccuracy/F1\nQNLI\nQA/NLI\n108k\n5.7k\n5.7k\n2\nAccuracy\nSTS-B\nSimilarity\n7k\n1.5k\n1.4k\n1\nPearson/Spearman corr\nSuperGLUE\nWSC\nCoreference\n554k\n104\n146\n2\nAccuracy\nBoolQ\nQA\n9,427\n3,270\n3,245\n2\nAccuracy\nCOPA\nQA\n400k\n100\n500\n2\nAccuracy\nCB\nNLI\n250\n57\n250\n3\nAccuracy/F1\nRTE\nNLI\n2.5k\n276\n3k\n2\nAccuracy\nWiC\nWSD\n2.5k\n276\n3k\n2\nAccuracy\nReCoRD\nMRC\n101k\n10k\n10k\n-\nExact Match (EM)/F1\nMultiRC\nMultiple choice\n5,100\n953\n1,800\n-\nExact Match (EM)/F1\nQuestion Answering\nSQuAD v1.1\nMRC\n87.6k\n10.5k\n9.5k\n-\nExact Match (EM)/F1\nSQuAD v2.0\nMRC\n130.3k\n11.9k\n8.9k\n-\nExact Match (EM)/F1\nRACE\nMRC\n87,866\n4,887\n4,934\n4\nAccuracy\nSWAG\nMultiple choice\n73.5k\n20k\n20k\n4\nAccuracy\nToken Classi\ufb01cation\nCoNLL 2003\nNER\n14,987\n3,466\n3,684\n8\nF1\nTable 6: Summary information of the NLP application benchmarks.\n\u201a GLUE. The General Language Understanding Evaluation (GLUE) benchmark is a collection of\nnine natural language understanding (NLU) tasks. As shown in Table 6, it includes question answer-\ning (Rajpurkar et al., 2016), linguistic acceptability (Warstadt et al., 2018), sentiment analysis (Socher\net al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan & Brockett, 2005), and\nnatural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al.,\n2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks\nmakes GLUE very suitable for evaluating the generalization and robustness of NLU models.\n\u201a SuperGLUE. SuperGLUE is an extension of the GLUE benchmark, but more dif\ufb01cult, which is\na collection of eight NLU tasks. It covers a various of tasks including question answering (Zhang\net al., 2018; Clark et al., 2019; Khashabi et al., 2018), natural language inference (Dagan et al.,\n2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; De Marneffe et al.,\n2019), coreference resolution (Levesque et al., 2012) and word sense disambiguation (Pilehvar &\nCamacho-Collados, 2019).\n\u201a RACE is a large-scale machine reading comprehension dataset, collected from English examinations\nin China, which are designed for middle school and high school students (Lai et al., 2017).\n\u201a SQuAD v1.1/v2.0 is the Stanford Question Answering Dataset (SQuAD) v1.1 and v2.0 (Rajpurkar\net al., 2016; 2018) are popular machine reading comprehension benchmarks. Their passages come\nfrom approximately 500 Wikipedia articles and the questions and answers are obtained by crowd-\nsourcing. The SQuAD v2.0 dataset includes unanswerable questions about the same paragraphs.\n15\n", []], "7 Acknowledgments": ["Published as a conference paper at ICLR 2021\n7\nACKNOWLEDGMENTS\nWe thank Jade Huang and Nikos Karampatziakis for proofreading the paper and providing insightful\ncomments. We thank Yoyo Liang, Saksham Singhal, Xia Song, and Saurabh Tiwary for their help\nwith large-scale model training. We also thank the anonymous reviewers for valuable discussions.\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo. The second PASCAL\nrecognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges\nWorkshop on Recognising Textual Entailment, 01 2006.\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The\n\ufb01fth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC\u201909,\n2009.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\n1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055, 2017.\nKezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D Forbus, and Jianfeng\nGao. Natural-to formal-language generation using tensor product representations. arXiv preprint\narXiv:1910.02339, 2019.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising dif\ufb01culty of natural yes/no questions. In Proceedings\nof NAACL-HLT 2019, 2019.\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training\ntext encoders as discriminators rather than generators. In ICLR, 2020.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\nchallenge. In Proceedings of the First International Conference on Machine Learning Chal-\nlenges: Evaluating Predictive Uncertainty Visual Object Classi\ufb01cation, and Recognizing Textual\nEntailment, MLCW\u201905, Berlin, Heidelberg, 2006.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a \ufb01xed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978\u20132988, 2019.\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: In-\nvestigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung,\nvolume 23, pp. 107\u2013124, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171\u20134186, 2019.\n10\n", []], "6 Conclusions": ["Published as a conference paper at ICLR 2021\nSecond, a convolution layer is added aside the \ufb01rst Transformer layer to induce n-gram knowledge of\nsub-word encodings and their outputs are summed up before feeding to the next Transformer layer 7.\nTable 5 reports the test results of SuperGLUE (Wang et al., 2019a) which is one of the most popular\nNLU benchmarks. SuperGLUE consists of a wide of NLU tasks, including Question Answering\n(Clark et al., 2019; Khashabi et al., 2018; Zhang et al., 2018), Natural Language Inference (Dagan\net al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), Word Sense\nDisambiguation (Pilehvar & Camacho-Collados, 2019), and Reasoning (Levesque et al., 2011;\nRoemmele et al., 2011). Since its release in 2019, top research teams around the world have been\ndeveloping large-scale PLMs that have driven striking performance improvement on SuperGLUE.\nThe signi\ufb01cant performance boost due to scaling DeBERTa to a larger model makes the single\nDeBERTa1.5B surpass the human performance on SuperGLUE for the \ufb01rst time in terms of macro-\naverage score (89.9 versus 89.8) as of December 29, 2020, and the ensemble DeBERTa model\n(DeBERTaEnsemble) sits atop the SuperGLUE benchmark rankings as of January 6, 2021, outper-\nforming the human baseline by a decent margin (90.3 versus 89.8). Compared to T5, which consists\nof 11 billion parameters, the 1.5-billion-parameter DeBERTa is much more energy ef\ufb01cient to train\nand maintain, and it is easier to compress and deploy to apps of various settings.\nModel\nBoolQ\nCB\nCOPA\nMultiRC\nReCoRD RTE WiC WSC Average\nAcc\nF1/Acc\nAcc\nF1a/EM\nF1/EM\nAcc\nAcc\nAcc\nScore\nRoBERTalarge\n87.1 90.5/95.2 90.6\n84.4/52.5 90.6/90.0 88.2\n69.9\n89.0\n84.6\nNEXHA-Plus\n87.8 94.4/96.0 93.6\n84.6/55.1 90.1/89.6 89.1\n74.6\n93.2\n86.7\nT511B\n91.2 93.9/96.8 94.8\n88.1/63.3 94.1/93.4 92.5\n76.9\n93.8\n89.3\nT511B+Meena\n91.3 95.8/97.6 97.4\n88.3/63.0 94.2/93.5 92.7\n77.9\n95.9\n90.2\nHuman\n89.0 95.8/98.9 100.0\n81.8/51.9 91.7/91.3 93.6\n80.0 100.0\n89.8\nDeBERTa1.5B+SiFT\n90.4 94.9/97.2 96.8\n88.2/63.7 94.5/94.1 93.2\n76.4\n95.9\n89.9\nDeBERTaEnsemble\n90.4 95.7/97.6 98.4\n88.2/63.7 94.5/94.1 93.2\n77.5\n95.9\n90.3\nTable 5: SuperGLUE test set results scored using the SuperGLUE evaluation server. All the results\nare obtained from https://super.gluebenchmark.com on January 6, 2021.\n6\nCONCLUSIONS\nThis paper presents a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled\nattention) that improves the BERT and RoBERTa models using two novel techniques. The \ufb01rst is the\ndisentangled attention mechanism, where each word is represented using two vectors that encode\nits content and position, respectively, and the attention weights among words are computed using\ndisentangled matrices on their contents and relative positions, respectively. The second is an enhanced\nmask decoder which incorporates absolute positions in the decoding layer to predict the masked\ntokens in model pre-training. In addition, a new virtual adversarial training method is used for\n\ufb01ne-tuning to improve model\u2019s generalization on downstream tasks.\nWe show through a comprehensive empirical study that these techniques signi\ufb01cantly improve the\nef\ufb01ciency of model pre-training and the performance of downstream tasks. The DeBERTa model\nwith 1.5 billion parameters surpasses the human performance on the SuperGLUE benchmark for the\n\ufb01rst time in terms of macro-average score.\nDeBERTa surpassing human performance on SuperGLUE marks an important milestone toward\ngeneral AI. Despite its promising results on SuperGLUE, the model is by no means reaching the\nhuman-level intelligence of NLU. Humans are extremely good at leveraging the knowledge learned\nfrom different tasks to solve a new task with no or little task-speci\ufb01c demonstration. This is referred\nto as compositional generalization, the ability to generalize to novel compositions (new tasks) of\nfamiliar constituents (subtasks or basic problem-solving skills). Moving forward, it is worth exploring\nhow to make DeBERTa incorporate compositional structures in a more explicit manner, which could\nallow combining neural and symbolic computation of natural language similar to what humans do.\n7Please refer to Table 12 in Appendix A.6 for the ablation study of different model sizes, and Table 13 in\nAppendix A.6 for the ablation study of new modi\ufb01cations.\n9\n", []], "5.3 Scale up to 1.5 billion parameters": ["Published as a conference paper at ICLR 2021\n5.2\nMODEL ANALYSIS\nIn this section, we \ufb01rst present an ablation study to quantify the relative contributions of different\ncomponents introduced in DeBERTa. Then, we study the convergence property to characterize the\nmodel training ef\ufb01ciency. We run experiments for analysis using the base model setting: a model is\npre-trained using the Wikipedia + Bookcorpus dataset for 1M steps with batch size 256 in 7 days\non a DGX-2 machine with 16 V-100 GPUs. Due to space limit, we visualize the different attention\npatterns of DeBERTa and RoBERTa in Appendix A.7.\n5.2.1\nABLATION STUDY\nTo verify our experimental setting, we pre-train the RoBERTa base model from scratch. The re-pre-\ntrained RoBERTa model is denoted as RoBERTa-ReImpbase. To investigate the relative contributions\nof different components in DeBERTa, we develop three variations:\n\u2022 -EMD is the DeBERTa base model without EMD.\n\u2022 -C2P is the DeBERTa base model without the content-to-position term ((c) in Eq. 4).\n\u2022 -P2C is the DeBERTa base model without the position-to-content term ((b) in Eq. 4). As\nXLNet also uses the relative position bias, this model is close to XLNet plus EMD.\nModel\nMNLI-m/mm\nSQuAD v1.1\nSQuAD v2.0\nRACE\nAcc\nF1/EM\nF1/EM\nAcc\nBERTbase Devlin et al. (2019)\n84.3/84.7\n88.5/81.0\n76.3/73.7\n65.0\nRoBERTabase Liu et al. (2019c)\n84.7/-\n90.6/-\n79.7/-\n65.6\nXLNetbase Yang et al. (2019)\n85.8/85.4\n-/-\n81.3/78.5\n66.7\nRoBERTa-ReImpbase\n84.9/85.1\n91.1/84.8\n79.5/76.0\n66.8\nDeBERTabase\n86.3/86.2\n92.1/86.1\n82.5/79.3\n71.7\n-EMD\n86.1/86.1\n91.8/85.8\n81.3/78.0\n70.3\n-C2P\n85.9/85.7\n91.6/85.8\n81.3/78.3\n69.3\n-P2C\n86.0/85.8\n91.7/85.7\n80.8/77.6\n69.6\n-(EMD+C2P)\n85.8/85.9\n91.5/85.3\n80.3/77.2\n68.1\n-(EMD+P2C)\n85.8/85.8\n91.3/85.1\n80.2/77.1\n68.5\nTable 4: Ablation study of the DeBERTa base model.\nTable 4 summarizes the results on four benchmark datasets. First, RoBERTa-ReImp performs\nsimilarly to RoBERTa across all benchmark datasets, ver\ufb01ying that our setting is reasonable. Second,\nwe see that removing any one component in DeBERTa results in a sheer performance drop. For\ninstance, removing EMD (-EMD) results in a loss of 1.4% (71.7% vs. 70.3%) on RACE, 0.3%\n(92.1% vs. 91.8%) on SQuAD v1.1, 1.2% (82.5% vs. 81.3%) on SQuAD v2.0, 0.2% (86.3% vs.\n86.1%) and 0.1% (86.2% vs. 86.1%) on MNLI-m/mm, respectively. Similarly, removing either\ncontent-to-position or position-to-content leads to inferior performance in all the benchmarks. As\nexpected, removing two components results in even more substantial loss in performance.\n5.3\nSCALE UP TO 1.5 BILLION PARAMETERS\nLarger pre-trained models have shown better generalization results (Raffel et al., 2020; Brown et al.,\n2020; Shoeybi et al., 2019). Thus, we have built a larger version of DeBERTa with 1.5 billion\nparameters, denoted as DeBERTa1.5B. The model consists of 48 layers with a hidden size of 1,536\nand 24 attention heads 6. DeBERTa1.5B is trained on a pre-training dataset amounting to 160G,\nsimilar to that in Liu et al. (2019c), with a new vocabulary of size 128K constructed using the dataset.\nTo train DeBERTa1.5B, we optimize the model architecture as follows. First, we share the projection\nmatrices of relative position embedding Wk,r, Wq,r with Wk,c, Wq,c, respectively, in all attention\nlayers to reduce the number of model parameters. Our ablation study in Table 13 on base models\nshows that the projection matrix sharing reduces the model size while retaining the model performance.\n6See Table 8 in Appendix for the model hyperparameters.\n8\n", []], "5.2.1 Ablation study": ["Published as a conference paper at ICLR 2021\n5.2\nMODEL ANALYSIS\nIn this section, we \ufb01rst present an ablation study to quantify the relative contributions of different\ncomponents introduced in DeBERTa. Then, we study the convergence property to characterize the\nmodel training ef\ufb01ciency. We run experiments for analysis using the base model setting: a model is\npre-trained using the Wikipedia + Bookcorpus dataset for 1M steps with batch size 256 in 7 days\non a DGX-2 machine with 16 V-100 GPUs. Due to space limit, we visualize the different attention\npatterns of DeBERTa and RoBERTa in Appendix A.7.\n5.2.1\nABLATION STUDY\nTo verify our experimental setting, we pre-train the RoBERTa base model from scratch. The re-pre-\ntrained RoBERTa model is denoted as RoBERTa-ReImpbase. To investigate the relative contributions\nof different components in DeBERTa, we develop three variations:\n\u2022 -EMD is the DeBERTa base model without EMD.\n\u2022 -C2P is the DeBERTa base model without the content-to-position term ((c) in Eq. 4).\n\u2022 -P2C is the DeBERTa base model without the position-to-content term ((b) in Eq. 4). As\nXLNet also uses the relative position bias, this model is close to XLNet plus EMD.\nModel\nMNLI-m/mm\nSQuAD v1.1\nSQuAD v2.0\nRACE\nAcc\nF1/EM\nF1/EM\nAcc\nBERTbase Devlin et al. (2019)\n84.3/84.7\n88.5/81.0\n76.3/73.7\n65.0\nRoBERTabase Liu et al. (2019c)\n84.7/-\n90.6/-\n79.7/-\n65.6\nXLNetbase Yang et al. (2019)\n85.8/85.4\n-/-\n81.3/78.5\n66.7\nRoBERTa-ReImpbase\n84.9/85.1\n91.1/84.8\n79.5/76.0\n66.8\nDeBERTabase\n86.3/86.2\n92.1/86.1\n82.5/79.3\n71.7\n-EMD\n86.1/86.1\n91.8/85.8\n81.3/78.0\n70.3\n-C2P\n85.9/85.7\n91.6/85.8\n81.3/78.3\n69.3\n-P2C\n86.0/85.8\n91.7/85.7\n80.8/77.6\n69.6\n-(EMD+C2P)\n85.8/85.9\n91.5/85.3\n80.3/77.2\n68.1\n-(EMD+P2C)\n85.8/85.8\n91.3/85.1\n80.2/77.1\n68.5\nTable 4: Ablation study of the DeBERTa base model.\nTable 4 summarizes the results on four benchmark datasets. First, RoBERTa-ReImp performs\nsimilarly to RoBERTa across all benchmark datasets, ver\ufb01ying that our setting is reasonable. Second,\nwe see that removing any one component in DeBERTa results in a sheer performance drop. For\ninstance, removing EMD (-EMD) results in a loss of 1.4% (71.7% vs. 70.3%) on RACE, 0.3%\n(92.1% vs. 91.8%) on SQuAD v1.1, 1.2% (82.5% vs. 81.3%) on SQuAD v2.0, 0.2% (86.3% vs.\n86.1%) and 0.1% (86.2% vs. 86.1%) on MNLI-m/mm, respectively. Similarly, removing either\ncontent-to-position or position-to-content leads to inferior performance in all the benchmarks. As\nexpected, removing two components results in even more substantial loss in performance.\n5.3\nSCALE UP TO 1.5 BILLION PARAMETERS\nLarger pre-trained models have shown better generalization results (Raffel et al., 2020; Brown et al.,\n2020; Shoeybi et al., 2019). Thus, we have built a larger version of DeBERTa with 1.5 billion\nparameters, denoted as DeBERTa1.5B. The model consists of 48 layers with a hidden size of 1,536\nand 24 attention heads 6. DeBERTa1.5B is trained on a pre-training dataset amounting to 160G,\nsimilar to that in Liu et al. (2019c), with a new vocabulary of size 128K constructed using the dataset.\nTo train DeBERTa1.5B, we optimize the model architecture as follows. First, we share the projection\nmatrices of relative position embedding Wk,r, Wq,r with Wk,c, Wq,c, respectively, in all attention\nlayers to reduce the number of model parameters. Our ablation study in Table 13 on base models\nshows that the projection matrix sharing reduces the model size while retaining the model performance.\n6See Table 8 in Appendix for the model hyperparameters.\n8\n", []], "5.2 Model Analysis": ["Published as a conference paper at ICLR 2021\n5.2\nMODEL ANALYSIS\nIn this section, we \ufb01rst present an ablation study to quantify the relative contributions of different\ncomponents introduced in DeBERTa. Then, we study the convergence property to characterize the\nmodel training ef\ufb01ciency. We run experiments for analysis using the base model setting: a model is\npre-trained using the Wikipedia + Bookcorpus dataset for 1M steps with batch size 256 in 7 days\non a DGX-2 machine with 16 V-100 GPUs. Due to space limit, we visualize the different attention\npatterns of DeBERTa and RoBERTa in Appendix A.7.\n5.2.1\nABLATION STUDY\nTo verify our experimental setting, we pre-train the RoBERTa base model from scratch. The re-pre-\ntrained RoBERTa model is denoted as RoBERTa-ReImpbase. To investigate the relative contributions\nof different components in DeBERTa, we develop three variations:\n\u2022 -EMD is the DeBERTa base model without EMD.\n\u2022 -C2P is the DeBERTa base model without the content-to-position term ((c) in Eq. 4).\n\u2022 -P2C is the DeBERTa base model without the position-to-content term ((b) in Eq. 4). As\nXLNet also uses the relative position bias, this model is close to XLNet plus EMD.\nModel\nMNLI-m/mm\nSQuAD v1.1\nSQuAD v2.0\nRACE\nAcc\nF1/EM\nF1/EM\nAcc\nBERTbase Devlin et al. (2019)\n84.3/84.7\n88.5/81.0\n76.3/73.7\n65.0\nRoBERTabase Liu et al. (2019c)\n84.7/-\n90.6/-\n79.7/-\n65.6\nXLNetbase Yang et al. (2019)\n85.8/85.4\n-/-\n81.3/78.5\n66.7\nRoBERTa-ReImpbase\n84.9/85.1\n91.1/84.8\n79.5/76.0\n66.8\nDeBERTabase\n86.3/86.2\n92.1/86.1\n82.5/79.3\n71.7\n-EMD\n86.1/86.1\n91.8/85.8\n81.3/78.0\n70.3\n-C2P\n85.9/85.7\n91.6/85.8\n81.3/78.3\n69.3\n-P2C\n86.0/85.8\n91.7/85.7\n80.8/77.6\n69.6\n-(EMD+C2P)\n85.8/85.9\n91.5/85.3\n80.3/77.2\n68.1\n-(EMD+P2C)\n85.8/85.8\n91.3/85.1\n80.2/77.1\n68.5\nTable 4: Ablation study of the DeBERTa base model.\nTable 4 summarizes the results on four benchmark datasets. First, RoBERTa-ReImp performs\nsimilarly to RoBERTa across all benchmark datasets, ver\ufb01ying that our setting is reasonable. Second,\nwe see that removing any one component in DeBERTa results in a sheer performance drop. For\ninstance, removing EMD (-EMD) results in a loss of 1.4% (71.7% vs. 70.3%) on RACE, 0.3%\n(92.1% vs. 91.8%) on SQuAD v1.1, 1.2% (82.5% vs. 81.3%) on SQuAD v2.0, 0.2% (86.3% vs.\n86.1%) and 0.1% (86.2% vs. 86.1%) on MNLI-m/mm, respectively. Similarly, removing either\ncontent-to-position or position-to-content leads to inferior performance in all the benchmarks. As\nexpected, removing two components results in even more substantial loss in performance.\n5.3\nSCALE UP TO 1.5 BILLION PARAMETERS\nLarger pre-trained models have shown better generalization results (Raffel et al., 2020; Brown et al.,\n2020; Shoeybi et al., 2019). Thus, we have built a larger version of DeBERTa with 1.5 billion\nparameters, denoted as DeBERTa1.5B. The model consists of 48 layers with a hidden size of 1,536\nand 24 attention heads 6. DeBERTa1.5B is trained on a pre-training dataset amounting to 160G,\nsimilar to that in Liu et al. (2019c), with a new vocabulary of size 128K constructed using the dataset.\nTo train DeBERTa1.5B, we optimize the model architecture as follows. First, we share the projection\nmatrices of relative position embedding Wk,r, Wq,r with Wk,c, Wq,c, respectively, in all attention\nlayers to reduce the number of model parameters. Our ablation study in Table 13 on base models\nshows that the projection matrix sharing reduces the model size while retaining the model performance.\n6See Table 8 in Appendix for the model hyperparameters.\n8\n", []], "5.1.2 Performance on Base Models": ["Published as a conference paper at ICLR 2021\nModel\nMNLI-m/mm SQuAD v1.1 SQuAD v2.0 RACE ReCoRD SWAG NER\nAcc\nF1/EM\nF1/EM\nAcc\nF1/EM\nAcc\nF1\nBERTlarge\n86.6/-\n90.9/84.1\n81.8/79.0\n72.0\n-\n86.6\n92.8\nALBERTlarge\n86.5/-\n91.8/85.2\n84.9/81.8\n75.2\n-\n-\n-\nRoBERTalarge\n90.2/90.2\n94.6/88.9\n89.4/86.5\n83.2\n90.6/90.0\n89.9\n93.4\nXLNetlarge\n90.8/90.8\n95.1/89.7\n90.6/87.9\n85.4\n-\n-\n-\nMegatron336M\n89.7/90.0\n94.2/88.0\n88.1/84.8\n83.0\n-\n-\n-\nDeBERTalarge\n91.1/91.1\n95.5/90.1\n90.7/88.0\n86.8\n91.4/91.0\n90.8\n93.8\nALBERTxxlarge\n90.8/-\n94.8/89.3\n90.2/87.4\n86.5\n-\n-\n-\nMegatron1.3B\n90.9/91.0\n94.9/89.1\n90.2/87.1\n87.3\n-\n-\n-\nMegatron3.9B\n91.4/91.4\n95.5/90.0\n91.2/88.5\n89.5\n-\n-\n-\nTable 2: Results on MNLI in/out-domain, SQuAD v1.1, SQuAD v2.0, RACE, ReCoRD, SWAG,\nCoNLL 2003 NER development set. Note that missing results in literature are signi\ufb01ed by \u201c-\u201d.\nIn addition to GLUE, DeBERTa is evaluated on three categories of NLU benchmarks: (1) Question\nAnswering: SQuAD v1.1 (Rajpurkar et al., 2016), SQuAD v2.0 (Rajpurkar et al., 2018), RACE (Lai\net al., 2017), ReCoRD (Zhang et al., 2018) and SWAG (Zellers et al., 2018); (2) Natural Language\nInference: MNLI (Williams et al., 2018); and (3) NER: CoNLL-2003. For comparison, we include\nALBERTxxlarge (Lan et al., 2019) 4 and Megatron (Shoeybi et al., 2019) with three different model\nsizes, denoted as Megatron336M, Megatron1.3B and Megatron3.9B, respectively, which are trained using\nthe same dataset as RoBERTa. Note that Megatron336M has a similar model size as other models\nmentioned above5.\nWe summarize the results in Table 2. Compared to the previous SOTA PLMs with a similar model\nsize (i.e., BERT, RoBERTa, XLNet, ALBERTlarge, and Megatron336M), DeBERTa shows superior\nperformance in all seven tasks. Taking the RACE benchmark as an example, DeBERTa signi\ufb01cantly\noutperforms XLNet by +1.4% (86.8% vs. 85.4%). Although Megatron1.3B is three times larger than\nDeBERTa, DeBERTa outperforms it in three of the four benchmarks. We further report DeBERTa on\ntext generation tasks in Appendix A.4.\n5.1.2\nPERFORMANCE ON BASE MODELS\nOur setting for base model pre-training is similar to that for large models. The base model structure\nfollows that of the BERT base model, i.e., L \u201c 12, H \u201c 768, A \u201c 12. We use 4 DGX-2 with 64\nV100 GPUs to train the base model. It takes 10 days to \ufb01nish a single pre-training of 1M training steps\nwith batch size 2048. We train DeBERTa using the same 78G dataset, and compare it to RoBERTa\nand XLNet trained on 160G text data.\nWe summarize the base model results in Table 3. Across all three tasks, DeBERTa consistently\noutperforms RoBERTa and XLNet by a larger margin than that in large models. For example, on\nMNLI-m, DeBERTabase obtains +1.2% (88.8% vs. 87.6%) over RoBERTabase, and +2% (88.8% vs.\n86.8%) over XLNetbase.\nModel\nMNLI-m/mm (Acc)\nSQuAD v1.1 (F1/EM)\nSQuAD v2.0 (F1/EM)\nRoBERTabase\n87.6/-\n91.5/84.6\n83.7/80.5\nXLNetbase\n86.8/-\n-/-\n-/80.2\nDeBERTabase\n88.8/88.5\n93.1/87.2\n86.2/83.1\nTable 3: Results on MNLI in/out-domain (m/mm), SQuAD v1.1 and v2.0 development set.\n4The hidden dimension of ALBERTxxlarge is 4 times of DeBERTa and the computation cost is about 4 times\nof DeBERTa.\n5T5 (Raffel et al., 2020) has more parameters (11B). Raffel et al. (2020) only report the test results of T5\nwhich are not comparable with other models.\n7\n", []], "5.1.1 Performance on Large Models": ["Published as a conference paper at ICLR 2021\nInspired by layer normalization (Ba et al., 2016), we propose the SiFT algorithm that improves the\ntraining stability by applying the perturbations to the normalized word embeddings. Speci\ufb01cally,\nwhen \ufb01ne-tuning DeBERTa to a downstream NLP task in our experiments, SiFT \ufb01rst normalizes the\nword embedding vectors into stochastic vectors, and then applies the perturbation to the normalized\nembedding vectors. We \ufb01nd that the normalization substantially improves the performance of the\n\ufb01ne-tuned models. The improvement is more prominent for larger DeBERTa models. Note that we\nonly apply SiFT to DeBERTa1.5B on SuperGLUE tasks in our experiments and we will provide a\nmore comprehensive study of SiFT in our future work.\n5\nEXPERIMENT\nThis section reports DeBERTa results on various NLU tasks.\n5.1\nMAIN RESULTS ON NLU TASKS\nFollowing previous studies of PLMs, we report results using large and base models.\n5.1.1\nPERFORMANCE ON LARGE MODELS\nModel\nCoLA QQP\nMNLI-m/mm\nSST-2\nSTS-B\nQNLI\nRTE\nMRPC\nAvg.\nMcc\nAcc\nAcc\nAcc\nCorr\nAcc\nAcc\nAcc\nBERTlarge\n60.6\n91.3\n86.6/-\n93.2\n90.0\n92.3\n70.4\n88.0\n84.05\nRoBERTalarge\n68.0\n92.2\n90.2/90.2\n96.4\n92.4\n93.9\n86.6\n90.9\n88.82\nXLNetlarge\n69.0\n92.3\n90.8/90.8\n97.0\n92.5\n94.9\n85.9\n90.8\n89.15\nELECTRAlarge\n69.1\n92.4\n90.9/-\n96.9\n92.6\n95.0\n88.0\n90.8\n89.46\nDeBERTalarge\n70.5\n92.3\n91.1/91.1\n96.8\n92.8\n95.3\n88.3\n91.9\n90.00\nTable 1: Comparison results on the GLUE development set.\nWe pre-train our large models following the setting of BERT (Devlin et al., 2019), except that we use\nthe BPE vocabulary of Radford et al. (2019); Liu et al. (2019c). For training data, we use Wikipedia\n(English Wikipedia dump3; 12GB), BookCorpus (Zhu et al., 2015) (6GB), OPENWEBTEXT (public\nReddit content (Gokaslan & Cohen, 2019); 38GB), and STORIES (a subset of CommonCrawl (Trinh\n& Le, 2018); 31GB). The total data size after data deduplication (Shoeybi et al., 2019) is about 78G.\nRefer to Appendix A.2 for a detailed description of the pre-training dataset.\nWe use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K\nbatch size and 1M steps takes about 20 days. Refer to Appendix A for the detailed hyperparamters.\nWe summarize the results on eight NLU tasks of GLUE (Wang et al., 2019b) in Table 1, where\nDeBERTa is compared DeBERTa with previous Transform-based PLMs of similar structures (i.e. 24\nlayers with hidden size of 1024) including BERT, RoBERTa, XLNet, ALBERT and ELECTRA. Note\nthat RoBERTa, XLNet and ELECTRA are pre-trained on 160G training data while DeBERTa is pre-\ntrained on 78G training data. RoBERTa and XLNet are pre-trained for 500K steps with 8K samples\nin a step, which amounts to four billion training samples. DeBERTa is pre-trained for one million\nsteps with 2K samples in each step. This amounts to two billion training samples, approximately\nhalf of either RoBERTa or XLNet. Table 1 shows that compared to BERT and RoBERTa, DeBERTa\nperforms consistently better across all the tasks. Meanwhile, DeBERTa outperforms XLNet in six out\nof eight tasks. Particularly, the improvements on MRPC (1.1% over XLNet and 1.0% over RoBERTa),\nRTE (2.4% over XLNet and 1.7% over RoBERTa) and CoLA (1.5% over XLNet and 2.5% over\nRoBERTa) are signi\ufb01cant. DeBERTa also outperforms other SOTA PLMs, i.e., ELECTRAlarge and\nXLNetlarge, in terms of average GLUE score.\nAmong all GLUE tasks, MNLI is most often used as an indicative task to monitor the research\nprogress of PLMs. DeBERTa signi\ufb01cantly outperforms all existing PLMs of similar size on MNLI\nand creates a new state of the art.\n3https://dumps.wikimedia.org/enwiki/\n6\n", []], "5.1 Main Results on NLU tasks": ["Published as a conference paper at ICLR 2021\nInspired by layer normalization (Ba et al., 2016), we propose the SiFT algorithm that improves the\ntraining stability by applying the perturbations to the normalized word embeddings. Speci\ufb01cally,\nwhen \ufb01ne-tuning DeBERTa to a downstream NLP task in our experiments, SiFT \ufb01rst normalizes the\nword embedding vectors into stochastic vectors, and then applies the perturbation to the normalized\nembedding vectors. We \ufb01nd that the normalization substantially improves the performance of the\n\ufb01ne-tuned models. The improvement is more prominent for larger DeBERTa models. Note that we\nonly apply SiFT to DeBERTa1.5B on SuperGLUE tasks in our experiments and we will provide a\nmore comprehensive study of SiFT in our future work.\n5\nEXPERIMENT\nThis section reports DeBERTa results on various NLU tasks.\n5.1\nMAIN RESULTS ON NLU TASKS\nFollowing previous studies of PLMs, we report results using large and base models.\n5.1.1\nPERFORMANCE ON LARGE MODELS\nModel\nCoLA QQP\nMNLI-m/mm\nSST-2\nSTS-B\nQNLI\nRTE\nMRPC\nAvg.\nMcc\nAcc\nAcc\nAcc\nCorr\nAcc\nAcc\nAcc\nBERTlarge\n60.6\n91.3\n86.6/-\n93.2\n90.0\n92.3\n70.4\n88.0\n84.05\nRoBERTalarge\n68.0\n92.2\n90.2/90.2\n96.4\n92.4\n93.9\n86.6\n90.9\n88.82\nXLNetlarge\n69.0\n92.3\n90.8/90.8\n97.0\n92.5\n94.9\n85.9\n90.8\n89.15\nELECTRAlarge\n69.1\n92.4\n90.9/-\n96.9\n92.6\n95.0\n88.0\n90.8\n89.46\nDeBERTalarge\n70.5\n92.3\n91.1/91.1\n96.8\n92.8\n95.3\n88.3\n91.9\n90.00\nTable 1: Comparison results on the GLUE development set.\nWe pre-train our large models following the setting of BERT (Devlin et al., 2019), except that we use\nthe BPE vocabulary of Radford et al. (2019); Liu et al. (2019c). For training data, we use Wikipedia\n(English Wikipedia dump3; 12GB), BookCorpus (Zhu et al., 2015) (6GB), OPENWEBTEXT (public\nReddit content (Gokaslan & Cohen, 2019); 38GB), and STORIES (a subset of CommonCrawl (Trinh\n& Le, 2018); 31GB). The total data size after data deduplication (Shoeybi et al., 2019) is about 78G.\nRefer to Appendix A.2 for a detailed description of the pre-training dataset.\nWe use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K\nbatch size and 1M steps takes about 20 days. Refer to Appendix A for the detailed hyperparamters.\nWe summarize the results on eight NLU tasks of GLUE (Wang et al., 2019b) in Table 1, where\nDeBERTa is compared DeBERTa with previous Transform-based PLMs of similar structures (i.e. 24\nlayers with hidden size of 1024) including BERT, RoBERTa, XLNet, ALBERT and ELECTRA. Note\nthat RoBERTa, XLNet and ELECTRA are pre-trained on 160G training data while DeBERTa is pre-\ntrained on 78G training data. RoBERTa and XLNet are pre-trained for 500K steps with 8K samples\nin a step, which amounts to four billion training samples. DeBERTa is pre-trained for one million\nsteps with 2K samples in each step. This amounts to two billion training samples, approximately\nhalf of either RoBERTa or XLNet. Table 1 shows that compared to BERT and RoBERTa, DeBERTa\nperforms consistently better across all the tasks. Meanwhile, DeBERTa outperforms XLNet in six out\nof eight tasks. Particularly, the improvements on MRPC (1.1% over XLNet and 1.0% over RoBERTa),\nRTE (2.4% over XLNet and 1.7% over RoBERTa) and CoLA (1.5% over XLNet and 2.5% over\nRoBERTa) are signi\ufb01cant. DeBERTa also outperforms other SOTA PLMs, i.e., ELECTRAlarge and\nXLNetlarge, in terms of average GLUE score.\nAmong all GLUE tasks, MNLI is most often used as an indicative task to monitor the research\nprogress of PLMs. DeBERTa signi\ufb01cantly outperforms all existing PLMs of similar size on MNLI\nand creates a new state of the art.\n3https://dumps.wikimedia.org/enwiki/\n6\n", []], "5 Experiment": ["Published as a conference paper at ICLR 2021\nInspired by layer normalization (Ba et al., 2016), we propose the SiFT algorithm that improves the\ntraining stability by applying the perturbations to the normalized word embeddings. Speci\ufb01cally,\nwhen \ufb01ne-tuning DeBERTa to a downstream NLP task in our experiments, SiFT \ufb01rst normalizes the\nword embedding vectors into stochastic vectors, and then applies the perturbation to the normalized\nembedding vectors. We \ufb01nd that the normalization substantially improves the performance of the\n\ufb01ne-tuned models. The improvement is more prominent for larger DeBERTa models. Note that we\nonly apply SiFT to DeBERTa1.5B on SuperGLUE tasks in our experiments and we will provide a\nmore comprehensive study of SiFT in our future work.\n5\nEXPERIMENT\nThis section reports DeBERTa results on various NLU tasks.\n5.1\nMAIN RESULTS ON NLU TASKS\nFollowing previous studies of PLMs, we report results using large and base models.\n5.1.1\nPERFORMANCE ON LARGE MODELS\nModel\nCoLA QQP\nMNLI-m/mm\nSST-2\nSTS-B\nQNLI\nRTE\nMRPC\nAvg.\nMcc\nAcc\nAcc\nAcc\nCorr\nAcc\nAcc\nAcc\nBERTlarge\n60.6\n91.3\n86.6/-\n93.2\n90.0\n92.3\n70.4\n88.0\n84.05\nRoBERTalarge\n68.0\n92.2\n90.2/90.2\n96.4\n92.4\n93.9\n86.6\n90.9\n88.82\nXLNetlarge\n69.0\n92.3\n90.8/90.8\n97.0\n92.5\n94.9\n85.9\n90.8\n89.15\nELECTRAlarge\n69.1\n92.4\n90.9/-\n96.9\n92.6\n95.0\n88.0\n90.8\n89.46\nDeBERTalarge\n70.5\n92.3\n91.1/91.1\n96.8\n92.8\n95.3\n88.3\n91.9\n90.00\nTable 1: Comparison results on the GLUE development set.\nWe pre-train our large models following the setting of BERT (Devlin et al., 2019), except that we use\nthe BPE vocabulary of Radford et al. (2019); Liu et al. (2019c). For training data, we use Wikipedia\n(English Wikipedia dump3; 12GB), BookCorpus (Zhu et al., 2015) (6GB), OPENWEBTEXT (public\nReddit content (Gokaslan & Cohen, 2019); 38GB), and STORIES (a subset of CommonCrawl (Trinh\n& Le, 2018); 31GB). The total data size after data deduplication (Shoeybi et al., 2019) is about 78G.\nRefer to Appendix A.2 for a detailed description of the pre-training dataset.\nWe use 6 DGX-2 machines (96 V100 GPUs) to train the models. A single model trained with 2K\nbatch size and 1M steps takes about 20 days. Refer to Appendix A for the detailed hyperparamters.\nWe summarize the results on eight NLU tasks of GLUE (Wang et al., 2019b) in Table 1, where\nDeBERTa is compared DeBERTa with previous Transform-based PLMs of similar structures (i.e. 24\nlayers with hidden size of 1024) including BERT, RoBERTa, XLNet, ALBERT and ELECTRA. Note\nthat RoBERTa, XLNet and ELECTRA are pre-trained on 160G training data while DeBERTa is pre-\ntrained on 78G training data. RoBERTa and XLNet are pre-trained for 500K steps with 8K samples\nin a step, which amounts to four billion training samples. DeBERTa is pre-trained for one million\nsteps with 2K samples in each step. This amounts to two billion training samples, approximately\nhalf of either RoBERTa or XLNet. Table 1 shows that compared to BERT and RoBERTa, DeBERTa\nperforms consistently better across all the tasks. Meanwhile, DeBERTa outperforms XLNet in six out\nof eight tasks. Particularly, the improvements on MRPC (1.1% over XLNet and 1.0% over RoBERTa),\nRTE (2.4% over XLNet and 1.7% over RoBERTa) and CoLA (1.5% over XLNet and 2.5% over\nRoBERTa) are signi\ufb01cant. DeBERTa also outperforms other SOTA PLMs, i.e., ELECTRAlarge and\nXLNetlarge, in terms of average GLUE score.\nAmong all GLUE tasks, MNLI is most often used as an indicative task to monitor the research\nprogress of PLMs. DeBERTa signi\ufb01cantly outperforms all existing PLMs of similar size on MNLI\nand creates a new state of the art.\n3https://dumps.wikimedia.org/enwiki/\n6\n", []], "4 Scale Invariant Fine-Tuning": ["Published as a conference paper at ICLR 2021\nof all possible relative positions are always a subset of Kr P R2k\u02c6d, then we can reuse Kr in the\nattention calculation for all the queries.\nIn our experiments, we set the maximum relative distance k to 512 for pre-training. The disentangled\nattention weights can be computed ef\ufb01ciently using Algorithm 1. Let \u03b4 be the relative position matrix\naccording to equation 3, i.e., \u03b4ri, js \u201c \u03b4pi, jq. Instead of allocating a different relative position\nembedding matrix for each query, we multiply each query vector Qcri, :s by K\u22ba\nr P Rd\u02c62k, as in line\n3 \u00b4 5. Then, we extract the attention weight using the relative position matrix \u03b4 as the index, as in\nline 6 \u00b4 10. To compute the position-to-content attention score, we calculate \u02dc\nAp\u00d1cr:, js, i.e., the\ncolumn vector of the attention matrix \u02dc\nAp\u00d1c, by multiplying each key vector Kcrj, :s by Q\u22ba\nr, as in\nline 11 \u00b4 13. Finally, we extract the corresponding attention score via the relative position matrix \u03b4\nas the index, as in line 14 \u00b4 18. In this way, we do not need to allocate memory to store a relative\nposition embedding for each query and thus reduce the space complexity to Opkdq (for storing Kr\nand Qr).\n3.2\nENHANCED MASK DECODER ACCOUNTS FOR ABSOLUTE WORD POSITIONS\nDeBERTa is pretrained using MLM, where a model is trained to use the words surrounding a mask\ntoken to predict what the masked word should be. DeBERTa uses the content and position information\nof the context words for MLM. The disentangled attention mechanism already considers the contents\nand relative positions of the context words, but not the absolute positions of these words, which in\nmany cases are crucial for the prediction.\nGiven a sentence \u201ca new store opened beside the new mall\u201d with the words \u201cstore\u201d and \u201cmall\u201d\nmasked for prediction. Using only the local context (e.g., relative positions and surrounding words)\nis insuf\ufb01cient for the model to distinguish store and mall in this sentence, since both follow the word\nnew with the same relative positions. To address this limitation, the model needs to take into account\nabsolute positions, as complement information to the relative positions. For example, the subject of\nthe sentence is \u201cstore\u201d not \u201cmall\u201d. These syntactical nuances depend, to a large degree, upon the\nwords\u2019 absolute positions in the sentence.\nThere are two methods of incorporating absolute positions. The BERT model incorporates absolute\npositions in the input layer. In DeBERTa, we incorporate them right after all the Transformer layers\nbut before the softmax layer for masked token prediction, as shown in Figure 2. In this way, DeBERTa\ncaptures the relative positions in all the Transformer layers and only uses absolute positions as\ncomplementary information when decoding the masked words. Thus, we call DeBERTa\u2019s decoding\ncomponent an Enhanced Mask Decoder (EMD). In the empirical study, we compare these two\nmethods of incorporating absolute positions and observe that EMD works much better. We conjecture\nthat the early incorporation of absolute positions used by BERT might undesirably hamper the model\nfrom learning suf\ufb01cient information of relative positions. In addition, EMD also enables us to\nintroduce other useful information, in addition to positions, for pre-training. We leave it to future\nwork.\n4\nSCALE INVARIANT FINE-TUNING\nThis section presents a new virtual adversarial training algorithm, Scale-invariant-Fine-Tuning (SiFT),\na variant to the algorithm described in Miyato et al. (2018); Jiang et al. (2020), for \ufb01ne-tuning.\nVirtual adversarial training is a regularization method for improving models\u2019 generalization. It does\nso by improving a model\u2019s robustness to adversarial examples, which are created by making small\nperturbations to the input. The model is regularized so that when given a task-speci\ufb01c example, the\nmodel produces the same output distribution as it produces on an adversarial perturbation of that\nexample.\nFor NLP tasks, the perturbation is applied to the word embedding instead of the original word\nsequence. However, the value ranges (norms) of the embedding vectors vary among different words\nand models. The variance gets larger for bigger models with billions of parameters, leading to some\ninstability of adversarial training.\n5\n", []], "3.2 Enhanced Mask Decoder Accounts for Absolute Word Positions": ["Published as a conference paper at ICLR 2021\nof all possible relative positions are always a subset of Kr P R2k\u02c6d, then we can reuse Kr in the\nattention calculation for all the queries.\nIn our experiments, we set the maximum relative distance k to 512 for pre-training. The disentangled\nattention weights can be computed ef\ufb01ciently using Algorithm 1. Let \u03b4 be the relative position matrix\naccording to equation 3, i.e., \u03b4ri, js \u201c \u03b4pi, jq. Instead of allocating a different relative position\nembedding matrix for each query, we multiply each query vector Qcri, :s by K\u22ba\nr P Rd\u02c62k, as in line\n3 \u00b4 5. Then, we extract the attention weight using the relative position matrix \u03b4 as the index, as in\nline 6 \u00b4 10. To compute the position-to-content attention score, we calculate \u02dc\nAp\u00d1cr:, js, i.e., the\ncolumn vector of the attention matrix \u02dc\nAp\u00d1c, by multiplying each key vector Kcrj, :s by Q\u22ba\nr, as in\nline 11 \u00b4 13. Finally, we extract the corresponding attention score via the relative position matrix \u03b4\nas the index, as in line 14 \u00b4 18. In this way, we do not need to allocate memory to store a relative\nposition embedding for each query and thus reduce the space complexity to Opkdq (for storing Kr\nand Qr).\n3.2\nENHANCED MASK DECODER ACCOUNTS FOR ABSOLUTE WORD POSITIONS\nDeBERTa is pretrained using MLM, where a model is trained to use the words surrounding a mask\ntoken to predict what the masked word should be. DeBERTa uses the content and position information\nof the context words for MLM. The disentangled attention mechanism already considers the contents\nand relative positions of the context words, but not the absolute positions of these words, which in\nmany cases are crucial for the prediction.\nGiven a sentence \u201ca new store opened beside the new mall\u201d with the words \u201cstore\u201d and \u201cmall\u201d\nmasked for prediction. Using only the local context (e.g., relative positions and surrounding words)\nis insuf\ufb01cient for the model to distinguish store and mall in this sentence, since both follow the word\nnew with the same relative positions. To address this limitation, the model needs to take into account\nabsolute positions, as complement information to the relative positions. For example, the subject of\nthe sentence is \u201cstore\u201d not \u201cmall\u201d. These syntactical nuances depend, to a large degree, upon the\nwords\u2019 absolute positions in the sentence.\nThere are two methods of incorporating absolute positions. The BERT model incorporates absolute\npositions in the input layer. In DeBERTa, we incorporate them right after all the Transformer layers\nbut before the softmax layer for masked token prediction, as shown in Figure 2. In this way, DeBERTa\ncaptures the relative positions in all the Transformer layers and only uses absolute positions as\ncomplementary information when decoding the masked words. Thus, we call DeBERTa\u2019s decoding\ncomponent an Enhanced Mask Decoder (EMD). In the empirical study, we compare these two\nmethods of incorporating absolute positions and observe that EMD works much better. We conjecture\nthat the early incorporation of absolute positions used by BERT might undesirably hamper the model\nfrom learning suf\ufb01cient information of relative positions. In addition, EMD also enables us to\nintroduce other useful information, in addition to positions, for pre-training. We leave it to future\nwork.\n4\nSCALE INVARIANT FINE-TUNING\nThis section presents a new virtual adversarial training algorithm, Scale-invariant-Fine-Tuning (SiFT),\na variant to the algorithm described in Miyato et al. (2018); Jiang et al. (2020), for \ufb01ne-tuning.\nVirtual adversarial training is a regularization method for improving models\u2019 generalization. It does\nso by improving a model\u2019s robustness to adversarial examples, which are created by making small\nperturbations to the input. The model is regularized so that when given a task-speci\ufb01c example, the\nmodel produces the same output distribution as it produces on an adversarial perturbation of that\nexample.\nFor NLP tasks, the perturbation is applied to the word embedding instead of the original word\nsequence. However, the value ranges (norms) of the embedding vectors vary among different words\nand models. The variance gets larger for bigger models with billions of parameters, leading to some\ninstability of adversarial training.\n5\n", []], "3.1.1 Efficient implementation": ["Published as a conference paper at ICLR 2021\nWe can represent the disentangled self-attention with relative position bias as equation 4,\nwhere Qc, Kc and Vc are the projected content vectors generated using projection matrices\nWq,c, Wk,c, Wv,c P Rd\u02c6d respectively, P P R2k\u02c6d represents the relative position embedding\nvectors shared across all layers (i.e., staying \ufb01xed during forward propagation), and Qr and Kr\nare projected relative position vectors generated using projection matrices Wq,r, Wk,r P Rd\u02c6d,\nrespectively.\nQc \u201c HWq,c, Kc \u201c HWk,c, Vc \u201c HWv,c, Qr \u201c P Wq,r, Kr \u201c P Wk,r\n\u02dcAi,j \u201c\nQc\niKc\nj\n\u22ba\nlooomooon\n(a) content-to-content\n`\nQc\niKr\n\u03b4pi,jq\n\u22ba\nlooooomooooon\n(b) content-to-position\n`\nKc\nj Qr\n\u03b4pj,iq\n\u22ba\nlooooomooooon\n(c) position-to-content\nHo \u201c softmaxp\n\u02dc\nA\n?\n3d\nqVc\n(4)\n\u02dcAi,j is the element of attention matrix \u02dc\nA, representing the attention score from token i to token\nj. Qc\ni is the i-th row of Qc. Kc\nj is the j-th row of Kc. Kr\n\u03b4pi,jq is the \u03b4pi, jq-th row of Kr with\nregarding to relative distance \u03b4pi, jq. Qr\n\u03b4pj,iq is the \u03b4pj, iq-th row of Qr with regarding to relative\ndistance \u03b4pj, iq. Note that we use \u03b4pj, iq rather than \u03b4pi, jq here. This is because for a given position\ni, position-to-content computes the attention weight of the key content at j with respect to the\nquery position at i, thus the relative distance is \u03b4pj, iq. The position-to-content term is calculated as\nKc\nj Qr\n\u03b4pj,iq\n\u22ba. The content-to-position term is calculated in a similar way.\nFinally, we apply a scaling factor of\n1\n?\n3d on \u02dc\nA. The factor is important for stabilizing model\ntraining (Vaswani et al., 2017), especially for large-scale PLMs.\nAlgorithm 1 Disentangled Attention\nInput: Hidden state H, relative distance embedding P , relative distance matrix \u03b4. Content projec-\ntion matrix Wk,c, Wq,c, Wv,c, position projection matrix Wk,r, Wq,r.\n1: Kc \u201c HWk,c, Qc \u201c HWq,c, Vc \u201c HWv,c, Kr \u201c P Wk,r, Qr \u201c P Wq,r\n2: Ac\u00d1c \u201c QcK\u22ba\nc\n3: for i \u201c 0, ..., N \u00b4 1 do\n4:\n\u02dc\nAc\u00d1pri, :s \u201c Qcri, :sK\u22ba\nr\n5: end for\n6: for i \u201c 0, ..., N \u00b4 1 do\n7:\nfor j \u201c 0, ..., N \u00b4 1 do\n8:\nAc\u00d1pri, js \u201c \u02dc\nAc\u00d1pri, \u03b4ri, jss\n9:\nend for\n10: end for\n11: for j \u201c 0, ..., N \u00b4 1 do\n12:\n\u02dc\nAp\u00d1cr:, js \u201c Kcrj, :sQ\u22ba\nr\n13: end for\n14: for j \u201c 0, ..., N \u00b4 1 do\n15:\nfor i \u201c 0, ..., N \u00b4 1 do\n16:\nAp\u00d1cri, js \u201c \u02dc\nAp\u00d1cr\u03b4rj, is, js\n17:\nend for\n18: end for\n19:\n\u02dc\nA \u201c Ac\u00d1c ` Ac\u00d1p ` Ap\u00d1c\n20: Ho \u201c softmaxp\n\u02dc\nA\n?\n3dqVc\nOutput: Ho\n3.1.1\nEFFICIENT IMPLEMENTATION\nFor an input sequence of length N, it requires a space complexity of OpN 2dq (Shaw et al., 2018;\nHuang et al., 2018; Dai et al., 2019) to store the relative position embedding for each token. However,\ntaking content-to-position as an example, we note that since \u03b4pi, jq P r0, 2kq and the embeddings\n4\n", []], "3.1 Disentangled Attention: A Two-Vector Approach to Content and Position Embedding": ["Published as a conference paper at ICLR 2021\nattention weights among words are computed using disentangled matrices on their contents and\nrelative positions, respectively.\n2.2\nMASKED LANGUAGE MODEL\nLarge-scale Transformer-based PLMs are typically pre-trained on large amounts of text to learn\ncontextual word representations using a self-supervision objective, known as Masked Language\nModel (MLM) (Devlin et al., 2019). Speci\ufb01cally, given a sequence X \u201c txiu, we corrupt it into\n\u02dc\nX by masking 15% of its tokens at random and then train a language model parameterized by \u03b8 to\nreconstruct X by predicting the masked tokens \u02dcx conditioned on \u02dc\nX:\nmax\n\u03b8\nlog p\u03b8pX| \u02dc\nXq \u201c max\n\u03b8\n\u00ff\niPC\nlog p\u03b8p\u02dcxi \u201c xi| \u02dc\nXq\n(1)\nwhere C is the index set of the masked tokens in the sequence. The authors of BERT propose to keep\n10% of the masked tokens unchanged, another 10% replaced with randomly picked tokens and the\nrest replaced with the [MASK] token.\n3\nTHE DEBERTA ARCHITECTURE\n3.1\nDISENTANGLED ATTENTION: A TWO-VECTOR APPROACH TO CONTENT AND POSITION\nEMBEDDING\nFor a token at position i in a sequence, we represent it using two vectors, tHiu and tPi|ju, which\nrepresent its content and relative position with the token at position j, respectively. The calculation of\nthe cross attention score between tokens i and j can be decomposed into four components as\nAi,j \u201c tHi, Pi|ju \u02c6 tHj, Pj|iu\u22ba\n\u201c HiH\u22ba\nj ` HiP \u22ba\nj|i ` Pi|jH\u22ba\nj ` Pi|jP \u22ba\nj|i\n(2)\nThat is, the attention weight of a word pair can be computed as a sum of four attention scores\nusing disentangled matrices on their contents and positions as content-to-content, content-to-position,\nposition-to-content, and position-to-position 2.\nExisting approaches to relative position encoding use a separate embedding matrix to compute the\nrelative position bias in computing attention weights (Shaw et al., 2018; Huang et al., 2018). This\nis equivalent to computing the attention weights using only the content-to-content and content-to-\nposition terms in equation 2. We argue that the position-to-content term is also important since the\nattention weight of a word pair depends not only on their contents but on their relative positions,\nwhich can only be fully modeled using both the content-to-position and position-to-content terms.\nSince we use relative position embedding, the position-to-position term does not provide much\nadditional information and is removed from equation 2 in our implementation.\nTaking single-head attention as an example, the standard self-attention operation (Vaswani et al.,\n2017) can be formulated as:\nQ \u201c HWq, K \u201c HWk, V \u201c HWv, A \u201c QK\u22ba\n?\nd\nHo \u201c softmaxpAqV\nwhere H P RN\u02c6d represents the input hidden vectors, Ho P RN\u02c6d the output of self-attention,\nWq, Wk, Wv P Rd\u02c6d the projection matrices, A P RN\u02c6N the attention matrix, N the length of the\ninput sequence, and d the dimension of hidden states.\nDenote k as the maximum relative distance, \u03b4pi, jq P r0, 2kq as the relative distance from token i to\ntoken j, which is de\ufb01ned as:\n\u03b4pi, jq \u201c\n#\n0\nfor\ni \u00b4 j \u010f \u00b4k\n2k \u00b4 1\nfor\ni \u00b4 j \u011b k\ni \u00b4 j ` k\nothers.\n(3)\n2In this sense, our model shares some similarity to Tensor Product Representation (Smolensky, 1990; Schlag\net al., 2019; Chen et al., 2019) where a word is represented using a tensor product of its \ufb01ller (content) vector\nand its role (position) vector.\n3\n", []], "3 The DeBERTa Architecture": ["Published as a conference paper at ICLR 2021\nattention weights among words are computed using disentangled matrices on their contents and\nrelative positions, respectively.\n2.2\nMASKED LANGUAGE MODEL\nLarge-scale Transformer-based PLMs are typically pre-trained on large amounts of text to learn\ncontextual word representations using a self-supervision objective, known as Masked Language\nModel (MLM) (Devlin et al., 2019). Speci\ufb01cally, given a sequence X \u201c txiu, we corrupt it into\n\u02dc\nX by masking 15% of its tokens at random and then train a language model parameterized by \u03b8 to\nreconstruct X by predicting the masked tokens \u02dcx conditioned on \u02dc\nX:\nmax\n\u03b8\nlog p\u03b8pX| \u02dc\nXq \u201c max\n\u03b8\n\u00ff\niPC\nlog p\u03b8p\u02dcxi \u201c xi| \u02dc\nXq\n(1)\nwhere C is the index set of the masked tokens in the sequence. The authors of BERT propose to keep\n10% of the masked tokens unchanged, another 10% replaced with randomly picked tokens and the\nrest replaced with the [MASK] token.\n3\nTHE DEBERTA ARCHITECTURE\n3.1\nDISENTANGLED ATTENTION: A TWO-VECTOR APPROACH TO CONTENT AND POSITION\nEMBEDDING\nFor a token at position i in a sequence, we represent it using two vectors, tHiu and tPi|ju, which\nrepresent its content and relative position with the token at position j, respectively. The calculation of\nthe cross attention score between tokens i and j can be decomposed into four components as\nAi,j \u201c tHi, Pi|ju \u02c6 tHj, Pj|iu\u22ba\n\u201c HiH\u22ba\nj ` HiP \u22ba\nj|i ` Pi|jH\u22ba\nj ` Pi|jP \u22ba\nj|i\n(2)\nThat is, the attention weight of a word pair can be computed as a sum of four attention scores\nusing disentangled matrices on their contents and positions as content-to-content, content-to-position,\nposition-to-content, and position-to-position 2.\nExisting approaches to relative position encoding use a separate embedding matrix to compute the\nrelative position bias in computing attention weights (Shaw et al., 2018; Huang et al., 2018). This\nis equivalent to computing the attention weights using only the content-to-content and content-to-\nposition terms in equation 2. We argue that the position-to-content term is also important since the\nattention weight of a word pair depends not only on their contents but on their relative positions,\nwhich can only be fully modeled using both the content-to-position and position-to-content terms.\nSince we use relative position embedding, the position-to-position term does not provide much\nadditional information and is removed from equation 2 in our implementation.\nTaking single-head attention as an example, the standard self-attention operation (Vaswani et al.,\n2017) can be formulated as:\nQ \u201c HWq, K \u201c HWk, V \u201c HWv, A \u201c QK\u22ba\n?\nd\nHo \u201c softmaxpAqV\nwhere H P RN\u02c6d represents the input hidden vectors, Ho P RN\u02c6d the output of self-attention,\nWq, Wk, Wv P Rd\u02c6d the projection matrices, A P RN\u02c6N the attention matrix, N the length of the\ninput sequence, and d the dimension of hidden states.\nDenote k as the maximum relative distance, \u03b4pi, jq P r0, 2kq as the relative distance from token i to\ntoken j, which is de\ufb01ned as:\n\u03b4pi, jq \u201c\n#\n0\nfor\ni \u00b4 j \u010f \u00b4k\n2k \u00b4 1\nfor\ni \u00b4 j \u011b k\ni \u00b4 j ` k\nothers.\n(3)\n2In this sense, our model shares some similarity to Tensor Product Representation (Smolensky, 1990; Schlag\net al., 2019; Chen et al., 2019) where a word is represented using a tensor product of its \ufb01ller (content) vector\nand its role (position) vector.\n3\n", []], "2.2 Masked Language Model": ["Published as a conference paper at ICLR 2021\nattention weights among words are computed using disentangled matrices on their contents and\nrelative positions, respectively.\n2.2\nMASKED LANGUAGE MODEL\nLarge-scale Transformer-based PLMs are typically pre-trained on large amounts of text to learn\ncontextual word representations using a self-supervision objective, known as Masked Language\nModel (MLM) (Devlin et al., 2019). Speci\ufb01cally, given a sequence X \u201c txiu, we corrupt it into\n\u02dc\nX by masking 15% of its tokens at random and then train a language model parameterized by \u03b8 to\nreconstruct X by predicting the masked tokens \u02dcx conditioned on \u02dc\nX:\nmax\n\u03b8\nlog p\u03b8pX| \u02dc\nXq \u201c max\n\u03b8\n\u00ff\niPC\nlog p\u03b8p\u02dcxi \u201c xi| \u02dc\nXq\n(1)\nwhere C is the index set of the masked tokens in the sequence. The authors of BERT propose to keep\n10% of the masked tokens unchanged, another 10% replaced with randomly picked tokens and the\nrest replaced with the [MASK] token.\n3\nTHE DEBERTA ARCHITECTURE\n3.1\nDISENTANGLED ATTENTION: A TWO-VECTOR APPROACH TO CONTENT AND POSITION\nEMBEDDING\nFor a token at position i in a sequence, we represent it using two vectors, tHiu and tPi|ju, which\nrepresent its content and relative position with the token at position j, respectively. The calculation of\nthe cross attention score between tokens i and j can be decomposed into four components as\nAi,j \u201c tHi, Pi|ju \u02c6 tHj, Pj|iu\u22ba\n\u201c HiH\u22ba\nj ` HiP \u22ba\nj|i ` Pi|jH\u22ba\nj ` Pi|jP \u22ba\nj|i\n(2)\nThat is, the attention weight of a word pair can be computed as a sum of four attention scores\nusing disentangled matrices on their contents and positions as content-to-content, content-to-position,\nposition-to-content, and position-to-position 2.\nExisting approaches to relative position encoding use a separate embedding matrix to compute the\nrelative position bias in computing attention weights (Shaw et al., 2018; Huang et al., 2018). This\nis equivalent to computing the attention weights using only the content-to-content and content-to-\nposition terms in equation 2. We argue that the position-to-content term is also important since the\nattention weight of a word pair depends not only on their contents but on their relative positions,\nwhich can only be fully modeled using both the content-to-position and position-to-content terms.\nSince we use relative position embedding, the position-to-position term does not provide much\nadditional information and is removed from equation 2 in our implementation.\nTaking single-head attention as an example, the standard self-attention operation (Vaswani et al.,\n2017) can be formulated as:\nQ \u201c HWq, K \u201c HWk, V \u201c HWv, A \u201c QK\u22ba\n?\nd\nHo \u201c softmaxpAqV\nwhere H P RN\u02c6d represents the input hidden vectors, Ho P RN\u02c6d the output of self-attention,\nWq, Wk, Wv P Rd\u02c6d the projection matrices, A P RN\u02c6N the attention matrix, N the length of the\ninput sequence, and d the dimension of hidden states.\nDenote k as the maximum relative distance, \u03b4pi, jq P r0, 2kq as the relative distance from token i to\ntoken j, which is de\ufb01ned as:\n\u03b4pi, jq \u201c\n#\n0\nfor\ni \u00b4 j \u010f \u00b4k\n2k \u00b4 1\nfor\ni \u00b4 j \u011b k\ni \u00b4 j ` k\nothers.\n(3)\n2In this sense, our model shares some similarity to Tensor Product Representation (Smolensky, 1990; Schlag\net al., 2019; Chen et al., 2019) where a word is represented using a tensor product of its \ufb01ller (content) vector\nand its role (position) vector.\n3\n", []], "2.1 Transformer": ["Published as a conference paper at ICLR 2021\nIn this paper, we propose a new Transformer-based neural language model DeBERTa (Decoding-\nenhanced BERT with disentangled attention), which improves previous state-of-the-art PLMs using\ntwo novel techniques: a disentangled attention mechanism, and an enhanced mask decoder.\nDisentangled attention.\nUnlike BERT where each word in the input layer is represented using\na vector which is the sum of its word (content) embedding and position embedding, each word in\nDeBERTa is represented using two vectors that encode its content and position, respectively, and the\nattention weights among words are computed using disentangled matrices based on their contents\nand relative positions, respectively. This is motivated by the observation that the attention weight of a\nword pair depends on not only their contents but their relative positions. For example, the dependency\nbetween the words \u201cdeep\u201d and \u201clearning\u201d is much stronger when they occur next to each other than\nwhen they occur in different sentences.\nEnhanced mask decoder.\nLike BERT, DeBERTa is pre-trained using masked language modeling\n(MLM). MLM is a \ufb01ll-in-the-blank task, where a model is taught to use the words surrounding a\nmask token to predict what the masked word should be. DeBERTa uses the content and position\ninformation of the context words for MLM. The disentangled attention mechanism already considers\nthe contents and relative positions of the context words, but not the absolute positions of these words,\nwhich in many cases are crucial for the prediction. Consider the sentence \u201ca new store opened beside\nthe new mall\u201d with the italicized words \u201cstore\u201d and \u201cmall\u201d masked for prediction. Although the local\ncontexts of the two words are similar, they play different syntactic roles in the sentence. (Here, the\nsubject of the sentence is \u201cstore\u201d not \u201cmall,\u201d for example.) These syntactical nuances depend, to a\nlarge degree, upon the words\u2019 absolute positions in the sentence, and so it is important to account\nfor a word\u2019s absolute position in the language modeling process. DeBERTa incorporates absolute\nword position embeddings right before the softmax layer where the model decodes the masked words\nbased on the aggregated contextual embeddings of word contents and positions.\nIn addition, we propose a new virtual adversarial training method for \ufb01ne-tuning PLMs to downstream\nNLP tasks. The method is effective in improving models\u2019 generalization.\nWe show through a comprehensive empirical study that these techniques substantially improve the\nef\ufb01ciency of pre-training and the performance of downstream tasks. In the NLU tasks, compared to\nRoBERTa-Large, a DeBERTa model trained on half the training data performs consistently better\non a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on\nSQuAD v2.0 by +2.3%(88.4% vs. 90.7%), and RACE by +3.6% (83.2% vs. 86.8%). In the NLG\ntasks, DeBERTa reduces the perplexity from 21.6 to 19.5 on the Wikitext-103 dataset. We further\nscale up DeBERTa by pre-training a larger model that consists of 48 Transformer layers with 1.5\nbillion parameters. The single 1.5B-parameter DeBERTa model substantially outperforms T5 with 11\nbillion parameters on the SuperGLUE benchmark (Wang et al., 2019a) by 0.6%(89.3% vs. 89.9%),\nand surpasses the human baseline (89.9 vs. 89.8) for the \ufb01rst time. The ensemble DeBERTa model\nsits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a\ndecent margin (90.3 versus 89.8).\n2\nBACKGROUND\n2.1\nTRANSFORMER\nA Transformer-based language model is composed of stacked Transformer blocks (Vaswani et al.,\n2017). Each block contains a multi-head self-attention layer followed by a fully connected positional\nfeed-forward network. The standard self-attention mechanism lacks a natural way to encode word\nposition information. Thus, existing approaches add a positional bias to each input word embedding\nso that each input word is represented by a vector whose value depends on its content and position.\nThe positional bias can be implemented using absolute position embedding (Vaswani et al., 2017;\nRadford et al., 2019; Devlin et al., 2019) or relative position embedding (Huang et al., 2018; Yang\net al., 2019). It has been shown that relative position representations are more effective for natural\nlanguage understanding and generation tasks (Dai et al., 2019; Shaw et al., 2018). The proposed\ndisentangled attention mechanism differs from all existing approaches in that we represent each\ninput word using two separate vectors that encode a word\u2019s content and position, respectively, and\n2\n", []], "2 Background": ["Published as a conference paper at ICLR 2021\nIn this paper, we propose a new Transformer-based neural language model DeBERTa (Decoding-\nenhanced BERT with disentangled attention), which improves previous state-of-the-art PLMs using\ntwo novel techniques: a disentangled attention mechanism, and an enhanced mask decoder.\nDisentangled attention.\nUnlike BERT where each word in the input layer is represented using\na vector which is the sum of its word (content) embedding and position embedding, each word in\nDeBERTa is represented using two vectors that encode its content and position, respectively, and the\nattention weights among words are computed using disentangled matrices based on their contents\nand relative positions, respectively. This is motivated by the observation that the attention weight of a\nword pair depends on not only their contents but their relative positions. For example, the dependency\nbetween the words \u201cdeep\u201d and \u201clearning\u201d is much stronger when they occur next to each other than\nwhen they occur in different sentences.\nEnhanced mask decoder.\nLike BERT, DeBERTa is pre-trained using masked language modeling\n(MLM). MLM is a \ufb01ll-in-the-blank task, where a model is taught to use the words surrounding a\nmask token to predict what the masked word should be. DeBERTa uses the content and position\ninformation of the context words for MLM. The disentangled attention mechanism already considers\nthe contents and relative positions of the context words, but not the absolute positions of these words,\nwhich in many cases are crucial for the prediction. Consider the sentence \u201ca new store opened beside\nthe new mall\u201d with the italicized words \u201cstore\u201d and \u201cmall\u201d masked for prediction. Although the local\ncontexts of the two words are similar, they play different syntactic roles in the sentence. (Here, the\nsubject of the sentence is \u201cstore\u201d not \u201cmall,\u201d for example.) These syntactical nuances depend, to a\nlarge degree, upon the words\u2019 absolute positions in the sentence, and so it is important to account\nfor a word\u2019s absolute position in the language modeling process. DeBERTa incorporates absolute\nword position embeddings right before the softmax layer where the model decodes the masked words\nbased on the aggregated contextual embeddings of word contents and positions.\nIn addition, we propose a new virtual adversarial training method for \ufb01ne-tuning PLMs to downstream\nNLP tasks. The method is effective in improving models\u2019 generalization.\nWe show through a comprehensive empirical study that these techniques substantially improve the\nef\ufb01ciency of pre-training and the performance of downstream tasks. In the NLU tasks, compared to\nRoBERTa-Large, a DeBERTa model trained on half the training data performs consistently better\non a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on\nSQuAD v2.0 by +2.3%(88.4% vs. 90.7%), and RACE by +3.6% (83.2% vs. 86.8%). In the NLG\ntasks, DeBERTa reduces the perplexity from 21.6 to 19.5 on the Wikitext-103 dataset. We further\nscale up DeBERTa by pre-training a larger model that consists of 48 Transformer layers with 1.5\nbillion parameters. The single 1.5B-parameter DeBERTa model substantially outperforms T5 with 11\nbillion parameters on the SuperGLUE benchmark (Wang et al., 2019a) by 0.6%(89.3% vs. 89.9%),\nand surpasses the human baseline (89.9 vs. 89.8) for the \ufb01rst time. The ensemble DeBERTa model\nsits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a\ndecent margin (90.3 versus 89.8).\n2\nBACKGROUND\n2.1\nTRANSFORMER\nA Transformer-based language model is composed of stacked Transformer blocks (Vaswani et al.,\n2017). Each block contains a multi-head self-attention layer followed by a fully connected positional\nfeed-forward network. The standard self-attention mechanism lacks a natural way to encode word\nposition information. Thus, existing approaches add a positional bias to each input word embedding\nso that each input word is represented by a vector whose value depends on its content and position.\nThe positional bias can be implemented using absolute position embedding (Vaswani et al., 2017;\nRadford et al., 2019; Devlin et al., 2019) or relative position embedding (Huang et al., 2018; Yang\net al., 2019). It has been shown that relative position representations are more effective for natural\nlanguage understanding and generation tasks (Dai et al., 2019; Shaw et al., 2018). The proposed\ndisentangled attention mechanism differs from all existing approaches in that we represent each\ninput word using two separate vectors that encode a word\u2019s content and position, respectively, and\n2\n", []], "1 Introduction": ["Published as a conference paper at ICLR 2021\nDEBERTA: DECODING-ENHANCED BERT WITH DIS-\nENTANGLED ATTENTION\nPengcheng He1, Xiaodong Liu2, Jianfeng Gao2, Weizhu Chen1\n1 Microsoft Dynamics 365 AI\n2 Microsoft Research\n{penhe,xiaodl,jfgao,wzchen}@microsoft.com\nABSTRACT\nRecent progress in pre-trained neural language models has signi\ufb01cantly improved\nthe performance of many natural language processing (NLP) tasks. In this pa-\nper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The \ufb01rst is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and position,\nrespectively, and the attention weights among words are computed using disen-\ntangled matrices on their contents and relative positions, respectively. Second,\nan enhanced mask decoder is used to incorporate absolute positions in the de-\ncoding layer to predict the masked tokens in model pre-training. In addition, a\nnew virtual adversarial training method is used for \ufb01ne-tuning to improve models\u2019\ngeneralization. We show that these techniques signi\ufb01cantly improve the ef\ufb01ciency\nof model pre-training and the performance of both natural language understand\n(NLU) and natural langauge generation (NLG) downstream tasks. Compared\nto RoBERTa-Large, a DeBERTa model trained on half of the training data per-\nforms consistently better on a wide range of NLP tasks, achieving improvements\non MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs.\n90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa\nby training a larger version that consists of 48 Transform layers with 1.5 bil-\nlion parameters. The signi\ufb01cant performance boost makes the single DeBERTa\nmodel surpass the human performance on the SuperGLUE benchmark (Wang et al.,\n2019a) for the \ufb01rst time in terms of macro-average score (89.9 versus 89.8), and\nthe ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of Jan-\nuary 6, 2021, outperforming the human baseline by a decent margin (90.3 versus\n89.8). The pre-trained DeBERTa models and the source code were released at:\nhttps://github.com/microsoft/DeBERTa1.\n1\nINTRODUCTION\nThe Transformer has become the most effective neural network architecture for neural language\nmodeling. Unlike recurrent neural networks (RNNs) that process text in sequence, Transformers\napply self-attention to compute in parallel every word from the input text an attention weight that\ngauges the in\ufb02uence each word has on another, thus allowing for much more parallelization than\nRNNs for large-scale model training (Vaswani et al., 2017). Since 2018, we have seen the rise of a\nset of large-scale Transformer-based Pre-trained Language Models (PLMs), such as GPT (Radford\net al., 2019; Brown et al., 2020), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), XLNet\n(Yang et al., 2019), UniLM (Dong et al., 2019), ELECTRA (Clark et al., 2020), T5 (Raffel et al.,\n2020), ALUM (Liu et al., 2020), StructBERT (Wang et al., 2019c) and ERINE (Sun et al., 2019) .\nThese PLMs have been \ufb01ne-tuned using task-speci\ufb01c labels and created new state of the art in many\ndownstream natural language processing (NLP) tasks (Liu et al., 2019b; Minaee et al., 2020; Jiang\net al., 2020; He et al., 2019a;b; Shen et al., 2020).\n1Our code and models are also available at HuggingFace Transformers: https://github.com/\nhuggingface/transformers, https://huggingface.co/models?filter=deberta\n1\narXiv:2006.03654v6  [cs.CL]  6 Oct 2021\n", []]}