{"References": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 20 of 27\ntask-specific data reduces the reliability on complex, sin-\ngle-use deep learning models and improve real-world per-\nformance by retraining on challenging urban scenarios. As \naforementioned, domain adaptation, zero-shot learning, few-\nshot learning, and foundation models are expected transfer \nlearning areas that serve this purpose.\nThe results of unsupervised methods like in Pawar and \nAttar (2021) can be further improved by online learning \nin crowded and challenging scenarios after deployment \non embedded hardware, as there is an unlimited supply of \nunlabeled data. The lack of theoretical performance analy-\nsis regarding the upper bound on false alarm rate in com-\nplex environments is discussed as an important aspect of \ndeep learning methods for anomaly detection in Doshi and \nYilmaz (2021). Future research is recommended to include \nthis analysis as well. It is hard to imagine complete reli-\nance on surveillance cameras for robust, widespread, and \neconomical traffic anomaly detection. The method in Parsa \net\u00a0al. (2020) includes traffic, network, demographic, land \nuse, and weather data sources to detect traffic. Such ideas \ncan be used in tandem with computer vision applications for \nbetter overall performance.\nFuture directions in the application of edge computing \nin ITS will consider multi-source data fusion along with \nonline learning (Xie et\u00a0al. 2021). Many factors like unseen \nshapes of vehicles, new surrounding environments, variable \ntraffic density, and rare events can be too challenging for DL \nmodels (Ferdowsi et\u00a0al. 2019). This new data could be used \nfor online training of the system. Traditional applications \ncan be extended using edge computing and IoV/IoT frame-\nworks. Vehicle re-identification from video is emerging as \nthe most robust solution to occlusion (Zhao et\u00a0al. 2021a). \nHowever, the inclusion of more spatio-temporal informa-\ntion for learning leads to greater memory and computational \nusage. Tracklets from one camera view can be matched with \nother views at different points in time using known features. \nInstead of using a fixed window, adaptive feature aggrega-\ntion based on similarity and quality, can be generalized to \nmany multi-object tracking tasks (Qian et\u00a0al. 2020).\nTransformers are good at learning dynamic interactions \nbetween heterogenous agents which will be particularly use-\nful in crowded urban environments for detection and trajec-\ntory prediction. They can also be used for the detection of \nanomalies and the prediction of potentially hazardous situa-\ntions like collisions in a multi-user heterogeneous scenario.\nConclusions\nIn real-world scenarios, most of the DL computer vision \nmethods suffer from severe performance degradation when \nfacing different challenges. In this paper, we review the spe-\ncific challenges for data, models, and complex environments \nin ITS and autonomous driving. Many related deep learn-\ning-based computer vision methods are reviewed, summa-\nrized, compared, and discussed. Furthermore, a number of \nrepresentative deep learning-based applications of ITS and \nautonomous driving are summarized and analyzed. Based \non our analysis and review, several potential future research \ndirections are provided. We expect that this paper could pro-\nvide useful research insights and inspire more progress in \nthe community.\nAcknowledgements\u2002 This work was supported in part by NSF 2215388.\nAuthor Contributions\u2002 Equal contribution from Ph.D. students TA and \nJL, supervised by professors HY and RK. Editorial inputs and iterative \nreviews by RLC and YL.\nFunding\u2002 NSF 2215388.\nAvailability of Data and Materials\u2002 Not applicable.\nDeclarations\u2002\nConflict of Interest\u2002 The authors declare that they have no conflict of \ninterest.\nEthical Approval\u2002 Not applicable.\nReferences\nAboah A, Shoman M, Mandal V, Davami S, Adu-Gyamfi Y, Sharma A \n(2021) A vision-based system for traffic anomaly detection using \ndeep learning and decision trees. In: CVPR\nAboah A, Boeding M, Adu-Gyamfi Y (2022) Mobile sensing for multi-\npurpose applications in transportation. J Big Data Analyt Transp \n4(2\u20133):171\u2013183\nAflalo E, Du M, Tseng S-Y, Liu Y, Wu C, Duan N, Lal V (2022) \nVl-interpret: an interactive visualization tool for interpreting \nvision-language transformers. In: Proceedings of the IEEE/\nCVF Conference on computer vision and pattern recognition, \npp 21406\u201321415\nAlbiol A, Albiol A, Mossi JM (2011) Video-based traffic queue length \nestimation, pp 1928\u20131932 . https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bICCVW.\u200b\n2011.\u200b61304\u200b84\nAmini A, Gilitschenski I, Phillips J, Moseyko J, Banerjee R, Karaman \nS, Rus D (2020) Learning robust control policies for end-to-end \nautonomous driving from data-driven simulation. IEEE Robot \nAutom Lett 5(2):1143\u20131150. https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bLRA.\u200b2020.\u200b\n29664\u200b14\nAmini A, Wang T-H, Gilitschenski I, Schwarting W, Liu Z, Han S, \nKaraman S, Rus D (2021) VISTA 2.0: an open, data-driven simu-\nlator for multimodal sensing and policy learning for autonomous \nvehicles. arXiv. https://\u200bdoi.\u200borg/\u200b10.\u200b48550/\u200bARXIV.\u200b2111.\u200b12083. \nhttps://\u200barxiv.\u200borg/\u200babs/\u200b2111.\u200b12083\nAnastasiu DC, Gaul J, Vazhaeparambil M, Gaba M, Sharma P (2020) \nEfficient city-wide multi-class multi-movement vehicle counting: \na survey. J Big Data Analyt Transp 2:235\u2013250\nArabi S, Haghighat A, Sharma A (2020) A deep-learning-based com-\nputer vision solution for construction vehicle detection. Comput-\nAided Civ Infrastruct Eng 35(7):753\u2013767\nAtakishiyev S, Salameh M, Yao H, Goebel R (2021) Towards safe, \nexplainable, and regulated autonomous driving. arXiv. https://\u200b\n", []], "Acknowledgements ": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 20 of 27\ntask-specific data reduces the reliability on complex, sin-\ngle-use deep learning models and improve real-world per-\nformance by retraining on challenging urban scenarios. As \naforementioned, domain adaptation, zero-shot learning, few-\nshot learning, and foundation models are expected transfer \nlearning areas that serve this purpose.\nThe results of unsupervised methods like in Pawar and \nAttar (2021) can be further improved by online learning \nin crowded and challenging scenarios after deployment \non embedded hardware, as there is an unlimited supply of \nunlabeled data. The lack of theoretical performance analy-\nsis regarding the upper bound on false alarm rate in com-\nplex environments is discussed as an important aspect of \ndeep learning methods for anomaly detection in Doshi and \nYilmaz (2021). Future research is recommended to include \nthis analysis as well. It is hard to imagine complete reli-\nance on surveillance cameras for robust, widespread, and \neconomical traffic anomaly detection. The method in Parsa \net\u00a0al. (2020) includes traffic, network, demographic, land \nuse, and weather data sources to detect traffic. Such ideas \ncan be used in tandem with computer vision applications for \nbetter overall performance.\nFuture directions in the application of edge computing \nin ITS will consider multi-source data fusion along with \nonline learning (Xie et\u00a0al. 2021). Many factors like unseen \nshapes of vehicles, new surrounding environments, variable \ntraffic density, and rare events can be too challenging for DL \nmodels (Ferdowsi et\u00a0al. 2019). This new data could be used \nfor online training of the system. Traditional applications \ncan be extended using edge computing and IoV/IoT frame-\nworks. Vehicle re-identification from video is emerging as \nthe most robust solution to occlusion (Zhao et\u00a0al. 2021a). \nHowever, the inclusion of more spatio-temporal informa-\ntion for learning leads to greater memory and computational \nusage. Tracklets from one camera view can be matched with \nother views at different points in time using known features. \nInstead of using a fixed window, adaptive feature aggrega-\ntion based on similarity and quality, can be generalized to \nmany multi-object tracking tasks (Qian et\u00a0al. 2020).\nTransformers are good at learning dynamic interactions \nbetween heterogenous agents which will be particularly use-\nful in crowded urban environments for detection and trajec-\ntory prediction. They can also be used for the detection of \nanomalies and the prediction of potentially hazardous situa-\ntions like collisions in a multi-user heterogeneous scenario.\nConclusions\nIn real-world scenarios, most of the DL computer vision \nmethods suffer from severe performance degradation when \nfacing different challenges. In this paper, we review the spe-\ncific challenges for data, models, and complex environments \nin ITS and autonomous driving. Many related deep learn-\ning-based computer vision methods are reviewed, summa-\nrized, compared, and discussed. Furthermore, a number of \nrepresentative deep learning-based applications of ITS and \nautonomous driving are summarized and analyzed. Based \non our analysis and review, several potential future research \ndirections are provided. We expect that this paper could pro-\nvide useful research insights and inspire more progress in \nthe community.\nAcknowledgements\u2002 This work was supported in part by NSF 2215388.\nAuthor Contributions\u2002 Equal contribution from Ph.D. students TA and \nJL, supervised by professors HY and RK. Editorial inputs and iterative \nreviews by RLC and YL.\nFunding\u2002 NSF 2215388.\nAvailability of Data and Materials\u2002 Not applicable.\nDeclarations\u2002\nConflict of Interest\u2002 The authors declare that they have no conflict of \ninterest.\nEthical Approval\u2002 Not applicable.\nReferences\nAboah A, Shoman M, Mandal V, Davami S, Adu-Gyamfi Y, Sharma A \n(2021) A vision-based system for traffic anomaly detection using \ndeep learning and decision trees. In: CVPR\nAboah A, Boeding M, Adu-Gyamfi Y (2022) Mobile sensing for multi-\npurpose applications in transportation. J Big Data Analyt Transp \n4(2\u20133):171\u2013183\nAflalo E, Du M, Tseng S-Y, Liu Y, Wu C, Duan N, Lal V (2022) \nVl-interpret: an interactive visualization tool for interpreting \nvision-language transformers. In: Proceedings of the IEEE/\nCVF Conference on computer vision and pattern recognition, \npp 21406\u201321415\nAlbiol A, Albiol A, Mossi JM (2011) Video-based traffic queue length \nestimation, pp 1928\u20131932 . https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bICCVW.\u200b\n2011.\u200b61304\u200b84\nAmini A, Gilitschenski I, Phillips J, Moseyko J, Banerjee R, Karaman \nS, Rus D (2020) Learning robust control policies for end-to-end \nautonomous driving from data-driven simulation. IEEE Robot \nAutom Lett 5(2):1143\u20131150. https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bLRA.\u200b2020.\u200b\n29664\u200b14\nAmini A, Wang T-H, Gilitschenski I, Schwarting W, Liu Z, Han S, \nKaraman S, Rus D (2021) VISTA 2.0: an open, data-driven simu-\nlator for multimodal sensing and policy learning for autonomous \nvehicles. arXiv. https://\u200bdoi.\u200borg/\u200b10.\u200b48550/\u200bARXIV.\u200b2111.\u200b12083. \nhttps://\u200barxiv.\u200borg/\u200babs/\u200b2111.\u200b12083\nAnastasiu DC, Gaul J, Vazhaeparambil M, Gaba M, Sharma P (2020) \nEfficient city-wide multi-class multi-movement vehicle counting: \na survey. J Big Data Analyt Transp 2:235\u2013250\nArabi S, Haghighat A, Sharma A (2020) A deep-learning-based com-\nputer vision solution for construction vehicle detection. Comput-\nAided Civ Infrastruct Eng 35(7):753\u2013767\nAtakishiyev S, Salameh M, Yao H, Goebel R (2021) Towards safe, \nexplainable, and regulated autonomous driving. arXiv. https://\u200b\n", []], "Conclusions": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 20 of 27\ntask-specific data reduces the reliability on complex, sin-\ngle-use deep learning models and improve real-world per-\nformance by retraining on challenging urban scenarios. As \naforementioned, domain adaptation, zero-shot learning, few-\nshot learning, and foundation models are expected transfer \nlearning areas that serve this purpose.\nThe results of unsupervised methods like in Pawar and \nAttar (2021) can be further improved by online learning \nin crowded and challenging scenarios after deployment \non embedded hardware, as there is an unlimited supply of \nunlabeled data. The lack of theoretical performance analy-\nsis regarding the upper bound on false alarm rate in com-\nplex environments is discussed as an important aspect of \ndeep learning methods for anomaly detection in Doshi and \nYilmaz (2021). Future research is recommended to include \nthis analysis as well. It is hard to imagine complete reli-\nance on surveillance cameras for robust, widespread, and \neconomical traffic anomaly detection. The method in Parsa \net\u00a0al. (2020) includes traffic, network, demographic, land \nuse, and weather data sources to detect traffic. Such ideas \ncan be used in tandem with computer vision applications for \nbetter overall performance.\nFuture directions in the application of edge computing \nin ITS will consider multi-source data fusion along with \nonline learning (Xie et\u00a0al. 2021). Many factors like unseen \nshapes of vehicles, new surrounding environments, variable \ntraffic density, and rare events can be too challenging for DL \nmodels (Ferdowsi et\u00a0al. 2019). This new data could be used \nfor online training of the system. Traditional applications \ncan be extended using edge computing and IoV/IoT frame-\nworks. Vehicle re-identification from video is emerging as \nthe most robust solution to occlusion (Zhao et\u00a0al. 2021a). \nHowever, the inclusion of more spatio-temporal informa-\ntion for learning leads to greater memory and computational \nusage. Tracklets from one camera view can be matched with \nother views at different points in time using known features. \nInstead of using a fixed window, adaptive feature aggrega-\ntion based on similarity and quality, can be generalized to \nmany multi-object tracking tasks (Qian et\u00a0al. 2020).\nTransformers are good at learning dynamic interactions \nbetween heterogenous agents which will be particularly use-\nful in crowded urban environments for detection and trajec-\ntory prediction. They can also be used for the detection of \nanomalies and the prediction of potentially hazardous situa-\ntions like collisions in a multi-user heterogeneous scenario.\nConclusions\nIn real-world scenarios, most of the DL computer vision \nmethods suffer from severe performance degradation when \nfacing different challenges. In this paper, we review the spe-\ncific challenges for data, models, and complex environments \nin ITS and autonomous driving. Many related deep learn-\ning-based computer vision methods are reviewed, summa-\nrized, compared, and discussed. Furthermore, a number of \nrepresentative deep learning-based applications of ITS and \nautonomous driving are summarized and analyzed. Based \non our analysis and review, several potential future research \ndirections are provided. We expect that this paper could pro-\nvide useful research insights and inspire more progress in \nthe community.\nAcknowledgements\u2002 This work was supported in part by NSF 2215388.\nAuthor Contributions\u2002 Equal contribution from Ph.D. students TA and \nJL, supervised by professors HY and RK. Editorial inputs and iterative \nreviews by RLC and YL.\nFunding\u2002 NSF 2215388.\nAvailability of Data and Materials\u2002 Not applicable.\nDeclarations\u2002\nConflict of Interest\u2002 The authors declare that they have no conflict of \ninterest.\nEthical Approval\u2002 Not applicable.\nReferences\nAboah A, Shoman M, Mandal V, Davami S, Adu-Gyamfi Y, Sharma A \n(2021) A vision-based system for traffic anomaly detection using \ndeep learning and decision trees. In: CVPR\nAboah A, Boeding M, Adu-Gyamfi Y (2022) Mobile sensing for multi-\npurpose applications in transportation. J Big Data Analyt Transp \n4(2\u20133):171\u2013183\nAflalo E, Du M, Tseng S-Y, Liu Y, Wu C, Duan N, Lal V (2022) \nVl-interpret: an interactive visualization tool for interpreting \nvision-language transformers. In: Proceedings of the IEEE/\nCVF Conference on computer vision and pattern recognition, \npp 21406\u201321415\nAlbiol A, Albiol A, Mossi JM (2011) Video-based traffic queue length \nestimation, pp 1928\u20131932 . https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bICCVW.\u200b\n2011.\u200b61304\u200b84\nAmini A, Gilitschenski I, Phillips J, Moseyko J, Banerjee R, Karaman \nS, Rus D (2020) Learning robust control policies for end-to-end \nautonomous driving from data-driven simulation. IEEE Robot \nAutom Lett 5(2):1143\u20131150. https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bLRA.\u200b2020.\u200b\n29664\u200b14\nAmini A, Wang T-H, Gilitschenski I, Schwarting W, Liu Z, Han S, \nKaraman S, Rus D (2021) VISTA 2.0: an open, data-driven simu-\nlator for multimodal sensing and policy learning for autonomous \nvehicles. arXiv. https://\u200bdoi.\u200borg/\u200b10.\u200b48550/\u200bARXIV.\u200b2111.\u200b12083. \nhttps://\u200barxiv.\u200borg/\u200babs/\u200b2111.\u200b12083\nAnastasiu DC, Gaul J, Vazhaeparambil M, Gaba M, Sharma P (2020) \nEfficient city-wide multi-class multi-movement vehicle counting: \na survey. J Big Data Analyt Transp 2:235\u2013250\nArabi S, Haghighat A, Sharma A (2020) A deep-learning-based com-\nputer vision solution for construction vehicle detection. Comput-\nAided Civ Infrastruct Eng 35(7):753\u2013767\nAtakishiyev S, Salameh M, Yao H, Goebel R (2021) Towards safe, \nexplainable, and regulated autonomous driving. arXiv. https://\u200b\n", []], "For Solving Complex Traffic Environment Challenges": ["Data Science for Transportation (2024) 6:1\t\nPage 19 of 27\u2003\n1\ncontrol (Amini et\u00a0al. 2020). Domain adaptation techniques \nare expected to be further extended to utilize synthetic data \nand conveniently collected data.\nSub-fields in transfer learning, especially few-shot learn-\ning and zero-shot learning, will be extensively applied with \nexpert knowledge to address the lack of data challenges, \nsuch as corner case recognition in ITS and AD. Likewise, \nnew unsupervised learning and semi-supervised learning \nmodels are expected in the general field of real-world com-\nputer vision. Future work in vision transformer explain-\nability will allow for more comprehensive insights based \non aggregated metrics over multiple samples (Aflalo et\u00a0al. \n2022). Interpretability research is also expected to evaluate \ndifferences between model-based and model-free reinforce-\nment learning approaches (Atakishiyev et\u00a0al. 2021).\nData decentralization is a well-recognized trend in ITS. \nTo address issues like data privacy, large-scale data pro-\ncessing, and efficiency, crowdsensing (Ning et\u00a0al. 2021) \nand federated learning on vision tasks (Liu et\u00a0al. 2020b) \nare unavoidable future directions in ITS and AD. Addition-\nally, instead of the traditional way of training a single model \nfor a single task, multiple downstream tasks learning with \na generalized foundation model, e.g., Florence Yuan et\u00a0al. \n(2021), is a promising trend to deal with various data chal-\nlenges. Another mechanism is data processing parallelism in \nITS coupled with edge computing for multi-task (e.g., traffic \nsurveillance and road surveillance) learning (Ke et\u00a0al. 2022).\nFor Solving Model Challenges\nDeep learning models are trained until they achieve good \naccuracy, but real-world testing often reveals weaknesses \nin edge cases and complex environmental conditions. There \nis a need for online learning for such models to continue to \nimprove and adapt to real-world scenarios otherwise they \ncannot be of practical use. If online training is not possi-\nble due to a lack of live feedback on the correctness of the \npredictions, the performance must be analyzed periodically \nwith real data stored and labeled by humans. This can serve \nas a sort of iterative feedback loop, where the model does not \nneed to be significantly changed, just incrementally retrained \nbased on the inputs it finds most challenging. One possible \nway to partially automate this would be to have multiple \ndifferent redundant architectures using the same input data \nto make predictions along with confidence scores. If the out-\nputs do not agree, or if the confidence scores are low for a \ncertain output, that data point can be manually labeled and \nadded to the training set for the next training iteration.\nComplex deep learning models deployed to edge devices \nneed to be more efficient through methods such as prun-\ning (Han et\u00a0al. 2015). Simple pruning methods can improve \nCNN performance by over 30% (Li et\u00a0al. 2016b). Depending \non the specific architecture, the models may also be split \ninto different functional blocks deployed on separate edge \nunits to minimize bandwidth and computation time (Sufian \net\u00a0al. 2021). A foreseeable future stage of edge AI is \u201cmodel \ntraining and inference both on the edge,\u201d without the par-\nticipation of cloud datacenters.\nIn recent years much work has been done towards \nexplainable AI, especially in computer vision. CNNs have \nbeen approached with three explainability methods: gradi-\nent-based saliency maps, Class Activation Mapping, and \nExcitation Backpropagation (Zhang et\u00a0al. 2018b). These \nmethods were extended for graph convolutional networks \nin Pope et\u00a0al. (2019), pointing out patterns in the input that \ncorrespond with the classification. Generic solutions for \nexplainability have been presented in Chefer et\u00a0al. (2021) for \nboth self-attention and co-attention transformer networks. \nWhile it is not straightforward to apply these methods to \ntransportation applications, some efforts have been made to \nunderstand deep spatio-temporal neural networks dealing \nwith video object segmentation and action recognition quan-\ntifying the static and dynamic information in the network \nand giving insight into the models and highlighting biases \nlearned from datasets (Kowal et\u00a0al. 2022).\nCooperative sensing model development is a necessary \nfuture direction for better perception in 3D, in order to miti-\ngate the effects of occlusion, noise, and sensor faults. V2X \nnetworks and vision transformers have been used for robust \ncooperative perception, which can support sensing in con-\nnected autonomous vehicle platforms (Xu et\u00a0al. 2021, 2022). \nConnected autonomous vehicles will also host other deep-\nlearning models that can learn from new data in a distributed \nmanner. Consensus-driven distributed perception is expected \nto make use of future network technologies like 6G V2X, \nresulting in low-latency model training that can enable true \nlevel 5 autonomous vehicles (Barbieri et\u00a0al. 2022).\nFor Solving Complex Traffic Environment Challenges\nMultimodal sensing and cooperative perception are neces-\nsary future avenues of practical research. Different modali-\nties like video, LiDAR, and audio can be used in combina-\ntions to improve the performance of methods purely based \non vision. Audio is especially useful for detecting anoma-\nlies earlier among pedestrians such as fights or commotions, \nand for vehicles in crowded intersections where the visual \nchaos may not immediately reveal problems like mechanical \nfaults, or minor accidents. Cooperative perception will allow \nmultiple sensor views of the same environment from differ-\nent vehicles to build a common picture that contains more \ninformation than any single agent can perceive thus solving \nproblems of occlusion and illumination.\nThere is an increasing trend of using transfer learning \nto improve model performance in real-world tasks. Initially \ntraining the model on synthetic data and fine-tuning with \n", []], "For Solving Model Challenges": ["Data Science for Transportation (2024) 6:1\t\nPage 19 of 27\u2003\n1\ncontrol (Amini et\u00a0al. 2020). Domain adaptation techniques \nare expected to be further extended to utilize synthetic data \nand conveniently collected data.\nSub-fields in transfer learning, especially few-shot learn-\ning and zero-shot learning, will be extensively applied with \nexpert knowledge to address the lack of data challenges, \nsuch as corner case recognition in ITS and AD. Likewise, \nnew unsupervised learning and semi-supervised learning \nmodels are expected in the general field of real-world com-\nputer vision. Future work in vision transformer explain-\nability will allow for more comprehensive insights based \non aggregated metrics over multiple samples (Aflalo et\u00a0al. \n2022). Interpretability research is also expected to evaluate \ndifferences between model-based and model-free reinforce-\nment learning approaches (Atakishiyev et\u00a0al. 2021).\nData decentralization is a well-recognized trend in ITS. \nTo address issues like data privacy, large-scale data pro-\ncessing, and efficiency, crowdsensing (Ning et\u00a0al. 2021) \nand federated learning on vision tasks (Liu et\u00a0al. 2020b) \nare unavoidable future directions in ITS and AD. Addition-\nally, instead of the traditional way of training a single model \nfor a single task, multiple downstream tasks learning with \na generalized foundation model, e.g., Florence Yuan et\u00a0al. \n(2021), is a promising trend to deal with various data chal-\nlenges. Another mechanism is data processing parallelism in \nITS coupled with edge computing for multi-task (e.g., traffic \nsurveillance and road surveillance) learning (Ke et\u00a0al. 2022).\nFor Solving Model Challenges\nDeep learning models are trained until they achieve good \naccuracy, but real-world testing often reveals weaknesses \nin edge cases and complex environmental conditions. There \nis a need for online learning for such models to continue to \nimprove and adapt to real-world scenarios otherwise they \ncannot be of practical use. If online training is not possi-\nble due to a lack of live feedback on the correctness of the \npredictions, the performance must be analyzed periodically \nwith real data stored and labeled by humans. This can serve \nas a sort of iterative feedback loop, where the model does not \nneed to be significantly changed, just incrementally retrained \nbased on the inputs it finds most challenging. One possible \nway to partially automate this would be to have multiple \ndifferent redundant architectures using the same input data \nto make predictions along with confidence scores. If the out-\nputs do not agree, or if the confidence scores are low for a \ncertain output, that data point can be manually labeled and \nadded to the training set for the next training iteration.\nComplex deep learning models deployed to edge devices \nneed to be more efficient through methods such as prun-\ning (Han et\u00a0al. 2015). Simple pruning methods can improve \nCNN performance by over 30% (Li et\u00a0al. 2016b). Depending \non the specific architecture, the models may also be split \ninto different functional blocks deployed on separate edge \nunits to minimize bandwidth and computation time (Sufian \net\u00a0al. 2021). A foreseeable future stage of edge AI is \u201cmodel \ntraining and inference both on the edge,\u201d without the par-\nticipation of cloud datacenters.\nIn recent years much work has been done towards \nexplainable AI, especially in computer vision. CNNs have \nbeen approached with three explainability methods: gradi-\nent-based saliency maps, Class Activation Mapping, and \nExcitation Backpropagation (Zhang et\u00a0al. 2018b). These \nmethods were extended for graph convolutional networks \nin Pope et\u00a0al. (2019), pointing out patterns in the input that \ncorrespond with the classification. Generic solutions for \nexplainability have been presented in Chefer et\u00a0al. (2021) for \nboth self-attention and co-attention transformer networks. \nWhile it is not straightforward to apply these methods to \ntransportation applications, some efforts have been made to \nunderstand deep spatio-temporal neural networks dealing \nwith video object segmentation and action recognition quan-\ntifying the static and dynamic information in the network \nand giving insight into the models and highlighting biases \nlearned from datasets (Kowal et\u00a0al. 2022).\nCooperative sensing model development is a necessary \nfuture direction for better perception in 3D, in order to miti-\ngate the effects of occlusion, noise, and sensor faults. V2X \nnetworks and vision transformers have been used for robust \ncooperative perception, which can support sensing in con-\nnected autonomous vehicle platforms (Xu et\u00a0al. 2021, 2022). \nConnected autonomous vehicles will also host other deep-\nlearning models that can learn from new data in a distributed \nmanner. Consensus-driven distributed perception is expected \nto make use of future network technologies like 6G V2X, \nresulting in low-latency model training that can enable true \nlevel 5 autonomous vehicles (Barbieri et\u00a0al. 2022).\nFor Solving Complex Traffic Environment Challenges\nMultimodal sensing and cooperative perception are neces-\nsary future avenues of practical research. Different modali-\nties like video, LiDAR, and audio can be used in combina-\ntions to improve the performance of methods purely based \non vision. Audio is especially useful for detecting anoma-\nlies earlier among pedestrians such as fights or commotions, \nand for vehicles in crowded intersections where the visual \nchaos may not immediately reveal problems like mechanical \nfaults, or minor accidents. Cooperative perception will allow \nmultiple sensor views of the same environment from differ-\nent vehicles to build a common picture that contains more \ninformation than any single agent can perceive thus solving \nproblems of occlusion and illumination.\nThere is an increasing trend of using transfer learning \nto improve model performance in real-world tasks. Initially \ntraining the model on synthetic data and fine-tuning with \n", []], "For Solving Data Challenges": ["Data Science for Transportation (2024) 6:1\t\nPage 17 of 27\u2003\n1\ntested on online datasets and on real cars and buses. The \ndetected events, along with CAN bus messages were used \nto filter irrelevant data, saving bandwidth for data collection. \nA practical deployment of parking surveillance using edge-\ncloud computing was presented in Ke et\u00a0al. (2021), the edge \ndevice performs detection and transmits the bounding box \nand object types to the server, which uses this information \nfor labeling and tracking. A different approach by Bura et\u00a0al. \n(2018) focused on vehicle tracking from top view cameras \nand number plate recognition from ground-level cameras \nfor real-time occupancy information and to automatically \ncharge a vehicle for the time it was parked. Large-scale traf-\nfic monitoring using computer vision and edge computing \nwas detailed in Liu et\u00a0al. (2021a) where edge nodes close \nto surveillance cameras can process low-resolution videos \nto monitor traffic, detect congestion, and detect speed if the \navailable bandwidth is low. If high bandwidth to the server \nis available, high-quality video will be sent for similar pro-\ncessing. Edge computing for vehicle detection is examined \nin Wan et\u00a0al. (2022). The algorithm divides the traffic video \ninto segments of interest and then uses YOLOv3 for vehicle \ndetection in real-time on the edge node, and the extracted \nclips are used as training data for the edge server.\nCurrent Methods to\u00a0Overcome Challenges\nOne problem with large-scale DL is that the huge quantity of \ndata produced cannot be sent to a cloud computer for train-\ning. Federated learning (Kone\u010dn\u00fd et\u00a0al. 2015) has emerged \nas a solution to this problem, especially considering the \nheterogeneous data sources, bandwidth, and privacy issues \n(Zhou et\u00a0al. 2021). Training can be performed on edge nodes \nor edge servers, with the results being sent to the cloud to \naggregate in the shared deep-learning model (Zhang and \nLetaief 2020). Federated learning is also robust to failure \nof individual edge nodes (Kairouz et\u00a0al. 2019). Concerns \nof bandwidth, data privacy, and power requirements are \naddressed in Song et\u00a0al. (2018) by transferring only inferred \ndata from edge nodes to the cloud, in the form of incremen-\ntal and unsupervised learning. In general, the processing \nof data on the edge to reduce bandwidth has the pleasant \nside effect of anonymizing the transmitted data (Barth\u00e9lemy \net\u00a0al. 2019). Another effort to reduce bandwidth require-\nments employs spectral clustering compression performed \non spatio-temporal features needed for traffic flow prediction \n(Chen et\u00a0al. 2021a).\nDeep learning models cannot be directly exported to \nmobile edge nodes, as they are usually too computationally \nintensive. Direct adaptation for vehicle counting resulted \nin 1\u20134 fps for models in AI city challenge if they ran at \nall (Anastasiu et\u00a0al. 2020). Neural network pruning both in \nterms of storage and computation was introduced in Han \net\u00a0al. (2015), while implementation of the resulting sparse \nnetwork on hardware is discussed in Zhang et\u00a0al. (2016a), \nachieving multiple orders of magnitude increase in effi-\nciency. A general lightweight CNN model was developed \nfor mobile edge units in Zhou et\u00a0al. (2019), matching or \noutperforming AlexNet and VGG-16 while being a fraction \nof the size and computation cost. Edge computing-based \ntraffic flow detection using deep learning was deployed by \nChen et\u00a0al. (2021b) where YOLOv3 was trained and pruned, \nalong with DeepSORT, to be deployed on the edge device \nfor real-time performance. A thorough review of deploying \ncompact DNNs on low-power edge computers for IoT appli-\ncations can be found in Zhang et\u00a0al. (2021). They note that \nthe diversity and quantity of DNN applications require an \nautomated method for model compression beyond traditional \npruning techniques.\nFuture Directions\nFor Solving Data Challenges\nWhile a large quantity of data is essential for training deep \nlearning models, often the quality is the limiting factor in \ntraining performance. Data curation is a necessary process to \ninclude edge cases and train the model on representative data \nfrom the real world. Labeling vision data, especially in com-\nplex urban environments is a labor-intensive task performed \nby humans. It can be sped up by first using existing object \ndetection or segmentation algorithms based on the relevant \ntask to automatically label the data. Then this can be fur-\nther checked by humans to eliminate errors by the machine, \nthus creating a useful labeled dataset. This approach has \ngreatly improved the quality of naturalistic driving datasets \n(Miao et\u00a0al. 2022). There is also a need for datasets that \ninclude multiple sensors from different views for training \ncooperative perception algorithms. Collecting such data is \nbound to be challenging because of hardware requirements \nand synchronization issues but it is possible to achieve with \nconnected vehicles and instrumented intersections similar to \nthe configuration that will be deployed. Crowd-sourcing via \nsmartphone apps is also a viable method of producing high-\nquality reliable data (Aboah et\u00a0al. 2022). Some examples of \nuseful datasets are collected in Table\u00a02.\nThe problems associated with poor quality or viewing \nangle of real-world cameras can be mitigated by using real-\nistic CCTV benchmarks and datasets that include a wide \nvariety of surveillance footage, including synthetic video \n(Revaud and Humenberger 2021). Data-driven simulators \nlike Amini et\u00a0al. (2021) use high-fidelity datasets to simu-\nlate cameras and LiDAR, which can be used to train DL \nmodels with data that is hard to capture in the real world \n(Azfar et\u00a0al. 2022). Such an approach has shown promise in \nend-to-end reinforcement learning of autonomous vehicle \n", []], "Future Directions": ["Data Science for Transportation (2024) 6:1\t\nPage 17 of 27\u2003\n1\ntested on online datasets and on real cars and buses. The \ndetected events, along with CAN bus messages were used \nto filter irrelevant data, saving bandwidth for data collection. \nA practical deployment of parking surveillance using edge-\ncloud computing was presented in Ke et\u00a0al. (2021), the edge \ndevice performs detection and transmits the bounding box \nand object types to the server, which uses this information \nfor labeling and tracking. A different approach by Bura et\u00a0al. \n(2018) focused on vehicle tracking from top view cameras \nand number plate recognition from ground-level cameras \nfor real-time occupancy information and to automatically \ncharge a vehicle for the time it was parked. Large-scale traf-\nfic monitoring using computer vision and edge computing \nwas detailed in Liu et\u00a0al. (2021a) where edge nodes close \nto surveillance cameras can process low-resolution videos \nto monitor traffic, detect congestion, and detect speed if the \navailable bandwidth is low. If high bandwidth to the server \nis available, high-quality video will be sent for similar pro-\ncessing. Edge computing for vehicle detection is examined \nin Wan et\u00a0al. (2022). The algorithm divides the traffic video \ninto segments of interest and then uses YOLOv3 for vehicle \ndetection in real-time on the edge node, and the extracted \nclips are used as training data for the edge server.\nCurrent Methods to\u00a0Overcome Challenges\nOne problem with large-scale DL is that the huge quantity of \ndata produced cannot be sent to a cloud computer for train-\ning. Federated learning (Kone\u010dn\u00fd et\u00a0al. 2015) has emerged \nas a solution to this problem, especially considering the \nheterogeneous data sources, bandwidth, and privacy issues \n(Zhou et\u00a0al. 2021). Training can be performed on edge nodes \nor edge servers, with the results being sent to the cloud to \naggregate in the shared deep-learning model (Zhang and \nLetaief 2020). Federated learning is also robust to failure \nof individual edge nodes (Kairouz et\u00a0al. 2019). Concerns \nof bandwidth, data privacy, and power requirements are \naddressed in Song et\u00a0al. (2018) by transferring only inferred \ndata from edge nodes to the cloud, in the form of incremen-\ntal and unsupervised learning. In general, the processing \nof data on the edge to reduce bandwidth has the pleasant \nside effect of anonymizing the transmitted data (Barth\u00e9lemy \net\u00a0al. 2019). Another effort to reduce bandwidth require-\nments employs spectral clustering compression performed \non spatio-temporal features needed for traffic flow prediction \n(Chen et\u00a0al. 2021a).\nDeep learning models cannot be directly exported to \nmobile edge nodes, as they are usually too computationally \nintensive. Direct adaptation for vehicle counting resulted \nin 1\u20134 fps for models in AI city challenge if they ran at \nall (Anastasiu et\u00a0al. 2020). Neural network pruning both in \nterms of storage and computation was introduced in Han \net\u00a0al. (2015), while implementation of the resulting sparse \nnetwork on hardware is discussed in Zhang et\u00a0al. (2016a), \nachieving multiple orders of magnitude increase in effi-\nciency. A general lightweight CNN model was developed \nfor mobile edge units in Zhou et\u00a0al. (2019), matching or \noutperforming AlexNet and VGG-16 while being a fraction \nof the size and computation cost. Edge computing-based \ntraffic flow detection using deep learning was deployed by \nChen et\u00a0al. (2021b) where YOLOv3 was trained and pruned, \nalong with DeepSORT, to be deployed on the edge device \nfor real-time performance. A thorough review of deploying \ncompact DNNs on low-power edge computers for IoT appli-\ncations can be found in Zhang et\u00a0al. (2021). They note that \nthe diversity and quantity of DNN applications require an \nautomated method for model compression beyond traditional \npruning techniques.\nFuture Directions\nFor Solving Data Challenges\nWhile a large quantity of data is essential for training deep \nlearning models, often the quality is the limiting factor in \ntraining performance. Data curation is a necessary process to \ninclude edge cases and train the model on representative data \nfrom the real world. Labeling vision data, especially in com-\nplex urban environments is a labor-intensive task performed \nby humans. It can be sped up by first using existing object \ndetection or segmentation algorithms based on the relevant \ntask to automatically label the data. Then this can be fur-\nther checked by humans to eliminate errors by the machine, \nthus creating a useful labeled dataset. This approach has \ngreatly improved the quality of naturalistic driving datasets \n(Miao et\u00a0al. 2022). There is also a need for datasets that \ninclude multiple sensors from different views for training \ncooperative perception algorithms. Collecting such data is \nbound to be challenging because of hardware requirements \nand synchronization issues but it is possible to achieve with \nconnected vehicles and instrumented intersections similar to \nthe configuration that will be deployed. Crowd-sourcing via \nsmartphone apps is also a viable method of producing high-\nquality reliable data (Aboah et\u00a0al. 2022). Some examples of \nuseful datasets are collected in Table\u00a02.\nThe problems associated with poor quality or viewing \nangle of real-world cameras can be mitigated by using real-\nistic CCTV benchmarks and datasets that include a wide \nvariety of surveillance footage, including synthetic video \n(Revaud and Humenberger 2021). Data-driven simulators \nlike Amini et\u00a0al. (2021) use high-fidelity datasets to simu-\nlate cameras and LiDAR, which can be used to train DL \nmodels with data that is hard to capture in the real world \n(Azfar et\u00a0al. 2022). Such an approach has shown promise in \nend-to-end reinforcement learning of autonomous vehicle \n", []], "Current Methods to\u00a0Overcome Challenges": ["Data Science for Transportation (2024) 6:1\t\nPage 17 of 27\u2003\n1\ntested on online datasets and on real cars and buses. The \ndetected events, along with CAN bus messages were used \nto filter irrelevant data, saving bandwidth for data collection. \nA practical deployment of parking surveillance using edge-\ncloud computing was presented in Ke et\u00a0al. (2021), the edge \ndevice performs detection and transmits the bounding box \nand object types to the server, which uses this information \nfor labeling and tracking. A different approach by Bura et\u00a0al. \n(2018) focused on vehicle tracking from top view cameras \nand number plate recognition from ground-level cameras \nfor real-time occupancy information and to automatically \ncharge a vehicle for the time it was parked. Large-scale traf-\nfic monitoring using computer vision and edge computing \nwas detailed in Liu et\u00a0al. (2021a) where edge nodes close \nto surveillance cameras can process low-resolution videos \nto monitor traffic, detect congestion, and detect speed if the \navailable bandwidth is low. If high bandwidth to the server \nis available, high-quality video will be sent for similar pro-\ncessing. Edge computing for vehicle detection is examined \nin Wan et\u00a0al. (2022). The algorithm divides the traffic video \ninto segments of interest and then uses YOLOv3 for vehicle \ndetection in real-time on the edge node, and the extracted \nclips are used as training data for the edge server.\nCurrent Methods to\u00a0Overcome Challenges\nOne problem with large-scale DL is that the huge quantity of \ndata produced cannot be sent to a cloud computer for train-\ning. Federated learning (Kone\u010dn\u00fd et\u00a0al. 2015) has emerged \nas a solution to this problem, especially considering the \nheterogeneous data sources, bandwidth, and privacy issues \n(Zhou et\u00a0al. 2021). Training can be performed on edge nodes \nor edge servers, with the results being sent to the cloud to \naggregate in the shared deep-learning model (Zhang and \nLetaief 2020). Federated learning is also robust to failure \nof individual edge nodes (Kairouz et\u00a0al. 2019). Concerns \nof bandwidth, data privacy, and power requirements are \naddressed in Song et\u00a0al. (2018) by transferring only inferred \ndata from edge nodes to the cloud, in the form of incremen-\ntal and unsupervised learning. In general, the processing \nof data on the edge to reduce bandwidth has the pleasant \nside effect of anonymizing the transmitted data (Barth\u00e9lemy \net\u00a0al. 2019). Another effort to reduce bandwidth require-\nments employs spectral clustering compression performed \non spatio-temporal features needed for traffic flow prediction \n(Chen et\u00a0al. 2021a).\nDeep learning models cannot be directly exported to \nmobile edge nodes, as they are usually too computationally \nintensive. Direct adaptation for vehicle counting resulted \nin 1\u20134 fps for models in AI city challenge if they ran at \nall (Anastasiu et\u00a0al. 2020). Neural network pruning both in \nterms of storage and computation was introduced in Han \net\u00a0al. (2015), while implementation of the resulting sparse \nnetwork on hardware is discussed in Zhang et\u00a0al. (2016a), \nachieving multiple orders of magnitude increase in effi-\nciency. A general lightweight CNN model was developed \nfor mobile edge units in Zhou et\u00a0al. (2019), matching or \noutperforming AlexNet and VGG-16 while being a fraction \nof the size and computation cost. Edge computing-based \ntraffic flow detection using deep learning was deployed by \nChen et\u00a0al. (2021b) where YOLOv3 was trained and pruned, \nalong with DeepSORT, to be deployed on the edge device \nfor real-time performance. A thorough review of deploying \ncompact DNNs on low-power edge computers for IoT appli-\ncations can be found in Zhang et\u00a0al. (2021). They note that \nthe diversity and quantity of DNN applications require an \nautomated method for model compression beyond traditional \npruning techniques.\nFuture Directions\nFor Solving Data Challenges\nWhile a large quantity of data is essential for training deep \nlearning models, often the quality is the limiting factor in \ntraining performance. Data curation is a necessary process to \ninclude edge cases and train the model on representative data \nfrom the real world. Labeling vision data, especially in com-\nplex urban environments is a labor-intensive task performed \nby humans. It can be sped up by first using existing object \ndetection or segmentation algorithms based on the relevant \ntask to automatically label the data. Then this can be fur-\nther checked by humans to eliminate errors by the machine, \nthus creating a useful labeled dataset. This approach has \ngreatly improved the quality of naturalistic driving datasets \n(Miao et\u00a0al. 2022). There is also a need for datasets that \ninclude multiple sensors from different views for training \ncooperative perception algorithms. Collecting such data is \nbound to be challenging because of hardware requirements \nand synchronization issues but it is possible to achieve with \nconnected vehicles and instrumented intersections similar to \nthe configuration that will be deployed. Crowd-sourcing via \nsmartphone apps is also a viable method of producing high-\nquality reliable data (Aboah et\u00a0al. 2022). Some examples of \nuseful datasets are collected in Table\u00a02.\nThe problems associated with poor quality or viewing \nangle of real-world cameras can be mitigated by using real-\nistic CCTV benchmarks and datasets that include a wide \nvariety of surveillance footage, including synthetic video \n(Revaud and Humenberger 2021). Data-driven simulators \nlike Amini et\u00a0al. (2021) use high-fidelity datasets to simu-\nlate cameras and LiDAR, which can be used to train DL \nmodels with data that is hard to capture in the real world \n(Azfar et\u00a0al. 2022). Such an approach has shown promise in \nend-to-end reinforcement learning of autonomous vehicle \n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 16 of 27\ncorresponding to normal traffic, which can be used to detect \nanomalies. The image description makes it more robust to \nlighting, perspective, and occlusions. Aboah et\u00a0al. (2021) \nproposed a decision tree-based DL approach for anomaly \ndetection using YOLOv5 for vehicle detection, followed by \nbackground estimation, then a decision tree considers fac-\ntors such as vehicle size, likelihood, and road feature mask \nto eliminate false positives. Adaptive thresholding allows for \nrobustness under variable illumination and weather condi-\ntions. A perspective map approach is discussed by Bai et\u00a0al. \n(2019), which models the background using road segmenta-\ntion based on a traffic flow frequency map, then the perspec-\ntive is detected from linear regression of object sizes based \non ResNet50. Finally, a spatial-temporal matrix discriminat-\ning module performs thresholding on consecutive frames to \ndetect anomalous states.\nCurrent Methods to\u00a0Overcome Challenges\nAnomaly detection relies on surveillance cameras which \nusually provide a view far along the road, but vehicles in \nthe distance occupy only a few pixels which make detection \ndifficult. Thus, Li et\u00a0al. (2020) uses pixel-level tracking in \naddition to box-level tracking for multi-granularity. The key \nidea is mask extraction based on frame difference and vehi-\ncle trajectory tracking based on the Gaussian Mixture Model \nto eliminate moving vehicles combined with segmentation \nbased on frame changes to also eliminate parking zones. \nAnomaly fusion uses the box and pixel-level tracking fea-\ntures with backtracking optimization to refine predictions. \nSurveillance cameras are prone to shaking in the wind, so \nvideo stabilization preprocessing was performed before \nusing two-stage vehicle detection in the form of Faster \nR-CNN and Cascade R-CNN (Zhao et\u00a0al. 2021b). An effi-\ncient real-time method for anomaly detection from surveil-\nlance video decouples the appearance and motion learning \ninto two parts (Li et\u00a0al. 2021c). First, an autoencoder learns \nappearance features, then 3D convolutional layers can use \nlatent codes from multiple past frames to predict features \nfor future frames. A significant difference between predicted \nand actual features indicates an anomaly. The model can \nbe deployed on edge nodes near the traffic cameras, and \nthe latent features appear to be robust to illumination and \nweather changes compared to pixel-wise methods.\nTo shed reliance on annotated data for anomalies, an \nunsupervised one-class approach in Pawar and Attar (2021) \napplies spatio-temporal convolutional autoencoder to get \nlatent features, stacks them together, and a sequence-to-\nsequence LSTM learns the temporal patterns. The method \nperforms well on multiple real-world surveillance footage \ndatasets, but not better than supervised training methods. \nThe advantage is that it can be indefinitely trained on normal \ntraffic data without any labeled anomalies.\nEdge Computing\nModels and\u00a0Algorithms\nComputer vision in ITS requires efficient infrastructure \narchitecture to analyze data in real time. If all acquired video \nstreams are sent to a single server, the required bandwidth \nand computation would not be able to provide a usable ser-\nvice. For example, edge computing architecture for real-time \nautomatic failure detection using a video usefulness metric \nwas explored in (Sun et\u00a0al. 2020a). Only video deemed to \nbe useful is transmitted to the server, while malfunction of \nthe surveillance camera, or obstruction of view, is automati-\ncally reported. Edge-cloud-based computing can implement \nDL models, not just for computer vision tasks, but also for \nresource allocation and efficiency (Xie et\u00a0al. 2021). Passive \nsurveillance has now been superseded in literature by the \nincreasing availability of sensor-equipped vehicles that can \nperform perception and mapping cooperatively (Zhang and \nLetaief 2020).\nOnboard computing resources in vehicles are often not \npowerful enough to process all sensor data in real time, and \napplications like localization and mapping can be very com-\nputationally intensive. The internet of things (IoT) archi-\ntecture allows for edge nodes to offload that computation \nand provide results at low latency to nearby users (Ferdowsi \net\u00a0al. 2019). This approach can avoid multiple cars doing \nthe same computation with similar inputs. One technique to \noffload computation tasks is discussed in Cui et\u00a0al. (2020), \ncombining integer linear programming for offline scheduling \noptimization and heuristics for online, real-world deploy-\nment. The authors compress 3D point cloud LIDAR data \ncollected from the vehicle\u2019s sensor and send it to the edge \nnode for classification and feature extraction. A deep rein-\nforcement learning algorithm known as Deep Determinis-\ntic Policy Gradient is proposed in Dai et\u00a0al. (2019), which \ncan dynamically allocate computing and caching resources \nthroughout the network. Future work in this direction will \nhandle multiple communication channels, interference man-\nagement, forecast handover, and bandwidth allocation. In \nthe macro scale, V2V communication can be used for traffic \nparameter estimation and management with sparse connec-\ntivity, while higher connected vehicle market penetration \nwill allow safety applications like collision avoidance (Dey \net\u00a0al. 2016).\nApplications for vehicles can include near-crash detec-\ntion, navigation, video streaming, and smart traffic lights. \nThe onboard unit can also be used as a mobile cache, and \ncommunicate with other vehicles via V2V networking. Real-\ntime near-crash detection using edge computing was devel-\noped in Ke et\u00a0al. (2020). The system uses dashcam video for \nSSD vehicle and pedestrian detection, followed by SORT \nfor tracking to estimate the time to collision (TTC). It was \nData Science for Transportation (2024) 6:1\t\nPage 15 of 27\u2003\n1\nLSTM to predict the trajectory. It can perform accurately in \ndense, heterogeneous, urban traffic conditions in real time. \nThe paper also contributes a new labeled dataset captured \nfrom crowded Asian cities. To be useful, trajectory predic-\ntion needs to take into account the motion of surrounding \nobjects and inter-object interactions in real time. Therefore, \na different approach to motion prediction is discussed in Li \net\u00a0al. (2019) based on the graph convolutional model, which \ntakes trajectory data as input and represents the interactions \nof nearby objects and extracts features. The graph model out-\nput is then passed into an encoder-decoder LSTM model for \nrobust predictions that can consider the interaction between \nvehicles. The method enables 30% higher prediction accu-\nracy in addition to 5x faster execution. The algorithm uses \ntrajectory data that has already been extracted from surveil-\nlance video data like NGSIM Colyar and Halkias (2007). In \nTripicchio and D\u2019Avella (2022), vehicle trajectory of vehi-\ncles is calculated using Lucas-Kanade algorithm on dash-\ncam video. Synthetic data was also used for augmenting the \ndataset to train an LSTM network to predict future motion \nand an SVM is used to classify the action, for eg. changing \nlanes. The method predicts the next 6 seconds of motion on \nhighways with 92% accuracy.\nCurrent Methods to\u00a0Overcome Challenges\nThe dynamics of vulnerable road users are described by a \nSwitching Linear Dynamical System (SLDS) in Kooij et\u00a0al. \n(2019) and extended with a dynamic bayesian network using \ncontext from features extracted from vehicle-mounted ste-\nreo cameras focusing on both static and dynamic cues. The \napproach can work in real-time, providing accurate predic-\ntions of road user trajectories. It can be improved by the \ninclusion of more context such as traffic lights and pedes-\ntrian crossings. The use of onboard camera and LiDAR \nalong with V2V communication is explored in Choi et\u00a0al. \n(2021b) to predict trajectories using the random forest and \nLSTM architecture. YOLO is used to detect cars and pro-\nvide bounding boxes, while LiDAR provides subtle changes \nin position, and V2V communication transmits raw values \nlike steering angles to reduce the uncertainty and latency of \npredictions.\nThe TRAF dataset was used in Chandra et\u00a0al. (2019b) \nfor robust end-to-end real-time trajectory prediction from \nstill or moving cameras. Mask R-CNN and reciprocal veloc-\nity obstacles algorithm are used for multi-vehicle tracking. \nThe last 3 seconds of tracking are used to predict the next \n5 seconds of trajectory as in Chandra et\u00a0al. (2019a), with \nthe added advantage of being end-to-end trainable and not \nrequiring annotated trajectory data. The paper also con-\ntributes TrackNPred, a python-based library that contains \nimplementations of different trajectory prediction methods. \nIt is a common interface for many trajectory prediction \napproaches and can be used for performance comparisons \nusing standard error measurement metrics on real-world \ndense and heterogeneous traffic datasets.\nMost DL methods for trajectory prediction do not uncover \nthe underlying reward function, instead, they only rely on \npreviously seen examples, which hinders generalizability \nand limits their scope. In Fernando et\u00a0al. (2021), inverse \nreinforcement learning is used to find the reward function so \nthat the model can be said to have a tangible goal, allowing \nit to be deployed in any environment. Transformer-based \nmotion prediction is performed in Liu et\u00a0al. (2021b) to \nachieve state-of-the-art multimodal trajectory prediction in \nthe Agroverse dataset. The network models both the road \ngeometry and interactions between the vehicles. Pedestrian \nintention in complex urban scenarios is predicted by graph \nconvolution networks on spatio-temporal graphs in Liu et\u00a0al. \n(2020a). The method considers the relationship between \npedestrians waiting to cross and the movement of vehicles. \nWhile achieving 80% accuracy on multiple datasets, it pre-\ndicts intent to cross one second in advance. On the other \nhand, pedestrians modeled as automatons, combined with \nSVM without the need for pose information, result in longer \npredictions but lack the consideration of contextual informa-\ntion (Jayaraman et\u00a0al. 2020).\nTraffic Anomaly Detection\nModels and\u00a0Algorithms\nTraffic surveillance cameras can be used to automatically \ndetect traffic anomalies like stopped vehicles and queues. \nThe detection of low-level image features like corners of \nvehicles has been used by Albiol et\u00a0al. (2011) to demonstrate \nqueue detection and queue length estimation without object \ntracking or background removal in different lighting condi-\ntions. Tracking methods based on optical flow can not only \nprovide queue length, but also speed, vehicle count, wait-\ning time, and time headway. In Shirazi and Morris (2015), \nthe authors use optical flow assuming constant short-term \nbrightness to detect vehicle features and successfully track \nthem even with occlusions. The speed of individual vehi-\ncles can be estimated, allowing the detection of stopped \nvehicles or queue formation. Trajectory analysis has also \nbeen deployed to identify illegal or dangerous movements \n(Nowosielski et\u00a0al. 2016). The background subtraction-based \napproaches are, however, limited to favorable scenarios and \ndo not generalize well.\nAn interesting method is applied in Li et\u00a0al. (2016a) \ninvolving partitioning the video into spatial and temporal \nblocks, local invariant features are then learned from traffic \nfootage to create a visual codebook of the image descriptors \nusing Locality-constrained Linear Coding. Then, a Gauss-\nian distribution model is trained to learn the probabilities \n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 14 of 27\nDashcam videos were used to train a Dynamic Spatial \nAssistance (DSA) network to distribute attention to objects \nand model temporal dependencies in Chan et\u00a0al. (2016). The \nmethod was able to predict crashes around 2 s in advance. \nUnderstanding multi-vehicle interaction in urban environ-\nments is challenging, and model-based methods may require \nprior knowledge, so a more general approach is explored \nin Zhang et\u00a0al. (2019), where YOLOv3 is used for object \ntracking from traffic camera video from the NGSIM dataset \nand a Gaussian velocity field is used to describe the inter-\naction behaviors between multiple vehicles. From this, an \n11-layer deep autoencoder learns the latent low-dimensional \nrepresentations for each frame, followed by a hidden semi-\nMarkov model with a hierarchical Dirichlet process, which \noptimizes the number of interaction patterns, to cluster rep-\nresentations into traffic primitives corresponding to the inter-\naction patterns. The pipeline can be used to analyze complex \nmulti-agent interactions from traffic video. DL methods are \nable to extract semantic descriptions from video, like in Li \net\u00a0al. (2021b), which can give advance warning of risky \nsituations. A scenario-wise spatio-temporal attention guid-\nance system was created by data mining from descriptive \nsemantic variables in fatal crash data to support the design \nof a model based on YOLOv3 for evaluating crash risk from \ndashcam footage. The attention guidance extracted seman-\ntic descriptions like \u201cpedestrian\u201d, \u201cschool bus\u201d and \u201catmos-\npheric condition\u201d, followed by DL to optimize attention on \nthese variables to identify clusters and associate scene fea-\ntures with a crash features.\nCurrent Methods to\u00a0Overcome Challenges\nWhile most vehicle interaction methods reviewed thus far \nmake little mention of the practical challenges in variable \nweather and lighting, Zhang et\u00a0al. (2018a) highlights a back-\nground learning method specifically to adapt to changing \nlighting conditions and headlight illumination in surveil-\nlance footage and even utilizes a threshold-based noise \nremoval for rainy conditions to detect near-miss events at \ngrade crossings. Domain adaptation, an example of transfer \nlearning, was employed in Li et\u00a0al. (2021a) to make use of \nlabeled daytime footage for vehicle detection in unlabeled \nnighttime images by a generative adversarial network called \nCycleGAN (Zhu et\u00a0al. 2017), which can be used with many \nreal-world deep learning computer vision applications. \nYouTube dashcam footage was used for crash detection in \nan ensemble multimodal DL method, based on the gated \nrecurrent unit (GRU) and CNN, which uses both video and \naudio data (Choi et\u00a0al. 2021a). The real-world data con-\nsists of positive clips containing crashes and negative clips \ncontaining normal driving. A crowd-sourced dashcam video \ndataset was also contributed by Chan et\u00a0al. (2016) for acci-\ndent anticipation containing scenarios like crowded streets, \ncomplicated road environments, and diversity of accidents. \nTo address low-visibility conditions like rain, fog, and \nnighttime footage, Wang et\u00a0al. (2020c) used Retinex image \nenhancement algorithm for preprocessing and YOLOv3 \nfor object detection, followed by a decision tree to classify \ncrashes. It balances dynamic range and enhances edges, but \ncongested mixed-flow traffic, lower-quality video, and fast \nvehicles are still major sources of error. The use of deep con-\nvolutional autoencoders for representation learning comple-\nmented with vehicle tracking is used to detect accidents from \nsurveillance footage in Singh and Mohan (2019). The test-\ning was performed on data collected during bright sunlight, \nnight, early morning, and also from a variety of cameras and \nangles. However, there are significant false alarms caused \nby low visibility, occlusions, and large variations in traffic \npatterns. The lack of near-miss data can be met by combin-\ning vehicle event recorder data and object detection from \nan onboard camera as proposed in Yamamoto et\u00a0al. (2022). \nBy extracting two deep feature representations that consider \nthe car status and the surrounding objects, the deep learning \nmethod can label near-miss events. While the method does \nnot claim to be real-time, it can generate large volumes of \nlabeled training data for near-crash events.\nA method to detect cycling near-misses from front view \nvideo is developed in Ibrahim et\u00a0al. (2021) using optical \nflow, CNN, LSTM, and a fully connected prediction stage. \nThe method was trained with complex urban environments \nand also contributes to a large dataset containing labeled \nnear-miss events. A ResNet-based model was used to detect \npedestrians and evaluate risk from a near-miss dataset in \nSuzuki et\u00a0al. (2017). The dataset contains videos from dif-\nferent vehicles, places (intersections, city, major roads), day \nand night time, and weather conditions. However, the model \nsuffers from overfitting as a result of having only near-miss \ndata for training.\nRoad User Behavior Prediction\nModels and\u00a0Algorithms\nTrajectory prediction from videos is useful for autonomous \ndriving, traffic forecasting, and congestion management. \nOlder works in this domain focused on homogeneous agents \nsuch as cars on a highway or pedestrians in a crowd, whereas \nheterogeneous agents were only considered in sparse sce-\nnarios with certain assumptions like lane-based driving. A \nlong short-term memory (LSTM) and CNN hybrid network, \nthat learns the relationship between pairs of heterogeneous \nagents, was developed in Chandra et\u00a0al. (2019a) to extract \nagent shape, velocity, and traffic concentration, which are \npassed through LSTMs to generate horizon and neighbor-\nhood maps, which then go through convolution networks to \nproduce latent representations that are passed through a final \n", []], "Models and\u00a0Algorithms": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 16 of 27\ncorresponding to normal traffic, which can be used to detect \nanomalies. The image description makes it more robust to \nlighting, perspective, and occlusions. Aboah et\u00a0al. (2021) \nproposed a decision tree-based DL approach for anomaly \ndetection using YOLOv5 for vehicle detection, followed by \nbackground estimation, then a decision tree considers fac-\ntors such as vehicle size, likelihood, and road feature mask \nto eliminate false positives. Adaptive thresholding allows for \nrobustness under variable illumination and weather condi-\ntions. A perspective map approach is discussed by Bai et\u00a0al. \n(2019), which models the background using road segmenta-\ntion based on a traffic flow frequency map, then the perspec-\ntive is detected from linear regression of object sizes based \non ResNet50. Finally, a spatial-temporal matrix discriminat-\ning module performs thresholding on consecutive frames to \ndetect anomalous states.\nCurrent Methods to\u00a0Overcome Challenges\nAnomaly detection relies on surveillance cameras which \nusually provide a view far along the road, but vehicles in \nthe distance occupy only a few pixels which make detection \ndifficult. Thus, Li et\u00a0al. (2020) uses pixel-level tracking in \naddition to box-level tracking for multi-granularity. The key \nidea is mask extraction based on frame difference and vehi-\ncle trajectory tracking based on the Gaussian Mixture Model \nto eliminate moving vehicles combined with segmentation \nbased on frame changes to also eliminate parking zones. \nAnomaly fusion uses the box and pixel-level tracking fea-\ntures with backtracking optimization to refine predictions. \nSurveillance cameras are prone to shaking in the wind, so \nvideo stabilization preprocessing was performed before \nusing two-stage vehicle detection in the form of Faster \nR-CNN and Cascade R-CNN (Zhao et\u00a0al. 2021b). An effi-\ncient real-time method for anomaly detection from surveil-\nlance video decouples the appearance and motion learning \ninto two parts (Li et\u00a0al. 2021c). First, an autoencoder learns \nappearance features, then 3D convolutional layers can use \nlatent codes from multiple past frames to predict features \nfor future frames. A significant difference between predicted \nand actual features indicates an anomaly. The model can \nbe deployed on edge nodes near the traffic cameras, and \nthe latent features appear to be robust to illumination and \nweather changes compared to pixel-wise methods.\nTo shed reliance on annotated data for anomalies, an \nunsupervised one-class approach in Pawar and Attar (2021) \napplies spatio-temporal convolutional autoencoder to get \nlatent features, stacks them together, and a sequence-to-\nsequence LSTM learns the temporal patterns. The method \nperforms well on multiple real-world surveillance footage \ndatasets, but not better than supervised training methods. \nThe advantage is that it can be indefinitely trained on normal \ntraffic data without any labeled anomalies.\nEdge Computing\nModels and\u00a0Algorithms\nComputer vision in ITS requires efficient infrastructure \narchitecture to analyze data in real time. If all acquired video \nstreams are sent to a single server, the required bandwidth \nand computation would not be able to provide a usable ser-\nvice. For example, edge computing architecture for real-time \nautomatic failure detection using a video usefulness metric \nwas explored in (Sun et\u00a0al. 2020a). Only video deemed to \nbe useful is transmitted to the server, while malfunction of \nthe surveillance camera, or obstruction of view, is automati-\ncally reported. Edge-cloud-based computing can implement \nDL models, not just for computer vision tasks, but also for \nresource allocation and efficiency (Xie et\u00a0al. 2021). Passive \nsurveillance has now been superseded in literature by the \nincreasing availability of sensor-equipped vehicles that can \nperform perception and mapping cooperatively (Zhang and \nLetaief 2020).\nOnboard computing resources in vehicles are often not \npowerful enough to process all sensor data in real time, and \napplications like localization and mapping can be very com-\nputationally intensive. The internet of things (IoT) archi-\ntecture allows for edge nodes to offload that computation \nand provide results at low latency to nearby users (Ferdowsi \net\u00a0al. 2019). This approach can avoid multiple cars doing \nthe same computation with similar inputs. One technique to \noffload computation tasks is discussed in Cui et\u00a0al. (2020), \ncombining integer linear programming for offline scheduling \noptimization and heuristics for online, real-world deploy-\nment. The authors compress 3D point cloud LIDAR data \ncollected from the vehicle\u2019s sensor and send it to the edge \nnode for classification and feature extraction. A deep rein-\nforcement learning algorithm known as Deep Determinis-\ntic Policy Gradient is proposed in Dai et\u00a0al. (2019), which \ncan dynamically allocate computing and caching resources \nthroughout the network. Future work in this direction will \nhandle multiple communication channels, interference man-\nagement, forecast handover, and bandwidth allocation. In \nthe macro scale, V2V communication can be used for traffic \nparameter estimation and management with sparse connec-\ntivity, while higher connected vehicle market penetration \nwill allow safety applications like collision avoidance (Dey \net\u00a0al. 2016).\nApplications for vehicles can include near-crash detec-\ntion, navigation, video streaming, and smart traffic lights. \nThe onboard unit can also be used as a mobile cache, and \ncommunicate with other vehicles via V2V networking. Real-\ntime near-crash detection using edge computing was devel-\noped in Ke et\u00a0al. (2020). The system uses dashcam video for \nSSD vehicle and pedestrian detection, followed by SORT \nfor tracking to estimate the time to collision (TTC). It was \nData Science for Transportation (2024) 6:1\t\nPage 15 of 27\u2003\n1\nLSTM to predict the trajectory. It can perform accurately in \ndense, heterogeneous, urban traffic conditions in real time. \nThe paper also contributes a new labeled dataset captured \nfrom crowded Asian cities. To be useful, trajectory predic-\ntion needs to take into account the motion of surrounding \nobjects and inter-object interactions in real time. Therefore, \na different approach to motion prediction is discussed in Li \net\u00a0al. (2019) based on the graph convolutional model, which \ntakes trajectory data as input and represents the interactions \nof nearby objects and extracts features. The graph model out-\nput is then passed into an encoder-decoder LSTM model for \nrobust predictions that can consider the interaction between \nvehicles. The method enables 30% higher prediction accu-\nracy in addition to 5x faster execution. The algorithm uses \ntrajectory data that has already been extracted from surveil-\nlance video data like NGSIM Colyar and Halkias (2007). In \nTripicchio and D\u2019Avella (2022), vehicle trajectory of vehi-\ncles is calculated using Lucas-Kanade algorithm on dash-\ncam video. Synthetic data was also used for augmenting the \ndataset to train an LSTM network to predict future motion \nand an SVM is used to classify the action, for eg. changing \nlanes. The method predicts the next 6 seconds of motion on \nhighways with 92% accuracy.\nCurrent Methods to\u00a0Overcome Challenges\nThe dynamics of vulnerable road users are described by a \nSwitching Linear Dynamical System (SLDS) in Kooij et\u00a0al. \n(2019) and extended with a dynamic bayesian network using \ncontext from features extracted from vehicle-mounted ste-\nreo cameras focusing on both static and dynamic cues. The \napproach can work in real-time, providing accurate predic-\ntions of road user trajectories. It can be improved by the \ninclusion of more context such as traffic lights and pedes-\ntrian crossings. The use of onboard camera and LiDAR \nalong with V2V communication is explored in Choi et\u00a0al. \n(2021b) to predict trajectories using the random forest and \nLSTM architecture. YOLO is used to detect cars and pro-\nvide bounding boxes, while LiDAR provides subtle changes \nin position, and V2V communication transmits raw values \nlike steering angles to reduce the uncertainty and latency of \npredictions.\nThe TRAF dataset was used in Chandra et\u00a0al. (2019b) \nfor robust end-to-end real-time trajectory prediction from \nstill or moving cameras. Mask R-CNN and reciprocal veloc-\nity obstacles algorithm are used for multi-vehicle tracking. \nThe last 3 seconds of tracking are used to predict the next \n5 seconds of trajectory as in Chandra et\u00a0al. (2019a), with \nthe added advantage of being end-to-end trainable and not \nrequiring annotated trajectory data. The paper also con-\ntributes TrackNPred, a python-based library that contains \nimplementations of different trajectory prediction methods. \nIt is a common interface for many trajectory prediction \napproaches and can be used for performance comparisons \nusing standard error measurement metrics on real-world \ndense and heterogeneous traffic datasets.\nMost DL methods for trajectory prediction do not uncover \nthe underlying reward function, instead, they only rely on \npreviously seen examples, which hinders generalizability \nand limits their scope. In Fernando et\u00a0al. (2021), inverse \nreinforcement learning is used to find the reward function so \nthat the model can be said to have a tangible goal, allowing \nit to be deployed in any environment. Transformer-based \nmotion prediction is performed in Liu et\u00a0al. (2021b) to \nachieve state-of-the-art multimodal trajectory prediction in \nthe Agroverse dataset. The network models both the road \ngeometry and interactions between the vehicles. Pedestrian \nintention in complex urban scenarios is predicted by graph \nconvolution networks on spatio-temporal graphs in Liu et\u00a0al. \n(2020a). The method considers the relationship between \npedestrians waiting to cross and the movement of vehicles. \nWhile achieving 80% accuracy on multiple datasets, it pre-\ndicts intent to cross one second in advance. On the other \nhand, pedestrians modeled as automatons, combined with \nSVM without the need for pose information, result in longer \npredictions but lack the consideration of contextual informa-\ntion (Jayaraman et\u00a0al. 2020).\nTraffic Anomaly Detection\nModels and\u00a0Algorithms\nTraffic surveillance cameras can be used to automatically \ndetect traffic anomalies like stopped vehicles and queues. \nThe detection of low-level image features like corners of \nvehicles has been used by Albiol et\u00a0al. (2011) to demonstrate \nqueue detection and queue length estimation without object \ntracking or background removal in different lighting condi-\ntions. Tracking methods based on optical flow can not only \nprovide queue length, but also speed, vehicle count, wait-\ning time, and time headway. In Shirazi and Morris (2015), \nthe authors use optical flow assuming constant short-term \nbrightness to detect vehicle features and successfully track \nthem even with occlusions. The speed of individual vehi-\ncles can be estimated, allowing the detection of stopped \nvehicles or queue formation. Trajectory analysis has also \nbeen deployed to identify illegal or dangerous movements \n(Nowosielski et\u00a0al. 2016). The background subtraction-based \napproaches are, however, limited to favorable scenarios and \ndo not generalize well.\nAn interesting method is applied in Li et\u00a0al. (2016a) \ninvolving partitioning the video into spatial and temporal \nblocks, local invariant features are then learned from traffic \nfootage to create a visual codebook of the image descriptors \nusing Locality-constrained Linear Coding. Then, a Gauss-\nian distribution model is trained to learn the probabilities \n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 14 of 27\nDashcam videos were used to train a Dynamic Spatial \nAssistance (DSA) network to distribute attention to objects \nand model temporal dependencies in Chan et\u00a0al. (2016). The \nmethod was able to predict crashes around 2 s in advance. \nUnderstanding multi-vehicle interaction in urban environ-\nments is challenging, and model-based methods may require \nprior knowledge, so a more general approach is explored \nin Zhang et\u00a0al. (2019), where YOLOv3 is used for object \ntracking from traffic camera video from the NGSIM dataset \nand a Gaussian velocity field is used to describe the inter-\naction behaviors between multiple vehicles. From this, an \n11-layer deep autoencoder learns the latent low-dimensional \nrepresentations for each frame, followed by a hidden semi-\nMarkov model with a hierarchical Dirichlet process, which \noptimizes the number of interaction patterns, to cluster rep-\nresentations into traffic primitives corresponding to the inter-\naction patterns. The pipeline can be used to analyze complex \nmulti-agent interactions from traffic video. DL methods are \nable to extract semantic descriptions from video, like in Li \net\u00a0al. (2021b), which can give advance warning of risky \nsituations. A scenario-wise spatio-temporal attention guid-\nance system was created by data mining from descriptive \nsemantic variables in fatal crash data to support the design \nof a model based on YOLOv3 for evaluating crash risk from \ndashcam footage. The attention guidance extracted seman-\ntic descriptions like \u201cpedestrian\u201d, \u201cschool bus\u201d and \u201catmos-\npheric condition\u201d, followed by DL to optimize attention on \nthese variables to identify clusters and associate scene fea-\ntures with a crash features.\nCurrent Methods to\u00a0Overcome Challenges\nWhile most vehicle interaction methods reviewed thus far \nmake little mention of the practical challenges in variable \nweather and lighting, Zhang et\u00a0al. (2018a) highlights a back-\nground learning method specifically to adapt to changing \nlighting conditions and headlight illumination in surveil-\nlance footage and even utilizes a threshold-based noise \nremoval for rainy conditions to detect near-miss events at \ngrade crossings. Domain adaptation, an example of transfer \nlearning, was employed in Li et\u00a0al. (2021a) to make use of \nlabeled daytime footage for vehicle detection in unlabeled \nnighttime images by a generative adversarial network called \nCycleGAN (Zhu et\u00a0al. 2017), which can be used with many \nreal-world deep learning computer vision applications. \nYouTube dashcam footage was used for crash detection in \nan ensemble multimodal DL method, based on the gated \nrecurrent unit (GRU) and CNN, which uses both video and \naudio data (Choi et\u00a0al. 2021a). The real-world data con-\nsists of positive clips containing crashes and negative clips \ncontaining normal driving. A crowd-sourced dashcam video \ndataset was also contributed by Chan et\u00a0al. (2016) for acci-\ndent anticipation containing scenarios like crowded streets, \ncomplicated road environments, and diversity of accidents. \nTo address low-visibility conditions like rain, fog, and \nnighttime footage, Wang et\u00a0al. (2020c) used Retinex image \nenhancement algorithm for preprocessing and YOLOv3 \nfor object detection, followed by a decision tree to classify \ncrashes. It balances dynamic range and enhances edges, but \ncongested mixed-flow traffic, lower-quality video, and fast \nvehicles are still major sources of error. The use of deep con-\nvolutional autoencoders for representation learning comple-\nmented with vehicle tracking is used to detect accidents from \nsurveillance footage in Singh and Mohan (2019). The test-\ning was performed on data collected during bright sunlight, \nnight, early morning, and also from a variety of cameras and \nangles. However, there are significant false alarms caused \nby low visibility, occlusions, and large variations in traffic \npatterns. The lack of near-miss data can be met by combin-\ning vehicle event recorder data and object detection from \nan onboard camera as proposed in Yamamoto et\u00a0al. (2022). \nBy extracting two deep feature representations that consider \nthe car status and the surrounding objects, the deep learning \nmethod can label near-miss events. While the method does \nnot claim to be real-time, it can generate large volumes of \nlabeled training data for near-crash events.\nA method to detect cycling near-misses from front view \nvideo is developed in Ibrahim et\u00a0al. (2021) using optical \nflow, CNN, LSTM, and a fully connected prediction stage. \nThe method was trained with complex urban environments \nand also contributes to a large dataset containing labeled \nnear-miss events. A ResNet-based model was used to detect \npedestrians and evaluate risk from a near-miss dataset in \nSuzuki et\u00a0al. (2017). The dataset contains videos from dif-\nferent vehicles, places (intersections, city, major roads), day \nand night time, and weather conditions. However, the model \nsuffers from overfitting as a result of having only near-miss \ndata for training.\nRoad User Behavior Prediction\nModels and\u00a0Algorithms\nTrajectory prediction from videos is useful for autonomous \ndriving, traffic forecasting, and congestion management. \nOlder works in this domain focused on homogeneous agents \nsuch as cars on a highway or pedestrians in a crowd, whereas \nheterogeneous agents were only considered in sparse sce-\nnarios with certain assumptions like lane-based driving. A \nlong short-term memory (LSTM) and CNN hybrid network, \nthat learns the relationship between pairs of heterogeneous \nagents, was developed in Chandra et\u00a0al. (2019a) to extract \nagent shape, velocity, and traffic concentration, which are \npassed through LSTMs to generate horizon and neighbor-\nhood maps, which then go through convolution networks to \nproduce latent representations that are passed through a final \nData Science for Transportation (2024) 6:1\t\nPage 13 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nRecent directions in segmentation include weakly-super-\nvised semantic segmentation\u00a0(Wang et\u00a0al. 2020e; Sun et\u00a0al. \n2020c), domain adaptation\u00a0(Chen et\u00a0al. 2019e; Liu et\u00a0al. \n2021c), multi-modal data fusion\u00a0(Feng et\u00a0al. 2020; Cortinhal \net\u00a0al. 2021), and real-time semantic segmentation\u00a0(Yu et\u00a0al. \n2018; Nirkin et\u00a0al. 2021; Gao et\u00a0al. 2021b).\nTS-Yolo Wan et\u00a0al. (2021b) is a CNN-based model for \naccurate traffic detection under severe weather conditions \nusing new samples from data augmentation. The data aug-\nmentation was conducted using copy\u2013paste strategy, and a \nlarge number of new samples were constructed from exist-\ning traffic-sign instances. Based on YoloV5, MixConv was \nalso used to mix different kernel sizes in a single convo-\nlution operation so that patterns with various resolutions \ncan be captured. Detecting and classifying real-life small \ntraffic signs from large input images is difficult due to them \noccupying fewer pixels relative to larger targets. To address \nthis, Dense-RefineDet\u00a0(Sun et\u00a0al. 2020) applies a single-shot, \nobject-detection framework to maintain a suitable accuracy-\nspeed trade-off. An end-to-end traffic sign detection frame-\nwork Feature Aggregation Multipath Network is proposed in \nOu et\u00a0al. (2019) to solve the problems of small object detec-\ntion and fine-grained classification in traffic sign detection.\nCooperative Perception\nModels and\u00a0Algorithms\nIn connected autonomous vehicles (CAV), cooperative per-\nception can be performed at three levels depending on the \ntype of data: early fusion (raw data), intermediate fusion \n(preprocessed data), where intermediate neural features are \nextracted and transmitted, and late fusion (processed data), \nwhere detection outputs (3D bounding box position, con-\nfidence score) are shared. Cooperative perception studies \nhow to leverage visual cues from neighboring connected \nvehicles and infrastructure to boost the overall perception \nperformance\u00a0(Xu et\u00a0al. 2022). \n(1)\t Early fusion: Chen et\u00a0al. (2019d) fuses the sensor data \ncollected from different positions and angles of con-\nnected vehicles using raw-data level LiDAR 3D point \nclouds, and a point cloud-based 3D object detection \nmethod is proposed to work on a diversity of aligned \npoint clouds. DiscoNet\u00a0(Li et\u00a0al. 2021d) leverages \nknowledge distillation to enhance training by constrain-\ning the corresponding features to the ones from the net-\nwork for early fusion.\n(2)\t Intermediate fusion: F-Cooper\u00a0(Chen et\u00a0al. 2019b) pro-\nvides both a new framework for applications On-Edge, \nservicing autonomous vehicles as well as new strategies \nfor 3D fusion detection. Wang et\u00a0al. (2020b) proposed \na vehicle-to-vehicle (V2V) approach for perception \nand prediction that transmits compressed intermediate \nrepresentations of the P &P neural network. Xu et\u00a0al. \n(2021) proposed an Attentive Intermediate Fusion pipe-\nline to better capture interactions between connected \nagents within the network. A robust cooperative per-\nception framework with vehicle-to-everything (V2X) \ncommunication using a novel vision Transformer is \npresented in Xu et\u00a0al. (2022).\n(3)\t Late fusion: Car2X-based perception\u00a0(Rauch et\u00a0al. \n2012) is modeled as a virtual sensor to integrate it into \na high-level sensor data fusion architecture.\nCurrent Methods to\u00a0Overcome Challenge\nTo reduce the communications load and overhead, an \nimproved algorithm for message generation rules in col-\nlective perception is proposed\u00a0(Thandavarayan et\u00a0al. 2020), \nwhich improves the reliability of V2X communications by \nreorganizing the transmission and content of collective per-\nception messages. This paper\u00a0(Yoon et\u00a0al. 2021) presents and \nevaluates a unified cooperative perception framework con-\ntaining a decentralized data association and fusion process \nthat is scalable with respect to participation variances. The \nevaluation considers the effects of communication losses in \nthe ad-hoc V2V network and the random vehicle motions in \ntraffic by adopting existing models along with a simplified \nalgorithm for individual vehicle\u2019s on-board sensor field of \nview. AICP is proposed in Zhou et\u00a0al. (2022), the first solu-\ntion that focuses on optimizing informativeness for perva-\nsive cooperative perception systems with efficient filtering \nat both the network and application layers. To facilitate sys-\ntem networking, they also use a networking protocol stack \nthat includes a dedicated data structure and a lightweight \nrouting protocol specifically for informativeness-focused \napplications.\nVehicle Interaction\nModels and\u00a0Algorithms\nComputer vision methods can be used to detect and clas-\nsify crash and near-crash events based on motion and tra-\njectories. CNN, in the form of modified YOLOv3, is used \nto detect objects and extract semantic information about the \nroad scene from onboard camera data from the SHRP2 data-\nset in Taccari et\u00a0al. (2018). Optical flow is calculated from \nconsecutive frames to track objects and to generate features \n(hard deceleration, the maximum area of the largest vehicle, \ntime to collision, etc.) that are combined with telematics \ndata (speed and 3-axis acceleration) to train a random forest \nclassifier on safe, near-crash, and crash events.\nData Science for Transportation (2024) 6:1\t\nPage 13 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nRecent directions in segmentation include weakly-super-\nvised semantic segmentation\u00a0(Wang et\u00a0al. 2020e; Sun et\u00a0al. \n2020c), domain adaptation\u00a0(Chen et\u00a0al. 2019e; Liu et\u00a0al. \n2021c), multi-modal data fusion\u00a0(Feng et\u00a0al. 2020; Cortinhal \net\u00a0al. 2021), and real-time semantic segmentation\u00a0(Yu et\u00a0al. \n2018; Nirkin et\u00a0al. 2021; Gao et\u00a0al. 2021b).\nTS-Yolo Wan et\u00a0al. (2021b) is a CNN-based model for \naccurate traffic detection under severe weather conditions \nusing new samples from data augmentation. The data aug-\nmentation was conducted using copy\u2013paste strategy, and a \nlarge number of new samples were constructed from exist-\ning traffic-sign instances. Based on YoloV5, MixConv was \nalso used to mix different kernel sizes in a single convo-\nlution operation so that patterns with various resolutions \ncan be captured. Detecting and classifying real-life small \ntraffic signs from large input images is difficult due to them \noccupying fewer pixels relative to larger targets. To address \nthis, Dense-RefineDet\u00a0(Sun et\u00a0al. 2020) applies a single-shot, \nobject-detection framework to maintain a suitable accuracy-\nspeed trade-off. An end-to-end traffic sign detection frame-\nwork Feature Aggregation Multipath Network is proposed in \nOu et\u00a0al. (2019) to solve the problems of small object detec-\ntion and fine-grained classification in traffic sign detection.\nCooperative Perception\nModels and\u00a0Algorithms\nIn connected autonomous vehicles (CAV), cooperative per-\nception can be performed at three levels depending on the \ntype of data: early fusion (raw data), intermediate fusion \n(preprocessed data), where intermediate neural features are \nextracted and transmitted, and late fusion (processed data), \nwhere detection outputs (3D bounding box position, con-\nfidence score) are shared. Cooperative perception studies \nhow to leverage visual cues from neighboring connected \nvehicles and infrastructure to boost the overall perception \nperformance\u00a0(Xu et\u00a0al. 2022). \n(1)\t Early fusion: Chen et\u00a0al. (2019d) fuses the sensor data \ncollected from different positions and angles of con-\nnected vehicles using raw-data level LiDAR 3D point \nclouds, and a point cloud-based 3D object detection \nmethod is proposed to work on a diversity of aligned \npoint clouds. DiscoNet\u00a0(Li et\u00a0al. 2021d) leverages \nknowledge distillation to enhance training by constrain-\ning the corresponding features to the ones from the net-\nwork for early fusion.\n(2)\t Intermediate fusion: F-Cooper\u00a0(Chen et\u00a0al. 2019b) pro-\nvides both a new framework for applications On-Edge, \nservicing autonomous vehicles as well as new strategies \nfor 3D fusion detection. Wang et\u00a0al. (2020b) proposed \na vehicle-to-vehicle (V2V) approach for perception \nand prediction that transmits compressed intermediate \nrepresentations of the P &P neural network. Xu et\u00a0al. \n(2021) proposed an Attentive Intermediate Fusion pipe-\nline to better capture interactions between connected \nagents within the network. A robust cooperative per-\nception framework with vehicle-to-everything (V2X) \ncommunication using a novel vision Transformer is \npresented in Xu et\u00a0al. (2022).\n(3)\t Late fusion: Car2X-based perception\u00a0(Rauch et\u00a0al. \n2012) is modeled as a virtual sensor to integrate it into \na high-level sensor data fusion architecture.\nCurrent Methods to\u00a0Overcome Challenge\nTo reduce the communications load and overhead, an \nimproved algorithm for message generation rules in col-\nlective perception is proposed\u00a0(Thandavarayan et\u00a0al. 2020), \nwhich improves the reliability of V2X communications by \nreorganizing the transmission and content of collective per-\nception messages. This paper\u00a0(Yoon et\u00a0al. 2021) presents and \nevaluates a unified cooperative perception framework con-\ntaining a decentralized data association and fusion process \nthat is scalable with respect to participation variances. The \nevaluation considers the effects of communication losses in \nthe ad-hoc V2V network and the random vehicle motions in \ntraffic by adopting existing models along with a simplified \nalgorithm for individual vehicle\u2019s on-board sensor field of \nview. AICP is proposed in Zhou et\u00a0al. (2022), the first solu-\ntion that focuses on optimizing informativeness for perva-\nsive cooperative perception systems with efficient filtering \nat both the network and application layers. To facilitate sys-\ntem networking, they also use a networking protocol stack \nthat includes a dedicated data structure and a lightweight \nrouting protocol specifically for informativeness-focused \napplications.\nVehicle Interaction\nModels and\u00a0Algorithms\nComputer vision methods can be used to detect and clas-\nsify crash and near-crash events based on motion and tra-\njectories. CNN, in the form of modified YOLOv3, is used \nto detect objects and extract semantic information about the \nroad scene from onboard camera data from the SHRP2 data-\nset in Taccari et\u00a0al. (2018). Optical flow is calculated from \nconsecutive frames to track objects and to generate features \n(hard deceleration, the maximum area of the largest vehicle, \ntime to collision, etc.) that are combined with telematics \ndata (speed and 3-axis acceleration) to train a random forest \nclassifier on safe, near-crash, and crash events.\n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 12 of 27\nand CNN-based one-stage detectors. the representative two-\nstage detectors include Faster R-CNN\u00a0(Zhang et\u00a0al. 2016), \nspatial pyramid pooling (SPP)-net\u00a0(He et\u00a0al. 2015), fea-\nture pyramid networks (FPN)\u00a0(Lin et\u00a0al. 2017), and Mask \nR-CNN\u00a0(He et\u00a0al. 2017). The representative one-stage detec-\ntors include YOLO\u00a0(Redmon et\u00a0al. 2016), Single Shot Multi-\nBox Detector (SSD)\u00a0(Liu et\u00a0al. 2016), and deeply supervised \nobject detectors (DSOD)\u00a0(Shen et\u00a0al. 2017). The two-step \nframework is a region proposal-based method, giving a \ncoarse scan of the whole image first and then focusing on \nregions of interest (RoIs). While one-step frameworks are \nbased on global regression/ classification, mapping straightly \nfrom image pixels to bounding box coordinates and class \nprobabilities. Based on these two frameworks, most of the \nworks get promising results in vehicle detection by combin-\ning other methods, such as multitask learning\u00a0(Brahmbhatt \net\u00a0al. 2017), multi-scale representation\u00a0(Bell et\u00a0al. 2016), and \ncontext modeling\u00a0(Kong et\u00a0al. 2016).\nCurrent Methods to\u00a0Overcome Challenge\nIn traffic sign detection, existing traffic sign datasets are \nlimited in terms of the type and severity of challenging \nconditions. Metadata corresponding to these conditions is \nunavailable and it is not possible to investigate the effect \nof a single factor because of the simultaneous changes in \nnumerous conditions. To overcome this, Temel et\u00a0al. (2019) \nintroduced the CURE-TSDReal dataset, based on simulated \nconditions that correspond to real-world environments. An \nend-to-end traffic sign detection framework Feature Aggre-\ngation Multipath Network (FAMN) is proposed in Ou et\u00a0al. \n(2019). It consists of two main structures named Feature \nAggregation and Multipath Network structure to solve the \nproblems of small object detection and fine-grained clas-\nsification in traffic sign detection.\nA vehicle highlight information-assisted neural net-\nwork for vehicle detection at night is presented in Mo et\u00a0al. \n(2019), which included two innovations: establishing the \nlabel hierarchy for vehicles based on their highlights and \ndesigning a multi-layer fused vehicle highlight informa-\ntion network. Real-time vehicle detection for nighttime \nsituations is presented in Bell et\u00a0al. (2021), where images \ninclude flashes that occupy large image regions, and the \nactual shape of vehicles is not well defined. By using a \nglobal image descriptor along with a grid of foveal classi-\nfiers, vehicle positions are accurately and efficiently esti-\nmated. AugGAN Lin et\u00a0al. (2020) is an unpaired image-to-\nimage translation network for domain adaptation in vehicle \ndetection. It quantitatively surpassed competing methods \nfor achieving higher nighttime vehicle detection accuracy \nbecause of better image-object preservation. A stepwise \ndomain adaptation (SDA) detection method is proposed \nto further improve the performance of CycleGAN by \nminimizing the divergence in cross-domain object detec-\ntion tasks in Li et\u00a0al. (2022). In the first step, an unpaired \nimage-to-image translator is trained to construct a fake \ntarget domain by translating the source images to similar \nones in the target domain. In the second step, to further \nminimize divergence across domains, an adaptive Center-\nNet is designed to align distributions at the feature level \nin an adversarial learning manner.\nAutonomous Driving Perception: Segmentation\nModels and\u00a0Algorithms\nImage segmentation contains three sub-tasks: semantic seg-\nmentation, instance segmentation, and panoptic segmenta-\ntion. Semantic segmentation is a fine prediction task to label \neach pixel of an image with a corresponding object class, \ninstance segmentation is designed to identify and segment \npixels that belong to each object instance, while panoptic \nsegmentation unifies semantic segmentation and instance \nsegmentation such that all pixels are given both a class label \nand an instance ID (Gu et\u00a0al. 2022).\nYOLACT\u00a0(Bolya et\u00a0al. 2019) splits instance segmenta-\ntion into two parallel sub-architectures. Protonet architec-\nture extracts spatial information by generating a certain \nnumber of prototype masks, and Head architecture gener-\nates the mask coefficients and object locations. In addi-\ntion, it employs Fast NMS rather than traditional NMS to \nreduce post-processing time. Path Aggregation Network \n(PANet)\u00a0(Liu et\u00a0al. 2018b) is proposed to integrate com-\nprehensively low-level location information and high-level \nsemantic information. Based on Feature Pyramid Networks \n(FPN)\u00a0(Lin et\u00a0al. 2017), PANet designs a bottom-up con-\ntext information aggregation structure, which can integrate \ndifferent levels of features. Hybrid Task Cascade (HTC) is \nproposed for instance segmentation in Chen et\u00a0al. (2019c). \nIt interweaves box and mask branches for joint multi-stage \nprocessing, adopts a semantic segmentation branch to pro-\nvide spatial context, and integrates complementary features \ntogether in each stage.\nIn Dong et\u00a0al. (2020), a novel real-time segmentation is \nproposed consisting of a convolutional attention module, \nspatial pyramid pooling, and a feature fusion network. It was \nevaluated on benchmark datasets Cityscapes and CamVid, \nwhich specifically target complex urban scenarios.\nMoving objects viewed from a moving platform pose a \nunique challenge for segmentation, which is addressed in \nZhou et\u00a0al. (2017) using time-consecutive stereo images. \nMotion likelihood estimates for each pixel aids in ego-\nmotion estimation, while segmentation is performed using \na graph-cut algorithm. However, computational complexity \nis a major limitation of this method.\nData Science for Transportation (2024) 6:1\t\nPage 11 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nCongestion detection performance can be improved using \nmultiple sensors based solutions including radar, lasers, and \nsensor fusion since it is hard to achieve ideal performance \nand accuracy using a single sensor in real-world scenarios. \nThere is a wide use of decision-making algorithms for pro-\ncessing fusion data acquired from multiple sensors\u00a0(Muham-\nmad et\u00a0al. 2020). A CNN-based model trained with bad \nweather condition datasets can improve the detection per-\nformance\u00a0(Sharma et\u00a0al. 2022), while generative adversarial \nnetwork (GAN) based Style Transfer methods have also been \napplied\u00a0(Lin et\u00a0al. 2020; Li et\u00a0al. 2021a). These approaches \nhelp to minimize the model challenges related to generaliz-\nability, which in turn improves real-world performance in a \nvariety of environments.\nAutonomous Driving Perception: Detection\nModels and\u00a0Algorithms\nCommon detection tasks that assist in AD are categorized \ninto traffic sign detection, traffic signal detection, road/lane \ndetection, pedestrian detection, and vehicle detection.\nTraffic signs There are two tasks in a typical traffic sign \nrecognition system: finding the locations and sizes of traf-\nfic signs in natural scene images (traffic sign detection) and \nclassifying the traffic signs into their specific sub-classes \n(traffic sign classification)\u00a0(Yang et\u00a0al. 2015). An improved \nSparse R-CNN was used for traffic sign detection in Cao \net\u00a0al. (2021), while an efficient algorithm based on YOLOv3 \nmodel for traffic sign detection was implemented in Wan \net\u00a0al. (2021a). SegU-Net, formed by merging the state-of-\nthe-art segmentation architectures SegNet and U-Net to \ndetect traffic signs from video sequences has been proposed \n(Kamal et\u00a0al. 2019). Several adaptations to Mask R-CNN \nwere tested in Tabernik and Sko\u010daj (2019) for detection \nand recognition with end-to-end learning in the domain of \ntraffic signs. They also proposed a data augmentation tech-\nnique based on the distribution of geometric and appearance \ndistortions.\nA method that uses an encoder-decoder DNN with focal \nregression loss to detect small traffic signals is proposed \nin Lee and Kim (2019). It is shown in Kim et\u00a0al. (2018) \nthat Faster R-CNN with Inception-Resnet-v2 model is more \nsuitable for traffic light detection than others. A practical \ntraffic light detection system in Ouyang et\u00a0al. (2019) com-\nbines CNN classifier model and heuristic region of interest \n(ROI) candidate detection on self-driving hardware plat-\nform Nvidia Jetpack Tx1/2 that can handle high-resolution \nimages. The recognition accuracy and processing speed are \nimproved by combining detection and tracking in Wang \net\u00a0al. (2021a) to enhance the practicality of the traffic signal \nrecognition system in autonomous vehicles using CNN and \nintegrated channel feature tracking to determine the coordi-\nnates and color for traffic lights.\nLane detection aims to identify the left and right lane \nboundaries from a processed image and apply an algorithm \nto track the road ahead. A novel hybrid neural network com-\nbining CNN and recurrent neural network (RNN) for robust \nlane detection in driving scenes has been proposed\u00a0(Zou \net\u00a0al. 2019). Features on each frame of the input video \nwere first abstracted by a CNN encoder and the sequential \nencoded features were processed by a ConvLSTM. The out-\nputs were fed into the CNN decoder for information recon-\nstruction and lane prediction. Another lane detection method \nis an anchor-based single-stage lane detection model called \nLaneATT\u00a0(Tabelini et\u00a0al. 2021). It uses a feature pooling \nmethod with a relatively lightweight backbone CNN while \nmaintaining high accuracy. A novel anchor-based attention \nmechanism to aggregate global information was also pro-\nposed. A new method to impose structure on badly posed \nsemantic segmentation problems is proposed in Ghafoorian \net\u00a0al. (2018) using a generative adversarial network architec-\nture with a discriminator that is trained on both predictions \nand labels at the same time.\nPedestrian detection A two-stage detector SDS-\nRCNN\u00a0(Brazil et\u00a0al. 2017) jointly learned pedestrian detec-\ntion and bounding-box aware semantic segmentation, \nthus encouraging model learning on pedestrian regions. \nRPN+BF\u00a0(Zhang et\u00a0al. 2016b) used a boosted forest to \nreplace second-stage learning and leveraged hard mining for \nproposals. However, involving such downstream classifiers \ncould bring more training complexity. AR-Ped\u00a0(Brazil and \nLiu 2019) exploited sequential labeling policy in the region \nproposal network to gradually filter out better proposals. The \nwork of Chen et\u00a0al. (2018) employed a two-stage pretrained \nperson detector (Faster R-CNN) and an instance segmenta-\ntion model for person re-identification. Each detected person \nis cropped out from the original image and fed to another \nnetwork. Wang et\u00a0al. (2018) introduced repulsion losses that \nprevent a predicted bounding box from shifting to neigh-\nboring overlapped objects to counter occlusions. Two-stage \ndetectors need to generate proposals in the first stage and \nthus are slow for inference in practice. One-stage detector \nGDFL\u00a0(Lin et\u00a0al. 2018) included semantic segmentation, \nwhich guided feature layers to emphasize pedestrian regions. \nLiu et\u00a0al. (2018a) extended the single-stage architecture with \nan asymptotic localization fitting module storing multiple \npredictors to evolve default anchor boxes. This improves \nthe quality of positive samples while enabling hard nega-\ntive mining with increased thresholds. Similar to pedestrian \ndetection, vehicle detection in ITS also is a popular and chal-\nlenging computer vision task\u00a0(Zhao et\u00a0al. 2019).\nVehicle detection Current generic vehicle detectors are \ndivided into two categories: CNN-based two-stage detectors \n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 10 of 27\nIn both cases the focus is on effectively detecting vehicles \nallowing for accurate counts in a given road segment, while \ntracking enables the estimation of average speed and move-\nment directions from traffic video.\nCurrent Methods to\u00a0Overcome Challenge\nA DL method at the edge of the ITS that performs real-\ntime vehicle detection, tracking, and counting in traffic sur-\nveillance video has been proposed in Chen et\u00a0al. (2021b). \nThe neural network detects individual vehicles at the sin-\ngle-frame level by capturing appearance features with the \nYOLOv3 object-detection method, deployed on edge devices \nto minimize bandwidth and power consumption, which are \nmajor practical hurdles in deployment. A vehicle detection \nand tracking approach in adverse weather conditions that \nachieves the best trade-off between accuracy and detection \nspeed in various traffic environments is discussed in Has-\nsaballah et\u00a0al. (2020). Also, a novel dataset called DAWN \n(Kenk and Hassaballah 2020) is introduced for vehicle \ndetection and tracking in adverse weather conditions like \nheavy fog, rain, snow, and sandstorms, to make training less \nbiased. Meanwhile, low resolution and slow framerate issues \nare specifically addressed in Wei et\u00a0al. (2019) to allow large-\nscale implementation on existing urban traffic surveillance \nsystems using SSD-Mobilenet for detection and VGG16 \nfeatures for tracking.\nTraffic Congestion Detection\nModels and\u00a0Algorithms\nThe methods that detect traffic congestion based on com-\nputer vision may also be divided into one-stage methods \nand multi-step methods. The one-stage methods identify \nvehicles from the video images and directly perform traffic \ncongestion detection. Among the one-stage methods are: (1) \nAlexNet and YOLO\u00a0(Chakraborty et\u00a0al. 2018) to distinguish \ncongestion and non-congestion, (2) AlexNet and VGG-\nNet\u00a0(Wang et\u00a0al. 2020d) which classify \u2018jam\u2019 and \u2018no jam\u2019; \nand (3) YOLO and Mask R-CNN\u00a0(Impedovo et\u00a0al. 2019) \nrecognize light, medium, and heavy congestion (identifying \nthe number of vehicles in each frame and then classify). The \nmulti-step methods first apply traffic flow estimation mod-\nels to measure traffic variables and then use the traffic flow \nvariables to infer congestion. Examples of two-stage traffic \ncongestion detection models are: (1) YOLOv3\u00a0(Rashmi and \nShantala 2020) and YOLOv4\u00a0(Sonnleitner et\u00a0al. 2020) for \nvehicle detection and counting, (2) counting vehicles using \nFaster R-CNN\u00a0(Gao et\u00a0al. 2021a) and applying regression \nfor traffic congestion. Beside these, the traffic congestion can \nbe evaluated by the traffic flow detection algorithms\u00a0(Kumar \nand Raubal 2021) using vehicle detection and tracking.\nTable\u202f1\u2002 \u2009(continued)\nApplication\nMethods\nMain challenges\nEdge computing\nInteger linear programming + fast heuristics (Cui et\u00a0al. 2020), SSD \n+ SORT (Ke et\u00a0al. 2020), Deep Deterministic Policy Gradient + \nV2V networking (Dai et\u00a0al. 2019), cloud-edge hybrid + Global \nforeground modeling + Gaussian mixture model (Liu et\u00a0al. 2021a), \nYOLOv3 Wan et\u00a0al. (2022), federated learning (Kairouz et\u00a0al. 2019), \nspectral clustering compression (Chen et\u00a0al. 2021a)\nPower consumption, heterogeneous data sources, cybersecurity, wire-\nless noise, scalability, neural network pruning and model compres-\nsion, installation and maintenance costs\n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 8 of 27\nimprove image segmentation performance in degraded \nimages.\nHeterogeneous, Urban Traffic Conditions\nDense urban traffic scenarios are full of complex visual \nelements, not only in quantity but also in the variety of \ndifferent vehicles and their interactions, as shown in \nFig.\u00a04. The presence of cars, buses, bicycles, and pedes-\ntrians in the same intersection is a significant problem for \nautonomous navigation and trajectory computation (Ma \net\u00a0al. 2018). The different sizes, turning radii, speeds, and \ndriver behaviors are further compounded by the interac-\ntions between these road users. From a DL perspective, \nit is easy to find videos of heterogeneous urban traffic, \nbut labeling for ground truth is very time-consuming. \nSimulation software usually cannot capture the complex \ndynamics of such scenarios, especially the traffic rule-\nbreaking behaviors seen in dense urban centers. In fact, a \nspecific dataset was created to represent these behaviors \nin Chandra et\u00a0al. (2019a). A simulator for unregulated \ndense traffic was created in Cai et\u00a0al. (2020) which is \nuseful for autonomous driving perception and control but \ndoes not represent the trajectory and interactions of real-\nworld road users.\nApplications\nTraffic Flow Estimation\nModels and\u00a0Algorithms\nTraffic flow variables include traffic volume, density, speed, \nand queue length. The algorithms and models to detect and \ntrack objects to estimate traffic flow variables from videos \nmay be classified into one-stage and two-stage methods. In \none-stage methods, the variables are estimated from detec-\ntion results and there is no further classification and location \noptimization, for example: (1) YOLOv3 + Feature stitching \n(Hong et\u00a0al. 2020; (2) YOLOv2 + spatial pyramid pool-\ning\u00a0(Kim et\u00a0al. 2019; (3) AlexNet + optical flow + Gaussian \nmixture model\u00a0(Ke et\u00a0al. 2018a; (4) CNN + optical flow \nbased on UAV video\u00a0(Ke et\u00a0al. 2018b; (5) SSD (single shot \ndetection) based on UAV video\u00a0(Tang et\u00a0al. 2017).\nTwo-stage methods first generate region proposals that \ncontain all potential targets in the input images and then \nconduct classification and location optimization. Exam-\nples of two-stage methods are: (1) Faster R-CNN + SORT \ntracker\u00a0(Fedorov et\u00a0al. 2019; (2) Faster R-CNN\u00a0(Peppa \net\u00a0al. 2018; Mhalla et\u00a0al. 2018; (3) Faster R-CNN based \non UAV video\u00a0(Peppa et\u00a0al. 2021; Brki\u0107 et\u00a0al. 2020).\nFig.\u202f4\u2002 \u2009Illustration of representative scenarios in complex traffic environments. Some demo images are adopted from Yang and Pun-Cheng (2018)\n", [144]], "Edge Computing": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 16 of 27\ncorresponding to normal traffic, which can be used to detect \nanomalies. The image description makes it more robust to \nlighting, perspective, and occlusions. Aboah et\u00a0al. (2021) \nproposed a decision tree-based DL approach for anomaly \ndetection using YOLOv5 for vehicle detection, followed by \nbackground estimation, then a decision tree considers fac-\ntors such as vehicle size, likelihood, and road feature mask \nto eliminate false positives. Adaptive thresholding allows for \nrobustness under variable illumination and weather condi-\ntions. A perspective map approach is discussed by Bai et\u00a0al. \n(2019), which models the background using road segmenta-\ntion based on a traffic flow frequency map, then the perspec-\ntive is detected from linear regression of object sizes based \non ResNet50. Finally, a spatial-temporal matrix discriminat-\ning module performs thresholding on consecutive frames to \ndetect anomalous states.\nCurrent Methods to\u00a0Overcome Challenges\nAnomaly detection relies on surveillance cameras which \nusually provide a view far along the road, but vehicles in \nthe distance occupy only a few pixels which make detection \ndifficult. Thus, Li et\u00a0al. (2020) uses pixel-level tracking in \naddition to box-level tracking for multi-granularity. The key \nidea is mask extraction based on frame difference and vehi-\ncle trajectory tracking based on the Gaussian Mixture Model \nto eliminate moving vehicles combined with segmentation \nbased on frame changes to also eliminate parking zones. \nAnomaly fusion uses the box and pixel-level tracking fea-\ntures with backtracking optimization to refine predictions. \nSurveillance cameras are prone to shaking in the wind, so \nvideo stabilization preprocessing was performed before \nusing two-stage vehicle detection in the form of Faster \nR-CNN and Cascade R-CNN (Zhao et\u00a0al. 2021b). An effi-\ncient real-time method for anomaly detection from surveil-\nlance video decouples the appearance and motion learning \ninto two parts (Li et\u00a0al. 2021c). First, an autoencoder learns \nappearance features, then 3D convolutional layers can use \nlatent codes from multiple past frames to predict features \nfor future frames. A significant difference between predicted \nand actual features indicates an anomaly. The model can \nbe deployed on edge nodes near the traffic cameras, and \nthe latent features appear to be robust to illumination and \nweather changes compared to pixel-wise methods.\nTo shed reliance on annotated data for anomalies, an \nunsupervised one-class approach in Pawar and Attar (2021) \napplies spatio-temporal convolutional autoencoder to get \nlatent features, stacks them together, and a sequence-to-\nsequence LSTM learns the temporal patterns. The method \nperforms well on multiple real-world surveillance footage \ndatasets, but not better than supervised training methods. \nThe advantage is that it can be indefinitely trained on normal \ntraffic data without any labeled anomalies.\nEdge Computing\nModels and\u00a0Algorithms\nComputer vision in ITS requires efficient infrastructure \narchitecture to analyze data in real time. If all acquired video \nstreams are sent to a single server, the required bandwidth \nand computation would not be able to provide a usable ser-\nvice. For example, edge computing architecture for real-time \nautomatic failure detection using a video usefulness metric \nwas explored in (Sun et\u00a0al. 2020a). Only video deemed to \nbe useful is transmitted to the server, while malfunction of \nthe surveillance camera, or obstruction of view, is automati-\ncally reported. Edge-cloud-based computing can implement \nDL models, not just for computer vision tasks, but also for \nresource allocation and efficiency (Xie et\u00a0al. 2021). Passive \nsurveillance has now been superseded in literature by the \nincreasing availability of sensor-equipped vehicles that can \nperform perception and mapping cooperatively (Zhang and \nLetaief 2020).\nOnboard computing resources in vehicles are often not \npowerful enough to process all sensor data in real time, and \napplications like localization and mapping can be very com-\nputationally intensive. The internet of things (IoT) archi-\ntecture allows for edge nodes to offload that computation \nand provide results at low latency to nearby users (Ferdowsi \net\u00a0al. 2019). This approach can avoid multiple cars doing \nthe same computation with similar inputs. One technique to \noffload computation tasks is discussed in Cui et\u00a0al. (2020), \ncombining integer linear programming for offline scheduling \noptimization and heuristics for online, real-world deploy-\nment. The authors compress 3D point cloud LIDAR data \ncollected from the vehicle\u2019s sensor and send it to the edge \nnode for classification and feature extraction. A deep rein-\nforcement learning algorithm known as Deep Determinis-\ntic Policy Gradient is proposed in Dai et\u00a0al. (2019), which \ncan dynamically allocate computing and caching resources \nthroughout the network. Future work in this direction will \nhandle multiple communication channels, interference man-\nagement, forecast handover, and bandwidth allocation. In \nthe macro scale, V2V communication can be used for traffic \nparameter estimation and management with sparse connec-\ntivity, while higher connected vehicle market penetration \nwill allow safety applications like collision avoidance (Dey \net\u00a0al. 2016).\nApplications for vehicles can include near-crash detec-\ntion, navigation, video streaming, and smart traffic lights. \nThe onboard unit can also be used as a mobile cache, and \ncommunicate with other vehicles via V2V networking. Real-\ntime near-crash detection using edge computing was devel-\noped in Ke et\u00a0al. (2020). The system uses dashcam video for \nSSD vehicle and pedestrian detection, followed by SORT \nfor tracking to estimate the time to collision (TTC). It was \n", []], "Traffic Anomaly Detection": ["Data Science for Transportation (2024) 6:1\t\nPage 15 of 27\u2003\n1\nLSTM to predict the trajectory. It can perform accurately in \ndense, heterogeneous, urban traffic conditions in real time. \nThe paper also contributes a new labeled dataset captured \nfrom crowded Asian cities. To be useful, trajectory predic-\ntion needs to take into account the motion of surrounding \nobjects and inter-object interactions in real time. Therefore, \na different approach to motion prediction is discussed in Li \net\u00a0al. (2019) based on the graph convolutional model, which \ntakes trajectory data as input and represents the interactions \nof nearby objects and extracts features. The graph model out-\nput is then passed into an encoder-decoder LSTM model for \nrobust predictions that can consider the interaction between \nvehicles. The method enables 30% higher prediction accu-\nracy in addition to 5x faster execution. The algorithm uses \ntrajectory data that has already been extracted from surveil-\nlance video data like NGSIM Colyar and Halkias (2007). In \nTripicchio and D\u2019Avella (2022), vehicle trajectory of vehi-\ncles is calculated using Lucas-Kanade algorithm on dash-\ncam video. Synthetic data was also used for augmenting the \ndataset to train an LSTM network to predict future motion \nand an SVM is used to classify the action, for eg. changing \nlanes. The method predicts the next 6 seconds of motion on \nhighways with 92% accuracy.\nCurrent Methods to\u00a0Overcome Challenges\nThe dynamics of vulnerable road users are described by a \nSwitching Linear Dynamical System (SLDS) in Kooij et\u00a0al. \n(2019) and extended with a dynamic bayesian network using \ncontext from features extracted from vehicle-mounted ste-\nreo cameras focusing on both static and dynamic cues. The \napproach can work in real-time, providing accurate predic-\ntions of road user trajectories. It can be improved by the \ninclusion of more context such as traffic lights and pedes-\ntrian crossings. The use of onboard camera and LiDAR \nalong with V2V communication is explored in Choi et\u00a0al. \n(2021b) to predict trajectories using the random forest and \nLSTM architecture. YOLO is used to detect cars and pro-\nvide bounding boxes, while LiDAR provides subtle changes \nin position, and V2V communication transmits raw values \nlike steering angles to reduce the uncertainty and latency of \npredictions.\nThe TRAF dataset was used in Chandra et\u00a0al. (2019b) \nfor robust end-to-end real-time trajectory prediction from \nstill or moving cameras. Mask R-CNN and reciprocal veloc-\nity obstacles algorithm are used for multi-vehicle tracking. \nThe last 3 seconds of tracking are used to predict the next \n5 seconds of trajectory as in Chandra et\u00a0al. (2019a), with \nthe added advantage of being end-to-end trainable and not \nrequiring annotated trajectory data. The paper also con-\ntributes TrackNPred, a python-based library that contains \nimplementations of different trajectory prediction methods. \nIt is a common interface for many trajectory prediction \napproaches and can be used for performance comparisons \nusing standard error measurement metrics on real-world \ndense and heterogeneous traffic datasets.\nMost DL methods for trajectory prediction do not uncover \nthe underlying reward function, instead, they only rely on \npreviously seen examples, which hinders generalizability \nand limits their scope. In Fernando et\u00a0al. (2021), inverse \nreinforcement learning is used to find the reward function so \nthat the model can be said to have a tangible goal, allowing \nit to be deployed in any environment. Transformer-based \nmotion prediction is performed in Liu et\u00a0al. (2021b) to \nachieve state-of-the-art multimodal trajectory prediction in \nthe Agroverse dataset. The network models both the road \ngeometry and interactions between the vehicles. Pedestrian \nintention in complex urban scenarios is predicted by graph \nconvolution networks on spatio-temporal graphs in Liu et\u00a0al. \n(2020a). The method considers the relationship between \npedestrians waiting to cross and the movement of vehicles. \nWhile achieving 80% accuracy on multiple datasets, it pre-\ndicts intent to cross one second in advance. On the other \nhand, pedestrians modeled as automatons, combined with \nSVM without the need for pose information, result in longer \npredictions but lack the consideration of contextual informa-\ntion (Jayaraman et\u00a0al. 2020).\nTraffic Anomaly Detection\nModels and\u00a0Algorithms\nTraffic surveillance cameras can be used to automatically \ndetect traffic anomalies like stopped vehicles and queues. \nThe detection of low-level image features like corners of \nvehicles has been used by Albiol et\u00a0al. (2011) to demonstrate \nqueue detection and queue length estimation without object \ntracking or background removal in different lighting condi-\ntions. Tracking methods based on optical flow can not only \nprovide queue length, but also speed, vehicle count, wait-\ning time, and time headway. In Shirazi and Morris (2015), \nthe authors use optical flow assuming constant short-term \nbrightness to detect vehicle features and successfully track \nthem even with occlusions. The speed of individual vehi-\ncles can be estimated, allowing the detection of stopped \nvehicles or queue formation. Trajectory analysis has also \nbeen deployed to identify illegal or dangerous movements \n(Nowosielski et\u00a0al. 2016). The background subtraction-based \napproaches are, however, limited to favorable scenarios and \ndo not generalize well.\nAn interesting method is applied in Li et\u00a0al. (2016a) \ninvolving partitioning the video into spatial and temporal \nblocks, local invariant features are then learned from traffic \nfootage to create a visual codebook of the image descriptors \nusing Locality-constrained Linear Coding. Then, a Gauss-\nian distribution model is trained to learn the probabilities \n", []], "Road User Behavior Prediction": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 14 of 27\nDashcam videos were used to train a Dynamic Spatial \nAssistance (DSA) network to distribute attention to objects \nand model temporal dependencies in Chan et\u00a0al. (2016). The \nmethod was able to predict crashes around 2 s in advance. \nUnderstanding multi-vehicle interaction in urban environ-\nments is challenging, and model-based methods may require \nprior knowledge, so a more general approach is explored \nin Zhang et\u00a0al. (2019), where YOLOv3 is used for object \ntracking from traffic camera video from the NGSIM dataset \nand a Gaussian velocity field is used to describe the inter-\naction behaviors between multiple vehicles. From this, an \n11-layer deep autoencoder learns the latent low-dimensional \nrepresentations for each frame, followed by a hidden semi-\nMarkov model with a hierarchical Dirichlet process, which \noptimizes the number of interaction patterns, to cluster rep-\nresentations into traffic primitives corresponding to the inter-\naction patterns. The pipeline can be used to analyze complex \nmulti-agent interactions from traffic video. DL methods are \nable to extract semantic descriptions from video, like in Li \net\u00a0al. (2021b), which can give advance warning of risky \nsituations. A scenario-wise spatio-temporal attention guid-\nance system was created by data mining from descriptive \nsemantic variables in fatal crash data to support the design \nof a model based on YOLOv3 for evaluating crash risk from \ndashcam footage. The attention guidance extracted seman-\ntic descriptions like \u201cpedestrian\u201d, \u201cschool bus\u201d and \u201catmos-\npheric condition\u201d, followed by DL to optimize attention on \nthese variables to identify clusters and associate scene fea-\ntures with a crash features.\nCurrent Methods to\u00a0Overcome Challenges\nWhile most vehicle interaction methods reviewed thus far \nmake little mention of the practical challenges in variable \nweather and lighting, Zhang et\u00a0al. (2018a) highlights a back-\nground learning method specifically to adapt to changing \nlighting conditions and headlight illumination in surveil-\nlance footage and even utilizes a threshold-based noise \nremoval for rainy conditions to detect near-miss events at \ngrade crossings. Domain adaptation, an example of transfer \nlearning, was employed in Li et\u00a0al. (2021a) to make use of \nlabeled daytime footage for vehicle detection in unlabeled \nnighttime images by a generative adversarial network called \nCycleGAN (Zhu et\u00a0al. 2017), which can be used with many \nreal-world deep learning computer vision applications. \nYouTube dashcam footage was used for crash detection in \nan ensemble multimodal DL method, based on the gated \nrecurrent unit (GRU) and CNN, which uses both video and \naudio data (Choi et\u00a0al. 2021a). The real-world data con-\nsists of positive clips containing crashes and negative clips \ncontaining normal driving. A crowd-sourced dashcam video \ndataset was also contributed by Chan et\u00a0al. (2016) for acci-\ndent anticipation containing scenarios like crowded streets, \ncomplicated road environments, and diversity of accidents. \nTo address low-visibility conditions like rain, fog, and \nnighttime footage, Wang et\u00a0al. (2020c) used Retinex image \nenhancement algorithm for preprocessing and YOLOv3 \nfor object detection, followed by a decision tree to classify \ncrashes. It balances dynamic range and enhances edges, but \ncongested mixed-flow traffic, lower-quality video, and fast \nvehicles are still major sources of error. The use of deep con-\nvolutional autoencoders for representation learning comple-\nmented with vehicle tracking is used to detect accidents from \nsurveillance footage in Singh and Mohan (2019). The test-\ning was performed on data collected during bright sunlight, \nnight, early morning, and also from a variety of cameras and \nangles. However, there are significant false alarms caused \nby low visibility, occlusions, and large variations in traffic \npatterns. The lack of near-miss data can be met by combin-\ning vehicle event recorder data and object detection from \nan onboard camera as proposed in Yamamoto et\u00a0al. (2022). \nBy extracting two deep feature representations that consider \nthe car status and the surrounding objects, the deep learning \nmethod can label near-miss events. While the method does \nnot claim to be real-time, it can generate large volumes of \nlabeled training data for near-crash events.\nA method to detect cycling near-misses from front view \nvideo is developed in Ibrahim et\u00a0al. (2021) using optical \nflow, CNN, LSTM, and a fully connected prediction stage. \nThe method was trained with complex urban environments \nand also contributes to a large dataset containing labeled \nnear-miss events. A ResNet-based model was used to detect \npedestrians and evaluate risk from a near-miss dataset in \nSuzuki et\u00a0al. (2017). The dataset contains videos from dif-\nferent vehicles, places (intersections, city, major roads), day \nand night time, and weather conditions. However, the model \nsuffers from overfitting as a result of having only near-miss \ndata for training.\nRoad User Behavior Prediction\nModels and\u00a0Algorithms\nTrajectory prediction from videos is useful for autonomous \ndriving, traffic forecasting, and congestion management. \nOlder works in this domain focused on homogeneous agents \nsuch as cars on a highway or pedestrians in a crowd, whereas \nheterogeneous agents were only considered in sparse sce-\nnarios with certain assumptions like lane-based driving. A \nlong short-term memory (LSTM) and CNN hybrid network, \nthat learns the relationship between pairs of heterogeneous \nagents, was developed in Chandra et\u00a0al. (2019a) to extract \nagent shape, velocity, and traffic concentration, which are \npassed through LSTMs to generate horizon and neighbor-\nhood maps, which then go through convolution networks to \nproduce latent representations that are passed through a final \n", []], "Vehicle Interaction": ["Data Science for Transportation (2024) 6:1\t\nPage 13 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nRecent directions in segmentation include weakly-super-\nvised semantic segmentation\u00a0(Wang et\u00a0al. 2020e; Sun et\u00a0al. \n2020c), domain adaptation\u00a0(Chen et\u00a0al. 2019e; Liu et\u00a0al. \n2021c), multi-modal data fusion\u00a0(Feng et\u00a0al. 2020; Cortinhal \net\u00a0al. 2021), and real-time semantic segmentation\u00a0(Yu et\u00a0al. \n2018; Nirkin et\u00a0al. 2021; Gao et\u00a0al. 2021b).\nTS-Yolo Wan et\u00a0al. (2021b) is a CNN-based model for \naccurate traffic detection under severe weather conditions \nusing new samples from data augmentation. The data aug-\nmentation was conducted using copy\u2013paste strategy, and a \nlarge number of new samples were constructed from exist-\ning traffic-sign instances. Based on YoloV5, MixConv was \nalso used to mix different kernel sizes in a single convo-\nlution operation so that patterns with various resolutions \ncan be captured. Detecting and classifying real-life small \ntraffic signs from large input images is difficult due to them \noccupying fewer pixels relative to larger targets. To address \nthis, Dense-RefineDet\u00a0(Sun et\u00a0al. 2020) applies a single-shot, \nobject-detection framework to maintain a suitable accuracy-\nspeed trade-off. An end-to-end traffic sign detection frame-\nwork Feature Aggregation Multipath Network is proposed in \nOu et\u00a0al. (2019) to solve the problems of small object detec-\ntion and fine-grained classification in traffic sign detection.\nCooperative Perception\nModels and\u00a0Algorithms\nIn connected autonomous vehicles (CAV), cooperative per-\nception can be performed at three levels depending on the \ntype of data: early fusion (raw data), intermediate fusion \n(preprocessed data), where intermediate neural features are \nextracted and transmitted, and late fusion (processed data), \nwhere detection outputs (3D bounding box position, con-\nfidence score) are shared. Cooperative perception studies \nhow to leverage visual cues from neighboring connected \nvehicles and infrastructure to boost the overall perception \nperformance\u00a0(Xu et\u00a0al. 2022). \n(1)\t Early fusion: Chen et\u00a0al. (2019d) fuses the sensor data \ncollected from different positions and angles of con-\nnected vehicles using raw-data level LiDAR 3D point \nclouds, and a point cloud-based 3D object detection \nmethod is proposed to work on a diversity of aligned \npoint clouds. DiscoNet\u00a0(Li et\u00a0al. 2021d) leverages \nknowledge distillation to enhance training by constrain-\ning the corresponding features to the ones from the net-\nwork for early fusion.\n(2)\t Intermediate fusion: F-Cooper\u00a0(Chen et\u00a0al. 2019b) pro-\nvides both a new framework for applications On-Edge, \nservicing autonomous vehicles as well as new strategies \nfor 3D fusion detection. Wang et\u00a0al. (2020b) proposed \na vehicle-to-vehicle (V2V) approach for perception \nand prediction that transmits compressed intermediate \nrepresentations of the P &P neural network. Xu et\u00a0al. \n(2021) proposed an Attentive Intermediate Fusion pipe-\nline to better capture interactions between connected \nagents within the network. A robust cooperative per-\nception framework with vehicle-to-everything (V2X) \ncommunication using a novel vision Transformer is \npresented in Xu et\u00a0al. (2022).\n(3)\t Late fusion: Car2X-based perception\u00a0(Rauch et\u00a0al. \n2012) is modeled as a virtual sensor to integrate it into \na high-level sensor data fusion architecture.\nCurrent Methods to\u00a0Overcome Challenge\nTo reduce the communications load and overhead, an \nimproved algorithm for message generation rules in col-\nlective perception is proposed\u00a0(Thandavarayan et\u00a0al. 2020), \nwhich improves the reliability of V2X communications by \nreorganizing the transmission and content of collective per-\nception messages. This paper\u00a0(Yoon et\u00a0al. 2021) presents and \nevaluates a unified cooperative perception framework con-\ntaining a decentralized data association and fusion process \nthat is scalable with respect to participation variances. The \nevaluation considers the effects of communication losses in \nthe ad-hoc V2V network and the random vehicle motions in \ntraffic by adopting existing models along with a simplified \nalgorithm for individual vehicle\u2019s on-board sensor field of \nview. AICP is proposed in Zhou et\u00a0al. (2022), the first solu-\ntion that focuses on optimizing informativeness for perva-\nsive cooperative perception systems with efficient filtering \nat both the network and application layers. To facilitate sys-\ntem networking, they also use a networking protocol stack \nthat includes a dedicated data structure and a lightweight \nrouting protocol specifically for informativeness-focused \napplications.\nVehicle Interaction\nModels and\u00a0Algorithms\nComputer vision methods can be used to detect and clas-\nsify crash and near-crash events based on motion and tra-\njectories. CNN, in the form of modified YOLOv3, is used \nto detect objects and extract semantic information about the \nroad scene from onboard camera data from the SHRP2 data-\nset in Taccari et\u00a0al. (2018). Optical flow is calculated from \nconsecutive frames to track objects and to generate features \n(hard deceleration, the maximum area of the largest vehicle, \ntime to collision, etc.) that are combined with telematics \ndata (speed and 3-axis acceleration) to train a random forest \nclassifier on safe, near-crash, and crash events.\n", []], "Current Methods to\u00a0Overcome Challenge": ["Data Science for Transportation (2024) 6:1\t\nPage 13 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nRecent directions in segmentation include weakly-super-\nvised semantic segmentation\u00a0(Wang et\u00a0al. 2020e; Sun et\u00a0al. \n2020c), domain adaptation\u00a0(Chen et\u00a0al. 2019e; Liu et\u00a0al. \n2021c), multi-modal data fusion\u00a0(Feng et\u00a0al. 2020; Cortinhal \net\u00a0al. 2021), and real-time semantic segmentation\u00a0(Yu et\u00a0al. \n2018; Nirkin et\u00a0al. 2021; Gao et\u00a0al. 2021b).\nTS-Yolo Wan et\u00a0al. (2021b) is a CNN-based model for \naccurate traffic detection under severe weather conditions \nusing new samples from data augmentation. The data aug-\nmentation was conducted using copy\u2013paste strategy, and a \nlarge number of new samples were constructed from exist-\ning traffic-sign instances. Based on YoloV5, MixConv was \nalso used to mix different kernel sizes in a single convo-\nlution operation so that patterns with various resolutions \ncan be captured. Detecting and classifying real-life small \ntraffic signs from large input images is difficult due to them \noccupying fewer pixels relative to larger targets. To address \nthis, Dense-RefineDet\u00a0(Sun et\u00a0al. 2020) applies a single-shot, \nobject-detection framework to maintain a suitable accuracy-\nspeed trade-off. An end-to-end traffic sign detection frame-\nwork Feature Aggregation Multipath Network is proposed in \nOu et\u00a0al. (2019) to solve the problems of small object detec-\ntion and fine-grained classification in traffic sign detection.\nCooperative Perception\nModels and\u00a0Algorithms\nIn connected autonomous vehicles (CAV), cooperative per-\nception can be performed at three levels depending on the \ntype of data: early fusion (raw data), intermediate fusion \n(preprocessed data), where intermediate neural features are \nextracted and transmitted, and late fusion (processed data), \nwhere detection outputs (3D bounding box position, con-\nfidence score) are shared. Cooperative perception studies \nhow to leverage visual cues from neighboring connected \nvehicles and infrastructure to boost the overall perception \nperformance\u00a0(Xu et\u00a0al. 2022). \n(1)\t Early fusion: Chen et\u00a0al. (2019d) fuses the sensor data \ncollected from different positions and angles of con-\nnected vehicles using raw-data level LiDAR 3D point \nclouds, and a point cloud-based 3D object detection \nmethod is proposed to work on a diversity of aligned \npoint clouds. DiscoNet\u00a0(Li et\u00a0al. 2021d) leverages \nknowledge distillation to enhance training by constrain-\ning the corresponding features to the ones from the net-\nwork for early fusion.\n(2)\t Intermediate fusion: F-Cooper\u00a0(Chen et\u00a0al. 2019b) pro-\nvides both a new framework for applications On-Edge, \nservicing autonomous vehicles as well as new strategies \nfor 3D fusion detection. Wang et\u00a0al. (2020b) proposed \na vehicle-to-vehicle (V2V) approach for perception \nand prediction that transmits compressed intermediate \nrepresentations of the P &P neural network. Xu et\u00a0al. \n(2021) proposed an Attentive Intermediate Fusion pipe-\nline to better capture interactions between connected \nagents within the network. A robust cooperative per-\nception framework with vehicle-to-everything (V2X) \ncommunication using a novel vision Transformer is \npresented in Xu et\u00a0al. (2022).\n(3)\t Late fusion: Car2X-based perception\u00a0(Rauch et\u00a0al. \n2012) is modeled as a virtual sensor to integrate it into \na high-level sensor data fusion architecture.\nCurrent Methods to\u00a0Overcome Challenge\nTo reduce the communications load and overhead, an \nimproved algorithm for message generation rules in col-\nlective perception is proposed\u00a0(Thandavarayan et\u00a0al. 2020), \nwhich improves the reliability of V2X communications by \nreorganizing the transmission and content of collective per-\nception messages. This paper\u00a0(Yoon et\u00a0al. 2021) presents and \nevaluates a unified cooperative perception framework con-\ntaining a decentralized data association and fusion process \nthat is scalable with respect to participation variances. The \nevaluation considers the effects of communication losses in \nthe ad-hoc V2V network and the random vehicle motions in \ntraffic by adopting existing models along with a simplified \nalgorithm for individual vehicle\u2019s on-board sensor field of \nview. AICP is proposed in Zhou et\u00a0al. (2022), the first solu-\ntion that focuses on optimizing informativeness for perva-\nsive cooperative perception systems with efficient filtering \nat both the network and application layers. To facilitate sys-\ntem networking, they also use a networking protocol stack \nthat includes a dedicated data structure and a lightweight \nrouting protocol specifically for informativeness-focused \napplications.\nVehicle Interaction\nModels and\u00a0Algorithms\nComputer vision methods can be used to detect and clas-\nsify crash and near-crash events based on motion and tra-\njectories. CNN, in the form of modified YOLOv3, is used \nto detect objects and extract semantic information about the \nroad scene from onboard camera data from the SHRP2 data-\nset in Taccari et\u00a0al. (2018). Optical flow is calculated from \nconsecutive frames to track objects and to generate features \n(hard deceleration, the maximum area of the largest vehicle, \ntime to collision, etc.) that are combined with telematics \ndata (speed and 3-axis acceleration) to train a random forest \nclassifier on safe, near-crash, and crash events.\nData Science for Transportation (2024) 6:1\t\nPage 13 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nRecent directions in segmentation include weakly-super-\nvised semantic segmentation\u00a0(Wang et\u00a0al. 2020e; Sun et\u00a0al. \n2020c), domain adaptation\u00a0(Chen et\u00a0al. 2019e; Liu et\u00a0al. \n2021c), multi-modal data fusion\u00a0(Feng et\u00a0al. 2020; Cortinhal \net\u00a0al. 2021), and real-time semantic segmentation\u00a0(Yu et\u00a0al. \n2018; Nirkin et\u00a0al. 2021; Gao et\u00a0al. 2021b).\nTS-Yolo Wan et\u00a0al. (2021b) is a CNN-based model for \naccurate traffic detection under severe weather conditions \nusing new samples from data augmentation. The data aug-\nmentation was conducted using copy\u2013paste strategy, and a \nlarge number of new samples were constructed from exist-\ning traffic-sign instances. Based on YoloV5, MixConv was \nalso used to mix different kernel sizes in a single convo-\nlution operation so that patterns with various resolutions \ncan be captured. Detecting and classifying real-life small \ntraffic signs from large input images is difficult due to them \noccupying fewer pixels relative to larger targets. To address \nthis, Dense-RefineDet\u00a0(Sun et\u00a0al. 2020) applies a single-shot, \nobject-detection framework to maintain a suitable accuracy-\nspeed trade-off. An end-to-end traffic sign detection frame-\nwork Feature Aggregation Multipath Network is proposed in \nOu et\u00a0al. (2019) to solve the problems of small object detec-\ntion and fine-grained classification in traffic sign detection.\nCooperative Perception\nModels and\u00a0Algorithms\nIn connected autonomous vehicles (CAV), cooperative per-\nception can be performed at three levels depending on the \ntype of data: early fusion (raw data), intermediate fusion \n(preprocessed data), where intermediate neural features are \nextracted and transmitted, and late fusion (processed data), \nwhere detection outputs (3D bounding box position, con-\nfidence score) are shared. Cooperative perception studies \nhow to leverage visual cues from neighboring connected \nvehicles and infrastructure to boost the overall perception \nperformance\u00a0(Xu et\u00a0al. 2022). \n(1)\t Early fusion: Chen et\u00a0al. (2019d) fuses the sensor data \ncollected from different positions and angles of con-\nnected vehicles using raw-data level LiDAR 3D point \nclouds, and a point cloud-based 3D object detection \nmethod is proposed to work on a diversity of aligned \npoint clouds. DiscoNet\u00a0(Li et\u00a0al. 2021d) leverages \nknowledge distillation to enhance training by constrain-\ning the corresponding features to the ones from the net-\nwork for early fusion.\n(2)\t Intermediate fusion: F-Cooper\u00a0(Chen et\u00a0al. 2019b) pro-\nvides both a new framework for applications On-Edge, \nservicing autonomous vehicles as well as new strategies \nfor 3D fusion detection. Wang et\u00a0al. (2020b) proposed \na vehicle-to-vehicle (V2V) approach for perception \nand prediction that transmits compressed intermediate \nrepresentations of the P &P neural network. Xu et\u00a0al. \n(2021) proposed an Attentive Intermediate Fusion pipe-\nline to better capture interactions between connected \nagents within the network. A robust cooperative per-\nception framework with vehicle-to-everything (V2X) \ncommunication using a novel vision Transformer is \npresented in Xu et\u00a0al. (2022).\n(3)\t Late fusion: Car2X-based perception\u00a0(Rauch et\u00a0al. \n2012) is modeled as a virtual sensor to integrate it into \na high-level sensor data fusion architecture.\nCurrent Methods to\u00a0Overcome Challenge\nTo reduce the communications load and overhead, an \nimproved algorithm for message generation rules in col-\nlective perception is proposed\u00a0(Thandavarayan et\u00a0al. 2020), \nwhich improves the reliability of V2X communications by \nreorganizing the transmission and content of collective per-\nception messages. This paper\u00a0(Yoon et\u00a0al. 2021) presents and \nevaluates a unified cooperative perception framework con-\ntaining a decentralized data association and fusion process \nthat is scalable with respect to participation variances. The \nevaluation considers the effects of communication losses in \nthe ad-hoc V2V network and the random vehicle motions in \ntraffic by adopting existing models along with a simplified \nalgorithm for individual vehicle\u2019s on-board sensor field of \nview. AICP is proposed in Zhou et\u00a0al. (2022), the first solu-\ntion that focuses on optimizing informativeness for perva-\nsive cooperative perception systems with efficient filtering \nat both the network and application layers. To facilitate sys-\ntem networking, they also use a networking protocol stack \nthat includes a dedicated data structure and a lightweight \nrouting protocol specifically for informativeness-focused \napplications.\nVehicle Interaction\nModels and\u00a0Algorithms\nComputer vision methods can be used to detect and clas-\nsify crash and near-crash events based on motion and tra-\njectories. CNN, in the form of modified YOLOv3, is used \nto detect objects and extract semantic information about the \nroad scene from onboard camera data from the SHRP2 data-\nset in Taccari et\u00a0al. (2018). Optical flow is calculated from \nconsecutive frames to track objects and to generate features \n(hard deceleration, the maximum area of the largest vehicle, \ntime to collision, etc.) that are combined with telematics \ndata (speed and 3-axis acceleration) to train a random forest \nclassifier on safe, near-crash, and crash events.\n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 12 of 27\nand CNN-based one-stage detectors. the representative two-\nstage detectors include Faster R-CNN\u00a0(Zhang et\u00a0al. 2016), \nspatial pyramid pooling (SPP)-net\u00a0(He et\u00a0al. 2015), fea-\nture pyramid networks (FPN)\u00a0(Lin et\u00a0al. 2017), and Mask \nR-CNN\u00a0(He et\u00a0al. 2017). The representative one-stage detec-\ntors include YOLO\u00a0(Redmon et\u00a0al. 2016), Single Shot Multi-\nBox Detector (SSD)\u00a0(Liu et\u00a0al. 2016), and deeply supervised \nobject detectors (DSOD)\u00a0(Shen et\u00a0al. 2017). The two-step \nframework is a region proposal-based method, giving a \ncoarse scan of the whole image first and then focusing on \nregions of interest (RoIs). While one-step frameworks are \nbased on global regression/ classification, mapping straightly \nfrom image pixels to bounding box coordinates and class \nprobabilities. Based on these two frameworks, most of the \nworks get promising results in vehicle detection by combin-\ning other methods, such as multitask learning\u00a0(Brahmbhatt \net\u00a0al. 2017), multi-scale representation\u00a0(Bell et\u00a0al. 2016), and \ncontext modeling\u00a0(Kong et\u00a0al. 2016).\nCurrent Methods to\u00a0Overcome Challenge\nIn traffic sign detection, existing traffic sign datasets are \nlimited in terms of the type and severity of challenging \nconditions. Metadata corresponding to these conditions is \nunavailable and it is not possible to investigate the effect \nof a single factor because of the simultaneous changes in \nnumerous conditions. To overcome this, Temel et\u00a0al. (2019) \nintroduced the CURE-TSDReal dataset, based on simulated \nconditions that correspond to real-world environments. An \nend-to-end traffic sign detection framework Feature Aggre-\ngation Multipath Network (FAMN) is proposed in Ou et\u00a0al. \n(2019). It consists of two main structures named Feature \nAggregation and Multipath Network structure to solve the \nproblems of small object detection and fine-grained clas-\nsification in traffic sign detection.\nA vehicle highlight information-assisted neural net-\nwork for vehicle detection at night is presented in Mo et\u00a0al. \n(2019), which included two innovations: establishing the \nlabel hierarchy for vehicles based on their highlights and \ndesigning a multi-layer fused vehicle highlight informa-\ntion network. Real-time vehicle detection for nighttime \nsituations is presented in Bell et\u00a0al. (2021), where images \ninclude flashes that occupy large image regions, and the \nactual shape of vehicles is not well defined. By using a \nglobal image descriptor along with a grid of foveal classi-\nfiers, vehicle positions are accurately and efficiently esti-\nmated. AugGAN Lin et\u00a0al. (2020) is an unpaired image-to-\nimage translation network for domain adaptation in vehicle \ndetection. It quantitatively surpassed competing methods \nfor achieving higher nighttime vehicle detection accuracy \nbecause of better image-object preservation. A stepwise \ndomain adaptation (SDA) detection method is proposed \nto further improve the performance of CycleGAN by \nminimizing the divergence in cross-domain object detec-\ntion tasks in Li et\u00a0al. (2022). In the first step, an unpaired \nimage-to-image translator is trained to construct a fake \ntarget domain by translating the source images to similar \nones in the target domain. In the second step, to further \nminimize divergence across domains, an adaptive Center-\nNet is designed to align distributions at the feature level \nin an adversarial learning manner.\nAutonomous Driving Perception: Segmentation\nModels and\u00a0Algorithms\nImage segmentation contains three sub-tasks: semantic seg-\nmentation, instance segmentation, and panoptic segmenta-\ntion. Semantic segmentation is a fine prediction task to label \neach pixel of an image with a corresponding object class, \ninstance segmentation is designed to identify and segment \npixels that belong to each object instance, while panoptic \nsegmentation unifies semantic segmentation and instance \nsegmentation such that all pixels are given both a class label \nand an instance ID (Gu et\u00a0al. 2022).\nYOLACT\u00a0(Bolya et\u00a0al. 2019) splits instance segmenta-\ntion into two parallel sub-architectures. Protonet architec-\nture extracts spatial information by generating a certain \nnumber of prototype masks, and Head architecture gener-\nates the mask coefficients and object locations. In addi-\ntion, it employs Fast NMS rather than traditional NMS to \nreduce post-processing time. Path Aggregation Network \n(PANet)\u00a0(Liu et\u00a0al. 2018b) is proposed to integrate com-\nprehensively low-level location information and high-level \nsemantic information. Based on Feature Pyramid Networks \n(FPN)\u00a0(Lin et\u00a0al. 2017), PANet designs a bottom-up con-\ntext information aggregation structure, which can integrate \ndifferent levels of features. Hybrid Task Cascade (HTC) is \nproposed for instance segmentation in Chen et\u00a0al. (2019c). \nIt interweaves box and mask branches for joint multi-stage \nprocessing, adopts a semantic segmentation branch to pro-\nvide spatial context, and integrates complementary features \ntogether in each stage.\nIn Dong et\u00a0al. (2020), a novel real-time segmentation is \nproposed consisting of a convolutional attention module, \nspatial pyramid pooling, and a feature fusion network. It was \nevaluated on benchmark datasets Cityscapes and CamVid, \nwhich specifically target complex urban scenarios.\nMoving objects viewed from a moving platform pose a \nunique challenge for segmentation, which is addressed in \nZhou et\u00a0al. (2017) using time-consecutive stereo images. \nMotion likelihood estimates for each pixel aids in ego-\nmotion estimation, while segmentation is performed using \na graph-cut algorithm. However, computational complexity \nis a major limitation of this method.\nData Science for Transportation (2024) 6:1\t\nPage 11 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nCongestion detection performance can be improved using \nmultiple sensors based solutions including radar, lasers, and \nsensor fusion since it is hard to achieve ideal performance \nand accuracy using a single sensor in real-world scenarios. \nThere is a wide use of decision-making algorithms for pro-\ncessing fusion data acquired from multiple sensors\u00a0(Muham-\nmad et\u00a0al. 2020). A CNN-based model trained with bad \nweather condition datasets can improve the detection per-\nformance\u00a0(Sharma et\u00a0al. 2022), while generative adversarial \nnetwork (GAN) based Style Transfer methods have also been \napplied\u00a0(Lin et\u00a0al. 2020; Li et\u00a0al. 2021a). These approaches \nhelp to minimize the model challenges related to generaliz-\nability, which in turn improves real-world performance in a \nvariety of environments.\nAutonomous Driving Perception: Detection\nModels and\u00a0Algorithms\nCommon detection tasks that assist in AD are categorized \ninto traffic sign detection, traffic signal detection, road/lane \ndetection, pedestrian detection, and vehicle detection.\nTraffic signs There are two tasks in a typical traffic sign \nrecognition system: finding the locations and sizes of traf-\nfic signs in natural scene images (traffic sign detection) and \nclassifying the traffic signs into their specific sub-classes \n(traffic sign classification)\u00a0(Yang et\u00a0al. 2015). An improved \nSparse R-CNN was used for traffic sign detection in Cao \net\u00a0al. (2021), while an efficient algorithm based on YOLOv3 \nmodel for traffic sign detection was implemented in Wan \net\u00a0al. (2021a). SegU-Net, formed by merging the state-of-\nthe-art segmentation architectures SegNet and U-Net to \ndetect traffic signs from video sequences has been proposed \n(Kamal et\u00a0al. 2019). Several adaptations to Mask R-CNN \nwere tested in Tabernik and Sko\u010daj (2019) for detection \nand recognition with end-to-end learning in the domain of \ntraffic signs. They also proposed a data augmentation tech-\nnique based on the distribution of geometric and appearance \ndistortions.\nA method that uses an encoder-decoder DNN with focal \nregression loss to detect small traffic signals is proposed \nin Lee and Kim (2019). It is shown in Kim et\u00a0al. (2018) \nthat Faster R-CNN with Inception-Resnet-v2 model is more \nsuitable for traffic light detection than others. A practical \ntraffic light detection system in Ouyang et\u00a0al. (2019) com-\nbines CNN classifier model and heuristic region of interest \n(ROI) candidate detection on self-driving hardware plat-\nform Nvidia Jetpack Tx1/2 that can handle high-resolution \nimages. The recognition accuracy and processing speed are \nimproved by combining detection and tracking in Wang \net\u00a0al. (2021a) to enhance the practicality of the traffic signal \nrecognition system in autonomous vehicles using CNN and \nintegrated channel feature tracking to determine the coordi-\nnates and color for traffic lights.\nLane detection aims to identify the left and right lane \nboundaries from a processed image and apply an algorithm \nto track the road ahead. A novel hybrid neural network com-\nbining CNN and recurrent neural network (RNN) for robust \nlane detection in driving scenes has been proposed\u00a0(Zou \net\u00a0al. 2019). Features on each frame of the input video \nwere first abstracted by a CNN encoder and the sequential \nencoded features were processed by a ConvLSTM. The out-\nputs were fed into the CNN decoder for information recon-\nstruction and lane prediction. Another lane detection method \nis an anchor-based single-stage lane detection model called \nLaneATT\u00a0(Tabelini et\u00a0al. 2021). It uses a feature pooling \nmethod with a relatively lightweight backbone CNN while \nmaintaining high accuracy. A novel anchor-based attention \nmechanism to aggregate global information was also pro-\nposed. A new method to impose structure on badly posed \nsemantic segmentation problems is proposed in Ghafoorian \net\u00a0al. (2018) using a generative adversarial network architec-\nture with a discriminator that is trained on both predictions \nand labels at the same time.\nPedestrian detection A two-stage detector SDS-\nRCNN\u00a0(Brazil et\u00a0al. 2017) jointly learned pedestrian detec-\ntion and bounding-box aware semantic segmentation, \nthus encouraging model learning on pedestrian regions. \nRPN+BF\u00a0(Zhang et\u00a0al. 2016b) used a boosted forest to \nreplace second-stage learning and leveraged hard mining for \nproposals. However, involving such downstream classifiers \ncould bring more training complexity. AR-Ped\u00a0(Brazil and \nLiu 2019) exploited sequential labeling policy in the region \nproposal network to gradually filter out better proposals. The \nwork of Chen et\u00a0al. (2018) employed a two-stage pretrained \nperson detector (Faster R-CNN) and an instance segmenta-\ntion model for person re-identification. Each detected person \nis cropped out from the original image and fed to another \nnetwork. Wang et\u00a0al. (2018) introduced repulsion losses that \nprevent a predicted bounding box from shifting to neigh-\nboring overlapped objects to counter occlusions. Two-stage \ndetectors need to generate proposals in the first stage and \nthus are slow for inference in practice. One-stage detector \nGDFL\u00a0(Lin et\u00a0al. 2018) included semantic segmentation, \nwhich guided feature layers to emphasize pedestrian regions. \nLiu et\u00a0al. (2018a) extended the single-stage architecture with \nan asymptotic localization fitting module storing multiple \npredictors to evolve default anchor boxes. This improves \nthe quality of positive samples while enabling hard nega-\ntive mining with increased thresholds. Similar to pedestrian \ndetection, vehicle detection in ITS also is a popular and chal-\nlenging computer vision task\u00a0(Zhao et\u00a0al. 2019).\nVehicle detection Current generic vehicle detectors are \ndivided into two categories: CNN-based two-stage detectors \n\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 10 of 27\nIn both cases the focus is on effectively detecting vehicles \nallowing for accurate counts in a given road segment, while \ntracking enables the estimation of average speed and move-\nment directions from traffic video.\nCurrent Methods to\u00a0Overcome Challenge\nA DL method at the edge of the ITS that performs real-\ntime vehicle detection, tracking, and counting in traffic sur-\nveillance video has been proposed in Chen et\u00a0al. (2021b). \nThe neural network detects individual vehicles at the sin-\ngle-frame level by capturing appearance features with the \nYOLOv3 object-detection method, deployed on edge devices \nto minimize bandwidth and power consumption, which are \nmajor practical hurdles in deployment. A vehicle detection \nand tracking approach in adverse weather conditions that \nachieves the best trade-off between accuracy and detection \nspeed in various traffic environments is discussed in Has-\nsaballah et\u00a0al. (2020). Also, a novel dataset called DAWN \n(Kenk and Hassaballah 2020) is introduced for vehicle \ndetection and tracking in adverse weather conditions like \nheavy fog, rain, snow, and sandstorms, to make training less \nbiased. Meanwhile, low resolution and slow framerate issues \nare specifically addressed in Wei et\u00a0al. (2019) to allow large-\nscale implementation on existing urban traffic surveillance \nsystems using SSD-Mobilenet for detection and VGG16 \nfeatures for tracking.\nTraffic Congestion Detection\nModels and\u00a0Algorithms\nThe methods that detect traffic congestion based on com-\nputer vision may also be divided into one-stage methods \nand multi-step methods. The one-stage methods identify \nvehicles from the video images and directly perform traffic \ncongestion detection. Among the one-stage methods are: (1) \nAlexNet and YOLO\u00a0(Chakraborty et\u00a0al. 2018) to distinguish \ncongestion and non-congestion, (2) AlexNet and VGG-\nNet\u00a0(Wang et\u00a0al. 2020d) which classify \u2018jam\u2019 and \u2018no jam\u2019; \nand (3) YOLO and Mask R-CNN\u00a0(Impedovo et\u00a0al. 2019) \nrecognize light, medium, and heavy congestion (identifying \nthe number of vehicles in each frame and then classify). The \nmulti-step methods first apply traffic flow estimation mod-\nels to measure traffic variables and then use the traffic flow \nvariables to infer congestion. Examples of two-stage traffic \ncongestion detection models are: (1) YOLOv3\u00a0(Rashmi and \nShantala 2020) and YOLOv4\u00a0(Sonnleitner et\u00a0al. 2020) for \nvehicle detection and counting, (2) counting vehicles using \nFaster R-CNN\u00a0(Gao et\u00a0al. 2021a) and applying regression \nfor traffic congestion. Beside these, the traffic congestion can \nbe evaluated by the traffic flow detection algorithms\u00a0(Kumar \nand Raubal 2021) using vehicle detection and tracking.\nTable\u202f1\u2002 \u2009(continued)\nApplication\nMethods\nMain challenges\nEdge computing\nInteger linear programming + fast heuristics (Cui et\u00a0al. 2020), SSD \n+ SORT (Ke et\u00a0al. 2020), Deep Deterministic Policy Gradient + \nV2V networking (Dai et\u00a0al. 2019), cloud-edge hybrid + Global \nforeground modeling + Gaussian mixture model (Liu et\u00a0al. 2021a), \nYOLOv3 Wan et\u00a0al. (2022), federated learning (Kairouz et\u00a0al. 2019), \nspectral clustering compression (Chen et\u00a0al. 2021a)\nPower consumption, heterogeneous data sources, cybersecurity, wire-\nless noise, scalability, neural network pruning and model compres-\nsion, installation and maintenance costs\n", []], "Cooperative Perception": ["Data Science for Transportation (2024) 6:1\t\nPage 13 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nRecent directions in segmentation include weakly-super-\nvised semantic segmentation\u00a0(Wang et\u00a0al. 2020e; Sun et\u00a0al. \n2020c), domain adaptation\u00a0(Chen et\u00a0al. 2019e; Liu et\u00a0al. \n2021c), multi-modal data fusion\u00a0(Feng et\u00a0al. 2020; Cortinhal \net\u00a0al. 2021), and real-time semantic segmentation\u00a0(Yu et\u00a0al. \n2018; Nirkin et\u00a0al. 2021; Gao et\u00a0al. 2021b).\nTS-Yolo Wan et\u00a0al. (2021b) is a CNN-based model for \naccurate traffic detection under severe weather conditions \nusing new samples from data augmentation. The data aug-\nmentation was conducted using copy\u2013paste strategy, and a \nlarge number of new samples were constructed from exist-\ning traffic-sign instances. Based on YoloV5, MixConv was \nalso used to mix different kernel sizes in a single convo-\nlution operation so that patterns with various resolutions \ncan be captured. Detecting and classifying real-life small \ntraffic signs from large input images is difficult due to them \noccupying fewer pixels relative to larger targets. To address \nthis, Dense-RefineDet\u00a0(Sun et\u00a0al. 2020) applies a single-shot, \nobject-detection framework to maintain a suitable accuracy-\nspeed trade-off. An end-to-end traffic sign detection frame-\nwork Feature Aggregation Multipath Network is proposed in \nOu et\u00a0al. (2019) to solve the problems of small object detec-\ntion and fine-grained classification in traffic sign detection.\nCooperative Perception\nModels and\u00a0Algorithms\nIn connected autonomous vehicles (CAV), cooperative per-\nception can be performed at three levels depending on the \ntype of data: early fusion (raw data), intermediate fusion \n(preprocessed data), where intermediate neural features are \nextracted and transmitted, and late fusion (processed data), \nwhere detection outputs (3D bounding box position, con-\nfidence score) are shared. Cooperative perception studies \nhow to leverage visual cues from neighboring connected \nvehicles and infrastructure to boost the overall perception \nperformance\u00a0(Xu et\u00a0al. 2022). \n(1)\t Early fusion: Chen et\u00a0al. (2019d) fuses the sensor data \ncollected from different positions and angles of con-\nnected vehicles using raw-data level LiDAR 3D point \nclouds, and a point cloud-based 3D object detection \nmethod is proposed to work on a diversity of aligned \npoint clouds. DiscoNet\u00a0(Li et\u00a0al. 2021d) leverages \nknowledge distillation to enhance training by constrain-\ning the corresponding features to the ones from the net-\nwork for early fusion.\n(2)\t Intermediate fusion: F-Cooper\u00a0(Chen et\u00a0al. 2019b) pro-\nvides both a new framework for applications On-Edge, \nservicing autonomous vehicles as well as new strategies \nfor 3D fusion detection. Wang et\u00a0al. (2020b) proposed \na vehicle-to-vehicle (V2V) approach for perception \nand prediction that transmits compressed intermediate \nrepresentations of the P &P neural network. Xu et\u00a0al. \n(2021) proposed an Attentive Intermediate Fusion pipe-\nline to better capture interactions between connected \nagents within the network. A robust cooperative per-\nception framework with vehicle-to-everything (V2X) \ncommunication using a novel vision Transformer is \npresented in Xu et\u00a0al. (2022).\n(3)\t Late fusion: Car2X-based perception\u00a0(Rauch et\u00a0al. \n2012) is modeled as a virtual sensor to integrate it into \na high-level sensor data fusion architecture.\nCurrent Methods to\u00a0Overcome Challenge\nTo reduce the communications load and overhead, an \nimproved algorithm for message generation rules in col-\nlective perception is proposed\u00a0(Thandavarayan et\u00a0al. 2020), \nwhich improves the reliability of V2X communications by \nreorganizing the transmission and content of collective per-\nception messages. This paper\u00a0(Yoon et\u00a0al. 2021) presents and \nevaluates a unified cooperative perception framework con-\ntaining a decentralized data association and fusion process \nthat is scalable with respect to participation variances. The \nevaluation considers the effects of communication losses in \nthe ad-hoc V2V network and the random vehicle motions in \ntraffic by adopting existing models along with a simplified \nalgorithm for individual vehicle\u2019s on-board sensor field of \nview. AICP is proposed in Zhou et\u00a0al. (2022), the first solu-\ntion that focuses on optimizing informativeness for perva-\nsive cooperative perception systems with efficient filtering \nat both the network and application layers. To facilitate sys-\ntem networking, they also use a networking protocol stack \nthat includes a dedicated data structure and a lightweight \nrouting protocol specifically for informativeness-focused \napplications.\nVehicle Interaction\nModels and\u00a0Algorithms\nComputer vision methods can be used to detect and clas-\nsify crash and near-crash events based on motion and tra-\njectories. CNN, in the form of modified YOLOv3, is used \nto detect objects and extract semantic information about the \nroad scene from onboard camera data from the SHRP2 data-\nset in Taccari et\u00a0al. (2018). Optical flow is calculated from \nconsecutive frames to track objects and to generate features \n(hard deceleration, the maximum area of the largest vehicle, \ntime to collision, etc.) that are combined with telematics \ndata (speed and 3-axis acceleration) to train a random forest \nclassifier on safe, near-crash, and crash events.\n", []], "Autonomous Driving Perception: Segmentation": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 12 of 27\nand CNN-based one-stage detectors. the representative two-\nstage detectors include Faster R-CNN\u00a0(Zhang et\u00a0al. 2016), \nspatial pyramid pooling (SPP)-net\u00a0(He et\u00a0al. 2015), fea-\nture pyramid networks (FPN)\u00a0(Lin et\u00a0al. 2017), and Mask \nR-CNN\u00a0(He et\u00a0al. 2017). The representative one-stage detec-\ntors include YOLO\u00a0(Redmon et\u00a0al. 2016), Single Shot Multi-\nBox Detector (SSD)\u00a0(Liu et\u00a0al. 2016), and deeply supervised \nobject detectors (DSOD)\u00a0(Shen et\u00a0al. 2017). The two-step \nframework is a region proposal-based method, giving a \ncoarse scan of the whole image first and then focusing on \nregions of interest (RoIs). While one-step frameworks are \nbased on global regression/ classification, mapping straightly \nfrom image pixels to bounding box coordinates and class \nprobabilities. Based on these two frameworks, most of the \nworks get promising results in vehicle detection by combin-\ning other methods, such as multitask learning\u00a0(Brahmbhatt \net\u00a0al. 2017), multi-scale representation\u00a0(Bell et\u00a0al. 2016), and \ncontext modeling\u00a0(Kong et\u00a0al. 2016).\nCurrent Methods to\u00a0Overcome Challenge\nIn traffic sign detection, existing traffic sign datasets are \nlimited in terms of the type and severity of challenging \nconditions. Metadata corresponding to these conditions is \nunavailable and it is not possible to investigate the effect \nof a single factor because of the simultaneous changes in \nnumerous conditions. To overcome this, Temel et\u00a0al. (2019) \nintroduced the CURE-TSDReal dataset, based on simulated \nconditions that correspond to real-world environments. An \nend-to-end traffic sign detection framework Feature Aggre-\ngation Multipath Network (FAMN) is proposed in Ou et\u00a0al. \n(2019). It consists of two main structures named Feature \nAggregation and Multipath Network structure to solve the \nproblems of small object detection and fine-grained clas-\nsification in traffic sign detection.\nA vehicle highlight information-assisted neural net-\nwork for vehicle detection at night is presented in Mo et\u00a0al. \n(2019), which included two innovations: establishing the \nlabel hierarchy for vehicles based on their highlights and \ndesigning a multi-layer fused vehicle highlight informa-\ntion network. Real-time vehicle detection for nighttime \nsituations is presented in Bell et\u00a0al. (2021), where images \ninclude flashes that occupy large image regions, and the \nactual shape of vehicles is not well defined. By using a \nglobal image descriptor along with a grid of foveal classi-\nfiers, vehicle positions are accurately and efficiently esti-\nmated. AugGAN Lin et\u00a0al. (2020) is an unpaired image-to-\nimage translation network for domain adaptation in vehicle \ndetection. It quantitatively surpassed competing methods \nfor achieving higher nighttime vehicle detection accuracy \nbecause of better image-object preservation. A stepwise \ndomain adaptation (SDA) detection method is proposed \nto further improve the performance of CycleGAN by \nminimizing the divergence in cross-domain object detec-\ntion tasks in Li et\u00a0al. (2022). In the first step, an unpaired \nimage-to-image translator is trained to construct a fake \ntarget domain by translating the source images to similar \nones in the target domain. In the second step, to further \nminimize divergence across domains, an adaptive Center-\nNet is designed to align distributions at the feature level \nin an adversarial learning manner.\nAutonomous Driving Perception: Segmentation\nModels and\u00a0Algorithms\nImage segmentation contains three sub-tasks: semantic seg-\nmentation, instance segmentation, and panoptic segmenta-\ntion. Semantic segmentation is a fine prediction task to label \neach pixel of an image with a corresponding object class, \ninstance segmentation is designed to identify and segment \npixels that belong to each object instance, while panoptic \nsegmentation unifies semantic segmentation and instance \nsegmentation such that all pixels are given both a class label \nand an instance ID (Gu et\u00a0al. 2022).\nYOLACT\u00a0(Bolya et\u00a0al. 2019) splits instance segmenta-\ntion into two parallel sub-architectures. Protonet architec-\nture extracts spatial information by generating a certain \nnumber of prototype masks, and Head architecture gener-\nates the mask coefficients and object locations. In addi-\ntion, it employs Fast NMS rather than traditional NMS to \nreduce post-processing time. Path Aggregation Network \n(PANet)\u00a0(Liu et\u00a0al. 2018b) is proposed to integrate com-\nprehensively low-level location information and high-level \nsemantic information. Based on Feature Pyramid Networks \n(FPN)\u00a0(Lin et\u00a0al. 2017), PANet designs a bottom-up con-\ntext information aggregation structure, which can integrate \ndifferent levels of features. Hybrid Task Cascade (HTC) is \nproposed for instance segmentation in Chen et\u00a0al. (2019c). \nIt interweaves box and mask branches for joint multi-stage \nprocessing, adopts a semantic segmentation branch to pro-\nvide spatial context, and integrates complementary features \ntogether in each stage.\nIn Dong et\u00a0al. (2020), a novel real-time segmentation is \nproposed consisting of a convolutional attention module, \nspatial pyramid pooling, and a feature fusion network. It was \nevaluated on benchmark datasets Cityscapes and CamVid, \nwhich specifically target complex urban scenarios.\nMoving objects viewed from a moving platform pose a \nunique challenge for segmentation, which is addressed in \nZhou et\u00a0al. (2017) using time-consecutive stereo images. \nMotion likelihood estimates for each pixel aids in ego-\nmotion estimation, while segmentation is performed using \na graph-cut algorithm. However, computational complexity \nis a major limitation of this method.\n", []], "Autonomous Driving Perception: Detection": ["Data Science for Transportation (2024) 6:1\t\nPage 11 of 27\u2003\n1\nCurrent Methods to\u00a0Overcome Challenge\nCongestion detection performance can be improved using \nmultiple sensors based solutions including radar, lasers, and \nsensor fusion since it is hard to achieve ideal performance \nand accuracy using a single sensor in real-world scenarios. \nThere is a wide use of decision-making algorithms for pro-\ncessing fusion data acquired from multiple sensors\u00a0(Muham-\nmad et\u00a0al. 2020). A CNN-based model trained with bad \nweather condition datasets can improve the detection per-\nformance\u00a0(Sharma et\u00a0al. 2022), while generative adversarial \nnetwork (GAN) based Style Transfer methods have also been \napplied\u00a0(Lin et\u00a0al. 2020; Li et\u00a0al. 2021a). These approaches \nhelp to minimize the model challenges related to generaliz-\nability, which in turn improves real-world performance in a \nvariety of environments.\nAutonomous Driving Perception: Detection\nModels and\u00a0Algorithms\nCommon detection tasks that assist in AD are categorized \ninto traffic sign detection, traffic signal detection, road/lane \ndetection, pedestrian detection, and vehicle detection.\nTraffic signs There are two tasks in a typical traffic sign \nrecognition system: finding the locations and sizes of traf-\nfic signs in natural scene images (traffic sign detection) and \nclassifying the traffic signs into their specific sub-classes \n(traffic sign classification)\u00a0(Yang et\u00a0al. 2015). An improved \nSparse R-CNN was used for traffic sign detection in Cao \net\u00a0al. (2021), while an efficient algorithm based on YOLOv3 \nmodel for traffic sign detection was implemented in Wan \net\u00a0al. (2021a). SegU-Net, formed by merging the state-of-\nthe-art segmentation architectures SegNet and U-Net to \ndetect traffic signs from video sequences has been proposed \n(Kamal et\u00a0al. 2019). Several adaptations to Mask R-CNN \nwere tested in Tabernik and Sko\u010daj (2019) for detection \nand recognition with end-to-end learning in the domain of \ntraffic signs. They also proposed a data augmentation tech-\nnique based on the distribution of geometric and appearance \ndistortions.\nA method that uses an encoder-decoder DNN with focal \nregression loss to detect small traffic signals is proposed \nin Lee and Kim (2019). It is shown in Kim et\u00a0al. (2018) \nthat Faster R-CNN with Inception-Resnet-v2 model is more \nsuitable for traffic light detection than others. A practical \ntraffic light detection system in Ouyang et\u00a0al. (2019) com-\nbines CNN classifier model and heuristic region of interest \n(ROI) candidate detection on self-driving hardware plat-\nform Nvidia Jetpack Tx1/2 that can handle high-resolution \nimages. The recognition accuracy and processing speed are \nimproved by combining detection and tracking in Wang \net\u00a0al. (2021a) to enhance the practicality of the traffic signal \nrecognition system in autonomous vehicles using CNN and \nintegrated channel feature tracking to determine the coordi-\nnates and color for traffic lights.\nLane detection aims to identify the left and right lane \nboundaries from a processed image and apply an algorithm \nto track the road ahead. A novel hybrid neural network com-\nbining CNN and recurrent neural network (RNN) for robust \nlane detection in driving scenes has been proposed\u00a0(Zou \net\u00a0al. 2019). Features on each frame of the input video \nwere first abstracted by a CNN encoder and the sequential \nencoded features were processed by a ConvLSTM. The out-\nputs were fed into the CNN decoder for information recon-\nstruction and lane prediction. Another lane detection method \nis an anchor-based single-stage lane detection model called \nLaneATT\u00a0(Tabelini et\u00a0al. 2021). It uses a feature pooling \nmethod with a relatively lightweight backbone CNN while \nmaintaining high accuracy. A novel anchor-based attention \nmechanism to aggregate global information was also pro-\nposed. A new method to impose structure on badly posed \nsemantic segmentation problems is proposed in Ghafoorian \net\u00a0al. (2018) using a generative adversarial network architec-\nture with a discriminator that is trained on both predictions \nand labels at the same time.\nPedestrian detection A two-stage detector SDS-\nRCNN\u00a0(Brazil et\u00a0al. 2017) jointly learned pedestrian detec-\ntion and bounding-box aware semantic segmentation, \nthus encouraging model learning on pedestrian regions. \nRPN+BF\u00a0(Zhang et\u00a0al. 2016b) used a boosted forest to \nreplace second-stage learning and leveraged hard mining for \nproposals. However, involving such downstream classifiers \ncould bring more training complexity. AR-Ped\u00a0(Brazil and \nLiu 2019) exploited sequential labeling policy in the region \nproposal network to gradually filter out better proposals. The \nwork of Chen et\u00a0al. (2018) employed a two-stage pretrained \nperson detector (Faster R-CNN) and an instance segmenta-\ntion model for person re-identification. Each detected person \nis cropped out from the original image and fed to another \nnetwork. Wang et\u00a0al. (2018) introduced repulsion losses that \nprevent a predicted bounding box from shifting to neigh-\nboring overlapped objects to counter occlusions. Two-stage \ndetectors need to generate proposals in the first stage and \nthus are slow for inference in practice. One-stage detector \nGDFL\u00a0(Lin et\u00a0al. 2018) included semantic segmentation, \nwhich guided feature layers to emphasize pedestrian regions. \nLiu et\u00a0al. (2018a) extended the single-stage architecture with \nan asymptotic localization fitting module storing multiple \npredictors to evolve default anchor boxes. This improves \nthe quality of positive samples while enabling hard nega-\ntive mining with increased thresholds. Similar to pedestrian \ndetection, vehicle detection in ITS also is a popular and chal-\nlenging computer vision task\u00a0(Zhao et\u00a0al. 2019).\nVehicle detection Current generic vehicle detectors are \ndivided into two categories: CNN-based two-stage detectors \n", []], "Traffic Congestion Detection": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 10 of 27\nIn both cases the focus is on effectively detecting vehicles \nallowing for accurate counts in a given road segment, while \ntracking enables the estimation of average speed and move-\nment directions from traffic video.\nCurrent Methods to\u00a0Overcome Challenge\nA DL method at the edge of the ITS that performs real-\ntime vehicle detection, tracking, and counting in traffic sur-\nveillance video has been proposed in Chen et\u00a0al. (2021b). \nThe neural network detects individual vehicles at the sin-\ngle-frame level by capturing appearance features with the \nYOLOv3 object-detection method, deployed on edge devices \nto minimize bandwidth and power consumption, which are \nmajor practical hurdles in deployment. A vehicle detection \nand tracking approach in adverse weather conditions that \nachieves the best trade-off between accuracy and detection \nspeed in various traffic environments is discussed in Has-\nsaballah et\u00a0al. (2020). Also, a novel dataset called DAWN \n(Kenk and Hassaballah 2020) is introduced for vehicle \ndetection and tracking in adverse weather conditions like \nheavy fog, rain, snow, and sandstorms, to make training less \nbiased. Meanwhile, low resolution and slow framerate issues \nare specifically addressed in Wei et\u00a0al. (2019) to allow large-\nscale implementation on existing urban traffic surveillance \nsystems using SSD-Mobilenet for detection and VGG16 \nfeatures for tracking.\nTraffic Congestion Detection\nModels and\u00a0Algorithms\nThe methods that detect traffic congestion based on com-\nputer vision may also be divided into one-stage methods \nand multi-step methods. The one-stage methods identify \nvehicles from the video images and directly perform traffic \ncongestion detection. Among the one-stage methods are: (1) \nAlexNet and YOLO\u00a0(Chakraborty et\u00a0al. 2018) to distinguish \ncongestion and non-congestion, (2) AlexNet and VGG-\nNet\u00a0(Wang et\u00a0al. 2020d) which classify \u2018jam\u2019 and \u2018no jam\u2019; \nand (3) YOLO and Mask R-CNN\u00a0(Impedovo et\u00a0al. 2019) \nrecognize light, medium, and heavy congestion (identifying \nthe number of vehicles in each frame and then classify). The \nmulti-step methods first apply traffic flow estimation mod-\nels to measure traffic variables and then use the traffic flow \nvariables to infer congestion. Examples of two-stage traffic \ncongestion detection models are: (1) YOLOv3\u00a0(Rashmi and \nShantala 2020) and YOLOv4\u00a0(Sonnleitner et\u00a0al. 2020) for \nvehicle detection and counting, (2) counting vehicles using \nFaster R-CNN\u00a0(Gao et\u00a0al. 2021a) and applying regression \nfor traffic congestion. Beside these, the traffic congestion can \nbe evaluated by the traffic flow detection algorithms\u00a0(Kumar \nand Raubal 2021) using vehicle detection and tracking.\nTable\u202f1\u2002 \u2009(continued)\nApplication\nMethods\nMain challenges\nEdge computing\nInteger linear programming + fast heuristics (Cui et\u00a0al. 2020), SSD \n+ SORT (Ke et\u00a0al. 2020), Deep Deterministic Policy Gradient + \nV2V networking (Dai et\u00a0al. 2019), cloud-edge hybrid + Global \nforeground modeling + Gaussian mixture model (Liu et\u00a0al. 2021a), \nYOLOv3 Wan et\u00a0al. (2022), federated learning (Kairouz et\u00a0al. 2019), \nspectral clustering compression (Chen et\u00a0al. 2021a)\nPower consumption, heterogeneous data sources, cybersecurity, wire-\nless noise, scalability, neural network pruning and model compres-\nsion, installation and maintenance costs\n", []], "Traffic Flow Estimation": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 8 of 27\nimprove image segmentation performance in degraded \nimages.\nHeterogeneous, Urban Traffic Conditions\nDense urban traffic scenarios are full of complex visual \nelements, not only in quantity but also in the variety of \ndifferent vehicles and their interactions, as shown in \nFig.\u00a04. The presence of cars, buses, bicycles, and pedes-\ntrians in the same intersection is a significant problem for \nautonomous navigation and trajectory computation (Ma \net\u00a0al. 2018). The different sizes, turning radii, speeds, and \ndriver behaviors are further compounded by the interac-\ntions between these road users. From a DL perspective, \nit is easy to find videos of heterogeneous urban traffic, \nbut labeling for ground truth is very time-consuming. \nSimulation software usually cannot capture the complex \ndynamics of such scenarios, especially the traffic rule-\nbreaking behaviors seen in dense urban centers. In fact, a \nspecific dataset was created to represent these behaviors \nin Chandra et\u00a0al. (2019a). A simulator for unregulated \ndense traffic was created in Cai et\u00a0al. (2020) which is \nuseful for autonomous driving perception and control but \ndoes not represent the trajectory and interactions of real-\nworld road users.\nApplications\nTraffic Flow Estimation\nModels and\u00a0Algorithms\nTraffic flow variables include traffic volume, density, speed, \nand queue length. The algorithms and models to detect and \ntrack objects to estimate traffic flow variables from videos \nmay be classified into one-stage and two-stage methods. In \none-stage methods, the variables are estimated from detec-\ntion results and there is no further classification and location \noptimization, for example: (1) YOLOv3 + Feature stitching \n(Hong et\u00a0al. 2020; (2) YOLOv2 + spatial pyramid pool-\ning\u00a0(Kim et\u00a0al. 2019; (3) AlexNet + optical flow + Gaussian \nmixture model\u00a0(Ke et\u00a0al. 2018a; (4) CNN + optical flow \nbased on UAV video\u00a0(Ke et\u00a0al. 2018b; (5) SSD (single shot \ndetection) based on UAV video\u00a0(Tang et\u00a0al. 2017).\nTwo-stage methods first generate region proposals that \ncontain all potential targets in the input images and then \nconduct classification and location optimization. Exam-\nples of two-stage methods are: (1) Faster R-CNN + SORT \ntracker\u00a0(Fedorov et\u00a0al. 2019; (2) Faster R-CNN\u00a0(Peppa \net\u00a0al. 2018; Mhalla et\u00a0al. 2018; (3) Faster R-CNN based \non UAV video\u00a0(Peppa et\u00a0al. 2021; Brki\u0107 et\u00a0al. 2020).\nFig.\u202f4\u2002 \u2009Illustration of representative scenarios in complex traffic environments. Some demo images are adopted from Yang and Pun-Cheng (2018)\n", [144]], "Applications": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 8 of 27\nimprove image segmentation performance in degraded \nimages.\nHeterogeneous, Urban Traffic Conditions\nDense urban traffic scenarios are full of complex visual \nelements, not only in quantity but also in the variety of \ndifferent vehicles and their interactions, as shown in \nFig.\u00a04. The presence of cars, buses, bicycles, and pedes-\ntrians in the same intersection is a significant problem for \nautonomous navigation and trajectory computation (Ma \net\u00a0al. 2018). The different sizes, turning radii, speeds, and \ndriver behaviors are further compounded by the interac-\ntions between these road users. From a DL perspective, \nit is easy to find videos of heterogeneous urban traffic, \nbut labeling for ground truth is very time-consuming. \nSimulation software usually cannot capture the complex \ndynamics of such scenarios, especially the traffic rule-\nbreaking behaviors seen in dense urban centers. In fact, a \nspecific dataset was created to represent these behaviors \nin Chandra et\u00a0al. (2019a). A simulator for unregulated \ndense traffic was created in Cai et\u00a0al. (2020) which is \nuseful for autonomous driving perception and control but \ndoes not represent the trajectory and interactions of real-\nworld road users.\nApplications\nTraffic Flow Estimation\nModels and\u00a0Algorithms\nTraffic flow variables include traffic volume, density, speed, \nand queue length. The algorithms and models to detect and \ntrack objects to estimate traffic flow variables from videos \nmay be classified into one-stage and two-stage methods. In \none-stage methods, the variables are estimated from detec-\ntion results and there is no further classification and location \noptimization, for example: (1) YOLOv3 + Feature stitching \n(Hong et\u00a0al. 2020; (2) YOLOv2 + spatial pyramid pool-\ning\u00a0(Kim et\u00a0al. 2019; (3) AlexNet + optical flow + Gaussian \nmixture model\u00a0(Ke et\u00a0al. 2018a; (4) CNN + optical flow \nbased on UAV video\u00a0(Ke et\u00a0al. 2018b; (5) SSD (single shot \ndetection) based on UAV video\u00a0(Tang et\u00a0al. 2017).\nTwo-stage methods first generate region proposals that \ncontain all potential targets in the input images and then \nconduct classification and location optimization. Exam-\nples of two-stage methods are: (1) Faster R-CNN + SORT \ntracker\u00a0(Fedorov et\u00a0al. 2019; (2) Faster R-CNN\u00a0(Peppa \net\u00a0al. 2018; Mhalla et\u00a0al. 2018; (3) Faster R-CNN based \non UAV video\u00a0(Peppa et\u00a0al. 2021; Brki\u0107 et\u00a0al. 2020).\nFig.\u202f4\u2002 \u2009Illustration of representative scenarios in complex traffic environments. Some demo images are adopted from Yang and Pun-Cheng (2018)\n", [144]], "Heterogeneous, Urban Traffic Conditions": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 8 of 27\nimprove image segmentation performance in degraded \nimages.\nHeterogeneous, Urban Traffic Conditions\nDense urban traffic scenarios are full of complex visual \nelements, not only in quantity but also in the variety of \ndifferent vehicles and their interactions, as shown in \nFig.\u00a04. The presence of cars, buses, bicycles, and pedes-\ntrians in the same intersection is a significant problem for \nautonomous navigation and trajectory computation (Ma \net\u00a0al. 2018). The different sizes, turning radii, speeds, and \ndriver behaviors are further compounded by the interac-\ntions between these road users. From a DL perspective, \nit is easy to find videos of heterogeneous urban traffic, \nbut labeling for ground truth is very time-consuming. \nSimulation software usually cannot capture the complex \ndynamics of such scenarios, especially the traffic rule-\nbreaking behaviors seen in dense urban centers. In fact, a \nspecific dataset was created to represent these behaviors \nin Chandra et\u00a0al. (2019a). A simulator for unregulated \ndense traffic was created in Cai et\u00a0al. (2020) which is \nuseful for autonomous driving perception and control but \ndoes not represent the trajectory and interactions of real-\nworld road users.\nApplications\nTraffic Flow Estimation\nModels and\u00a0Algorithms\nTraffic flow variables include traffic volume, density, speed, \nand queue length. The algorithms and models to detect and \ntrack objects to estimate traffic flow variables from videos \nmay be classified into one-stage and two-stage methods. In \none-stage methods, the variables are estimated from detec-\ntion results and there is no further classification and location \noptimization, for example: (1) YOLOv3 + Feature stitching \n(Hong et\u00a0al. 2020; (2) YOLOv2 + spatial pyramid pool-\ning\u00a0(Kim et\u00a0al. 2019; (3) AlexNet + optical flow + Gaussian \nmixture model\u00a0(Ke et\u00a0al. 2018a; (4) CNN + optical flow \nbased on UAV video\u00a0(Ke et\u00a0al. 2018b; (5) SSD (single shot \ndetection) based on UAV video\u00a0(Tang et\u00a0al. 2017).\nTwo-stage methods first generate region proposals that \ncontain all potential targets in the input images and then \nconduct classification and location optimization. Exam-\nples of two-stage methods are: (1) Faster R-CNN + SORT \ntracker\u00a0(Fedorov et\u00a0al. 2019; (2) Faster R-CNN\u00a0(Peppa \net\u00a0al. 2018; Mhalla et\u00a0al. 2018; (3) Faster R-CNN based \non UAV video\u00a0(Peppa et\u00a0al. 2021; Brki\u0107 et\u00a0al. 2020).\nFig.\u202f4\u2002 \u2009Illustration of representative scenarios in complex traffic environments. Some demo images are adopted from Yang and Pun-Cheng (2018)\n", [144]], "Camera Blur and\u00a0Degraded Images": ["Data Science for Transportation (2024) 6:1\t\nPage 7 of 27\u2003\n1\nand road-marking corners can introduce significant errors. \nSimilarly, cost-effective city-wide real-time vehicle count \nsolutions are scalable, but accuracy drops due to shallow \ntraffic camera angles (Huang and Sharma 2020). The model \nin Aboah et\u00a0al. (2021) can identify anomalies near the cam-\nera, including their start and end times, but is not accurate \nfor anomalies in the distance since the vehicles occupy only \na few pixels.\nAn earlier survey on anomaly detection from surveil-\nlance video concluded that illumination, camera angle, het-\nerogeneous objects, and a lack of real-world datasets are \nthe major challenges (Santhosh et\u00a0al. 2020). The methods \nused for sparse and dense traffic conditions are different and \nlack generalizability. Matching objects in different views \nis another major problem in a multi-view vision scene, as \nmulti-view ITS applications need to process data across the \ndifferent images captured by different cameras at the same \ntime\u00a0(Xie et\u00a0al. 2021).\nCamera Blur and\u00a0Degraded Images\nSurveillance cameras are subject to weather elements. \nWater, dust, and particulate matter can accumulate on the \nlens causing image quality degradation. Strong wind can \ncause a camera to shake, resulting in motion blur in the \nwhole image. Front-facing cameras on autonomous vehi-\ncles also face this, as insects can smash onto the glass, \ncausing blind spots in the camera\u2019s field of view. Spe-\ncifically, object detection and segmentation algorithms \nsuffer greatly (Vasiljevic et\u00a0al. 2017), and unless prepara-\ntions are made in the model, false detections can cause \nserious safety issues in AD and miss important events \nin surveillance applications. Some approaches to address \nthis include using degraded images for training, image \nrestoration preprocessing, and fine-tuning pre-trained \nnetworks to learn from degraded images. For example, \nDense-Gram networks are used in Guo et\u00a0al. (2019) which \nFig.\u202f3\u2002 \u2009Illustration of representative model challenges. Some demo images are adopted from Bianco et\u00a0al. (2018); Bornstein (2016)\n", [123, 144]], "Camera Angle": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 6 of 27\nvaried performance that depends strongly on the random \nseeds used to initialize training.\nPractical systems need to be efficient in terms of mem-\nory and computation for real-time processing on a variety \nof low-cost hardware (Bai et\u00a0al. 2021). Some approaches \ntowards efficient and low-cost computation include param-\neter pruning, network quantization, low-rank factorization, \nand model distillation. Approaches like Cui et\u00a0al. (2019) are \nefficient and capable of real-time trajectory prediction but \nare not end-to-end because they assume the prior existence \nof an object-tracking system to estimate the states of sur-\nrounding vehicles.\nVulnerable road users (VRU) such as pedestrians and \nbicyclists present a unique problem, since they can change \ntheir direction and speeds very rapidly, and interact with \nthe traffic environment differently than vehicles (Saleh et\u00a0al. \n2017).\nSome of the major barriers to the practical deployment of \ncomputer vision models in ITS are the heterogeneity of data \nsources and software, sensor hardware failure, and extreme \nor unusual sensing cases (Zhou et\u00a0al. 2021). Furthermore, \nrecent frameworks such as those based on edge computing \ndirectly expose the wireless communication signals of a \nmultitude of heterogeneous devices with various security \nimplementations, creating an ever-increasing potential attack \nsurface for malicious actors (Contreras-Castillo et\u00a0al. 2018; \nHaghighat et\u00a0al. 2020). Deep learning models have been \ndeveloped to detect these attacks, however real-time appli-\ncation and online learning are still areas of active research \n(Chen et\u00a0al. 2019a).\nIoV faces fundamental practical issues arising from the \nfact that moving vehicles will present highly variable pro-\ncessing requirements on the edge nodes, while each vehicle \ncan also have many concurrent edge and cloud-related appli-\ncations running, along with harsh wireless communication \nenvironments (Zhang and Letaief 2020). Other challenges \nrelated to edge computing for autonomous vehicles include \ncooperative sensing, cooperative decisions, and cybersecu-\nrity (Liu et\u00a0al. 2019). Attackers can use lasers and bright \ninfrared light to interfere with cameras and LiDAR, change \ntraffic signage, and replay attacks over the communication \nchannel. A visual depiction of model challenges can be seen \nin Fig.\u00a03.\nComplex Traffic Environments\nShadow, Lighting, Weather\nSituations like shadows, adverse weather, similarity between \nbackground and foreground, strong or insufficient illumina-\ntion in the real world are cited as common issues\u00a0(Lin et\u00a0al. \n2019; Song et\u00a0al. 2020). The appearance of camera images is \nknown to be affected by adverse weather conditions, such as \nheavy fog, sleeting rain, snowstorms, and dust storms\u00a0(Has-\nsaballah et\u00a0al. 2020).\nA real-time crash detection method in Jiansheng (2014) \nutilizes foreground extraction using the Gaussian Mixture \nModel, then tracks vehicles using a mean shift algorithm. \nThe position, speed, and acceleration of the vehicles are \npassed through a threshold functions to determine the detec-\ntion of a crash. While computationally efficient, such meth-\nods suffer significantly in the presence of noise, complex \ntraffic environment, and change in weather.\nIn harsh weather conditions, vehicles captured by traffic \nsurveillance cameras exhibit issues such as underexposure, \nblurring, and partial occlusion. At the same time, raindrops, \nand snowflakes that appear in traffic scenes add difficulty \nfor the algorithm to extract vehicle targets\u00a0(Yang and Pun-\nCheng 2018). At night, or in tunnels with vehicles driving \ntowards the camera, the scene may be masked completely \nbecause of the high beam glare\u00a0(Sonnleitner et\u00a0al. 2020).\nOcclusion\nOcclusion is one of the most challenging issues, where a \ntarget object is only partially visible to the camera or sensor \ndue to obstruction by another foreground object. Occlusion \nexists in various forms ranging from partial occlusion to \nheavy occlusion\u00a0(Gilroy et\u00a0al. 2019). In AD, target objects \ncan be occluded by static objects such as buildings and \nlampposts. Dynamic objects such as moving vehicles or \nother road users may occlude one another, such as in crowds. \nFigure\u00a04 shows how a single bus can occlude multiple vehi-\ncles. Occlusion is also a common issue in object tracking \n(Nowosielski et\u00a0al. 2016) because once the tracked vehicle \ndisappears from view and reappears, it is considered a differ-\nent vehicle causing tracking and trajectory information to be \ninaccurate. In fact, when the vehicle reappears, it is double \ncounted by detection and tracking algorithms regardless of \nthe model used resulting in exaggerated counts (Mandal and \nAdu-Gyamfi 2020). Data imputation and post-processing for \nerror correction are important steps for practical applications \ninvolving tracking through occlusion but these often require \na manual analysis of results (Dhatbale and Chilukuri 2021).\nCamera Angle\nIn the applications of transportation infrastructure, the diver-\nsity of surveillance cameras and their viewing angles pose \nchallenges to DL methods trained on limited types of camera \nviews (Buch et\u00a0al. 2011; Santhosh et\u00a0al. 2020). While the \nqueue length estimation in Albiol et\u00a0al. (2011) is compu-\ntationally efficient and can work in varying lighting condi-\ntions and traffic density scenarios, lower-pitch camera views \n", [123]], "Occlusion": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 6 of 27\nvaried performance that depends strongly on the random \nseeds used to initialize training.\nPractical systems need to be efficient in terms of mem-\nory and computation for real-time processing on a variety \nof low-cost hardware (Bai et\u00a0al. 2021). Some approaches \ntowards efficient and low-cost computation include param-\neter pruning, network quantization, low-rank factorization, \nand model distillation. Approaches like Cui et\u00a0al. (2019) are \nefficient and capable of real-time trajectory prediction but \nare not end-to-end because they assume the prior existence \nof an object-tracking system to estimate the states of sur-\nrounding vehicles.\nVulnerable road users (VRU) such as pedestrians and \nbicyclists present a unique problem, since they can change \ntheir direction and speeds very rapidly, and interact with \nthe traffic environment differently than vehicles (Saleh et\u00a0al. \n2017).\nSome of the major barriers to the practical deployment of \ncomputer vision models in ITS are the heterogeneity of data \nsources and software, sensor hardware failure, and extreme \nor unusual sensing cases (Zhou et\u00a0al. 2021). Furthermore, \nrecent frameworks such as those based on edge computing \ndirectly expose the wireless communication signals of a \nmultitude of heterogeneous devices with various security \nimplementations, creating an ever-increasing potential attack \nsurface for malicious actors (Contreras-Castillo et\u00a0al. 2018; \nHaghighat et\u00a0al. 2020). Deep learning models have been \ndeveloped to detect these attacks, however real-time appli-\ncation and online learning are still areas of active research \n(Chen et\u00a0al. 2019a).\nIoV faces fundamental practical issues arising from the \nfact that moving vehicles will present highly variable pro-\ncessing requirements on the edge nodes, while each vehicle \ncan also have many concurrent edge and cloud-related appli-\ncations running, along with harsh wireless communication \nenvironments (Zhang and Letaief 2020). Other challenges \nrelated to edge computing for autonomous vehicles include \ncooperative sensing, cooperative decisions, and cybersecu-\nrity (Liu et\u00a0al. 2019). Attackers can use lasers and bright \ninfrared light to interfere with cameras and LiDAR, change \ntraffic signage, and replay attacks over the communication \nchannel. A visual depiction of model challenges can be seen \nin Fig.\u00a03.\nComplex Traffic Environments\nShadow, Lighting, Weather\nSituations like shadows, adverse weather, similarity between \nbackground and foreground, strong or insufficient illumina-\ntion in the real world are cited as common issues\u00a0(Lin et\u00a0al. \n2019; Song et\u00a0al. 2020). The appearance of camera images is \nknown to be affected by adverse weather conditions, such as \nheavy fog, sleeting rain, snowstorms, and dust storms\u00a0(Has-\nsaballah et\u00a0al. 2020).\nA real-time crash detection method in Jiansheng (2014) \nutilizes foreground extraction using the Gaussian Mixture \nModel, then tracks vehicles using a mean shift algorithm. \nThe position, speed, and acceleration of the vehicles are \npassed through a threshold functions to determine the detec-\ntion of a crash. While computationally efficient, such meth-\nods suffer significantly in the presence of noise, complex \ntraffic environment, and change in weather.\nIn harsh weather conditions, vehicles captured by traffic \nsurveillance cameras exhibit issues such as underexposure, \nblurring, and partial occlusion. At the same time, raindrops, \nand snowflakes that appear in traffic scenes add difficulty \nfor the algorithm to extract vehicle targets\u00a0(Yang and Pun-\nCheng 2018). At night, or in tunnels with vehicles driving \ntowards the camera, the scene may be masked completely \nbecause of the high beam glare\u00a0(Sonnleitner et\u00a0al. 2020).\nOcclusion\nOcclusion is one of the most challenging issues, where a \ntarget object is only partially visible to the camera or sensor \ndue to obstruction by another foreground object. Occlusion \nexists in various forms ranging from partial occlusion to \nheavy occlusion\u00a0(Gilroy et\u00a0al. 2019). In AD, target objects \ncan be occluded by static objects such as buildings and \nlampposts. Dynamic objects such as moving vehicles or \nother road users may occlude one another, such as in crowds. \nFigure\u00a04 shows how a single bus can occlude multiple vehi-\ncles. Occlusion is also a common issue in object tracking \n(Nowosielski et\u00a0al. 2016) because once the tracked vehicle \ndisappears from view and reappears, it is considered a differ-\nent vehicle causing tracking and trajectory information to be \ninaccurate. In fact, when the vehicle reappears, it is double \ncounted by detection and tracking algorithms regardless of \nthe model used resulting in exaggerated counts (Mandal and \nAdu-Gyamfi 2020). Data imputation and post-processing for \nerror correction are important steps for practical applications \ninvolving tracking through occlusion but these often require \na manual analysis of results (Dhatbale and Chilukuri 2021).\nCamera Angle\nIn the applications of transportation infrastructure, the diver-\nsity of surveillance cameras and their viewing angles pose \nchallenges to DL methods trained on limited types of camera \nviews (Buch et\u00a0al. 2011; Santhosh et\u00a0al. 2020). While the \nqueue length estimation in Albiol et\u00a0al. (2011) is compu-\ntationally efficient and can work in varying lighting condi-\ntions and traffic density scenarios, lower-pitch camera views \n", []], "Shadow, Lighting, Weather": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 6 of 27\nvaried performance that depends strongly on the random \nseeds used to initialize training.\nPractical systems need to be efficient in terms of mem-\nory and computation for real-time processing on a variety \nof low-cost hardware (Bai et\u00a0al. 2021). Some approaches \ntowards efficient and low-cost computation include param-\neter pruning, network quantization, low-rank factorization, \nand model distillation. Approaches like Cui et\u00a0al. (2019) are \nefficient and capable of real-time trajectory prediction but \nare not end-to-end because they assume the prior existence \nof an object-tracking system to estimate the states of sur-\nrounding vehicles.\nVulnerable road users (VRU) such as pedestrians and \nbicyclists present a unique problem, since they can change \ntheir direction and speeds very rapidly, and interact with \nthe traffic environment differently than vehicles (Saleh et\u00a0al. \n2017).\nSome of the major barriers to the practical deployment of \ncomputer vision models in ITS are the heterogeneity of data \nsources and software, sensor hardware failure, and extreme \nor unusual sensing cases (Zhou et\u00a0al. 2021). Furthermore, \nrecent frameworks such as those based on edge computing \ndirectly expose the wireless communication signals of a \nmultitude of heterogeneous devices with various security \nimplementations, creating an ever-increasing potential attack \nsurface for malicious actors (Contreras-Castillo et\u00a0al. 2018; \nHaghighat et\u00a0al. 2020). Deep learning models have been \ndeveloped to detect these attacks, however real-time appli-\ncation and online learning are still areas of active research \n(Chen et\u00a0al. 2019a).\nIoV faces fundamental practical issues arising from the \nfact that moving vehicles will present highly variable pro-\ncessing requirements on the edge nodes, while each vehicle \ncan also have many concurrent edge and cloud-related appli-\ncations running, along with harsh wireless communication \nenvironments (Zhang and Letaief 2020). Other challenges \nrelated to edge computing for autonomous vehicles include \ncooperative sensing, cooperative decisions, and cybersecu-\nrity (Liu et\u00a0al. 2019). Attackers can use lasers and bright \ninfrared light to interfere with cameras and LiDAR, change \ntraffic signage, and replay attacks over the communication \nchannel. A visual depiction of model challenges can be seen \nin Fig.\u00a03.\nComplex Traffic Environments\nShadow, Lighting, Weather\nSituations like shadows, adverse weather, similarity between \nbackground and foreground, strong or insufficient illumina-\ntion in the real world are cited as common issues\u00a0(Lin et\u00a0al. \n2019; Song et\u00a0al. 2020). The appearance of camera images is \nknown to be affected by adverse weather conditions, such as \nheavy fog, sleeting rain, snowstorms, and dust storms\u00a0(Has-\nsaballah et\u00a0al. 2020).\nA real-time crash detection method in Jiansheng (2014) \nutilizes foreground extraction using the Gaussian Mixture \nModel, then tracks vehicles using a mean shift algorithm. \nThe position, speed, and acceleration of the vehicles are \npassed through a threshold functions to determine the detec-\ntion of a crash. While computationally efficient, such meth-\nods suffer significantly in the presence of noise, complex \ntraffic environment, and change in weather.\nIn harsh weather conditions, vehicles captured by traffic \nsurveillance cameras exhibit issues such as underexposure, \nblurring, and partial occlusion. At the same time, raindrops, \nand snowflakes that appear in traffic scenes add difficulty \nfor the algorithm to extract vehicle targets\u00a0(Yang and Pun-\nCheng 2018). At night, or in tunnels with vehicles driving \ntowards the camera, the scene may be masked completely \nbecause of the high beam glare\u00a0(Sonnleitner et\u00a0al. 2020).\nOcclusion\nOcclusion is one of the most challenging issues, where a \ntarget object is only partially visible to the camera or sensor \ndue to obstruction by another foreground object. Occlusion \nexists in various forms ranging from partial occlusion to \nheavy occlusion\u00a0(Gilroy et\u00a0al. 2019). In AD, target objects \ncan be occluded by static objects such as buildings and \nlampposts. Dynamic objects such as moving vehicles or \nother road users may occlude one another, such as in crowds. \nFigure\u00a04 shows how a single bus can occlude multiple vehi-\ncles. Occlusion is also a common issue in object tracking \n(Nowosielski et\u00a0al. 2016) because once the tracked vehicle \ndisappears from view and reappears, it is considered a differ-\nent vehicle causing tracking and trajectory information to be \ninaccurate. In fact, when the vehicle reappears, it is double \ncounted by detection and tracking algorithms regardless of \nthe model used resulting in exaggerated counts (Mandal and \nAdu-Gyamfi 2020). Data imputation and post-processing for \nerror correction are important steps for practical applications \ninvolving tracking through occlusion but these often require \na manual analysis of results (Dhatbale and Chilukuri 2021).\nCamera Angle\nIn the applications of transportation infrastructure, the diver-\nsity of surveillance cameras and their viewing angles pose \nchallenges to DL methods trained on limited types of camera \nviews (Buch et\u00a0al. 2011; Santhosh et\u00a0al. 2020). While the \nqueue length estimation in Albiol et\u00a0al. (2011) is compu-\ntationally efficient and can work in varying lighting condi-\ntions and traffic density scenarios, lower-pitch camera views \n", []], "Complex Traffic Environments": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 6 of 27\nvaried performance that depends strongly on the random \nseeds used to initialize training.\nPractical systems need to be efficient in terms of mem-\nory and computation for real-time processing on a variety \nof low-cost hardware (Bai et\u00a0al. 2021). Some approaches \ntowards efficient and low-cost computation include param-\neter pruning, network quantization, low-rank factorization, \nand model distillation. Approaches like Cui et\u00a0al. (2019) are \nefficient and capable of real-time trajectory prediction but \nare not end-to-end because they assume the prior existence \nof an object-tracking system to estimate the states of sur-\nrounding vehicles.\nVulnerable road users (VRU) such as pedestrians and \nbicyclists present a unique problem, since they can change \ntheir direction and speeds very rapidly, and interact with \nthe traffic environment differently than vehicles (Saleh et\u00a0al. \n2017).\nSome of the major barriers to the practical deployment of \ncomputer vision models in ITS are the heterogeneity of data \nsources and software, sensor hardware failure, and extreme \nor unusual sensing cases (Zhou et\u00a0al. 2021). Furthermore, \nrecent frameworks such as those based on edge computing \ndirectly expose the wireless communication signals of a \nmultitude of heterogeneous devices with various security \nimplementations, creating an ever-increasing potential attack \nsurface for malicious actors (Contreras-Castillo et\u00a0al. 2018; \nHaghighat et\u00a0al. 2020). Deep learning models have been \ndeveloped to detect these attacks, however real-time appli-\ncation and online learning are still areas of active research \n(Chen et\u00a0al. 2019a).\nIoV faces fundamental practical issues arising from the \nfact that moving vehicles will present highly variable pro-\ncessing requirements on the edge nodes, while each vehicle \ncan also have many concurrent edge and cloud-related appli-\ncations running, along with harsh wireless communication \nenvironments (Zhang and Letaief 2020). Other challenges \nrelated to edge computing for autonomous vehicles include \ncooperative sensing, cooperative decisions, and cybersecu-\nrity (Liu et\u00a0al. 2019). Attackers can use lasers and bright \ninfrared light to interfere with cameras and LiDAR, change \ntraffic signage, and replay attacks over the communication \nchannel. A visual depiction of model challenges can be seen \nin Fig.\u00a03.\nComplex Traffic Environments\nShadow, Lighting, Weather\nSituations like shadows, adverse weather, similarity between \nbackground and foreground, strong or insufficient illumina-\ntion in the real world are cited as common issues\u00a0(Lin et\u00a0al. \n2019; Song et\u00a0al. 2020). The appearance of camera images is \nknown to be affected by adverse weather conditions, such as \nheavy fog, sleeting rain, snowstorms, and dust storms\u00a0(Has-\nsaballah et\u00a0al. 2020).\nA real-time crash detection method in Jiansheng (2014) \nutilizes foreground extraction using the Gaussian Mixture \nModel, then tracks vehicles using a mean shift algorithm. \nThe position, speed, and acceleration of the vehicles are \npassed through a threshold functions to determine the detec-\ntion of a crash. While computationally efficient, such meth-\nods suffer significantly in the presence of noise, complex \ntraffic environment, and change in weather.\nIn harsh weather conditions, vehicles captured by traffic \nsurveillance cameras exhibit issues such as underexposure, \nblurring, and partial occlusion. At the same time, raindrops, \nand snowflakes that appear in traffic scenes add difficulty \nfor the algorithm to extract vehicle targets\u00a0(Yang and Pun-\nCheng 2018). At night, or in tunnels with vehicles driving \ntowards the camera, the scene may be masked completely \nbecause of the high beam glare\u00a0(Sonnleitner et\u00a0al. 2020).\nOcclusion\nOcclusion is one of the most challenging issues, where a \ntarget object is only partially visible to the camera or sensor \ndue to obstruction by another foreground object. Occlusion \nexists in various forms ranging from partial occlusion to \nheavy occlusion\u00a0(Gilroy et\u00a0al. 2019). In AD, target objects \ncan be occluded by static objects such as buildings and \nlampposts. Dynamic objects such as moving vehicles or \nother road users may occlude one another, such as in crowds. \nFigure\u00a04 shows how a single bus can occlude multiple vehi-\ncles. Occlusion is also a common issue in object tracking \n(Nowosielski et\u00a0al. 2016) because once the tracked vehicle \ndisappears from view and reappears, it is considered a differ-\nent vehicle causing tracking and trajectory information to be \ninaccurate. In fact, when the vehicle reappears, it is double \ncounted by detection and tracking algorithms regardless of \nthe model used resulting in exaggerated counts (Mandal and \nAdu-Gyamfi 2020). Data imputation and post-processing for \nerror correction are important steps for practical applications \ninvolving tracking through occlusion but these often require \na manual analysis of results (Dhatbale and Chilukuri 2021).\nCamera Angle\nIn the applications of transportation infrastructure, the diver-\nsity of surveillance cameras and their viewing angles pose \nchallenges to DL methods trained on limited types of camera \nviews (Buch et\u00a0al. 2011; Santhosh et\u00a0al. 2020). While the \nqueue length estimation in Albiol et\u00a0al. (2011) is compu-\ntationally efficient and can work in varying lighting condi-\ntions and traffic density scenarios, lower-pitch camera views \n", []], "Real-World Testing": ["Data Science for Transportation (2024) 6:1\t\nPage 5 of 27\u2003\n1\nby correlating with other information, even with a blurred \nlicense plate.\nModel Challenges\nComplexity\nDL computer vision models have a high complexity with \nrespect to neural network structures and training procedures. \nMany DL models are designed to run on high-performance \ncloud centers or AI workstations, and a good model requires \nweeks or months of training as well as high power consump-\ntion driven by modern Graphical Processing Units (GPUs) \nor Tensor Processing Units (TPUs).\nIn ITS and AV field applications,\u00a0many have a require-\nment for real-time or near real-time operations (Ke 2020) \nfor the sake of functionality and traffic safety. The DL model \ncomplexity adds a high cost to the training and inference in \nreal-time applications; particularly, the trend of ITS and AV \nis towards large-scale on-device processing closer to where \nthe traffic data is generated, e.g., crowdsensing. Three popu-\nlar embedded devices are compared in Arabi et\u00a0al. (2020), \nwith the Nvidia Jetson Nano yielding the highest inference \nefficiency but it is too little computation power for complex \napplications.\nReal-time applications usually make some modifications \nlike resizing video to lower resolution or model quantization \nand pruning which can lead to loss of performance. The \nmodel complexity of the state-of-the-art DL methods needs \nto be reduced in many practical applications to meet the effi-\nciency and accuracy requirements. For example, multi-scale \ndeformable attention has been used with vision transformer \nneural networks in object detection for high performance \nand fast convergence leading to faster training and inference \n(Zhu et\u00a0al. 2021).\nLack of\u00a0Explainability\nDNNs are largely seen as black boxes with many layers of \nprocessing, the working of which can be examined using \nstatistics, but the learned internal representations of the net-\nwork are based on millions or billions of parameters, making \nanalysis extremely difficult (Ras et\u00a0al. 2018). This means \nthat the behavior is essentially unpredictable, and very lit-\ntle explanation can be given of the decisions. It also makes \nsystem validation and verification impossible for critical use \ncases like autonomous driving (Samek et\u00a0al. 2017).\nThe common assumption that a complex black box is \nnecessary for good performance is being challenged (Rudin \n2019). Recent research is attempting to make DNNs more \nexplainable. A visualization tool for vision transformers is \npresented in Aflalo et\u00a0al. (2022), which can be used to see \nthe inner mechanisms, such as hidden parameters, and gain \ninsight into specific parts of the input that influenced the \npredictions. A framework for safety, explainability, and regu-\nlations for autonomous driving was evaluated in post-acci-\ndent scenarios (Atakishiyev et\u00a0al. 2021). The results showed \nmany benefits including transparency and debugging. A \nconvolutional neural network (CNN)-based architecture is \nproposed to detect action-inducing objects for autonomous \nvehicles, while also providing explanations for the actions \n(Xu et\u00a0al. 2020).\nTransferability and\u00a0Generalizability\nGeneralization to out-of-distribution data is natural to \nhumans yet challenging for machines because most learning \nalgorithms strongly rely on the independent and identically \ndistributed (i.i.d.) assumption training at testing data, which \nis often violated in practice due to domain shift. Domain \ngeneralization aims to generalize models to new domains \nwithout knowledge of the target distribution during training. \nDifferent methods have been proposed for learning gener-\nalizable and transferable representations\u00a0(Dou et\u00a0al. 2019).\nMost existing approaches belong to the category of \ndomain alignment, where the main idea is to minimize the \ndifference between source domains for learning domain-\ninvariant representations. Features that are invariant to the \nsource domain shift should also be robust to any unseen \ntarget domain shift. Data augmentation has been a common \npractice to regularize the training of machine learning mod-\nels to avoid overfitting and improve generalization\u00a0(LeCun \net\u00a0al. 2015), which is particularly important for over-param-\neterized DNNs.\nVisual attention in CNNs can be used to highlight the \nregions of the image involved in a decision, with causal fil-\ntering to find the most relevant parts (Kim and Canny 2018). \nThe importance of individual pixels is estimated in Petsiuk \net\u00a0al. (2018) using randomly masked versions of images and \ncomparing the output predictions. This approach does not \napply to spatio-temporal methods or those that consider rela-\ntionships between objects in complex environments.\nReal\u2011World Testing\nIn general, DL methods have been shown to be prone to \nunderspecification, a problem that appears regardless of \nmodel type or application. Among other domains, under-\nspecification of computer vision is analyzed in D\u2019Amour \net\u00a0al. (2020), specifically for DL models such as the com-\nmonly used ResNet-50 and a scaled-up transfer learning \nimage classification model, Big Transfer (BiT) Kolesnikov \net\u00a0al. (2019). It is shown that while benchmark scores \nimproved with more model complexity and training data, \ntesting with real-world distortions results in poor and highly \n", []], "Transferability and\u00a0Generalizability": ["Data Science for Transportation (2024) 6:1\t\nPage 5 of 27\u2003\n1\nby correlating with other information, even with a blurred \nlicense plate.\nModel Challenges\nComplexity\nDL computer vision models have a high complexity with \nrespect to neural network structures and training procedures. \nMany DL models are designed to run on high-performance \ncloud centers or AI workstations, and a good model requires \nweeks or months of training as well as high power consump-\ntion driven by modern Graphical Processing Units (GPUs) \nor Tensor Processing Units (TPUs).\nIn ITS and AV field applications,\u00a0many have a require-\nment for real-time or near real-time operations (Ke 2020) \nfor the sake of functionality and traffic safety. The DL model \ncomplexity adds a high cost to the training and inference in \nreal-time applications; particularly, the trend of ITS and AV \nis towards large-scale on-device processing closer to where \nthe traffic data is generated, e.g., crowdsensing. Three popu-\nlar embedded devices are compared in Arabi et\u00a0al. (2020), \nwith the Nvidia Jetson Nano yielding the highest inference \nefficiency but it is too little computation power for complex \napplications.\nReal-time applications usually make some modifications \nlike resizing video to lower resolution or model quantization \nand pruning which can lead to loss of performance. The \nmodel complexity of the state-of-the-art DL methods needs \nto be reduced in many practical applications to meet the effi-\nciency and accuracy requirements. For example, multi-scale \ndeformable attention has been used with vision transformer \nneural networks in object detection for high performance \nand fast convergence leading to faster training and inference \n(Zhu et\u00a0al. 2021).\nLack of\u00a0Explainability\nDNNs are largely seen as black boxes with many layers of \nprocessing, the working of which can be examined using \nstatistics, but the learned internal representations of the net-\nwork are based on millions or billions of parameters, making \nanalysis extremely difficult (Ras et\u00a0al. 2018). This means \nthat the behavior is essentially unpredictable, and very lit-\ntle explanation can be given of the decisions. It also makes \nsystem validation and verification impossible for critical use \ncases like autonomous driving (Samek et\u00a0al. 2017).\nThe common assumption that a complex black box is \nnecessary for good performance is being challenged (Rudin \n2019). Recent research is attempting to make DNNs more \nexplainable. A visualization tool for vision transformers is \npresented in Aflalo et\u00a0al. (2022), which can be used to see \nthe inner mechanisms, such as hidden parameters, and gain \ninsight into specific parts of the input that influenced the \npredictions. A framework for safety, explainability, and regu-\nlations for autonomous driving was evaluated in post-acci-\ndent scenarios (Atakishiyev et\u00a0al. 2021). The results showed \nmany benefits including transparency and debugging. A \nconvolutional neural network (CNN)-based architecture is \nproposed to detect action-inducing objects for autonomous \nvehicles, while also providing explanations for the actions \n(Xu et\u00a0al. 2020).\nTransferability and\u00a0Generalizability\nGeneralization to out-of-distribution data is natural to \nhumans yet challenging for machines because most learning \nalgorithms strongly rely on the independent and identically \ndistributed (i.i.d.) assumption training at testing data, which \nis often violated in practice due to domain shift. Domain \ngeneralization aims to generalize models to new domains \nwithout knowledge of the target distribution during training. \nDifferent methods have been proposed for learning gener-\nalizable and transferable representations\u00a0(Dou et\u00a0al. 2019).\nMost existing approaches belong to the category of \ndomain alignment, where the main idea is to minimize the \ndifference between source domains for learning domain-\ninvariant representations. Features that are invariant to the \nsource domain shift should also be robust to any unseen \ntarget domain shift. Data augmentation has been a common \npractice to regularize the training of machine learning mod-\nels to avoid overfitting and improve generalization\u00a0(LeCun \net\u00a0al. 2015), which is particularly important for over-param-\neterized DNNs.\nVisual attention in CNNs can be used to highlight the \nregions of the image involved in a decision, with causal fil-\ntering to find the most relevant parts (Kim and Canny 2018). \nThe importance of individual pixels is estimated in Petsiuk \net\u00a0al. (2018) using randomly masked versions of images and \ncomparing the output predictions. This approach does not \napply to spatio-temporal methods or those that consider rela-\ntionships between objects in complex environments.\nReal\u2011World Testing\nIn general, DL methods have been shown to be prone to \nunderspecification, a problem that appears regardless of \nmodel type or application. Among other domains, under-\nspecification of computer vision is analyzed in D\u2019Amour \net\u00a0al. (2020), specifically for DL models such as the com-\nmonly used ResNet-50 and a scaled-up transfer learning \nimage classification model, Big Transfer (BiT) Kolesnikov \net\u00a0al. (2019). It is shown that while benchmark scores \nimproved with more model complexity and training data, \ntesting with real-world distortions results in poor and highly \n", []], "Lack of\u00a0Explainability": ["Data Science for Transportation (2024) 6:1\t\nPage 5 of 27\u2003\n1\nby correlating with other information, even with a blurred \nlicense plate.\nModel Challenges\nComplexity\nDL computer vision models have a high complexity with \nrespect to neural network structures and training procedures. \nMany DL models are designed to run on high-performance \ncloud centers or AI workstations, and a good model requires \nweeks or months of training as well as high power consump-\ntion driven by modern Graphical Processing Units (GPUs) \nor Tensor Processing Units (TPUs).\nIn ITS and AV field applications,\u00a0many have a require-\nment for real-time or near real-time operations (Ke 2020) \nfor the sake of functionality and traffic safety. The DL model \ncomplexity adds a high cost to the training and inference in \nreal-time applications; particularly, the trend of ITS and AV \nis towards large-scale on-device processing closer to where \nthe traffic data is generated, e.g., crowdsensing. Three popu-\nlar embedded devices are compared in Arabi et\u00a0al. (2020), \nwith the Nvidia Jetson Nano yielding the highest inference \nefficiency but it is too little computation power for complex \napplications.\nReal-time applications usually make some modifications \nlike resizing video to lower resolution or model quantization \nand pruning which can lead to loss of performance. The \nmodel complexity of the state-of-the-art DL methods needs \nto be reduced in many practical applications to meet the effi-\nciency and accuracy requirements. For example, multi-scale \ndeformable attention has been used with vision transformer \nneural networks in object detection for high performance \nand fast convergence leading to faster training and inference \n(Zhu et\u00a0al. 2021).\nLack of\u00a0Explainability\nDNNs are largely seen as black boxes with many layers of \nprocessing, the working of which can be examined using \nstatistics, but the learned internal representations of the net-\nwork are based on millions or billions of parameters, making \nanalysis extremely difficult (Ras et\u00a0al. 2018). This means \nthat the behavior is essentially unpredictable, and very lit-\ntle explanation can be given of the decisions. It also makes \nsystem validation and verification impossible for critical use \ncases like autonomous driving (Samek et\u00a0al. 2017).\nThe common assumption that a complex black box is \nnecessary for good performance is being challenged (Rudin \n2019). Recent research is attempting to make DNNs more \nexplainable. A visualization tool for vision transformers is \npresented in Aflalo et\u00a0al. (2022), which can be used to see \nthe inner mechanisms, such as hidden parameters, and gain \ninsight into specific parts of the input that influenced the \npredictions. A framework for safety, explainability, and regu-\nlations for autonomous driving was evaluated in post-acci-\ndent scenarios (Atakishiyev et\u00a0al. 2021). The results showed \nmany benefits including transparency and debugging. A \nconvolutional neural network (CNN)-based architecture is \nproposed to detect action-inducing objects for autonomous \nvehicles, while also providing explanations for the actions \n(Xu et\u00a0al. 2020).\nTransferability and\u00a0Generalizability\nGeneralization to out-of-distribution data is natural to \nhumans yet challenging for machines because most learning \nalgorithms strongly rely on the independent and identically \ndistributed (i.i.d.) assumption training at testing data, which \nis often violated in practice due to domain shift. Domain \ngeneralization aims to generalize models to new domains \nwithout knowledge of the target distribution during training. \nDifferent methods have been proposed for learning gener-\nalizable and transferable representations\u00a0(Dou et\u00a0al. 2019).\nMost existing approaches belong to the category of \ndomain alignment, where the main idea is to minimize the \ndifference between source domains for learning domain-\ninvariant representations. Features that are invariant to the \nsource domain shift should also be robust to any unseen \ntarget domain shift. Data augmentation has been a common \npractice to regularize the training of machine learning mod-\nels to avoid overfitting and improve generalization\u00a0(LeCun \net\u00a0al. 2015), which is particularly important for over-param-\neterized DNNs.\nVisual attention in CNNs can be used to highlight the \nregions of the image involved in a decision, with causal fil-\ntering to find the most relevant parts (Kim and Canny 2018). \nThe importance of individual pixels is estimated in Petsiuk \net\u00a0al. (2018) using randomly masked versions of images and \ncomparing the output predictions. This approach does not \napply to spatio-temporal methods or those that consider rela-\ntionships between objects in complex environments.\nReal\u2011World Testing\nIn general, DL methods have been shown to be prone to \nunderspecification, a problem that appears regardless of \nmodel type or application. Among other domains, under-\nspecification of computer vision is analyzed in D\u2019Amour \net\u00a0al. (2020), specifically for DL models such as the com-\nmonly used ResNet-50 and a scaled-up transfer learning \nimage classification model, Big Transfer (BiT) Kolesnikov \net\u00a0al. (2019). It is shown that while benchmark scores \nimproved with more model complexity and training data, \ntesting with real-world distortions results in poor and highly \n", []], "Complexity": ["Data Science for Transportation (2024) 6:1\t\nPage 5 of 27\u2003\n1\nby correlating with other information, even with a blurred \nlicense plate.\nModel Challenges\nComplexity\nDL computer vision models have a high complexity with \nrespect to neural network structures and training procedures. \nMany DL models are designed to run on high-performance \ncloud centers or AI workstations, and a good model requires \nweeks or months of training as well as high power consump-\ntion driven by modern Graphical Processing Units (GPUs) \nor Tensor Processing Units (TPUs).\nIn ITS and AV field applications,\u00a0many have a require-\nment for real-time or near real-time operations (Ke 2020) \nfor the sake of functionality and traffic safety. The DL model \ncomplexity adds a high cost to the training and inference in \nreal-time applications; particularly, the trend of ITS and AV \nis towards large-scale on-device processing closer to where \nthe traffic data is generated, e.g., crowdsensing. Three popu-\nlar embedded devices are compared in Arabi et\u00a0al. (2020), \nwith the Nvidia Jetson Nano yielding the highest inference \nefficiency but it is too little computation power for complex \napplications.\nReal-time applications usually make some modifications \nlike resizing video to lower resolution or model quantization \nand pruning which can lead to loss of performance. The \nmodel complexity of the state-of-the-art DL methods needs \nto be reduced in many practical applications to meet the effi-\nciency and accuracy requirements. For example, multi-scale \ndeformable attention has been used with vision transformer \nneural networks in object detection for high performance \nand fast convergence leading to faster training and inference \n(Zhu et\u00a0al. 2021).\nLack of\u00a0Explainability\nDNNs are largely seen as black boxes with many layers of \nprocessing, the working of which can be examined using \nstatistics, but the learned internal representations of the net-\nwork are based on millions or billions of parameters, making \nanalysis extremely difficult (Ras et\u00a0al. 2018). This means \nthat the behavior is essentially unpredictable, and very lit-\ntle explanation can be given of the decisions. It also makes \nsystem validation and verification impossible for critical use \ncases like autonomous driving (Samek et\u00a0al. 2017).\nThe common assumption that a complex black box is \nnecessary for good performance is being challenged (Rudin \n2019). Recent research is attempting to make DNNs more \nexplainable. A visualization tool for vision transformers is \npresented in Aflalo et\u00a0al. (2022), which can be used to see \nthe inner mechanisms, such as hidden parameters, and gain \ninsight into specific parts of the input that influenced the \npredictions. A framework for safety, explainability, and regu-\nlations for autonomous driving was evaluated in post-acci-\ndent scenarios (Atakishiyev et\u00a0al. 2021). The results showed \nmany benefits including transparency and debugging. A \nconvolutional neural network (CNN)-based architecture is \nproposed to detect action-inducing objects for autonomous \nvehicles, while also providing explanations for the actions \n(Xu et\u00a0al. 2020).\nTransferability and\u00a0Generalizability\nGeneralization to out-of-distribution data is natural to \nhumans yet challenging for machines because most learning \nalgorithms strongly rely on the independent and identically \ndistributed (i.i.d.) assumption training at testing data, which \nis often violated in practice due to domain shift. Domain \ngeneralization aims to generalize models to new domains \nwithout knowledge of the target distribution during training. \nDifferent methods have been proposed for learning gener-\nalizable and transferable representations\u00a0(Dou et\u00a0al. 2019).\nMost existing approaches belong to the category of \ndomain alignment, where the main idea is to minimize the \ndifference between source domains for learning domain-\ninvariant representations. Features that are invariant to the \nsource domain shift should also be robust to any unseen \ntarget domain shift. Data augmentation has been a common \npractice to regularize the training of machine learning mod-\nels to avoid overfitting and improve generalization\u00a0(LeCun \net\u00a0al. 2015), which is particularly important for over-param-\neterized DNNs.\nVisual attention in CNNs can be used to highlight the \nregions of the image involved in a decision, with causal fil-\ntering to find the most relevant parts (Kim and Canny 2018). \nThe importance of individual pixels is estimated in Petsiuk \net\u00a0al. (2018) using randomly masked versions of images and \ncomparing the output predictions. This approach does not \napply to spatio-temporal methods or those that consider rela-\ntionships between objects in complex environments.\nReal\u2011World Testing\nIn general, DL methods have been shown to be prone to \nunderspecification, a problem that appears regardless of \nmodel type or application. Among other domains, under-\nspecification of computer vision is analyzed in D\u2019Amour \net\u00a0al. (2020), specifically for DL models such as the com-\nmonly used ResNet-50 and a scaled-up transfer learning \nimage classification model, Big Transfer (BiT) Kolesnikov \net\u00a0al. (2019). It is shown that while benchmark scores \nimproved with more model complexity and training data, \ntesting with real-world distortions results in poor and highly \n", []], "Model Challenges": ["Data Science for Transportation (2024) 6:1\t\nPage 5 of 27\u2003\n1\nby correlating with other information, even with a blurred \nlicense plate.\nModel Challenges\nComplexity\nDL computer vision models have a high complexity with \nrespect to neural network structures and training procedures. \nMany DL models are designed to run on high-performance \ncloud centers or AI workstations, and a good model requires \nweeks or months of training as well as high power consump-\ntion driven by modern Graphical Processing Units (GPUs) \nor Tensor Processing Units (TPUs).\nIn ITS and AV field applications,\u00a0many have a require-\nment for real-time or near real-time operations (Ke 2020) \nfor the sake of functionality and traffic safety. The DL model \ncomplexity adds a high cost to the training and inference in \nreal-time applications; particularly, the trend of ITS and AV \nis towards large-scale on-device processing closer to where \nthe traffic data is generated, e.g., crowdsensing. Three popu-\nlar embedded devices are compared in Arabi et\u00a0al. (2020), \nwith the Nvidia Jetson Nano yielding the highest inference \nefficiency but it is too little computation power for complex \napplications.\nReal-time applications usually make some modifications \nlike resizing video to lower resolution or model quantization \nand pruning which can lead to loss of performance. The \nmodel complexity of the state-of-the-art DL methods needs \nto be reduced in many practical applications to meet the effi-\nciency and accuracy requirements. For example, multi-scale \ndeformable attention has been used with vision transformer \nneural networks in object detection for high performance \nand fast convergence leading to faster training and inference \n(Zhu et\u00a0al. 2021).\nLack of\u00a0Explainability\nDNNs are largely seen as black boxes with many layers of \nprocessing, the working of which can be examined using \nstatistics, but the learned internal representations of the net-\nwork are based on millions or billions of parameters, making \nanalysis extremely difficult (Ras et\u00a0al. 2018). This means \nthat the behavior is essentially unpredictable, and very lit-\ntle explanation can be given of the decisions. It also makes \nsystem validation and verification impossible for critical use \ncases like autonomous driving (Samek et\u00a0al. 2017).\nThe common assumption that a complex black box is \nnecessary for good performance is being challenged (Rudin \n2019). Recent research is attempting to make DNNs more \nexplainable. A visualization tool for vision transformers is \npresented in Aflalo et\u00a0al. (2022), which can be used to see \nthe inner mechanisms, such as hidden parameters, and gain \ninsight into specific parts of the input that influenced the \npredictions. A framework for safety, explainability, and regu-\nlations for autonomous driving was evaluated in post-acci-\ndent scenarios (Atakishiyev et\u00a0al. 2021). The results showed \nmany benefits including transparency and debugging. A \nconvolutional neural network (CNN)-based architecture is \nproposed to detect action-inducing objects for autonomous \nvehicles, while also providing explanations for the actions \n(Xu et\u00a0al. 2020).\nTransferability and\u00a0Generalizability\nGeneralization to out-of-distribution data is natural to \nhumans yet challenging for machines because most learning \nalgorithms strongly rely on the independent and identically \ndistributed (i.i.d.) assumption training at testing data, which \nis often violated in practice due to domain shift. Domain \ngeneralization aims to generalize models to new domains \nwithout knowledge of the target distribution during training. \nDifferent methods have been proposed for learning gener-\nalizable and transferable representations\u00a0(Dou et\u00a0al. 2019).\nMost existing approaches belong to the category of \ndomain alignment, where the main idea is to minimize the \ndifference between source domains for learning domain-\ninvariant representations. Features that are invariant to the \nsource domain shift should also be robust to any unseen \ntarget domain shift. Data augmentation has been a common \npractice to regularize the training of machine learning mod-\nels to avoid overfitting and improve generalization\u00a0(LeCun \net\u00a0al. 2015), which is particularly important for over-param-\neterized DNNs.\nVisual attention in CNNs can be used to highlight the \nregions of the image involved in a decision, with causal fil-\ntering to find the most relevant parts (Kim and Canny 2018). \nThe importance of individual pixels is estimated in Petsiuk \net\u00a0al. (2018) using randomly masked versions of images and \ncomparing the output predictions. This approach does not \napply to spatio-temporal methods or those that consider rela-\ntionships between objects in complex environments.\nReal\u2011World Testing\nIn general, DL methods have been shown to be prone to \nunderspecification, a problem that appears regardless of \nmodel type or application. Among other domains, under-\nspecification of computer vision is analyzed in D\u2019Amour \net\u00a0al. (2020), specifically for DL models such as the com-\nmonly used ResNet-50 and a scaled-up transfer learning \nimage classification model, Big Transfer (BiT) Kolesnikov \net\u00a0al. (2019). It is shown that while benchmark scores \nimproved with more model complexity and training data, \ntesting with real-world distortions results in poor and highly \n", []], "Security and\u00a0Privacy": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 4 of 27\nof this training data\u00a0(Wang et\u00a0al. 2022; Fu et\u00a0al. 2021). Some \nmethods like data re-sampling\u00a0(Mahajan et\u00a0al. 2018), loss re-\nweighting\u00a0(Wang et\u00a0al. 2021b), and cost-sensitive learning \n(Formosa et\u00a0al. 2023), can compensate the underrepresented \nclasses. However, they need to partition the categories into \nseveral groups based on their category frequency prior. Such \nhard division between head and tail classes brings two prob-\nlems: training inconsistency between adjacent categories and \nlack of discriminative power for rare categories\u00a0(Wang et\u00a0al. \n2021c).\nAn object detection study focusing on construction vehi-\ncles found that training a deep model with a huge general \ntraining dataset did not perform as well as a smaller model \ntrained specifically on construction vehicles (Arabi et\u00a0al. \n2020). Another model based on YOLOv2 for vehicle size \nestimation performed well on commonly seen sizes but var-\nied considerably with uncommon sizes (Wu et\u00a0al. 2019). The \ndataset used by Carranza-Garc\u00eda et\u00a0al. (2021) for autono-\nmous vehicle object detection had severe class imbalance \nwith only 1% cyclists represented. A number of weight-\nbased learning strategies were employed to address this, \ngiving higher weight to underrepresented classes, showing \nsignificant improvements.\nGeneral object detectors can be improved using transfer \nlearning with the underrepresented data for task-specific per-\nformance benefits (Zhao et\u00a0al. 2019). In addition, it is noted \nin Ras et\u00a0al. (2018) that model bias may not always be appar-\nent from just the training set, and explanability methods are \nneeded to address the problem.\nHigh Data Volume\nVisual data is composed of over 90% of the Internet traf-\nfic, and video transmission, computation, and storage pose \nincreasing challenges in ITS and AV fields (Ke 2020). The \nhigh volume of traffic and vehicular-based video data from \nthe roadside and onboard sensors via the traffic camera net-\nwork or the Internet of Vehicles (IoV) network poses compu-\ntational and bandwidth bottlenecks that cannot be solved by \nusing more powerful equipment (Xu et\u00a0al. 2018). As many \napplications in connected or autonomous vehicles rely on \nDL, vehicle-cloud architecture is emerging as an effective \ndistributed computing technique (Wang et\u00a0al. 2011). With \nthe integration of Road Side Units (RSU), these edge nodes \ncan process faster and provide low communication latency.\nSecurity and\u00a0Privacy\nPrivacy concerns are an important human factor that can-\nnot be overlooked in the design and operation of ITS appli-\ncations\u00a0(Fries et\u00a0al. 2012). Observing and tracking the \nmassive amounts of pedestrian and vehicle information \ncauses security and privacy concerns in ITS environments. \nFor example, UAVs are capable of collecting traffic data \n(through onboard video cameras). However, privacy con-\ncerns restrict them from being a regular part of the ITS sen-\nsor network \u00a0(Khan et\u00a0al. 2021). Video surveillance systems \nconstantly collect human faces and license plates. Personal \nprivacy is exchanged for security or safety services provided \nby the surveillance (Mart\u00ednez-Ballest\u00e9 et\u00a0al. 2013). Systems \ndeployed in practice might need to de-identify faces and \nlicense plates in real-time if raw video data is being sent \nor stored (Mart\u00ednez-Ballest\u00e9 et\u00a0al. 2012). Any processing \nwould ideally be done on the local edge unit, limiting the \npropagation of private information. Full anonymity is dif-\nficult to guarantee, for example, an uncommon model car \nwith a distinctive paint pattern can be traced to its owner \nFig.\u202f2\u2002 \u2009Illustration of representa-\ntive challenges associated with \ndata in computer vision applica-\ntions for transportation\n", [62]], "High Data Volume": ["\t\nData Science for Transportation (2024) 6:1\n1\u2003\nPage 4 of 27\nof this training data\u00a0(Wang et\u00a0al. 2022; Fu et\u00a0al. 2021). Some \nmethods like data re-sampling\u00a0(Mahajan et\u00a0al. 2018), loss re-\nweighting\u00a0(Wang et\u00a0al. 2021b), and cost-sensitive learning \n(Formosa et\u00a0al. 2023), can compensate the underrepresented \nclasses. However, they need to partition the categories into \nseveral groups based on their category frequency prior. Such \nhard division between head and tail classes brings two prob-\nlems: training inconsistency between adjacent categories and \nlack of discriminative power for rare categories\u00a0(Wang et\u00a0al. \n2021c).\nAn object detection study focusing on construction vehi-\ncles found that training a deep model with a huge general \ntraining dataset did not perform as well as a smaller model \ntrained specifically on construction vehicles (Arabi et\u00a0al. \n2020). Another model based on YOLOv2 for vehicle size \nestimation performed well on commonly seen sizes but var-\nied considerably with uncommon sizes (Wu et\u00a0al. 2019). The \ndataset used by Carranza-Garc\u00eda et\u00a0al. (2021) for autono-\nmous vehicle object detection had severe class imbalance \nwith only 1% cyclists represented. A number of weight-\nbased learning strategies were employed to address this, \ngiving higher weight to underrepresented classes, showing \nsignificant improvements.\nGeneral object detectors can be improved using transfer \nlearning with the underrepresented data for task-specific per-\nformance benefits (Zhao et\u00a0al. 2019). In addition, it is noted \nin Ras et\u00a0al. (2018) that model bias may not always be appar-\nent from just the training set, and explanability methods are \nneeded to address the problem.\nHigh Data Volume\nVisual data is composed of over 90% of the Internet traf-\nfic, and video transmission, computation, and storage pose \nincreasing challenges in ITS and AV fields (Ke 2020). The \nhigh volume of traffic and vehicular-based video data from \nthe roadside and onboard sensors via the traffic camera net-\nwork or the Internet of Vehicles (IoV) network poses compu-\ntational and bandwidth bottlenecks that cannot be solved by \nusing more powerful equipment (Xu et\u00a0al. 2018). As many \napplications in connected or autonomous vehicles rely on \nDL, vehicle-cloud architecture is emerging as an effective \ndistributed computing technique (Wang et\u00a0al. 2011). With \nthe integration of Road Side Units (RSU), these edge nodes \ncan process faster and provide low communication latency.\nSecurity and\u00a0Privacy\nPrivacy concerns are an important human factor that can-\nnot be overlooked in the design and operation of ITS appli-\ncations\u00a0(Fries et\u00a0al. 2012). Observing and tracking the \nmassive amounts of pedestrian and vehicle information \ncauses security and privacy concerns in ITS environments. \nFor example, UAVs are capable of collecting traffic data \n(through onboard video cameras). However, privacy con-\ncerns restrict them from being a regular part of the ITS sen-\nsor network \u00a0(Khan et\u00a0al. 2021). Video surveillance systems \nconstantly collect human faces and license plates. Personal \nprivacy is exchanged for security or safety services provided \nby the surveillance (Mart\u00ednez-Ballest\u00e9 et\u00a0al. 2013). Systems \ndeployed in practice might need to de-identify faces and \nlicense plates in real-time if raw video data is being sent \nor stored (Mart\u00ednez-Ballest\u00e9 et\u00a0al. 2012). Any processing \nwould ideally be done on the local edge unit, limiting the \npropagation of private information. Full anonymity is dif-\nficult to guarantee, for example, an uncommon model car \nwith a distinctive paint pattern can be traced to its owner \nFig.\u202f2\u2002 \u2009Illustration of representa-\ntive challenges associated with \ndata in computer vision applica-\ntions for transportation\n", [62]], "Data Bias": ["Data Science for Transportation (2024) 6:1\t\nPage 3 of 27\u2003\n1\n\u2022\t A review of DL models used for some representative \ncomputer vision applications susceptible to the chal-\nlenges.\n\u2022\t Specific techniques are already being used to mitigate the \nchallenges.\n\u2022\t Future directions of research to improve DL models for \nreal world complex traffic environments.\nData Challenges\nData Communication\nData communication, while not considered in most ITS and \nAV computer vision studies in the lab, is critical in practical \napplications. Individual-camera-based deep learning tasks \nin practice commonly require data communication between \nthe camera and the cloud server at TMC. Video data entails \ngreater network utilization, which can cause potential data \ncommunication issues, such as transmission delay and pack-\nage loss. In a cooperative camera-sensing environment, there \nare not only data communications with the server but also \namong different sensors. Therefore, two additional issues are \nmulti-sensor calibration and data synchronization.\nCalibration in a cooperative environment aims to deter-\nmine the perspective transformation between sensors to be \nable to merge acquired data from several views at a given \nframe (Caillot et\u00a0al. 2022). This task is quite challenging in \na multi-user environment because the transformation matrix \nbetween sensors constantly changes as the vehicles move. In \na cooperative context, calibration relies on the synchroniza-\ntion of the elements in a background image to determine \nthe transformation between static or mobile sensors\u00a0(Yang \net\u00a0al. 2021). There are multiple sources of desynchroniza-\ntion, such as an offset between the clocks or variable com-\nmunication delays. Although clocks may be synchronized, \nit is difficult to ensure the data acquisitions are triggered at \nthe same moment which adds uncertainty towards merging \nthe acquired data. Similarly, different sampling rates require \ninterpolation between acquired or predicted data, also add-\ning uncertainty.\nQuality of\u00a0Training Data and\u00a0Benchmarks\nTraffic cameras are widely deployed on roadways and vehi-\ncles (Ke 2020). TMCs at DOTs and cities constantly col-\nlect network-wide traffic camera data, which are required \nfor various ITS applications, such as event recognition and \nvehicle detection. However, labeled training data is much \nless common than unlabeled data (Halevy et\u00a0al. 2009; Luo \net\u00a0al. 2018). The lack of annotated datasets for many appli-\ncations is slowly being overcome with synthetic data, as \ngraphical fidelity and simulated physics have become more \nand more realistic. For example, ground truth 3D informa-\ntion in Hu et\u00a0al. (2019) needs high accuracy during training \nfor monocular 3D detection and tracking, so video game \ndata was used. In addition to realistic appearance, simulated \nscenarios do not need to be manually labeled as the labels \nare already generated by the simulation, and can support \na wide variety of illuminations, viewpoints, and vehicle \nbehaviors (Yao et\u00a0al. 2020). The 2020 AI City challenge for \nvehicle re-identification winner utilized a hybrid dataset to \nsignificantly improve the performance (Zheng et\u00a0al. 2020) \nby generating examples from real-world data and adding \nother simulated views and environments. However, if using \nsynthetic data, additional learning procedures, e.g., domain \nadaptation, are still needed for real-world applications. Low-\nfidelity simulated data were used to train a real-world object \ndetector with domain randomization transfer learning (Tobin \net\u00a0al. 2017).\nThe lack of good quality crash and near-crash data is \noften cited as a practical limitation (Taccari et\u00a0al. 2018). \nMore crash data will update the attention guidance in AD, \nallowing it to capture long-term crash characteristics, \nthereby improving crash risk estimation (Li et\u00a0al. 2021b). \nThere is also a lack of representation in the literature regard-\ning bicycles as the ego vehicle as mentioned in Ibrahim \net\u00a0al. (2021). A near-miss incident database was developed \nin Kataoka et\u00a0al. (2018) to compensate for the unavailabil-\nity, however, it is private because of copyright issues. A \nreview of vehicle behavior prediction methods (Mozaffari \net\u00a0al. 2022) discusses the lack of a benchmark for evaluat-\ning existing studies, preventing a fair comparison of differ-\nent DL techniques, or classical methods like Bayesian or \nMarkov decision process. It also highlights that faulty or \nlimited sensors, constrained computational resources, and \ngeneralizability to any driving scenario are current barriers \nto practical deployment and represent a significant research \ngap. Some of these issues can be addressed by sensor fusion, \ninternet of vehicles (IoV), and edge computing (Wang et\u00a0al. \n2020a).\nData Bias\nAlthough current vehicle detection algorithms perform \nwell on balanced datasets, they suffer from performance \ndegradation on tail classes when facing imbalanced data-\nsets. In real-world scenarios, data tends to obey the Zip-\nfian distribution\u00a0(Reed 2001) where a large number of tail \ncategories have fewer samples. A typical example of this \ncan be seen in the histogram in Fig.\u00a02. In long-tail data-\nsets, a few head classes (frequent classes) contribute most \nof the training samples, while tail classes (rare classes) are \nunderrepresented. Most DL models trained with such data \nminimize empirical risk on long-tail training data, and are \nbiased towards head categories since they contribute most \n", [62]], "Quality of\u00a0Training Data and\u00a0Benchmarks": ["Data Science for Transportation (2024) 6:1\t\nPage 3 of 27\u2003\n1\n\u2022\t A review of DL models used for some representative \ncomputer vision applications susceptible to the chal-\nlenges.\n\u2022\t Specific techniques are already being used to mitigate the \nchallenges.\n\u2022\t Future directions of research to improve DL models for \nreal world complex traffic environments.\nData Challenges\nData Communication\nData communication, while not considered in most ITS and \nAV computer vision studies in the lab, is critical in practical \napplications. Individual-camera-based deep learning tasks \nin practice commonly require data communication between \nthe camera and the cloud server at TMC. Video data entails \ngreater network utilization, which can cause potential data \ncommunication issues, such as transmission delay and pack-\nage loss. In a cooperative camera-sensing environment, there \nare not only data communications with the server but also \namong different sensors. Therefore, two additional issues are \nmulti-sensor calibration and data synchronization.\nCalibration in a cooperative environment aims to deter-\nmine the perspective transformation between sensors to be \nable to merge acquired data from several views at a given \nframe (Caillot et\u00a0al. 2022). This task is quite challenging in \na multi-user environment because the transformation matrix \nbetween sensors constantly changes as the vehicles move. In \na cooperative context, calibration relies on the synchroniza-\ntion of the elements in a background image to determine \nthe transformation between static or mobile sensors\u00a0(Yang \net\u00a0al. 2021). There are multiple sources of desynchroniza-\ntion, such as an offset between the clocks or variable com-\nmunication delays. Although clocks may be synchronized, \nit is difficult to ensure the data acquisitions are triggered at \nthe same moment which adds uncertainty towards merging \nthe acquired data. Similarly, different sampling rates require \ninterpolation between acquired or predicted data, also add-\ning uncertainty.\nQuality of\u00a0Training Data and\u00a0Benchmarks\nTraffic cameras are widely deployed on roadways and vehi-\ncles (Ke 2020). TMCs at DOTs and cities constantly col-\nlect network-wide traffic camera data, which are required \nfor various ITS applications, such as event recognition and \nvehicle detection. However, labeled training data is much \nless common than unlabeled data (Halevy et\u00a0al. 2009; Luo \net\u00a0al. 2018). The lack of annotated datasets for many appli-\ncations is slowly being overcome with synthetic data, as \ngraphical fidelity and simulated physics have become more \nand more realistic. For example, ground truth 3D informa-\ntion in Hu et\u00a0al. (2019) needs high accuracy during training \nfor monocular 3D detection and tracking, so video game \ndata was used. In addition to realistic appearance, simulated \nscenarios do not need to be manually labeled as the labels \nare already generated by the simulation, and can support \na wide variety of illuminations, viewpoints, and vehicle \nbehaviors (Yao et\u00a0al. 2020). The 2020 AI City challenge for \nvehicle re-identification winner utilized a hybrid dataset to \nsignificantly improve the performance (Zheng et\u00a0al. 2020) \nby generating examples from real-world data and adding \nother simulated views and environments. However, if using \nsynthetic data, additional learning procedures, e.g., domain \nadaptation, are still needed for real-world applications. Low-\nfidelity simulated data were used to train a real-world object \ndetector with domain randomization transfer learning (Tobin \net\u00a0al. 2017).\nThe lack of good quality crash and near-crash data is \noften cited as a practical limitation (Taccari et\u00a0al. 2018). \nMore crash data will update the attention guidance in AD, \nallowing it to capture long-term crash characteristics, \nthereby improving crash risk estimation (Li et\u00a0al. 2021b). \nThere is also a lack of representation in the literature regard-\ning bicycles as the ego vehicle as mentioned in Ibrahim \net\u00a0al. (2021). A near-miss incident database was developed \nin Kataoka et\u00a0al. (2018) to compensate for the unavailabil-\nity, however, it is private because of copyright issues. A \nreview of vehicle behavior prediction methods (Mozaffari \net\u00a0al. 2022) discusses the lack of a benchmark for evaluat-\ning existing studies, preventing a fair comparison of differ-\nent DL techniques, or classical methods like Bayesian or \nMarkov decision process. It also highlights that faulty or \nlimited sensors, constrained computational resources, and \ngeneralizability to any driving scenario are current barriers \nto practical deployment and represent a significant research \ngap. Some of these issues can be addressed by sensor fusion, \ninternet of vehicles (IoV), and edge computing (Wang et\u00a0al. \n2020a).\nData Bias\nAlthough current vehicle detection algorithms perform \nwell on balanced datasets, they suffer from performance \ndegradation on tail classes when facing imbalanced data-\nsets. In real-world scenarios, data tends to obey the Zip-\nfian distribution\u00a0(Reed 2001) where a large number of tail \ncategories have fewer samples. A typical example of this \ncan be seen in the histogram in Fig.\u00a02. In long-tail data-\nsets, a few head classes (frequent classes) contribute most \nof the training samples, while tail classes (rare classes) are \nunderrepresented. Most DL models trained with such data \nminimize empirical risk on long-tail training data, and are \nbiased towards head categories since they contribute most \n", []], "Data Communication": ["Data Science for Transportation (2024) 6:1\t\nPage 3 of 27\u2003\n1\n\u2022\t A review of DL models used for some representative \ncomputer vision applications susceptible to the chal-\nlenges.\n\u2022\t Specific techniques are already being used to mitigate the \nchallenges.\n\u2022\t Future directions of research to improve DL models for \nreal world complex traffic environments.\nData Challenges\nData Communication\nData communication, while not considered in most ITS and \nAV computer vision studies in the lab, is critical in practical \napplications. Individual-camera-based deep learning tasks \nin practice commonly require data communication between \nthe camera and the cloud server at TMC. Video data entails \ngreater network utilization, which can cause potential data \ncommunication issues, such as transmission delay and pack-\nage loss. In a cooperative camera-sensing environment, there \nare not only data communications with the server but also \namong different sensors. Therefore, two additional issues are \nmulti-sensor calibration and data synchronization.\nCalibration in a cooperative environment aims to deter-\nmine the perspective transformation between sensors to be \nable to merge acquired data from several views at a given \nframe (Caillot et\u00a0al. 2022). This task is quite challenging in \na multi-user environment because the transformation matrix \nbetween sensors constantly changes as the vehicles move. In \na cooperative context, calibration relies on the synchroniza-\ntion of the elements in a background image to determine \nthe transformation between static or mobile sensors\u00a0(Yang \net\u00a0al. 2021). There are multiple sources of desynchroniza-\ntion, such as an offset between the clocks or variable com-\nmunication delays. Although clocks may be synchronized, \nit is difficult to ensure the data acquisitions are triggered at \nthe same moment which adds uncertainty towards merging \nthe acquired data. Similarly, different sampling rates require \ninterpolation between acquired or predicted data, also add-\ning uncertainty.\nQuality of\u00a0Training Data and\u00a0Benchmarks\nTraffic cameras are widely deployed on roadways and vehi-\ncles (Ke 2020). TMCs at DOTs and cities constantly col-\nlect network-wide traffic camera data, which are required \nfor various ITS applications, such as event recognition and \nvehicle detection. However, labeled training data is much \nless common than unlabeled data (Halevy et\u00a0al. 2009; Luo \net\u00a0al. 2018). The lack of annotated datasets for many appli-\ncations is slowly being overcome with synthetic data, as \ngraphical fidelity and simulated physics have become more \nand more realistic. For example, ground truth 3D informa-\ntion in Hu et\u00a0al. (2019) needs high accuracy during training \nfor monocular 3D detection and tracking, so video game \ndata was used. In addition to realistic appearance, simulated \nscenarios do not need to be manually labeled as the labels \nare already generated by the simulation, and can support \na wide variety of illuminations, viewpoints, and vehicle \nbehaviors (Yao et\u00a0al. 2020). The 2020 AI City challenge for \nvehicle re-identification winner utilized a hybrid dataset to \nsignificantly improve the performance (Zheng et\u00a0al. 2020) \nby generating examples from real-world data and adding \nother simulated views and environments. However, if using \nsynthetic data, additional learning procedures, e.g., domain \nadaptation, are still needed for real-world applications. Low-\nfidelity simulated data were used to train a real-world object \ndetector with domain randomization transfer learning (Tobin \net\u00a0al. 2017).\nThe lack of good quality crash and near-crash data is \noften cited as a practical limitation (Taccari et\u00a0al. 2018). \nMore crash data will update the attention guidance in AD, \nallowing it to capture long-term crash characteristics, \nthereby improving crash risk estimation (Li et\u00a0al. 2021b). \nThere is also a lack of representation in the literature regard-\ning bicycles as the ego vehicle as mentioned in Ibrahim \net\u00a0al. (2021). A near-miss incident database was developed \nin Kataoka et\u00a0al. (2018) to compensate for the unavailabil-\nity, however, it is private because of copyright issues. A \nreview of vehicle behavior prediction methods (Mozaffari \net\u00a0al. 2022) discusses the lack of a benchmark for evaluat-\ning existing studies, preventing a fair comparison of differ-\nent DL techniques, or classical methods like Bayesian or \nMarkov decision process. It also highlights that faulty or \nlimited sensors, constrained computational resources, and \ngeneralizability to any driving scenario are current barriers \nto practical deployment and represent a significant research \ngap. Some of these issues can be addressed by sensor fusion, \ninternet of vehicles (IoV), and edge computing (Wang et\u00a0al. \n2020a).\nData Bias\nAlthough current vehicle detection algorithms perform \nwell on balanced datasets, they suffer from performance \ndegradation on tail classes when facing imbalanced data-\nsets. In real-world scenarios, data tends to obey the Zip-\nfian distribution\u00a0(Reed 2001) where a large number of tail \ncategories have fewer samples. A typical example of this \ncan be seen in the histogram in Fig.\u00a02. In long-tail data-\nsets, a few head classes (frequent classes) contribute most \nof the training samples, while tail classes (rare classes) are \nunderrepresented. Most DL models trained with such data \nminimize empirical risk on long-tail training data, and are \nbiased towards head categories since they contribute most \n", []], "Data Challenges": ["Data Science for Transportation (2024) 6:1\t\nPage 3 of 27\u2003\n1\n\u2022\t A review of DL models used for some representative \ncomputer vision applications susceptible to the chal-\nlenges.\n\u2022\t Specific techniques are already being used to mitigate the \nchallenges.\n\u2022\t Future directions of research to improve DL models for \nreal world complex traffic environments.\nData Challenges\nData Communication\nData communication, while not considered in most ITS and \nAV computer vision studies in the lab, is critical in practical \napplications. Individual-camera-based deep learning tasks \nin practice commonly require data communication between \nthe camera and the cloud server at TMC. Video data entails \ngreater network utilization, which can cause potential data \ncommunication issues, such as transmission delay and pack-\nage loss. In a cooperative camera-sensing environment, there \nare not only data communications with the server but also \namong different sensors. Therefore, two additional issues are \nmulti-sensor calibration and data synchronization.\nCalibration in a cooperative environment aims to deter-\nmine the perspective transformation between sensors to be \nable to merge acquired data from several views at a given \nframe (Caillot et\u00a0al. 2022). This task is quite challenging in \na multi-user environment because the transformation matrix \nbetween sensors constantly changes as the vehicles move. In \na cooperative context, calibration relies on the synchroniza-\ntion of the elements in a background image to determine \nthe transformation between static or mobile sensors\u00a0(Yang \net\u00a0al. 2021). There are multiple sources of desynchroniza-\ntion, such as an offset between the clocks or variable com-\nmunication delays. Although clocks may be synchronized, \nit is difficult to ensure the data acquisitions are triggered at \nthe same moment which adds uncertainty towards merging \nthe acquired data. Similarly, different sampling rates require \ninterpolation between acquired or predicted data, also add-\ning uncertainty.\nQuality of\u00a0Training Data and\u00a0Benchmarks\nTraffic cameras are widely deployed on roadways and vehi-\ncles (Ke 2020). TMCs at DOTs and cities constantly col-\nlect network-wide traffic camera data, which are required \nfor various ITS applications, such as event recognition and \nvehicle detection. However, labeled training data is much \nless common than unlabeled data (Halevy et\u00a0al. 2009; Luo \net\u00a0al. 2018). The lack of annotated datasets for many appli-\ncations is slowly being overcome with synthetic data, as \ngraphical fidelity and simulated physics have become more \nand more realistic. For example, ground truth 3D informa-\ntion in Hu et\u00a0al. (2019) needs high accuracy during training \nfor monocular 3D detection and tracking, so video game \ndata was used. In addition to realistic appearance, simulated \nscenarios do not need to be manually labeled as the labels \nare already generated by the simulation, and can support \na wide variety of illuminations, viewpoints, and vehicle \nbehaviors (Yao et\u00a0al. 2020). The 2020 AI City challenge for \nvehicle re-identification winner utilized a hybrid dataset to \nsignificantly improve the performance (Zheng et\u00a0al. 2020) \nby generating examples from real-world data and adding \nother simulated views and environments. However, if using \nsynthetic data, additional learning procedures, e.g., domain \nadaptation, are still needed for real-world applications. Low-\nfidelity simulated data were used to train a real-world object \ndetector with domain randomization transfer learning (Tobin \net\u00a0al. 2017).\nThe lack of good quality crash and near-crash data is \noften cited as a practical limitation (Taccari et\u00a0al. 2018). \nMore crash data will update the attention guidance in AD, \nallowing it to capture long-term crash characteristics, \nthereby improving crash risk estimation (Li et\u00a0al. 2021b). \nThere is also a lack of representation in the literature regard-\ning bicycles as the ego vehicle as mentioned in Ibrahim \net\u00a0al. (2021). A near-miss incident database was developed \nin Kataoka et\u00a0al. (2018) to compensate for the unavailabil-\nity, however, it is private because of copyright issues. A \nreview of vehicle behavior prediction methods (Mozaffari \net\u00a0al. 2022) discusses the lack of a benchmark for evaluat-\ning existing studies, preventing a fair comparison of differ-\nent DL techniques, or classical methods like Bayesian or \nMarkov decision process. It also highlights that faulty or \nlimited sensors, constrained computational resources, and \ngeneralizability to any driving scenario are current barriers \nto practical deployment and represent a significant research \ngap. Some of these issues can be addressed by sensor fusion, \ninternet of vehicles (IoV), and edge computing (Wang et\u00a0al. \n2020a).\nData Bias\nAlthough current vehicle detection algorithms perform \nwell on balanced datasets, they suffer from performance \ndegradation on tail classes when facing imbalanced data-\nsets. In real-world scenarios, data tends to obey the Zip-\nfian distribution\u00a0(Reed 2001) where a large number of tail \ncategories have fewer samples. A typical example of this \ncan be seen in the histogram in Fig.\u00a02. In long-tail data-\nsets, a few head classes (frequent classes) contribute most \nof the training samples, while tail classes (rare classes) are \nunderrepresented. Most DL models trained with such data \nminimize empirical risk on long-tail training data, and are \nbiased towards head categories since they contribute most \n", []], "Introduction": ["Vol.:(0123456789)\nData Science for Transportation (2024) 6:1 \nhttps://doi.org/10.1007/s42421-023-00086-7\nRESEARCH\nDeep Learning\u2011Based Computer Vision Methods for\u00a0Complex Traffic \nEnvironments Perception: A\u00a0Review\nTalha\u00a0Azfar1\u00a0\u00b7 Jinlong\u00a0Li2\u00a0\u00b7 Hongkai\u00a0Yu2\u00a0\u00b7 Ruey\u00a0L.\u00a0Cheu3\u00a0\u00b7 Yisheng\u00a0Lv4\u00a0\u00b7 Ruimin\u00a0Ke1\nReceived: 2 May 2023 / Revised: 7 October 2023 / Accepted: 1 November 2023 / Published online: 8 January 2024 \n\u00a9 The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024\nAbstract\nComputer vision applications in intelligent transportation systems (ITS) and autonomous driving (AD) have gravitated \ntowards deep neural network architectures in recent years. While performance seems to be improving on benchmark datasets, \nmany real-world challenges are yet to be adequately considered in research. This paper conducted an extensive literature \nreview on the applications of computer vision in ITS and AD, and discusses challenges related to data, models, and complex \nurban environments. The data challenges are associated with the collection and labeling of training data and its relevance \nto real-world conditions, bias inherent in datasets, the high volume of data needed to be processed, and privacy concerns. \nDeep learning (DL) models are commonly too complex for real-time processing on embedded hardware, lack explainability \nand generalizability, and are hard to test in real-world settings. Complex urban traffic environments have irregular lighting \nand occlusions, and surveillance cameras can be mounted at a variety of angles, gather dirt, and shake in the wind, while the \ntraffic conditions are highly heterogeneous, with violation of rules and complex interactions in crowded scenarios. Some \nrepresentative applications that suffer from these problems are traffic flow estimation, congestion detection, autonomous \ndriving perception, vehicle interaction, and edge computing for practical deployment. The possible ways of dealing with the \nchallenges are also explored while prioritizing practical deployment.\nKeywords\u2002 Deep learning\u00a0\u00b7 Intelligent Transportation systems\u00a0\u00b7 Computer vision\u00a0\u00b7 Autonomous driving\u00a0\u00b7 Complex traffic \nenvironment\nIntroduction\nVideo cameras have been used to monitor traffic and provide \nvaluable information to traffic management center (TMC) \noperators. The manual process of having TMC operators \nobserve numerous video screens has given way to automated \nand semi-automated computer vision approaches for faster \nprocessing and response times with some humans in the loop \nto interpret and verify the data. Artificial neural networks \nare being increasingly used in computer vision in ITS and \nautonomous driving (AD) applications, showing benefits in \ntraffic monitoring, traffic flow estimation, incident detection, \netc. However, the use of deep neural networks (DNN) brings \nsome issues and concerns that should be studied in further \ndetail as they need to be more accurate, more reliable, and \npractical enough for large-scale deployment as part of ITS \ninfrastructure or in autonomous vehicles.\nDeep learning (DL) refers to machine learning archi-\ntectures of multiple layers such as large (deep) neural net-\nworks spanning many layers in a variety of configurations. \n *\t Ruimin Ke \n\t\nker@rpi.edu\n\t\nTalha Azfar \n\t\nazfart@rpi.edu\n\t\nJinlong Li \n\t\nj.li56@vikes.csuohio.edu\n\t\nHongkai Yu \n\t\nh.yu19@csuohio.edu\n\t\nRuey L. Cheu \n\t\nrcheu@utep.edu\n\t\nYisheng Lv \n\t\nyisheng.lv@ia.ac.cn\n1\t\nRensselaer Polytechnic Institute, Troy, NY\u00a012180, USA\n2\t\nCleveland State University, Cleveland, OH\u00a044115, USA\n3\t\nThe University of\u00a0Texas at\u00a0El Paso, El\u00a0Paso, TX\u00a079968, USA\n4\t\nInstitute of\u00a0Automation, Chinese Academy of\u00a0Sciences, \nBeijing\u00a0100190, China\n", [17]], "Abstract": ["Vol.:(0123456789)\nData Science for Transportation (2024) 6:1 \nhttps://doi.org/10.1007/s42421-023-00086-7\nRESEARCH\nDeep Learning\u2011Based Computer Vision Methods for\u00a0Complex Traffic \nEnvironments Perception: A\u00a0Review\nTalha\u00a0Azfar1\u00a0\u00b7 Jinlong\u00a0Li2\u00a0\u00b7 Hongkai\u00a0Yu2\u00a0\u00b7 Ruey\u00a0L.\u00a0Cheu3\u00a0\u00b7 Yisheng\u00a0Lv4\u00a0\u00b7 Ruimin\u00a0Ke1\nReceived: 2 May 2023 / Revised: 7 October 2023 / Accepted: 1 November 2023 / Published online: 8 January 2024 \n\u00a9 The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024\nAbstract\nComputer vision applications in intelligent transportation systems (ITS) and autonomous driving (AD) have gravitated \ntowards deep neural network architectures in recent years. While performance seems to be improving on benchmark datasets, \nmany real-world challenges are yet to be adequately considered in research. This paper conducted an extensive literature \nreview on the applications of computer vision in ITS and AD, and discusses challenges related to data, models, and complex \nurban environments. The data challenges are associated with the collection and labeling of training data and its relevance \nto real-world conditions, bias inherent in datasets, the high volume of data needed to be processed, and privacy concerns. \nDeep learning (DL) models are commonly too complex for real-time processing on embedded hardware, lack explainability \nand generalizability, and are hard to test in real-world settings. Complex urban traffic environments have irregular lighting \nand occlusions, and surveillance cameras can be mounted at a variety of angles, gather dirt, and shake in the wind, while the \ntraffic conditions are highly heterogeneous, with violation of rules and complex interactions in crowded scenarios. Some \nrepresentative applications that suffer from these problems are traffic flow estimation, congestion detection, autonomous \ndriving perception, vehicle interaction, and edge computing for practical deployment. The possible ways of dealing with the \nchallenges are also explored while prioritizing practical deployment.\nKeywords\u2002 Deep learning\u00a0\u00b7 Intelligent Transportation systems\u00a0\u00b7 Computer vision\u00a0\u00b7 Autonomous driving\u00a0\u00b7 Complex traffic \nenvironment\nIntroduction\nVideo cameras have been used to monitor traffic and provide \nvaluable information to traffic management center (TMC) \noperators. The manual process of having TMC operators \nobserve numerous video screens has given way to automated \nand semi-automated computer vision approaches for faster \nprocessing and response times with some humans in the loop \nto interpret and verify the data. Artificial neural networks \nare being increasingly used in computer vision in ITS and \nautonomous driving (AD) applications, showing benefits in \ntraffic monitoring, traffic flow estimation, incident detection, \netc. However, the use of deep neural networks (DNN) brings \nsome issues and concerns that should be studied in further \ndetail as they need to be more accurate, more reliable, and \npractical enough for large-scale deployment as part of ITS \ninfrastructure or in autonomous vehicles.\nDeep learning (DL) refers to machine learning archi-\ntectures of multiple layers such as large (deep) neural net-\nworks spanning many layers in a variety of configurations. \n *\t Ruimin Ke \n\t\nker@rpi.edu\n\t\nTalha Azfar \n\t\nazfart@rpi.edu\n\t\nJinlong Li \n\t\nj.li56@vikes.csuohio.edu\n\t\nHongkai Yu \n\t\nh.yu19@csuohio.edu\n\t\nRuey L. Cheu \n\t\nrcheu@utep.edu\n\t\nYisheng Lv \n\t\nyisheng.lv@ia.ac.cn\n1\t\nRensselaer Polytechnic Institute, Troy, NY\u00a012180, USA\n2\t\nCleveland State University, Cleveland, OH\u00a044115, USA\n3\t\nThe University of\u00a0Texas at\u00a0El Paso, El\u00a0Paso, TX\u00a079968, USA\n4\t\nInstitute of\u00a0Automation, Chinese Academy of\u00a0Sciences, \nBeijing\u00a0100190, China\n", []], "Deep Learning-Based Computer Vision Methods for\u00a0Complex Traffic Environments Perception: A\u00a0Review": ["Vol.:(0123456789)\nData Science for Transportation (2024) 6:1 \nhttps://doi.org/10.1007/s42421-023-00086-7\nRESEARCH\nDeep Learning\u2011Based Computer Vision Methods for\u00a0Complex Traffic \nEnvironments Perception: A\u00a0Review\nTalha\u00a0Azfar1\u00a0\u00b7 Jinlong\u00a0Li2\u00a0\u00b7 Hongkai\u00a0Yu2\u00a0\u00b7 Ruey\u00a0L.\u00a0Cheu3\u00a0\u00b7 Yisheng\u00a0Lv4\u00a0\u00b7 Ruimin\u00a0Ke1\nReceived: 2 May 2023 / Revised: 7 October 2023 / Accepted: 1 November 2023 / Published online: 8 January 2024 \n\u00a9 The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024\nAbstract\nComputer vision applications in intelligent transportation systems (ITS) and autonomous driving (AD) have gravitated \ntowards deep neural network architectures in recent years. While performance seems to be improving on benchmark datasets, \nmany real-world challenges are yet to be adequately considered in research. This paper conducted an extensive literature \nreview on the applications of computer vision in ITS and AD, and discusses challenges related to data, models, and complex \nurban environments. The data challenges are associated with the collection and labeling of training data and its relevance \nto real-world conditions, bias inherent in datasets, the high volume of data needed to be processed, and privacy concerns. \nDeep learning (DL) models are commonly too complex for real-time processing on embedded hardware, lack explainability \nand generalizability, and are hard to test in real-world settings. Complex urban traffic environments have irregular lighting \nand occlusions, and surveillance cameras can be mounted at a variety of angles, gather dirt, and shake in the wind, while the \ntraffic conditions are highly heterogeneous, with violation of rules and complex interactions in crowded scenarios. Some \nrepresentative applications that suffer from these problems are traffic flow estimation, congestion detection, autonomous \ndriving perception, vehicle interaction, and edge computing for practical deployment. The possible ways of dealing with the \nchallenges are also explored while prioritizing practical deployment.\nKeywords\u2002 Deep learning\u00a0\u00b7 Intelligent Transportation systems\u00a0\u00b7 Computer vision\u00a0\u00b7 Autonomous driving\u00a0\u00b7 Complex traffic \nenvironment\nIntroduction\nVideo cameras have been used to monitor traffic and provide \nvaluable information to traffic management center (TMC) \noperators. The manual process of having TMC operators \nobserve numerous video screens has given way to automated \nand semi-automated computer vision approaches for faster \nprocessing and response times with some humans in the loop \nto interpret and verify the data. Artificial neural networks \nare being increasingly used in computer vision in ITS and \nautonomous driving (AD) applications, showing benefits in \ntraffic monitoring, traffic flow estimation, incident detection, \netc. However, the use of deep neural networks (DNN) brings \nsome issues and concerns that should be studied in further \ndetail as they need to be more accurate, more reliable, and \npractical enough for large-scale deployment as part of ITS \ninfrastructure or in autonomous vehicles.\nDeep learning (DL) refers to machine learning archi-\ntectures of multiple layers such as large (deep) neural net-\nworks spanning many layers in a variety of configurations. \n *\t Ruimin Ke \n\t\nker@rpi.edu\n\t\nTalha Azfar \n\t\nazfart@rpi.edu\n\t\nJinlong Li \n\t\nj.li56@vikes.csuohio.edu\n\t\nHongkai Yu \n\t\nh.yu19@csuohio.edu\n\t\nRuey L. Cheu \n\t\nrcheu@utep.edu\n\t\nYisheng Lv \n\t\nyisheng.lv@ia.ac.cn\n1\t\nRensselaer Polytechnic Institute, Troy, NY\u00a012180, USA\n2\t\nCleveland State University, Cleveland, OH\u00a044115, USA\n3\t\nThe University of\u00a0Texas at\u00a0El Paso, El\u00a0Paso, TX\u00a079968, USA\n4\t\nInstitute of\u00a0Automation, Chinese Academy of\u00a0Sciences, \nBeijing\u00a0100190, China\n", []]}