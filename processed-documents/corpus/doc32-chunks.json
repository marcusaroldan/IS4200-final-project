{"References": ["68:22\nSungsoo Ray Hong et al.\n6\nLIMITATIONS\nWhile we devoted considerable effort to interviewing participants from a broad range of domains,\nour analysis misses other important domains, such as governance (e.g., predictive policing), agri-\nculture (e.g., crop monitoring, animal health monitoring), and more. We focused on cases where\nmodel predictions are consumed by a human. We did not cover application areas where a model\u2019s\npredictions are used in fully automated and/or semi-automated settings (e.g., autonomous driv-\ning, or warehouse robots). Finally, many of our participants were data scientists with a technical\nbackground, therefore this work reflects their specific point of view, as interpreted by the authors\nthrough systematic qualitative analysis. Although we identified key stakeholder roles in model\ninterpretability, we did not directly hear from every role that we identified as relevant to model\ninterpretability. Future work might pursue ethnographic and other observational approaches to\ncorroborate these results.\n7\nCONCLUSION\nIn this work, we empirically investigated industry practitioners\u2019 model interpretability practices,\nchallenges, and needs. We describe roles, processes, goals, and strategies around interpretability\npracticed by ML practitioners working in a variety of domains. We draw out important contrasts\nbetween our results and the predominant framing of interpretability in existing research as a\nproperty of a model that can be defined solely in light of an individual versus a ML model\u2019s\nalignment. Our results lead us to characterize interpretability as inherently social and negotiated,\naimed at fostering trust both in people and in models, context-dependent, and arising from careful\ncomparisons of human mental models. Our identification of unaddressed technological needs\namong practitioners is intended to help researchers direct their attention to unaddressed, yet\nprevalent, challenges in industry.\n8\nACKNOWLEDGEMENT\nWe wish to express our deepest gratitude to our participants who were willing to share their\nexperience and insights about their model interpretability practice and challenges. We also thank\nfor DARPA Data-Driven Discovery of Models (D3M) Program for generously supporting this\nresearch. Any opinions, findings, and conclusions or recommendations expressed in this material\nare those of the authors and do not necessarily reflect the views of DARPA.\nREFERENCES\n[1] Josh M Attenberg, Pagagiotis G Ipeirotis, and Foster Provost. 2011. Beat the machine: Challenging workers to find the\nunknown unknowns. In Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence.\n[2] Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz, and Marzyeh Ghassemi. 2018. The Effect of Heterogeneous\nData for Alzheimer\u2019s Disease Detection from Speech. arXiv preprint arXiv:1811.12254 (2018).\n[3] Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in human-ai\nteams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 33. 2429\u20132437.\n[4] Adrien Bibal and Beno\u00eet Frenay. 2016. Interpretability of Machine Learning Models and Representations: an Introduction.\nIn 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. 77\u201382.\n[5] Ann Bostrom, Baruch Fischhoff, and M Granger Morgan. 1992. Characterizing mental models of hazardous processes:\nA methodology and an application to radon. Journal of social issues 48, 4 (1992), 85\u2013100.\n[6] Leo Breiman. 2017. Classification and Regression Trees. Routledge.\n[7] Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The role of explanations on trust and reliance in\nclinical decision support systems. In 2015 International Conference on Healthcare Informatics. IEEE, 160\u2013169.\n[8] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models\nfor healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "8 Acknowledgement": ["68:22\nSungsoo Ray Hong et al.\n6\nLIMITATIONS\nWhile we devoted considerable effort to interviewing participants from a broad range of domains,\nour analysis misses other important domains, such as governance (e.g., predictive policing), agri-\nculture (e.g., crop monitoring, animal health monitoring), and more. We focused on cases where\nmodel predictions are consumed by a human. We did not cover application areas where a model\u2019s\npredictions are used in fully automated and/or semi-automated settings (e.g., autonomous driv-\ning, or warehouse robots). Finally, many of our participants were data scientists with a technical\nbackground, therefore this work reflects their specific point of view, as interpreted by the authors\nthrough systematic qualitative analysis. Although we identified key stakeholder roles in model\ninterpretability, we did not directly hear from every role that we identified as relevant to model\ninterpretability. Future work might pursue ethnographic and other observational approaches to\ncorroborate these results.\n7\nCONCLUSION\nIn this work, we empirically investigated industry practitioners\u2019 model interpretability practices,\nchallenges, and needs. We describe roles, processes, goals, and strategies around interpretability\npracticed by ML practitioners working in a variety of domains. We draw out important contrasts\nbetween our results and the predominant framing of interpretability in existing research as a\nproperty of a model that can be defined solely in light of an individual versus a ML model\u2019s\nalignment. Our results lead us to characterize interpretability as inherently social and negotiated,\naimed at fostering trust both in people and in models, context-dependent, and arising from careful\ncomparisons of human mental models. Our identification of unaddressed technological needs\namong practitioners is intended to help researchers direct their attention to unaddressed, yet\nprevalent, challenges in industry.\n8\nACKNOWLEDGEMENT\nWe wish to express our deepest gratitude to our participants who were willing to share their\nexperience and insights about their model interpretability practice and challenges. We also thank\nfor DARPA Data-Driven Discovery of Models (D3M) Program for generously supporting this\nresearch. Any opinions, findings, and conclusions or recommendations expressed in this material\nare those of the authors and do not necessarily reflect the views of DARPA.\nREFERENCES\n[1] Josh M Attenberg, Pagagiotis G Ipeirotis, and Foster Provost. 2011. Beat the machine: Challenging workers to find the\nunknown unknowns. In Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence.\n[2] Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz, and Marzyeh Ghassemi. 2018. The Effect of Heterogeneous\nData for Alzheimer\u2019s Disease Detection from Speech. arXiv preprint arXiv:1811.12254 (2018).\n[3] Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in human-ai\nteams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 33. 2429\u20132437.\n[4] Adrien Bibal and Beno\u00eet Frenay. 2016. Interpretability of Machine Learning Models and Representations: an Introduction.\nIn 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. 77\u201382.\n[5] Ann Bostrom, Baruch Fischhoff, and M Granger Morgan. 1992. Characterizing mental models of hazardous processes:\nA methodology and an application to radon. Journal of social issues 48, 4 (1992), 85\u2013100.\n[6] Leo Breiman. 2017. Classification and Regression Trees. Routledge.\n[7] Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The role of explanations on trust and reliance in\nclinical decision support systems. In 2015 International Conference on Healthcare Informatics. IEEE, 160\u2013169.\n[8] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models\nfor healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "7 Conclusion": ["68:22\nSungsoo Ray Hong et al.\n6\nLIMITATIONS\nWhile we devoted considerable effort to interviewing participants from a broad range of domains,\nour analysis misses other important domains, such as governance (e.g., predictive policing), agri-\nculture (e.g., crop monitoring, animal health monitoring), and more. We focused on cases where\nmodel predictions are consumed by a human. We did not cover application areas where a model\u2019s\npredictions are used in fully automated and/or semi-automated settings (e.g., autonomous driv-\ning, or warehouse robots). Finally, many of our participants were data scientists with a technical\nbackground, therefore this work reflects their specific point of view, as interpreted by the authors\nthrough systematic qualitative analysis. Although we identified key stakeholder roles in model\ninterpretability, we did not directly hear from every role that we identified as relevant to model\ninterpretability. Future work might pursue ethnographic and other observational approaches to\ncorroborate these results.\n7\nCONCLUSION\nIn this work, we empirically investigated industry practitioners\u2019 model interpretability practices,\nchallenges, and needs. We describe roles, processes, goals, and strategies around interpretability\npracticed by ML practitioners working in a variety of domains. We draw out important contrasts\nbetween our results and the predominant framing of interpretability in existing research as a\nproperty of a model that can be defined solely in light of an individual versus a ML model\u2019s\nalignment. Our results lead us to characterize interpretability as inherently social and negotiated,\naimed at fostering trust both in people and in models, context-dependent, and arising from careful\ncomparisons of human mental models. Our identification of unaddressed technological needs\namong practitioners is intended to help researchers direct their attention to unaddressed, yet\nprevalent, challenges in industry.\n8\nACKNOWLEDGEMENT\nWe wish to express our deepest gratitude to our participants who were willing to share their\nexperience and insights about their model interpretability practice and challenges. We also thank\nfor DARPA Data-Driven Discovery of Models (D3M) Program for generously supporting this\nresearch. Any opinions, findings, and conclusions or recommendations expressed in this material\nare those of the authors and do not necessarily reflect the views of DARPA.\nREFERENCES\n[1] Josh M Attenberg, Pagagiotis G Ipeirotis, and Foster Provost. 2011. Beat the machine: Challenging workers to find the\nunknown unknowns. In Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence.\n[2] Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz, and Marzyeh Ghassemi. 2018. The Effect of Heterogeneous\nData for Alzheimer\u2019s Disease Detection from Speech. arXiv preprint arXiv:1811.12254 (2018).\n[3] Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in human-ai\nteams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 33. 2429\u20132437.\n[4] Adrien Bibal and Beno\u00eet Frenay. 2016. Interpretability of Machine Learning Models and Representations: an Introduction.\nIn 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. 77\u201382.\n[5] Ann Bostrom, Baruch Fischhoff, and M Granger Morgan. 1992. Characterizing mental models of hazardous processes:\nA methodology and an application to radon. Journal of social issues 48, 4 (1992), 85\u2013100.\n[6] Leo Breiman. 2017. Classification and Regression Trees. Routledge.\n[7] Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The role of explanations on trust and reliance in\nclinical decision support systems. In 2015 International Conference on Healthcare Informatics. IEEE, 160\u2013169.\n[8] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models\nfor healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "6 Limitations": ["68:22\nSungsoo Ray Hong et al.\n6\nLIMITATIONS\nWhile we devoted considerable effort to interviewing participants from a broad range of domains,\nour analysis misses other important domains, such as governance (e.g., predictive policing), agri-\nculture (e.g., crop monitoring, animal health monitoring), and more. We focused on cases where\nmodel predictions are consumed by a human. We did not cover application areas where a model\u2019s\npredictions are used in fully automated and/or semi-automated settings (e.g., autonomous driv-\ning, or warehouse robots). Finally, many of our participants were data scientists with a technical\nbackground, therefore this work reflects their specific point of view, as interpreted by the authors\nthrough systematic qualitative analysis. Although we identified key stakeholder roles in model\ninterpretability, we did not directly hear from every role that we identified as relevant to model\ninterpretability. Future work might pursue ethnographic and other observational approaches to\ncorroborate these results.\n7\nCONCLUSION\nIn this work, we empirically investigated industry practitioners\u2019 model interpretability practices,\nchallenges, and needs. We describe roles, processes, goals, and strategies around interpretability\npracticed by ML practitioners working in a variety of domains. We draw out important contrasts\nbetween our results and the predominant framing of interpretability in existing research as a\nproperty of a model that can be defined solely in light of an individual versus a ML model\u2019s\nalignment. Our results lead us to characterize interpretability as inherently social and negotiated,\naimed at fostering trust both in people and in models, context-dependent, and arising from careful\ncomparisons of human mental models. Our identification of unaddressed technological needs\namong practitioners is intended to help researchers direct their attention to unaddressed, yet\nprevalent, challenges in industry.\n8\nACKNOWLEDGEMENT\nWe wish to express our deepest gratitude to our participants who were willing to share their\nexperience and insights about their model interpretability practice and challenges. We also thank\nfor DARPA Data-Driven Discovery of Models (D3M) Program for generously supporting this\nresearch. Any opinions, findings, and conclusions or recommendations expressed in this material\nare those of the authors and do not necessarily reflect the views of DARPA.\nREFERENCES\n[1] Josh M Attenberg, Pagagiotis G Ipeirotis, and Foster Provost. 2011. Beat the machine: Challenging workers to find the\nunknown unknowns. In Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence.\n[2] Aparna Balagopalan, Jekaterina Novikova, Frank Rudzicz, and Marzyeh Ghassemi. 2018. The Effect of Heterogeneous\nData for Alzheimer\u2019s Disease Detection from Speech. arXiv preprint arXiv:1811.12254 (2018).\n[3] Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in human-ai\nteams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference\non Artificial Intelligence, Vol. 33. 2429\u20132437.\n[4] Adrien Bibal and Beno\u00eet Frenay. 2016. Interpretability of Machine Learning Models and Representations: an Introduction.\nIn 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. 77\u201382.\n[5] Ann Bostrom, Baruch Fischhoff, and M Granger Morgan. 1992. Characterizing mental models of hazardous processes:\nA methodology and an application to radon. Journal of social issues 48, 4 (1992), 85\u2013100.\n[6] Leo Breiman. 2017. Classification and Regression Trees. Routledge.\n[7] Adrian Bussone, Simone Stumpf, and Dympna O\u2019Sullivan. 2015. The role of explanations on trust and reliance in\nclinical decision support systems. In 2015 International Conference on Healthcare Informatics. IEEE, 160\u2013169.\n[8] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models\nfor healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "5.4 Post-Deployment Support": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:21\nOne possibly fruitful research direction is research which focuses on hiding the complexity\nbehind building an ML model and making model-building more accessible to non-specialists [27, 71].\nAnother direction may be building more intelligible visualizations that can effectively present the\nbehavior of a model to the general public. For instance, presenting uncertainty in model predictions\nusing more intuitive visual abstractions [28, 33] may be beneficial.\n5.3\nScalable and Integratable Interpretability Tools\nMany participants shared their concerns with integrating interpretability tools in their complex\nworkflows and organizational infrastructure. Problems commonly cited are that: methods are not\navailable because they were developed in academic settings, where creating robust and dependable\nsoftware is not the norm; tools were not easy to integrate in existing platforms because they cannot\nbe adapted to their specific environment; or simply that tools do not scale to the sheer size of their\ndata sets. We introduce two emerging interpretability-related use cases: model comparison, and\ndata preparation.\nParticipants pointed out that existing model interpretability approaches seem to overlook im-\nportant, but perhaps less principled, aspects of approaches to comparing the outputs of multiple\nmodels to build a deeper understanding. We identified several different cases for model comparison:\nwhen comparing different parameters, selection of features, timestamps, and more. Though model\ncomparison emerged as a ubiquitous task in model development, according to our results, existing\ninterpretability tools have very limited support (if any) for model comparison (a notable exception\nbeing the Manifold tool [76]).\nInterpretability tools are also needed in the ideation stage of the model lifecycle. Better tools\nare needed before training to assist with \u201cdata debugging\u201d, to detect issues with data before they\nare used for training, as well as for feature engineering and model design. Several participants\nsuggested that there should be a better way for a model designer to test hypotheses about what\neffect different decisions on feature sets may have on model performance without necessarily\nbuilding all these models entirely.\n5.4\nPost-Deployment Support\nMultiple participants described their efforts to, and struggles with, connecting not only decision\noutcomes but the value of those outcomes to interpretability work. For example, for P16 \u201cto have a\nframework for people to understand how the explanations either drive the decision making, improve\ndecision making, or otherwise ... I have this model, but did it actually solve the business problem?\u201d\nIn the post-training phase, better tools are needed to monitor models once they have been\ndeployed. P16 also described how \u201cThe next step is when you\u2019re ... say you\u2019ve communicated and\neverybody\u2019s happy with the model and you\u2019re ready to deploy it. Once you go out to meet with the\nengineers who are running the software stack, the first thing they\u2019re going to ask you is \u2018How do I test\nand debug this? How do I continuously evaluate whether it\u2019s working?\u2019 And that\u2019s where your tools\nreally really fall short.\u201d\nData visualization tools are one way to facilitate monitoring model behavior. Automated anomaly\ndetection may be useful in addition, to identify and surface potentially troubling behaviors, given an\nappropriate specification of critical expectations that should hold such as one learned from domain\nexperts. Multiple participants identified a need for better tools to support \u201croot cause analysis\u201d:\nonce issues with the model have been detected, it is crucial to have tools that can quickly point\nusers to a potential source of the problem. As several participants alluded to, this is particularly\nhard in production because problems may stem from different modules of complex data architecture\nand not only from the model itself.\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "5.3 Scalable and Integratable Interpretability Tools": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:21\nOne possibly fruitful research direction is research which focuses on hiding the complexity\nbehind building an ML model and making model-building more accessible to non-specialists [27, 71].\nAnother direction may be building more intelligible visualizations that can effectively present the\nbehavior of a model to the general public. For instance, presenting uncertainty in model predictions\nusing more intuitive visual abstractions [28, 33] may be beneficial.\n5.3\nScalable and Integratable Interpretability Tools\nMany participants shared their concerns with integrating interpretability tools in their complex\nworkflows and organizational infrastructure. Problems commonly cited are that: methods are not\navailable because they were developed in academic settings, where creating robust and dependable\nsoftware is not the norm; tools were not easy to integrate in existing platforms because they cannot\nbe adapted to their specific environment; or simply that tools do not scale to the sheer size of their\ndata sets. We introduce two emerging interpretability-related use cases: model comparison, and\ndata preparation.\nParticipants pointed out that existing model interpretability approaches seem to overlook im-\nportant, but perhaps less principled, aspects of approaches to comparing the outputs of multiple\nmodels to build a deeper understanding. We identified several different cases for model comparison:\nwhen comparing different parameters, selection of features, timestamps, and more. Though model\ncomparison emerged as a ubiquitous task in model development, according to our results, existing\ninterpretability tools have very limited support (if any) for model comparison (a notable exception\nbeing the Manifold tool [76]).\nInterpretability tools are also needed in the ideation stage of the model lifecycle. Better tools\nare needed before training to assist with \u201cdata debugging\u201d, to detect issues with data before they\nare used for training, as well as for feature engineering and model design. Several participants\nsuggested that there should be a better way for a model designer to test hypotheses about what\neffect different decisions on feature sets may have on model performance without necessarily\nbuilding all these models entirely.\n5.4\nPost-Deployment Support\nMultiple participants described their efforts to, and struggles with, connecting not only decision\noutcomes but the value of those outcomes to interpretability work. For example, for P16 \u201cto have a\nframework for people to understand how the explanations either drive the decision making, improve\ndecision making, or otherwise ... I have this model, but did it actually solve the business problem?\u201d\nIn the post-training phase, better tools are needed to monitor models once they have been\ndeployed. P16 also described how \u201cThe next step is when you\u2019re ... say you\u2019ve communicated and\neverybody\u2019s happy with the model and you\u2019re ready to deploy it. Once you go out to meet with the\nengineers who are running the software stack, the first thing they\u2019re going to ask you is \u2018How do I test\nand debug this? How do I continuously evaluate whether it\u2019s working?\u2019 And that\u2019s where your tools\nreally really fall short.\u201d\nData visualization tools are one way to facilitate monitoring model behavior. Automated anomaly\ndetection may be useful in addition, to identify and surface potentially troubling behaviors, given an\nappropriate specification of critical expectations that should hold such as one learned from domain\nexperts. Multiple participants identified a need for better tools to support \u201croot cause analysis\u201d:\nonce issues with the model have been detected, it is crucial to have tools that can quickly point\nusers to a potential source of the problem. As several participants alluded to, this is particularly\nhard in production because problems may stem from different modules of complex data architecture\nand not only from the model itself.\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "5.2 Communicating and Summarizing Model Behavior": ["68:20\nSungsoo Ray Hong et al.\nproviding answers to users\u2019 queries based on conditional probabilities may be another promising\napproach [47, 65].\nMultiple participants described the discussions that they or their team members repeatedly\nengaged in with other stakeholders to identify such cases as difficult yet crucial. Our finding that\ndata scientists often engage in such collaborative work aligns with recent findings [75]. Aligning\nschedules to make meetings happen was not always possible, leading at least one participant\nto feel regretful about missing the value that more extensive discussions with end-users could\nprovide. Having access to other stakeholders\u2019 responses to model results was implied to be valuable\nduring the stage of model development and verification. For example, P13 described how, \u201cIf I can\nproactively test or verify the interpreted results, it would be super helpful. So, now the models can give\nme some results. I can just look at the result and make the decision for myself. But I still have to define\nanother set of rules, actions to verify if it actually works. If there\u2019s a way for us to easily, interactively,\nverify ...\u201d One approach that may be fruitful is to develop interactive applications that an end-user\nor domain expert can use to provide their (asynchronous) feedback more efficiently. While some\nresearch has demonstrated the efficacy of using large scale crowdsourcing platforms to identify\nerrors made by predictive models [1] or getting collective answers from a group of people [10],\nwe are not aware of research aimed at developing similar approaches that are better suited to\nsmaller numbers of potentially expert users. Interactive approaches could provide both domain\nexperts and ML experts an ability to engage asynchronously where needed, reserving valuable\nin-person discussion time for interrogating these instances or sharing knowledge about decision\nprocesses. Though many participants described challenges to gaining feedback, none mentioned\nhaving developed in-house tools to support this common task, nor having obtained tools from a\nthird party.\n5.2\nCommunicating and Summarizing Model Behavior\nWe observed several ways in which the communication that comprised interpretability work,\nbetween stakeholders within an organization or occurring as a form of \u201cdialogue\u201d between a human\nand a model, can often result in misinterpretation. Several participants remarked about the need for\nmore accessible interpretability solutions. P20 said \u201dI want interpretability for everyone,\u201d referring\nto solutions that would be appropriate for both domain experts and practitioners in the healthcare\nsetting in which he worked. Similarly, P17 desired methods that came closer to \u201cpresenting the\npredictions in a colloquial way\u201d to facilitate easier interpretation on the part of users.\nThe cooperative and social nature of much of the interpretability work our participants described\nalso suggests that transferring knowledge in the form of model \u201chand-offs\u201d happens frequently.\nIn these situations, having representations, such as visualizations, that can concisely capture how\nthose receiving a model might expect it to behave in relation to their mental models could be\nuseful to provide the types of model \u201cbug\u201d discoveries that some participants described occurring\nunexpectedly after deployment. These representations could be developed along with the algorithms\nmentioned above for efficiently identifying important test or edge cases for summarizing a model\u2019s\ninterpretability status.\nA related concern voiced by some participants involved finding ways to improve existing in-\nterpretability approaches to provide more direct and customized information on why a model\ndisagreed with a human mental model. For P12, tools that could help more quickly resolve discrep-\nancies between model behavior and representations of her own or her users\u2019 expectations in the\nform of shape constraints seemed important: \u201cif your hypothesis is wrong and it really hurts the\naccuracy to make that constraint, then you have to ask, what\u2019s going on in there. I feel like we\ndon\u2019t have very good debug tools necessarily to figure out why is the model not monotonic when I\nthink it should be.\u201d\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "5.1 Identifying, Representing, and Integrating Human Expectations": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:19\nform of explanation to go with it, that\u2019s our standard immediately\u201d: \u201cif you are a nurse, if you are a\ndoctor, you should always challenge every result that people gave you. Challenge it.\u201d\nSome participants also implied that the impacts of interpretability work were intimately related\nto decision-making infrastructures that could be complex. P12 described: \u201cYou have to figure out\nis that revenue gain coming in a way that\u2019s really truly beneficial for all of the stakeholders. And\nso there\u2019s a lot of analysis that will happen there and a lot of decision-makers and just I don\u2019t know,\nlayers of decision-makers. These systems often are pretty interconnected and might also destabilize\nother things. So sometimes you might build a model and then maybe some other people are using it in\nsome new way and you need to make sure you\u2019re not breaking anything from their point of view.\u201d\n5\nDESIGN OPPORTUNITIES FOR INTERPRETABILITY CHALLENGES\nOur interviews surfaced a number of challenges faced by ML experts working toward interpretable\nmodels in industry. While an exhaustive listing of opportunities for technology to better support\nthis work is beyond the scope of this work, we present a select set of challenges that we distilled\nfrom the issues raised by our participants. These challenges were chosen in part because they\nrepresent areas where existing research has not focused much attention, but where progress may\nhave a strong impact on ML/HCI research and practice.\n5.1\nIdentifying, Representing, and Integrating Human Expectations\nThe prominence of attempts to identify and handle edge cases in the work of our participants sug-\ngests that more systematic approaches to identifying these important \u201canchors\u201d for interpretability\nwork would increase interpretability-related task productivity. The edge case identification could\nhappen in two ways, between a model and an individual, and a model and a group of people.\nOne opportunity lies in eliciting and representing an individual\u2019s expectations in ways that are\ncompatible with the model representation that ML experts are accustomed to, even if through\nproxies. During the interview, P12 discussed the importance of regulating the model\u2019s outcome when\nthe shape does not align with her \u201ccommon-sense\u201d. She described discovering shape constraints\nas a powerful representation that her team can use to easily regulate a model\u2019s behavior when\nthe model doesn\u2019t align with the common-sense level of human expectations. However, which\n\u201cside\u201d, the human or the model, is correct in the case of a mismatch may not always be obvious. P2\nintroduced his term \u201csuperstition\u201d which refers to an inaccurate assumption that people can have:\nwe knew what I called the superstition of the client that X was always true and this was important; so\nfind out those internal expectations they had, most of which will be true and some of which will be\nfolk wisdom that isn\u2019t factual. So it\u2019s [ML model] going to take those values that they\u2019re not expecting\nto look at.\nResolving such tension between a human and a model is an active research topic in the CSCW\nand CHI communities [77]. P12 mentioned tools that could help more quickly resolve discrepancies\nbetween a model\u2019s prediction and her expectations: \u201cif your hypothesis is wrong and it really hurts\nthe accuracy to make that constraint, then you have to ask, what\u2019s going on in there. I feel like we don\u2019t\nhave very good debug tools necessarily to figure out why is the model not monotonic when I think it\nshould be.\u201d We propose that interpretability research would benefit from interfaces and mechanisms\nto help a human (1) articulate their expectations around predictions in model interpretation tasks\n(2) efficiently recognize the gaps between their expectations and model representations, (3) gain\ncontext through further evidence available to the model that may help them assess the gap, and (4)\nenable them to debug/change model behavior based on their findings. Looking to research in belief\nelicitation and mental model discovery within psychology, economics, and related disciplines in\ndesigning such approaches may be informative (e.g., [5, 17, 35, 36, 58]). Bayesian approaches to\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "5 Design Opportunities for Interpretability Challenges": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:19\nform of explanation to go with it, that\u2019s our standard immediately\u201d: \u201cif you are a nurse, if you are a\ndoctor, you should always challenge every result that people gave you. Challenge it.\u201d\nSome participants also implied that the impacts of interpretability work were intimately related\nto decision-making infrastructures that could be complex. P12 described: \u201cYou have to figure out\nis that revenue gain coming in a way that\u2019s really truly beneficial for all of the stakeholders. And\nso there\u2019s a lot of analysis that will happen there and a lot of decision-makers and just I don\u2019t know,\nlayers of decision-makers. These systems often are pretty interconnected and might also destabilize\nother things. So sometimes you might build a model and then maybe some other people are using it in\nsome new way and you need to make sure you\u2019re not breaking anything from their point of view.\u201d\n5\nDESIGN OPPORTUNITIES FOR INTERPRETABILITY CHALLENGES\nOur interviews surfaced a number of challenges faced by ML experts working toward interpretable\nmodels in industry. While an exhaustive listing of opportunities for technology to better support\nthis work is beyond the scope of this work, we present a select set of challenges that we distilled\nfrom the issues raised by our participants. These challenges were chosen in part because they\nrepresent areas where existing research has not focused much attention, but where progress may\nhave a strong impact on ML/HCI research and practice.\n5.1\nIdentifying, Representing, and Integrating Human Expectations\nThe prominence of attempts to identify and handle edge cases in the work of our participants sug-\ngests that more systematic approaches to identifying these important \u201canchors\u201d for interpretability\nwork would increase interpretability-related task productivity. The edge case identification could\nhappen in two ways, between a model and an individual, and a model and a group of people.\nOne opportunity lies in eliciting and representing an individual\u2019s expectations in ways that are\ncompatible with the model representation that ML experts are accustomed to, even if through\nproxies. During the interview, P12 discussed the importance of regulating the model\u2019s outcome when\nthe shape does not align with her \u201ccommon-sense\u201d. She described discovering shape constraints\nas a powerful representation that her team can use to easily regulate a model\u2019s behavior when\nthe model doesn\u2019t align with the common-sense level of human expectations. However, which\n\u201cside\u201d, the human or the model, is correct in the case of a mismatch may not always be obvious. P2\nintroduced his term \u201csuperstition\u201d which refers to an inaccurate assumption that people can have:\nwe knew what I called the superstition of the client that X was always true and this was important; so\nfind out those internal expectations they had, most of which will be true and some of which will be\nfolk wisdom that isn\u2019t factual. So it\u2019s [ML model] going to take those values that they\u2019re not expecting\nto look at.\nResolving such tension between a human and a model is an active research topic in the CSCW\nand CHI communities [77]. P12 mentioned tools that could help more quickly resolve discrepancies\nbetween a model\u2019s prediction and her expectations: \u201cif your hypothesis is wrong and it really hurts\nthe accuracy to make that constraint, then you have to ask, what\u2019s going on in there. I feel like we don\u2019t\nhave very good debug tools necessarily to figure out why is the model not monotonic when I think it\nshould be.\u201d We propose that interpretability research would benefit from interfaces and mechanisms\nto help a human (1) articulate their expectations around predictions in model interpretation tasks\n(2) efficiently recognize the gaps between their expectations and model representations, (3) gain\ncontext through further evidence available to the model that may help them assess the gap, and (4)\nenable them to debug/change model behavior based on their findings. Looking to research in belief\nelicitation and mental model discovery within psychology, economics, and related disciplines in\ndesigning such approaches may be informative (e.g., [5, 17, 35, 36, 58]). Bayesian approaches to\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "4.2 Themes: Characterizing Interpretability Work": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:15\na perspective of what makes software that we\u2019re using right now reliable, elegant, available, and\nscalable.\u201d\nBeing able to concisely and effectively explain how a model works and why it should be trusted\nis not easy, especially because some of the stakeholders may need to understand this at a high level\nof abstraction and with little technical detail. Several participants mentioned that visualization\nmethods can play a major role in the communication stage of interpretability.\nThe high priority on building trust as a modeling goal for model developers means that they\nperceive the need to build transparent models or to use model extraction methods to explain to\nothers how a given model works. Many of our participants described this need as one of the major\n\u201cpain points\u201d of model interpretability. P11, one of our participants working in banking, mentioned\nthat many colleagues in his organization are still \u201cscared\u201d to adopt black-box models for this reason.\n4.2\nThemes: Characterizing Interpretability Work\nResearch on interpretability aimed at producing new tools or insights has primarily focused on the\ninteraction between an individual and a model. Interpretability is described as a property of the\nmodel class in general based on its relation to human expectations (e.g., [40]) or constraints like\nmonotonicity [22] or additivity [8], or human-driven constraints that come from domain knowledge\nor the presentation of a specific model\u2019s results that enables a human to understand why a label\nwas applied to an instance (e.g., LIME [64] or SHAP [45]).\nWhile many of our participants described understandings of interpretability that corroborate its\nimportance to building an individual\u2019s trust in a model and its role in processes like model debugging,\nseveral prominent themes in our results stand in contrast to prior framings of interpretability. We\nsummarize four themes that emerged from our results which extend and may problematize existing\ncharacterizations.\n4.2.1\nInterpretability is Cooperative. A theme that recurred again and again in participants\u2019 de-\nscriptions of how they conceived of and practiced interpretability in their organization was one of\ncollaboration and coordinating of values and knowledge between stakeholder roles. This theme\nwas firstly exemplified by mentions of discussion with other stakeholder groups, such as domain\nexperts, by more than half of our participants, who described these discussions occurring frequently\nduring ideation and model building and validation, but also after deployment. Many participants\nexplicitly remarked on how collaboration around interpretability was important for improving their\nbusiness reasoning and convincing them that the models they built brought value. As P9 remarked:\n\u201cincluding domain experts at the very beginning of the process is absolutely vital from an efficiency\nstandpoint. And ultimately from the standpoint of building a good data product.\u201d. P22 described how\ninducing different roles\u2019 perspectives can produce clear modeling implications for their team which\nwould be difficult to arrive at in other ways: \u201cAnd sometimes we realize that certain features are\nmaybe not properly coded. Sometimes certain features are overwhelmingly biasing ... sometimes it\u2019s a\nnew discovery like, oh, we never thought of that.\u201d\nInterpretability\u2019s role in building and maintaining trust between people in an organization, as\nmentioned by many of our participants, was another clear way in which its social nature was\nevident. Trust and building of common ground were perceived by some participants as resulting\nfrom communication affordances of interpretability: \u201cWe can easily use the interpretability using the\nvisualization to convey this information to data scientists or even to the operation people. And they\ncan fully use this information to convey the message to the rest of the operation. Interpretability is\nabout communication between people, not necessarily just about person and model\u201d (P13). Similarly,\nP2 described how \u201cat the end of the day, people respond to use cases and anecdotes way better than\nthey respond to math, so I find that the proof and the need for interpretability is something I kind of\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "4.1 Interpretability Roles, Stages, Goals": ["68:8\nSungsoo Ray Hong et al.\nplace and \u2018When\u2019 in planning, building and deploying, and managing ML models, and (3) Goals de-\nscribing \u2018Why\u2019 interpretability is perceived as being sought. We then describe themes that emerged\nfrom the intersection of these anchors which extend and problematize prior characterizations of\ninterpretability.\n4.1\nInterpretability Roles, Stages, Goals\n4.1.1\nWho?: Interpretability Roles. We organize the stakeholder roles described by our participants\naround three main categories: Model Builders, Model Breakers, and Model Consumers.\nR1. Model Builders: Model builders are individuals responsible for designing, developing and\ntesting models as well as for integrating them in the data infrastructure of the organization. The\nmost common job title of professionals found in this category are Data Scientists (or often called\nML Engineers) and Data Engineers, with the former typically responsible for model development\nand validation and the latter responsible for integrating the model in a larger data infrastructure.\nR2. Model Breakers: Model breakers are people who have the domain knowledge to verify\nthat models meet desired goals and behave as expected, but may not necessarily have professional\nlevel of knowledge about ML. They work with model builders to give them feedback about things\nthat need to be improved in building their model. Model breakers contribute different perspectives\nto help identify and fix potential issues with the model, hence the name \u201cmodel breakers\u201d. Within\nthe category of model breakers we identified three main sub-classes of breakers. Domain Experts,\nwho are individuals with a deep knowledge of the real-world phenomena the model is supposed to\ncapture (e.g., physicians in the development of a healthcare product, business analysts working for\nbuilding credit evaluation models). Product managers, who are responsible for the development of\nthe actual product the organization needs. They communicate requirements and business goals\nand verify that they are met. Auditors, who examine models from the legal perspective. Their goal\nis to make sure the model satisfies legal requirements from the compliance standpoint.\nR3. Model Consumers: Model consumers are the intended end-users of the information and\ndecisions produced by the models. This group includes a large variety of professionals who aim\nat making high-stakes decisions supported by ML models. Examples include physicians working\nwith mortality and readmission models; bank representatives who use loan approval models for\nhandling their customer\u2019s loan request and explaining outcomes, engineers predicting failures with\nexpensive and mission critical machinery, and biologists testing potentially adverse effects of new\ndrugs. Many of the professionals we interviewed explained that often the actual end-users of a\nmodel are other individuals employed in the same company they work for.\nOne important aspect to keep in mind regarding these roles is that they do not necessarily define\na single individual. At any given time, the same individual can cover more than one of the roles we\nhave outlined above.\n4.1.2\nWhat? When?: Interpretability Stages. Our participants described in detail how interpretability\nplays a role at many different stages of the model and product development process.\nS1. Ideation and Conceptualization Stage: Before developing an actual model, model de-\nvelopers spend considerable time exploring the problem space and conceptualizing alternative\nstrategies for model building. We found that interpretability plays a major role already at this stage\nof the process, even before an actual model is developed. For example, P11, referring to feature\nengineering, remarked: \u201c... this is the first step toward making interpretable models, even though\nwe don\u2019t have any model yet.\u201d. In particular, we found several data scientists complement feature\nengineering considerations pertaining exclusively to model performance with considerations on\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "4 Results": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:7\nidentified himself as a UX researcher who has been collaborating with ML engineers to build models\nused in various product applications, such as personal photograph ranking, face recognition, and\nvoice transcription. All of them worked in teams of data scientists and engineers who need to\ncommunicate with product managers, customers, and other stakeholders.\n3.2\nInterview Guide\nWe used a semi-structured interview approach. To prepare for the interviews, we developed a\nslide deck with a script and a set of questions organized around the following main themes: (1)\nparticipant\u2019s background and role; (2) type of projects developed in the organization; (3) experience\nwith interpretability in their work (when, why, how interpretability plays a role, and who else it\ninvolves); (4) challenges commonly faced in the organization and desirable future developments.\nWe shared the slide deck with the participant and used it to direct the conversation during the call.\nWe provide the interview protocols in the supplementary material.\nBefore starting an interview, we obtained permission from our interviewee to audio-record the\ninterviews. We also noted that we would share the outcomes of the study with the participant in\nthe event they wished to provide further comments. In total, we collected 19 hours and 10 minutes\nof audio recorded interviews from 22 interview sessions. Each interview lasted an average of 52\nminutes, normally lasting just a few minutes longer or shorter.\n3.3\nQualitative Coding Process\nAll the audio recordings were transcribed by professional transcribers. The first and third authors\nworked together to perform an initial qualitative analysis, with the second author corroborating\nresults and contributing further insights from an independent pass after the initial round of analysis.\nThe analyses we present represent the perspectives of our participants systematically curated in an\ninterpretive framework which was developed through discussions among the authors.\nWe used an iterative qualitative coding process [66] characterized by alternate phases of coding\n(to tag specific text segments with codes), analytic memo writing (to collect ideas and insights)\nand diagramming (to build themes and categories from the codes). More specifically, we moved\nthrough the following steps. In the beginning, the first and third authors each created, using a\nsubset of the interviews, initial sets of codes and memos (i.e., open coding, pre-coding [41]). Then,\nwe shared our work and compared our codes to find commonalities and discrepancies and to\nagree on a common structure. At this stage, a small set of anchoring concepts emerged from open\ncoding - roles, processes, goals, and strategies - which were influenced by our goal of understanding\nhow interpretability manifests in real workplaces. We also explicitly noted descriptions of where\nroles interacted around interpretability, conceptual definitions and tensions perceived around\ninterpretability, and challenges relating to achieving interpretability in the organization. Equipped\nwith this knowledge, we went through the second round of coding, this time including the rest\nof the interviews and keeping track of insightful quotes. At this stage, we met again to share our\nwork and to come up with a newly refined common structure emerging from our coding activity,\nwith the remaining author joining in the discussion after an independent code pass. Finally, we\nreviewed together all our coded text snippets and memos to tweak and enrich our structure with\nrelevant details. The final structure is reflected in the organization of this paper and comprises:\nroles, processes, goals, and a set of cross-cutting interpretability themes.\n4\nRESULTS\nWe organize our results around the anchoring concepts we identified in our qualitative coding:\n(1) Interpretability Roles, describing \u2018Who\u2019 is involved, (2) Stages, describing \u2018What\u2019 activities take\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "3.3 Qualitative Coding Process": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:7\nidentified himself as a UX researcher who has been collaborating with ML engineers to build models\nused in various product applications, such as personal photograph ranking, face recognition, and\nvoice transcription. All of them worked in teams of data scientists and engineers who need to\ncommunicate with product managers, customers, and other stakeholders.\n3.2\nInterview Guide\nWe used a semi-structured interview approach. To prepare for the interviews, we developed a\nslide deck with a script and a set of questions organized around the following main themes: (1)\nparticipant\u2019s background and role; (2) type of projects developed in the organization; (3) experience\nwith interpretability in their work (when, why, how interpretability plays a role, and who else it\ninvolves); (4) challenges commonly faced in the organization and desirable future developments.\nWe shared the slide deck with the participant and used it to direct the conversation during the call.\nWe provide the interview protocols in the supplementary material.\nBefore starting an interview, we obtained permission from our interviewee to audio-record the\ninterviews. We also noted that we would share the outcomes of the study with the participant in\nthe event they wished to provide further comments. In total, we collected 19 hours and 10 minutes\nof audio recorded interviews from 22 interview sessions. Each interview lasted an average of 52\nminutes, normally lasting just a few minutes longer or shorter.\n3.3\nQualitative Coding Process\nAll the audio recordings were transcribed by professional transcribers. The first and third authors\nworked together to perform an initial qualitative analysis, with the second author corroborating\nresults and contributing further insights from an independent pass after the initial round of analysis.\nThe analyses we present represent the perspectives of our participants systematically curated in an\ninterpretive framework which was developed through discussions among the authors.\nWe used an iterative qualitative coding process [66] characterized by alternate phases of coding\n(to tag specific text segments with codes), analytic memo writing (to collect ideas and insights)\nand diagramming (to build themes and categories from the codes). More specifically, we moved\nthrough the following steps. In the beginning, the first and third authors each created, using a\nsubset of the interviews, initial sets of codes and memos (i.e., open coding, pre-coding [41]). Then,\nwe shared our work and compared our codes to find commonalities and discrepancies and to\nagree on a common structure. At this stage, a small set of anchoring concepts emerged from open\ncoding - roles, processes, goals, and strategies - which were influenced by our goal of understanding\nhow interpretability manifests in real workplaces. We also explicitly noted descriptions of where\nroles interacted around interpretability, conceptual definitions and tensions perceived around\ninterpretability, and challenges relating to achieving interpretability in the organization. Equipped\nwith this knowledge, we went through the second round of coding, this time including the rest\nof the interviews and keeping track of insightful quotes. At this stage, we met again to share our\nwork and to come up with a newly refined common structure emerging from our coding activity,\nwith the remaining author joining in the discussion after an independent code pass. Finally, we\nreviewed together all our coded text snippets and memos to tweak and enrich our structure with\nrelevant details. The final structure is reflected in the organization of this paper and comprises:\nroles, processes, goals, and a set of cross-cutting interpretability themes.\n4\nRESULTS\nWe organize our results around the anchoring concepts we identified in our qualitative coding:\n(1) Interpretability Roles, describing \u2018Who\u2019 is involved, (2) Stages, describing \u2018What\u2019 activities take\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "3.2 Interview Guide": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:7\nidentified himself as a UX researcher who has been collaborating with ML engineers to build models\nused in various product applications, such as personal photograph ranking, face recognition, and\nvoice transcription. All of them worked in teams of data scientists and engineers who need to\ncommunicate with product managers, customers, and other stakeholders.\n3.2\nInterview Guide\nWe used a semi-structured interview approach. To prepare for the interviews, we developed a\nslide deck with a script and a set of questions organized around the following main themes: (1)\nparticipant\u2019s background and role; (2) type of projects developed in the organization; (3) experience\nwith interpretability in their work (when, why, how interpretability plays a role, and who else it\ninvolves); (4) challenges commonly faced in the organization and desirable future developments.\nWe shared the slide deck with the participant and used it to direct the conversation during the call.\nWe provide the interview protocols in the supplementary material.\nBefore starting an interview, we obtained permission from our interviewee to audio-record the\ninterviews. We also noted that we would share the outcomes of the study with the participant in\nthe event they wished to provide further comments. In total, we collected 19 hours and 10 minutes\nof audio recorded interviews from 22 interview sessions. Each interview lasted an average of 52\nminutes, normally lasting just a few minutes longer or shorter.\n3.3\nQualitative Coding Process\nAll the audio recordings were transcribed by professional transcribers. The first and third authors\nworked together to perform an initial qualitative analysis, with the second author corroborating\nresults and contributing further insights from an independent pass after the initial round of analysis.\nThe analyses we present represent the perspectives of our participants systematically curated in an\ninterpretive framework which was developed through discussions among the authors.\nWe used an iterative qualitative coding process [66] characterized by alternate phases of coding\n(to tag specific text segments with codes), analytic memo writing (to collect ideas and insights)\nand diagramming (to build themes and categories from the codes). More specifically, we moved\nthrough the following steps. In the beginning, the first and third authors each created, using a\nsubset of the interviews, initial sets of codes and memos (i.e., open coding, pre-coding [41]). Then,\nwe shared our work and compared our codes to find commonalities and discrepancies and to\nagree on a common structure. At this stage, a small set of anchoring concepts emerged from open\ncoding - roles, processes, goals, and strategies - which were influenced by our goal of understanding\nhow interpretability manifests in real workplaces. We also explicitly noted descriptions of where\nroles interacted around interpretability, conceptual definitions and tensions perceived around\ninterpretability, and challenges relating to achieving interpretability in the organization. Equipped\nwith this knowledge, we went through the second round of coding, this time including the rest\nof the interviews and keeping track of insightful quotes. At this stage, we met again to share our\nwork and to come up with a newly refined common structure emerging from our coding activity,\nwith the remaining author joining in the discussion after an independent code pass. Finally, we\nreviewed together all our coded text snippets and memos to tweak and enrich our structure with\nrelevant details. The final structure is reflected in the organization of this paper and comprises:\nroles, processes, goals, and a set of cross-cutting interpretability themes.\n4\nRESULTS\nWe organize our results around the anchoring concepts we identified in our qualitative coding:\n(1) Interpretability Roles, describing \u2018Who\u2019 is involved, (2) Stages, describing \u2018What\u2019 activities take\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "3.1 Recruiting Participants": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:5\nacross contexts in the real world is necessary for developing a science of interpretability [32]. For\ninstance, Miller argues that the majority of studies discuss model interpretability based on the unit\nof an individual, but few studies explain how humans describe how models work to one another [51].\nWhile some recent work presents insights regarding how laypeople use automated tools in building\nML models [71, 74] or ML experts use specific data science packages in determining insights related\nto model interpretability [32], little research to date focuses on investigating holistic model inter-\npretability processes or communication strategies among different roles in organizations. Inspired\nby studies that emphasize workplace settings [44, 75], our work characterizes interpretability\nvia how data scientists and others involved in this work perceive the different stakeholders and\nstrategies that characterize interpretability work in organizations using ML. In doing so, our work\nproblematizes attempts to define interpretability through measurable, fixed aspects of a single\nhuman\u2019s interactions with a model by uncovering how those engaged with interpretability portray\nit as an ongoing result of negotiations between people within an organization.\nOur ultimate goal is to help the scientific community ground their work on practical needs by\ndescribing current practices and gaps that need to be bridged. Our work is therefore in line with\nsimilar attempts at providing a window to the world of practitioners in ML and Data Science, such\nas Holstein et al.\u2019s interview study on ML fairness [26]; Yang et al.\u2019s, survey on how laypeople\nuse ML [74]; and Kandel et al.\u2019s interview study on how enterprise analysts use data analysis and\nvisualization tools [31].\n3\nRESEARCH METHOD\nThe goals of our research are to (1) characterize how ML experts conceive of interpretability and\nhow interpretability manifests in their organization, and (2) identify aspects of interpretability\npractices that are not sufficiently supported by existing technology. To achieve these goals, we\nconducted open-ended conversational interviews with professional ML experts. While ethnographic\nmethods like participant observation are also likely to support these goals, based on the lack of\nprior work focusing on interpretability work in real organizational settings, we opted for interviews\nas they allowed us to reach a broader set of people working with ML in industry, helping ensure\nour results were general enough to apply across multiple domains. We describe the methodology\nwe followed to conduct the interviews and to analyze the data we collected.\n3.1\nRecruiting Participants\nTo recruit practitioners, we used convenience and snowball sampling strategies [12]. We first\ncommunicated with industry acquaintances who are building and using state-of-the-art ML so-\nlutions in their fields. We invited these contacts to participate in a voluntary interview if they\nhad had personal experience with model interpretability issues and solutions, and/or needed to\ncommunicate the results of their ML models to others. We also asked them to suggest practitioners\nfrom other companies and domains who may have relevant experience for our study. This snowball\nsampling helped ensure that we recruited participants representing a variety of application areas\nand enterprise settings. As we continued our interviews, we identified new directions in need of\nfurther investigation as our understanding of several initial topics became saturated.\nTable 1 shows details of the 22 individuals who participated in our interviews. We had 22 indi-\nviduals from 20 different companies (6 females and 16 males). Our participants span a wide variety\nof domains including banking, finance, healthcare, software companies, social media, transporta-\ntion, consulting, manufacturing, internet services, and transportation. Among our participants, 17\ndescribed themselves as data scientists or machine learning engineers, 2 identified themselves as\nsoftware engineers whose goal is to build infrastructure related to model interpretability (model\ndata pipelines, model interpretability tools), 2 identified themselves as product managers. One\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "3 Research Method": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:5\nacross contexts in the real world is necessary for developing a science of interpretability [32]. For\ninstance, Miller argues that the majority of studies discuss model interpretability based on the unit\nof an individual, but few studies explain how humans describe how models work to one another [51].\nWhile some recent work presents insights regarding how laypeople use automated tools in building\nML models [71, 74] or ML experts use specific data science packages in determining insights related\nto model interpretability [32], little research to date focuses on investigating holistic model inter-\npretability processes or communication strategies among different roles in organizations. Inspired\nby studies that emphasize workplace settings [44, 75], our work characterizes interpretability\nvia how data scientists and others involved in this work perceive the different stakeholders and\nstrategies that characterize interpretability work in organizations using ML. In doing so, our work\nproblematizes attempts to define interpretability through measurable, fixed aspects of a single\nhuman\u2019s interactions with a model by uncovering how those engaged with interpretability portray\nit as an ongoing result of negotiations between people within an organization.\nOur ultimate goal is to help the scientific community ground their work on practical needs by\ndescribing current practices and gaps that need to be bridged. Our work is therefore in line with\nsimilar attempts at providing a window to the world of practitioners in ML and Data Science, such\nas Holstein et al.\u2019s interview study on ML fairness [26]; Yang et al.\u2019s, survey on how laypeople\nuse ML [74]; and Kandel et al.\u2019s interview study on how enterprise analysts use data analysis and\nvisualization tools [31].\n3\nRESEARCH METHOD\nThe goals of our research are to (1) characterize how ML experts conceive of interpretability and\nhow interpretability manifests in their organization, and (2) identify aspects of interpretability\npractices that are not sufficiently supported by existing technology. To achieve these goals, we\nconducted open-ended conversational interviews with professional ML experts. While ethnographic\nmethods like participant observation are also likely to support these goals, based on the lack of\nprior work focusing on interpretability work in real organizational settings, we opted for interviews\nas they allowed us to reach a broader set of people working with ML in industry, helping ensure\nour results were general enough to apply across multiple domains. We describe the methodology\nwe followed to conduct the interviews and to analyze the data we collected.\n3.1\nRecruiting Participants\nTo recruit practitioners, we used convenience and snowball sampling strategies [12]. We first\ncommunicated with industry acquaintances who are building and using state-of-the-art ML so-\nlutions in their fields. We invited these contacts to participate in a voluntary interview if they\nhad had personal experience with model interpretability issues and solutions, and/or needed to\ncommunicate the results of their ML models to others. We also asked them to suggest practitioners\nfrom other companies and domains who may have relevant experience for our study. This snowball\nsampling helped ensure that we recruited participants representing a variety of application areas\nand enterprise settings. As we continued our interviews, we identified new directions in need of\nfurther investigation as our understanding of several initial topics became saturated.\nTable 1 shows details of the 22 individuals who participated in our interviews. We had 22 indi-\nviduals from 20 different companies (6 females and 16 males). Our participants span a wide variety\nof domains including banking, finance, healthcare, software companies, social media, transporta-\ntion, consulting, manufacturing, internet services, and transportation. Among our participants, 17\ndescribed themselves as data scientists or machine learning engineers, 2 identified themselves as\nsoftware engineers whose goal is to build infrastructure related to model interpretability (model\ndata pipelines, model interpretability tools), 2 identified themselves as product managers. One\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "2.2 Limitations and Criticism of Model Interpretability": ["68:4\nSungsoo Ray Hong et al.\nfrom the model [18] as well as methods that build whole surrogates that try to mimic the model\u2019s\nbehavior by building a more transparent equivalent [11].\nLocal interpretability is about providing explanations at a single instance level. Some approaches,\nsuch as LIME [64] and SHAP [45] focus on providing a set of \u201cweights\u201d aimed at providing insights\nabout how an instance is processed by the model. Anchors aims at providing instance explanations\nusing a rule structure [63]. Some methods provide explanations more in the form of \u201ccounterfactual\u201d,\nthat is, the minimal set of changes necessary to change the instance\u2019s predicted outcome [48].\nIn addition to model extraction techniques, researchers have developed interactive visualization\ninterfaces to help data scientists\u2019 build, interpret, and debug models [24]. For instance, EnsembleMa-\ntrix presents multiple confusion matrices to help combine multiple models [70]. Squares visualizes\nthe performance of multiple models in a single view, enabling fast model comparison [62]. Some\nother approaches present visual analytics systems specialized in a specific model type, such as\nregression models [13], neural networks [29, 69, 73], and surrogate modes [52].\n2.2\nLimitations and Criticism of Model Interpretability\nAs interpretability research grows, critiques have also grown more prominent. While the existing\nliterature emphasizes the importance of model interpretability as a tool that humans can leverage\nto align their mental model with ML models [39], some studies paint a less positive picture.\nFor example, Bussone et al., found users of clinical decision-support models tend to over-rely on a\nmodel recommendation over their own expertise [7]. Aligning with their findings, Kaur et al., found\nthat data scientists often \u201cmisuse\u201d interpretability tools and over-trust the results [32]. Narayanan\net al., investigate how explanation complexity (defined as explanation size, cognitive chunks, and\nthe number of repeated terms) impacts accuracy, efficiency and user satisfaction. Initial results show\nthat explanation complexity has a negative impact on efficiency and satisfaction but not necessarily\non accuracy [56]. Green and Chen\u2019s findings suggest that people\u2019s decisions when presented with\nmodel predictions may not achieve desiderata related to reliability and fairness [20]. Forough et al.,\ninvestigated how the number of input features and model transparency affect simulatability (the\nuser\u2019s ability to predict model behavior) and trust [61]. Their results suggest that simpler and more\ntransparent models don\u2019t necessarily help people build better mental models and that transparency\nmay even hinder people\u2019s ability to correct inaccurate predictions. This line of research suggests\nthat the impact of model interpretability can be complex, and may be dependent on properties of\nthe consumers and context on the top of the tools available in practice.\nMore broadly, scholars have argued that the concept of model interpretability is ill-defined and\ncouched in ambiguous terminology [4, 43, 61]. For example, Lipton discusses how although one of\nthe goals for interpreting models is gaining trust, the intended meaning of trust can vary, from faith\nin a model\u2019s ability to perform accurately, to acquiring a \u201clow-level mechanistic understanding\u201d\nof a model [43]. The use of many different terms, such as comprehensibility, mental fit, explain-\nability, acceptability, and justifiability, to refer to similar concepts [4] makes it hard to understand\ninterpretability\u2019s scope and to define metrics for its evaluation [61].\nSuch critiques have prompted reflection and at times more formal definitions. Doshi-Velez and\nKim see model interpretability as a proxy for many \u201cauxiliary criteria\u201d such as safety, fairness,\nnon-discrimination, avoiding technical debt, privacy, reliability, providing the right to explanation,\ntrust, and more [14]. Attempts to operationalize interpretability approximate it using the relative\ntime it takes a human or group of humans (such as crowd workers) to predict a model\u2019s label\nfor an instance [40], or the overlap between human annotations for multiple instances and those\ngenerated by a model in text classification tasks [67] and image recognition tasks [77].\nRecent studies suggest that the contexts in which data scientists work with ML models are\ndiversifying [75]. A deeper understanding of how practitioners work with model interpretability\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "2.1 ML Interpretability: Background and Approaches": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:3\naligning an ML model with a single notion of the human mental model to achieve interpretability as\na property of the model: it is simultaneously about the process of negotiating interpretability and its\nrole in creating insight and decisions that occurs through considering the mental models between\ndifferent stakeholders. While interpretability work is intended to foster trust in models, as discussed\nin existing interpretability literature, our interviewees also highlighted how interpretability plays an\nequally important role in building organizational trust among data scientists and other stakeholders\nthrough the three aforementioned stages of conceptualization, building, and deployment.\nThese results suggest important avenues for future work aimed at bridging the gap between\ninterpretability as understood in research versus practice. We conclude by using our results to\nmotivate design opportunities for technology aimed at addressing core communication-oriented\nchallenges of interpretability work in organizations.\n2\nRELATED WORK\nWe discuss the motivations behind model interpretability research and present key approaches to\nunderstanding and realizing interpretability in HCI, CSCW, and ML communities. We explain the\nlimitations and criticism discussed in the literature. Finally, we describe relevant empirical studies\nthat motivated our work.\n2.1\nML Interpretability: Background and Approaches\nModels are often categorized into two classes in light of the ease with which they can be interpreted\nby humans: white-box models and black-box models. White-box models tend to have a transparent\nstructure that permits human observers to understand the logic that drives model decisions. Ex-\namples include decision trees, logistic regression, and many varieties of rules. Black-box models\nhave complex structures that create much less intelligible relationships between input and output.\nExamples include a large variety of neural networks, random forests, and ensemble models.\nThe fast-growing interest in model interpretability is closely related to the raise of black-box\nmodels as practitioners are concerned with their opacity. But white-box models can also be trou-\nblesome as their structure, though transparent, can become very large and complex. For instance,\nlinear models with a high number of variables, trees with a high number of nodes and rule lists\nwith a large set of conditions also pose interpretability challenges to humans.\nWhile successful machine-human collaboration can yield outcomes that outperform either human\nor machine acting alone [3, 9, 37, 60], how to align human mental models with ML models in ways\nthat are helpful rather than harmful has become crucial in practice, and remains a topic of active\nresearch. For example, even though black-box models are known to provide better performance than\nwhite-box models in many cases [21], over-trusting models may result in an increased risk of making\na decision with severe negative consequences [54] and/or lower trust in model predictions [46]. As\nseveral studies have shown, ML models can learn spurious associations [8, 19] and make decisions\nbased on correlations that do not result from causal relationships. Models can also learn relationships\nin data that exemplify undesirable forms of bias [30] or are mislabeled [57]. Hence in practice,\nmodel accuracy cannot be fully trusted as a reliable metric for automation and decision making [38],\ngrounding a need for interpretability efforts.\nModel interpretability techniques are often categorized into two classes: global and local [15,\n21, 55]. Global interpretability refers to showing the logical structure of a model to explain how it\nworks globally. This type of information is readily available in white-box methods with the caveat\nmentioned above that even transparent models can become hard to understand when they become\ntoo large [52]. When the goal is to understand a black-box model at a global level model extraction\nmethods are needed, that is, methods that use the input-output behavior of the model to infer\na structure able to describe its behavior. Examples include methods that infer feature relevance\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "2 Related Work": ["Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs\n68:3\naligning an ML model with a single notion of the human mental model to achieve interpretability as\na property of the model: it is simultaneously about the process of negotiating interpretability and its\nrole in creating insight and decisions that occurs through considering the mental models between\ndifferent stakeholders. While interpretability work is intended to foster trust in models, as discussed\nin existing interpretability literature, our interviewees also highlighted how interpretability plays an\nequally important role in building organizational trust among data scientists and other stakeholders\nthrough the three aforementioned stages of conceptualization, building, and deployment.\nThese results suggest important avenues for future work aimed at bridging the gap between\ninterpretability as understood in research versus practice. We conclude by using our results to\nmotivate design opportunities for technology aimed at addressing core communication-oriented\nchallenges of interpretability work in organizations.\n2\nRELATED WORK\nWe discuss the motivations behind model interpretability research and present key approaches to\nunderstanding and realizing interpretability in HCI, CSCW, and ML communities. We explain the\nlimitations and criticism discussed in the literature. Finally, we describe relevant empirical studies\nthat motivated our work.\n2.1\nML Interpretability: Background and Approaches\nModels are often categorized into two classes in light of the ease with which they can be interpreted\nby humans: white-box models and black-box models. White-box models tend to have a transparent\nstructure that permits human observers to understand the logic that drives model decisions. Ex-\namples include decision trees, logistic regression, and many varieties of rules. Black-box models\nhave complex structures that create much less intelligible relationships between input and output.\nExamples include a large variety of neural networks, random forests, and ensemble models.\nThe fast-growing interest in model interpretability is closely related to the raise of black-box\nmodels as practitioners are concerned with their opacity. But white-box models can also be trou-\nblesome as their structure, though transparent, can become very large and complex. For instance,\nlinear models with a high number of variables, trees with a high number of nodes and rule lists\nwith a large set of conditions also pose interpretability challenges to humans.\nWhile successful machine-human collaboration can yield outcomes that outperform either human\nor machine acting alone [3, 9, 37, 60], how to align human mental models with ML models in ways\nthat are helpful rather than harmful has become crucial in practice, and remains a topic of active\nresearch. For example, even though black-box models are known to provide better performance than\nwhite-box models in many cases [21], over-trusting models may result in an increased risk of making\na decision with severe negative consequences [54] and/or lower trust in model predictions [46]. As\nseveral studies have shown, ML models can learn spurious associations [8, 19] and make decisions\nbased on correlations that do not result from causal relationships. Models can also learn relationships\nin data that exemplify undesirable forms of bias [30] or are mislabeled [57]. Hence in practice,\nmodel accuracy cannot be fully trusted as a reliable metric for automation and decision making [38],\ngrounding a need for interpretability efforts.\nModel interpretability techniques are often categorized into two classes: global and local [15,\n21, 55]. Global interpretability refers to showing the logical structure of a model to explain how it\nworks globally. This type of information is readily available in white-box methods with the caveat\nmentioned above that even transparent models can become hard to understand when they become\ntoo large [52]. When the goal is to understand a black-box model at a global level model extraction\nmethods are needed, that is, methods that use the input-output behavior of the model to infer\na structure able to describe its behavior. Examples include methods that infer feature relevance\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", []], "1 Introduction": ["68\nHuman Factors in Model Interpretability: Industry Practices,\nChallenges, and Needs\nSUNGSOO RAY HONG, New York University, USA\nJESSICA HULLMAN, Northwestern University, USA\nENRICO BERTINI, New York University, USA\nAs the use of machine learning (ML) models in product development and data-driven decision-making processes\nbecame pervasive in many domains, people\u2019s focus on building a well-performing model has increasingly\nshifted to understanding how their model works. While scholarly interest in model interpretability has grown\nrapidly in research communities like HCI, ML, and beyond, little is known about how practitioners perceive\nand aim to provide interpretability in the context of their existing workflows. This lack of understanding\nof interpretability as practiced may prevent interpretability research from addressing important needs, or\nlead to unrealistic solutions. To bridge this gap, we conducted 22 semi-structured interviews with industry\npractitioners to understand how they conceive of and design for interpretability while they plan, build, and use\ntheir models. Based on a qualitative analysis of our results, we differentiate interpretability roles, processes,\ngoals and strategies as they exist within organizations making heavy use of ML models. The characterization of\ninterpretability work that emerges from our analysis suggests that model interpretability frequently involves\ncooperation and mental model comparison between people in different roles, often aimed at building trust not\nonly between people and models but also between people within the organization. We present implications\nfor design that discuss gaps between the interpretability challenges that practitioners face in their practice\nand approaches proposed in the literature, highlighting possible research directions that can better address\nreal-world needs.\nCCS Concepts: \u2022 Human-centered computing \u2192Human computer interaction (HCI); Collaborative\nand social computing theory, concepts and paradigms; \u2022 Computing methodologies \u2192Machine\nlearning.\nAdditional Key Words and Phrases: machine learning; model interpretability; explainable AI; empirical study;\ndata scientist; domain expert; subject matter expert; mental model; sense-making; group work\nACM Reference Format:\nSungsoo Ray Hong, Jessica Hullman, and Enrico Bertini. 2020. Human Factors in Model Interpretability:\nIndustry Practices, Challenges, and Needs. Proc. ACM Hum.-Comput. Interact. 4, CSCW1, Article 68 (May 2020),\n26 pages. https://doi.org/10.1145/3392878\n1\nINTRODUCTION\nRecent years have witnessed a rapid increase in the deployment of machine learning (ML) in a large\nvariety of practical application areas, such as finance [54], healthcare [2, 16, 38], governance [50],\nAuthors\u2019 addresses: Sungsoo Ray Hong, rayhong@nyu.edu, New York University, New York City, NY, USA; Jessica Hullman,\njhullman@northwestern.edu, Northwestern University, Evanston, IL, USA; Enrico Bertini, enrico.bertini@nyu.edu, New\nYork University, New York City, NY, USA.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2020 Association for Computing Machinery.\n2573-0142/2020/5-ART68 $15.00\nhttps://doi.org/10.1145/3392878\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", [1, 2]], "Abstract": ["68\nHuman Factors in Model Interpretability: Industry Practices,\nChallenges, and Needs\nSUNGSOO RAY HONG, New York University, USA\nJESSICA HULLMAN, Northwestern University, USA\nENRICO BERTINI, New York University, USA\nAs the use of machine learning (ML) models in product development and data-driven decision-making processes\nbecame pervasive in many domains, people\u2019s focus on building a well-performing model has increasingly\nshifted to understanding how their model works. While scholarly interest in model interpretability has grown\nrapidly in research communities like HCI, ML, and beyond, little is known about how practitioners perceive\nand aim to provide interpretability in the context of their existing workflows. This lack of understanding\nof interpretability as practiced may prevent interpretability research from addressing important needs, or\nlead to unrealistic solutions. To bridge this gap, we conducted 22 semi-structured interviews with industry\npractitioners to understand how they conceive of and design for interpretability while they plan, build, and use\ntheir models. Based on a qualitative analysis of our results, we differentiate interpretability roles, processes,\ngoals and strategies as they exist within organizations making heavy use of ML models. The characterization of\ninterpretability work that emerges from our analysis suggests that model interpretability frequently involves\ncooperation and mental model comparison between people in different roles, often aimed at building trust not\nonly between people and models but also between people within the organization. We present implications\nfor design that discuss gaps between the interpretability challenges that practitioners face in their practice\nand approaches proposed in the literature, highlighting possible research directions that can better address\nreal-world needs.\nCCS Concepts: \u2022 Human-centered computing \u2192Human computer interaction (HCI); Collaborative\nand social computing theory, concepts and paradigms; \u2022 Computing methodologies \u2192Machine\nlearning.\nAdditional Key Words and Phrases: machine learning; model interpretability; explainable AI; empirical study;\ndata scientist; domain expert; subject matter expert; mental model; sense-making; group work\nACM Reference Format:\nSungsoo Ray Hong, Jessica Hullman, and Enrico Bertini. 2020. Human Factors in Model Interpretability:\nIndustry Practices, Challenges, and Needs. Proc. ACM Hum.-Comput. Interact. 4, CSCW1, Article 68 (May 2020),\n26 pages. https://doi.org/10.1145/3392878\n1\nINTRODUCTION\nRecent years have witnessed a rapid increase in the deployment of machine learning (ML) in a large\nvariety of practical application areas, such as finance [54], healthcare [2, 16, 38], governance [50],\nAuthors\u2019 addresses: Sungsoo Ray Hong, rayhong@nyu.edu, New York University, New York City, NY, USA; Jessica Hullman,\njhullman@northwestern.edu, Northwestern University, Evanston, IL, USA; Enrico Bertini, enrico.bertini@nyu.edu, New\nYork University, New York City, NY, USA.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2020 Association for Computing Machinery.\n2573-0142/2020/5-ART68 $15.00\nhttps://doi.org/10.1145/3392878\nProc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW1, Article 68. Publication date: May 2020.\n", [1, 2]]}