{"References": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 51 of 53\u2003\n61\nAcknowledgements\u2002 Ms Mary Simons contributed to the development of the search strategy and Profes-\nsor Wendy Rogers and Associate Professor Farah Magrabi to its refinement. Dr Catalin Tufanaru pro-\nvided guidance on the scoping review process and the selection of risk of bias tools for critical appraisal.\nAuthor Contributions\u2002 MG, EA, RC-W conceived the review, MG conducted the search. MG, EA, RC-W \nscreened titles and abstracts and full texts. MG, EA, RC-W extracted data and undertook the critical \nappraisal. MG undertook the qualitative analysis. MG wrote the introduction and the discussion. EA, \nRC-W revised the first draft of the paper and the final draft. All authors approved the final version.\nFunding\u2002 None.\nAvailability of Data and Material\u2002 All data relevant to the study are included in the article of uploaded as \nsupplementary information.\nDeclarations\u2002\nConflict of interest\u2002 The authors declare that they have no conflict of interest.\nReferences\nAbramoff, M. D., Tobey, D., & Char, D. S. (2020). Lessons learned about autonomous AI: Finding a safe, \nefficacious, and ethical path through the development process. American Journal of Ophthalmology, \n214, 134\u2013142.\nAnderson, M., Anderson, S. L., & Berenz, V. (2019). A value-driven eldercare robot: Virtual and physi-\ncal instantiations of a case supported principle-based behavior paradigm. Proceedings of the IEEE, \n107(3), 526\u2013540.\nBattistuzzi, L., Papadopoulos, C., Papadopoulos, I., Koulouglioti, C., Sgorbissa, A., Maciejewski, A. A., \net\u00a0al. (2018). Embedding ethics in the design of culturally competent socially assistive robots. IEEE/\nRSJ International Conference on Intelligent Robots and Systems, 2018, 1996\u20132001.\nBeauchamp, T. L., & Childress, J. F. (2013). Principles of biomedical ethics (7th ed.). Oxford University \nPress.\nBeil, M., Proft, I., van Heerden, D., Sviri, S., & van Heerden, P. V. (2019). Ethical considerations about \nartificial intelligence for prognostication in intensive care. Intensive Care Medicine Experimental, \n7(1), 70.\nBird, E., Fox-Skelly, J., Jenner, N., Larbey, R., Weitkamp, E., & Winfield, A. (2020). The ethics of artifi-\ncial intelligence: Issues and initiatives. Panel for the Future of Science and Technology.\nBuston, O., Strukelj, N., & Fenech, M. (2019). Explainability in Data-Driven Health and Care Technol-\nogy. Future Advocacy.\nCarter, S. M., Rogers, W., Win, K. T., Frazer, H., Richards, B., & Houssami, N. (2020). The ethical, \nlegal and social implications of using artificial intelligence systems in breast cancer care. Breast, 49, \n25\u201332.\nCawthorne, D., & Robbins-van Wynsberghe, A. (2019). From healthdrone to frugaldrone: Value-sensitive \ndesign of a blood sample transportation drone. 2019 IEEE International Symposium on Technology \nand Society.\nCawthorne, D., & Robbins-van Wynsberghe, A. (2020). An ethical framework for the design, develop-\nment, implementation, and assessment of drones used in public healthcare. Science and Engineering \nEthics.\nChanna, R., Wolf, R., & Abramoff, M. D. (2020). Autonomous artificial intelligence in diabetic retinopa-\nthy: From algorithm to clinical application. Journal of Diabetes Science and Technology.\nCommision, A. H. R. (2019). Human Rights and Technology: Discussion Paper. Australian Human \nRights Commision.\nCooney, M. D., & Menezes, M. L. R. (2018). Design for an art therapy robot: An explorative review of \nthe theoretical foundations for engaging in emotional and creative painting with a robot. Multimodal \nTechnologies and Interaction, 2(3).\n", []], "Acknowledgements ": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 51 of 53\u2003\n61\nAcknowledgements\u2002 Ms Mary Simons contributed to the development of the search strategy and Profes-\nsor Wendy Rogers and Associate Professor Farah Magrabi to its refinement. Dr Catalin Tufanaru pro-\nvided guidance on the scoping review process and the selection of risk of bias tools for critical appraisal.\nAuthor Contributions\u2002 MG, EA, RC-W conceived the review, MG conducted the search. MG, EA, RC-W \nscreened titles and abstracts and full texts. MG, EA, RC-W extracted data and undertook the critical \nappraisal. MG undertook the qualitative analysis. MG wrote the introduction and the discussion. EA, \nRC-W revised the first draft of the paper and the final draft. All authors approved the final version.\nFunding\u2002 None.\nAvailability of Data and Material\u2002 All data relevant to the study are included in the article of uploaded as \nsupplementary information.\nDeclarations\u2002\nConflict of interest\u2002 The authors declare that they have no conflict of interest.\nReferences\nAbramoff, M. D., Tobey, D., & Char, D. S. (2020). Lessons learned about autonomous AI: Finding a safe, \nefficacious, and ethical path through the development process. American Journal of Ophthalmology, \n214, 134\u2013142.\nAnderson, M., Anderson, S. L., & Berenz, V. (2019). A value-driven eldercare robot: Virtual and physi-\ncal instantiations of a case supported principle-based behavior paradigm. Proceedings of the IEEE, \n107(3), 526\u2013540.\nBattistuzzi, L., Papadopoulos, C., Papadopoulos, I., Koulouglioti, C., Sgorbissa, A., Maciejewski, A. A., \net\u00a0al. (2018). Embedding ethics in the design of culturally competent socially assistive robots. IEEE/\nRSJ International Conference on Intelligent Robots and Systems, 2018, 1996\u20132001.\nBeauchamp, T. L., & Childress, J. F. (2013). Principles of biomedical ethics (7th ed.). Oxford University \nPress.\nBeil, M., Proft, I., van Heerden, D., Sviri, S., & van Heerden, P. V. (2019). Ethical considerations about \nartificial intelligence for prognostication in intensive care. Intensive Care Medicine Experimental, \n7(1), 70.\nBird, E., Fox-Skelly, J., Jenner, N., Larbey, R., Weitkamp, E., & Winfield, A. (2020). The ethics of artifi-\ncial intelligence: Issues and initiatives. Panel for the Future of Science and Technology.\nBuston, O., Strukelj, N., & Fenech, M. (2019). Explainability in Data-Driven Health and Care Technol-\nogy. Future Advocacy.\nCarter, S. M., Rogers, W., Win, K. T., Frazer, H., Richards, B., & Houssami, N. (2020). The ethical, \nlegal and social implications of using artificial intelligence systems in breast cancer care. Breast, 49, \n25\u201332.\nCawthorne, D., & Robbins-van Wynsberghe, A. (2019). From healthdrone to frugaldrone: Value-sensitive \ndesign of a blood sample transportation drone. 2019 IEEE International Symposium on Technology \nand Society.\nCawthorne, D., & Robbins-van Wynsberghe, A. (2020). An ethical framework for the design, develop-\nment, implementation, and assessment of drones used in public healthcare. Science and Engineering \nEthics.\nChanna, R., Wolf, R., & Abramoff, M. D. (2020). Autonomous artificial intelligence in diabetic retinopa-\nthy: From algorithm to clinical application. Journal of Diabetes Science and Technology.\nCommision, A. H. R. (2019). Human Rights and Technology: Discussion Paper. Australian Human \nRights Commision.\nCooney, M. D., & Menezes, M. L. R. (2018). Design for an art therapy robot: An explorative review of \nthe theoretical foundations for engaging in emotional and creative painting with a robot. Multimodal \nTechnologies and Interaction, 2(3).\n", []], "Conclusion": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 17 of 53\u2003\n61\nConclusion\nOperationalising ethics frameworks in AIHA is a complex endeavour, and not \nwidely reported in the literature. Recommended and implemented ethics frame-\nworks for AIHA draw from medical ethics, technology-specific ethics and profes-\nsional ethics. Yet there has been a limited uptake of AI ethics frameworks in health-\ncare. One challenge when operationalising an ethics framework is that there is no \none size fits all approach and a need for contextualisation. While an AI ethics frame-\nwork for AIHA may be application-agnostic, the implementation of the framework \nneeds to be adapted to the environment of the AIHA and the same AIHA may face \ndifferent ethical issues when deployed into different environments with different \npopulations. Ultimately, there is agreement on the need for proactive and inclusive \napproaches. Implementing an ethics framework should not be remedial and should \ntake into consideration all stakeholders\u2019 needs. Embedding an ethics framework is \nparticularly important due to the danger of power imbalance between the recipients \nor consumers and the AIHA providers. Measures of a successful implementation of \nAIHA ethics frameworks are beginning to emerge, bringing much needed clarity \nand transparency to the whole process. Cross-pollination between computer science, \nengineering, medical sciences, and social sciences promises to yield the most com-\nprehensive ways to tackle the wicked issue of ethics in AIHA.\nAppendix\u00a01: Grey literature search strategy\n\u2022\t Scienceresearch.com limited to Food and Drug Administration using the follow-\ning query: (artificial intelligence or machine learning or deep learning or robot* \nor chatbot* or intelligent assistive or decision support) AND (ethic* or privacy \nor fairness or equity).\n\u2022\t Manual search of the websites of the following organisations:\nOrganisation name\nOrganisation type\nAccenture\nConsultancy\nACM\nProfessional organisation\nAmazon care\nTechnology company\nApple\nTechnology company\nBain & co\nConsultancy\nCap Gemini\nConsulting\nCEPEJ (Council of Europe European Commission for the Efficiency of \nJustice)\nGovernment\nCNIL\nGovernment\nDeepMind\nTechnology company\n", []], "Strengths and\u00a0Limitations": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 16 of 53\nbroader consequences of the deployment of such applications on society. One circles \nback to the effects on developers being exposed to possibly disturbing information \nwhen having to label training datasets (Peters et\u00a0al., 2020). The other enquires about \nthe impact on the environment, and societal issues such as job losses, but also whether \nthe introduction of a \u201cgood drone\u201d could pave the way for acceptance of drones which \nmay not be as beneficial, lowering our guard in some ways (Cawthorne & Robbins-\nvan Wynsberghe, 2019, 2020). Such thinking implies the need for vigilance and calls \nfor ethics frameworks to adjust or evolve with societal context. An abductive usage \nof the framework is recommended. Not unlike the way an AI model works, it begs to \nkeep learning, monitoring and improving to be able to respond adequately.\nNo Clear Measure of\u00a0Success\nWhile some strategies include ways of evaluating the implementation of the ethics \nframework, we could not find clear measures of success and few explicitly reported \non whether these implementations were successful (Anderson et\u00a0al., 2019; Kretzsch-\nmar et\u00a0al., 2019). The absence of clearly defined measures of successful implemen-\ntation of ethics frameworks could indicate a lack of maturity in this emerging field. \nIt could also be another symptom of the complexity of operationalising an ethics \nframework and reinforce the need for a common language between the different \nstakeholders. Clear measures of success are critical to evaluate whether the selected \nframework is fit-for-purpose and whether the implementation of the ethics frame-\nwork was effective.\nStrengths and\u00a0Limitations\nWe used a comprehensive search strategy including academic databases and grey \nliterature, and followed a rigorous process to screen, and appraise articles. None-\ntheless, private sector initiatives are not necessarily accessible through a public \nliterature search, and likely underrepresented in this review. Due to the interdisci-\nplinary nature of the topic, the data were heterogeneous and did not always lend \nitself neatly to the process of a systematic scoping review. As such, the screening \nand eligibility determination was not straightforward, and the data capture messy. \nHaving two pairs of reviewers with different backgrounds helped alleviate possible \nbiases and potential gaps in the data capture. Because of the limitations to English \nand French languages, China, Korea and Japan are likely underrepresented in the \ncurrent review while being prominent players in the field of AI. Finally, a number of \nthe authors within our data set operate within the same network. Out of 33 articles, \none pair (Ienca et\u00a0al., 2017, 2018) had the same first author, two pairs (Cawthorne & \nRobbins-van Wynsberghe, 2019, 2020; Poulsen & Burmeister, 2019; Poulsen et\u00a0al., \n2018) had the same first and second authors and studied the same application, and \nthe second author of one of these pairs was the first author of another article (Van \nWynsberghe, 2016). Another two articles shared the same author and drew from the \nsame application (Abramoff et\u00a0al., 2020; Channa et\u00a0al., 2020). While it gives more \nperspectives and depth to these approaches, it is likely that the interconnectedness of \nthe authors may overrepresent their strategies.\n", []], "No Clear Measure of\u00a0Success": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 16 of 53\nbroader consequences of the deployment of such applications on society. One circles \nback to the effects on developers being exposed to possibly disturbing information \nwhen having to label training datasets (Peters et\u00a0al., 2020). The other enquires about \nthe impact on the environment, and societal issues such as job losses, but also whether \nthe introduction of a \u201cgood drone\u201d could pave the way for acceptance of drones which \nmay not be as beneficial, lowering our guard in some ways (Cawthorne & Robbins-\nvan Wynsberghe, 2019, 2020). Such thinking implies the need for vigilance and calls \nfor ethics frameworks to adjust or evolve with societal context. An abductive usage \nof the framework is recommended. Not unlike the way an AI model works, it begs to \nkeep learning, monitoring and improving to be able to respond adequately.\nNo Clear Measure of\u00a0Success\nWhile some strategies include ways of evaluating the implementation of the ethics \nframework, we could not find clear measures of success and few explicitly reported \non whether these implementations were successful (Anderson et\u00a0al., 2019; Kretzsch-\nmar et\u00a0al., 2019). The absence of clearly defined measures of successful implemen-\ntation of ethics frameworks could indicate a lack of maturity in this emerging field. \nIt could also be another symptom of the complexity of operationalising an ethics \nframework and reinforce the need for a common language between the different \nstakeholders. Clear measures of success are critical to evaluate whether the selected \nframework is fit-for-purpose and whether the implementation of the ethics frame-\nwork was effective.\nStrengths and\u00a0Limitations\nWe used a comprehensive search strategy including academic databases and grey \nliterature, and followed a rigorous process to screen, and appraise articles. None-\ntheless, private sector initiatives are not necessarily accessible through a public \nliterature search, and likely underrepresented in this review. Due to the interdisci-\nplinary nature of the topic, the data were heterogeneous and did not always lend \nitself neatly to the process of a systematic scoping review. As such, the screening \nand eligibility determination was not straightforward, and the data capture messy. \nHaving two pairs of reviewers with different backgrounds helped alleviate possible \nbiases and potential gaps in the data capture. Because of the limitations to English \nand French languages, China, Korea and Japan are likely underrepresented in the \ncurrent review while being prominent players in the field of AI. Finally, a number of \nthe authors within our data set operate within the same network. Out of 33 articles, \none pair (Ienca et\u00a0al., 2017, 2018) had the same first author, two pairs (Cawthorne & \nRobbins-van Wynsberghe, 2019, 2020; Poulsen & Burmeister, 2019; Poulsen et\u00a0al., \n2018) had the same first and second authors and studied the same application, and \nthe second author of one of these pairs was the first author of another article (Van \nWynsberghe, 2016). Another two articles shared the same author and drew from the \nsame application (Abramoff et\u00a0al., 2020; Channa et\u00a0al., 2020). While it gives more \nperspectives and depth to these approaches, it is likely that the interconnectedness of \nthe authors may overrepresent their strategies.\n", []], "Some Strategies Strive to\u00a0be Holistic": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 15 of 53\u2003\n61\nChallenges Cannot be Overcome with\u00a0Technology Alone\nThe five levels of challenges identified (ethics principles, design, technology, \norganisational and regulatory) reinforce the need for an interdisciplinary approach \nand indicate that it can\u2019t be fixed with technology alone. In one CDSS application \n(Sendak et\u00a0al., 2020), the authors highlighted how trust in the AIHA was rooted in \nrelationships, not the features and functions of the technology. Another CDSS article \nthat was technology focused (Rajkomar et\u00a0al., 2018) stressed the importance of edu-\ncating clinicians on ML technology to be able to understand ML output and orient \ndesign choices. The Australian Human Rights Commission report on the usage of \nAI (Commision, 2019) advocates educating public servants and regulatory advisors \ninvolved in making decisions about the acquisition and deployment of AI systems \nfor them to better understand the implications and consequences of ML technology. \nConversely, people involved in the design and development of ML technology need \nto be equipped to assess the ethical and societal consequences of their development \n(Commision, 2019; Villani, 2018). There is a need to create a common language \nand understanding on both sides about what is at play among all stakeholders and \nto coordinate effort from researchers, developers, end-users, and implementers to \nensure the gap between principles and practice is closed (Morley et\u00a0al., 2020).\nBalance of\u00a0Power is\u00a0Unclear in\u00a0Strategies\nStrategies to implement ethics frameworks were mostly proactive and involved some \ndegree of stakeholders\u2019 consultation, yet it is unclear how decisions were made in \nparticular in the case of conflicts between ethics principles and/or between stake-\nholders needs. It is also unclear how much education was being provided for stake-\nholders to understand the technology and its implications in these consultations. \nHow well stakeholders\u2019 voices are being heard, and who has the last say on how eth-\nics and values are operationalised are known issues in AI in general and not specific \nto AIHA (Crawford et\u00a0al., 2019).\nSome Strategies Strive to\u00a0be Holistic\nMost strategies were multi-pronged combining some of the identified approaches \n(contextual, technological, checklist-oriented, organisational and evidence-based). \nOnly one spanned the entire lifecycle of an application, and in particular, the commer-\ncialisation/procurement stage is the least represented. This could be a concern given \nthe importance of the economic interests at stakes. Two strategies included consid-\nerations about the consequences of the AIHA on its broader environment, or second \nand third-order consequences usually seen as unintended consequences (Cawthorne \n& Robbins-van Wynsberghe, 2020; Peters et\u00a0al., 2020). Both applications are on the \nfringe of the healthcare system, as one is a drone delivering blood sample, and the \nother a mental health counselling app. It is possible that because they are not con-\ntained within a medical environment, they are better positioned to enquire about \n", []], "Balance of\u00a0Power is\u00a0Unclear in\u00a0Strategies": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 15 of 53\u2003\n61\nChallenges Cannot be Overcome with\u00a0Technology Alone\nThe five levels of challenges identified (ethics principles, design, technology, \norganisational and regulatory) reinforce the need for an interdisciplinary approach \nand indicate that it can\u2019t be fixed with technology alone. In one CDSS application \n(Sendak et\u00a0al., 2020), the authors highlighted how trust in the AIHA was rooted in \nrelationships, not the features and functions of the technology. Another CDSS article \nthat was technology focused (Rajkomar et\u00a0al., 2018) stressed the importance of edu-\ncating clinicians on ML technology to be able to understand ML output and orient \ndesign choices. The Australian Human Rights Commission report on the usage of \nAI (Commision, 2019) advocates educating public servants and regulatory advisors \ninvolved in making decisions about the acquisition and deployment of AI systems \nfor them to better understand the implications and consequences of ML technology. \nConversely, people involved in the design and development of ML technology need \nto be equipped to assess the ethical and societal consequences of their development \n(Commision, 2019; Villani, 2018). There is a need to create a common language \nand understanding on both sides about what is at play among all stakeholders and \nto coordinate effort from researchers, developers, end-users, and implementers to \nensure the gap between principles and practice is closed (Morley et\u00a0al., 2020).\nBalance of\u00a0Power is\u00a0Unclear in\u00a0Strategies\nStrategies to implement ethics frameworks were mostly proactive and involved some \ndegree of stakeholders\u2019 consultation, yet it is unclear how decisions were made in \nparticular in the case of conflicts between ethics principles and/or between stake-\nholders needs. It is also unclear how much education was being provided for stake-\nholders to understand the technology and its implications in these consultations. \nHow well stakeholders\u2019 voices are being heard, and who has the last say on how eth-\nics and values are operationalised are known issues in AI in general and not specific \nto AIHA (Crawford et\u00a0al., 2019).\nSome Strategies Strive to\u00a0be Holistic\nMost strategies were multi-pronged combining some of the identified approaches \n(contextual, technological, checklist-oriented, organisational and evidence-based). \nOnly one spanned the entire lifecycle of an application, and in particular, the commer-\ncialisation/procurement stage is the least represented. This could be a concern given \nthe importance of the economic interests at stakes. Two strategies included consid-\nerations about the consequences of the AIHA on its broader environment, or second \nand third-order consequences usually seen as unintended consequences (Cawthorne \n& Robbins-van Wynsberghe, 2020; Peters et\u00a0al., 2020). Both applications are on the \nfringe of the healthcare system, as one is a drone delivering blood sample, and the \nother a mental health counselling app. It is possible that because they are not con-\ntained within a medical environment, they are better positioned to enquire about \n", []], "Challenges Cannot be Overcome with\u00a0Technology Alone": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 15 of 53\u2003\n61\nChallenges Cannot be Overcome with\u00a0Technology Alone\nThe five levels of challenges identified (ethics principles, design, technology, \norganisational and regulatory) reinforce the need for an interdisciplinary approach \nand indicate that it can\u2019t be fixed with technology alone. In one CDSS application \n(Sendak et\u00a0al., 2020), the authors highlighted how trust in the AIHA was rooted in \nrelationships, not the features and functions of the technology. Another CDSS article \nthat was technology focused (Rajkomar et\u00a0al., 2018) stressed the importance of edu-\ncating clinicians on ML technology to be able to understand ML output and orient \ndesign choices. The Australian Human Rights Commission report on the usage of \nAI (Commision, 2019) advocates educating public servants and regulatory advisors \ninvolved in making decisions about the acquisition and deployment of AI systems \nfor them to better understand the implications and consequences of ML technology. \nConversely, people involved in the design and development of ML technology need \nto be equipped to assess the ethical and societal consequences of their development \n(Commision, 2019; Villani, 2018). There is a need to create a common language \nand understanding on both sides about what is at play among all stakeholders and \nto coordinate effort from researchers, developers, end-users, and implementers to \nensure the gap between principles and practice is closed (Morley et\u00a0al., 2020).\nBalance of\u00a0Power is\u00a0Unclear in\u00a0Strategies\nStrategies to implement ethics frameworks were mostly proactive and involved some \ndegree of stakeholders\u2019 consultation, yet it is unclear how decisions were made in \nparticular in the case of conflicts between ethics principles and/or between stake-\nholders needs. It is also unclear how much education was being provided for stake-\nholders to understand the technology and its implications in these consultations. \nHow well stakeholders\u2019 voices are being heard, and who has the last say on how eth-\nics and values are operationalised are known issues in AI in general and not specific \nto AIHA (Crawford et\u00a0al., 2019).\nSome Strategies Strive to\u00a0be Holistic\nMost strategies were multi-pronged combining some of the identified approaches \n(contextual, technological, checklist-oriented, organisational and evidence-based). \nOnly one spanned the entire lifecycle of an application, and in particular, the commer-\ncialisation/procurement stage is the least represented. This could be a concern given \nthe importance of the economic interests at stakes. Two strategies included consid-\nerations about the consequences of the AIHA on its broader environment, or second \nand third-order consequences usually seen as unintended consequences (Cawthorne \n& Robbins-van Wynsberghe, 2020; Peters et\u00a0al., 2020). Both applications are on the \nfringe of the healthcare system, as one is a drone delivering blood sample, and the \nother a mental health counselling app. It is possible that because they are not con-\ntained within a medical environment, they are better positioned to enquire about \n", []], "Need for\u00a0Examining the\u00a0Motives for\u00a0Implementing an\u00a0Ethics Framework": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 14 of 53\n2019; D. Peters et\u00a0 al., 2020) mention an AI-specific ethics framework. Of the \neight, only three report on the adoption of an ethics framework for deployed \napplications (Channa et\u00a0al., 2020; CPAIS, 2019; Joerin et\u00a0al., 2020) and one on \na pilot (Cawthorne & Robbins-van Wynsberghe, 2020). Among these four AI-\nspecific frameworks, only one was specific to healthcare developed by the Ameri-\ncan Medical Association (Channa et\u00a0al., 2020). The limited uptake of AI-specific \nethics frameworks in AIHA may be due to the novelty of the frameworks, or their \nlack of healthcare specialisation. The limited uptake could also signal the com-\nplexity of operationalising ethics in AIHA and harmonising the cultures of the \nhealthcare and high-tech industry.\nTrust is\u00a0a\u00a0Wicked Problem\nTrust is a wicked problem, one difficult to solve because of compounding fac-\ntors such as the challenges around ML explicability, and reluctance to adopt \nbecause of ethical concerns resulting in unknown variables. Critically, technol-\nogy performance is heavily dependent on data whose collection depends on adop-\ntion (Hasenauer et\u00a0al., 2019; Kretzschmar et\u00a0al., 2019; Sendak et\u00a0al., 2020). For \ninstance, mental health chatbots users need to trust the app to use it and under-\nstand its limitations for safety reasons. It means privacy, confidentiality but also \ntransparency about the limitations of the app, and how the collected data are \nused need to be communicated, while the data are needed to increase the per-\nformance of the app, hence its safety. In the case of a CDSS whose users were \nclinicians, the trust in relationships between the different parties was more impor-\ntant than the trust in the technology (Sendak et\u00a0al., 2020). Nonetheless, the chal-\nlenge AI applications are facing with adoption is circular because of their large \nneed for data to keep learning and improving, hence becoming safer, hence more \ntrustworthy.\nNeed for\u00a0Examining the\u00a0Motives for\u00a0Implementing an\u00a0Ethics Framework\nApplications involving IATs, and robots in particular are the most represented and \nspread over the five-year window. CDSSs are becoming more prominent only in the \nlast two years and equally represented as IATs in 2020. Because IATs interface with \npatients directly, ethical issues may be more visible and pressing, prompting more \nstudies. In one instance (Hasenauer et\u00a0al., 2019), the implementation of an ethics \nframework for a care robot in a nursing home was motivated by the need for the \nconsumers to adopt the robot after they expressed reservations and ethical concerns \nabout it. In other words, the implementation of an ethics framework was motivated \nby the need to gain the trust of the consumers, and the assumption that the care robot \nis the adequate solution is not challenged. It is noteworthy that only two strategies \n(Carter et\u00a0al., 2020; Van Wynsberghe, 2016) call for explicitly investigating whether \nan AIHA is even a good idea in the given context first.\n", []], "Trust is\u00a0a\u00a0Wicked Problem": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 14 of 53\n2019; D. Peters et\u00a0 al., 2020) mention an AI-specific ethics framework. Of the \neight, only three report on the adoption of an ethics framework for deployed \napplications (Channa et\u00a0al., 2020; CPAIS, 2019; Joerin et\u00a0al., 2020) and one on \na pilot (Cawthorne & Robbins-van Wynsberghe, 2020). Among these four AI-\nspecific frameworks, only one was specific to healthcare developed by the Ameri-\ncan Medical Association (Channa et\u00a0al., 2020). The limited uptake of AI-specific \nethics frameworks in AIHA may be due to the novelty of the frameworks, or their \nlack of healthcare specialisation. The limited uptake could also signal the com-\nplexity of operationalising ethics in AIHA and harmonising the cultures of the \nhealthcare and high-tech industry.\nTrust is\u00a0a\u00a0Wicked Problem\nTrust is a wicked problem, one difficult to solve because of compounding fac-\ntors such as the challenges around ML explicability, and reluctance to adopt \nbecause of ethical concerns resulting in unknown variables. Critically, technol-\nogy performance is heavily dependent on data whose collection depends on adop-\ntion (Hasenauer et\u00a0al., 2019; Kretzschmar et\u00a0al., 2019; Sendak et\u00a0al., 2020). For \ninstance, mental health chatbots users need to trust the app to use it and under-\nstand its limitations for safety reasons. It means privacy, confidentiality but also \ntransparency about the limitations of the app, and how the collected data are \nused need to be communicated, while the data are needed to increase the per-\nformance of the app, hence its safety. In the case of a CDSS whose users were \nclinicians, the trust in relationships between the different parties was more impor-\ntant than the trust in the technology (Sendak et\u00a0al., 2020). Nonetheless, the chal-\nlenge AI applications are facing with adoption is circular because of their large \nneed for data to keep learning and improving, hence becoming safer, hence more \ntrustworthy.\nNeed for\u00a0Examining the\u00a0Motives for\u00a0Implementing an\u00a0Ethics Framework\nApplications involving IATs, and robots in particular are the most represented and \nspread over the five-year window. CDSSs are becoming more prominent only in the \nlast two years and equally represented as IATs in 2020. Because IATs interface with \npatients directly, ethical issues may be more visible and pressing, prompting more \nstudies. In one instance (Hasenauer et\u00a0al., 2019), the implementation of an ethics \nframework for a care robot in a nursing home was motivated by the need for the \nconsumers to adopt the robot after they expressed reservations and ethical concerns \nabout it. In other words, the implementation of an ethics framework was motivated \nby the need to gain the trust of the consumers, and the assumption that the care robot \nis the adequate solution is not challenged. It is noteworthy that only two strategies \n(Carter et\u00a0al., 2020; Van Wynsberghe, 2016) call for explicitly investigating whether \nan AIHA is even a good idea in the given context first.\n", []], "Limited Uptake of\u00a0AI-Specific Ethics Frameworks": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 13 of 53\u2003\n61\nperform very differently in another context. It is therefore important to prove the \nsystem before deploying it into a clinical setting.\nEvaluation of\u00a0the\u00a0Implementations of\u00a0Ethics Frameworks into\u00a0AIHA\nLess than a third of the proposed implementation strategies included specifics about \nthe evaluations of the implementation (n\u2009=\u20099, 27%) (Beil et\u00a0al., 2019; Cawthorne & \nRobbins-van Wynsberghe, 2020; CPAIS, 2019; Dantas et\u00a0al., 2017; Hasenauer et\u00a0al., \n2019; Klein & Schl\u00f6mer, 2018; Kretzschmar et\u00a0al., 2019; McBride, 2020; Morch \net\u00a0 al., 2019). Measures of success included better outcomes for the patients than \nwithout AI (Abramoff et\u00a0al., 2020; Carter et\u00a0al., 2020; Channa et\u00a0al., 2020), efficacy \n(Abramoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019; Poulsen et\u00a0al., 2018), safety (Abra-\nmoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019), equity (Abramoff et\u00a0al., 2020; Carter \net\u00a0al., 2020), effectiveness (Poulsen et\u00a0al., 2018), cost-effectiveness and reduction \n(Carter et\u00a0 al., 2020; Hasenauer et\u00a0 al., 2019), privacy (Kretzschmar et\u00a0 al., 2019), \nclear professional responsibilities, autonomy, usage in relevance settings, and that \ntrust is maintained (Carter et\u00a0al., 2020). The approach, whether a design framework, \nquestionnaire or stakeholders\u2019 consultations, acts as an evaluation methodology. One \narticle featuring an actual care robot application reported a successful evaluation as \nthe robot behaved as expected (Anderson et\u00a0al., 2019). One article concluded mixed \nresults in their evaluations of different mental health mobile applications (Kretzsch-\nmar et\u00a0al., 2019).\nDiscussion\nDearth of\u00a0Studies About the\u00a0Implementation of\u00a0Ethics Frameworks in\u00a0AIHA\nIn this review, we aimed to examine the implementation of ethics frameworks in \nAIHA. While 4444 articles were screened out of which 480 were eligible for full-\ntext review, only 0.75% (n\u2009=\u200933) were ultimately included. The attrition of articles \nbetween screening, eligibility and inclusion indicates a dearth of studies on how eth-\nics are implemented in AIHA. Additionally, only a third of the articles were about \ndeployed applications. This lack of data could be due to the novelty of the ethics \nframeworks for AI as according to Jobin et\u00a0al. (2019), 88% of them have been issued \nafter 2016, the culture of non-disclosure of business practices in the private sector, \nand/or indicative of the complexity of the task (assuming failures don\u2019t get reported).\nLimited Uptake of\u00a0AI\u2011Specific Ethics Frameworks\nThe review reveals that while there have been at least 84 AI-specific ethics frame-\nworks published in the last 6\u00a0years (see (Jobin et\u00a0al., 2019) for full list), the adop-\ntion of these frameworks in healthcare has been limited. Eight or 24% of the \narticles (Beil et\u00a0al., 2019; Buston et\u00a0al., 2019; Cawthorne & Robbins-van Wyns-\nberghe, 2020; Channa et\u00a0al., 2020; CPAIS, 2019; Joerin et\u00a0al., 2020; Milosevic, \n", []], "Dearth of\u00a0Studies About the\u00a0Implementation of\u00a0Ethics Frameworks in\u00a0AIHA": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 13 of 53\u2003\n61\nperform very differently in another context. It is therefore important to prove the \nsystem before deploying it into a clinical setting.\nEvaluation of\u00a0the\u00a0Implementations of\u00a0Ethics Frameworks into\u00a0AIHA\nLess than a third of the proposed implementation strategies included specifics about \nthe evaluations of the implementation (n\u2009=\u20099, 27%) (Beil et\u00a0al., 2019; Cawthorne & \nRobbins-van Wynsberghe, 2020; CPAIS, 2019; Dantas et\u00a0al., 2017; Hasenauer et\u00a0al., \n2019; Klein & Schl\u00f6mer, 2018; Kretzschmar et\u00a0al., 2019; McBride, 2020; Morch \net\u00a0 al., 2019). Measures of success included better outcomes for the patients than \nwithout AI (Abramoff et\u00a0al., 2020; Carter et\u00a0al., 2020; Channa et\u00a0al., 2020), efficacy \n(Abramoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019; Poulsen et\u00a0al., 2018), safety (Abra-\nmoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019), equity (Abramoff et\u00a0al., 2020; Carter \net\u00a0al., 2020), effectiveness (Poulsen et\u00a0al., 2018), cost-effectiveness and reduction \n(Carter et\u00a0 al., 2020; Hasenauer et\u00a0 al., 2019), privacy (Kretzschmar et\u00a0 al., 2019), \nclear professional responsibilities, autonomy, usage in relevance settings, and that \ntrust is maintained (Carter et\u00a0al., 2020). The approach, whether a design framework, \nquestionnaire or stakeholders\u2019 consultations, acts as an evaluation methodology. One \narticle featuring an actual care robot application reported a successful evaluation as \nthe robot behaved as expected (Anderson et\u00a0al., 2019). One article concluded mixed \nresults in their evaluations of different mental health mobile applications (Kretzsch-\nmar et\u00a0al., 2019).\nDiscussion\nDearth of\u00a0Studies About the\u00a0Implementation of\u00a0Ethics Frameworks in\u00a0AIHA\nIn this review, we aimed to examine the implementation of ethics frameworks in \nAIHA. While 4444 articles were screened out of which 480 were eligible for full-\ntext review, only 0.75% (n\u2009=\u200933) were ultimately included. The attrition of articles \nbetween screening, eligibility and inclusion indicates a dearth of studies on how eth-\nics are implemented in AIHA. Additionally, only a third of the articles were about \ndeployed applications. This lack of data could be due to the novelty of the ethics \nframeworks for AI as according to Jobin et\u00a0al. (2019), 88% of them have been issued \nafter 2016, the culture of non-disclosure of business practices in the private sector, \nand/or indicative of the complexity of the task (assuming failures don\u2019t get reported).\nLimited Uptake of\u00a0AI\u2011Specific Ethics Frameworks\nThe review reveals that while there have been at least 84 AI-specific ethics frame-\nworks published in the last 6\u00a0years (see (Jobin et\u00a0al., 2019) for full list), the adop-\ntion of these frameworks in healthcare has been limited. Eight or 24% of the \narticles (Beil et\u00a0al., 2019; Buston et\u00a0al., 2019; Cawthorne & Robbins-van Wyns-\nberghe, 2020; Channa et\u00a0al., 2020; CPAIS, 2019; Joerin et\u00a0al., 2020; Milosevic, \n", []], "Discussion": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 13 of 53\u2003\n61\nperform very differently in another context. It is therefore important to prove the \nsystem before deploying it into a clinical setting.\nEvaluation of\u00a0the\u00a0Implementations of\u00a0Ethics Frameworks into\u00a0AIHA\nLess than a third of the proposed implementation strategies included specifics about \nthe evaluations of the implementation (n\u2009=\u20099, 27%) (Beil et\u00a0al., 2019; Cawthorne & \nRobbins-van Wynsberghe, 2020; CPAIS, 2019; Dantas et\u00a0al., 2017; Hasenauer et\u00a0al., \n2019; Klein & Schl\u00f6mer, 2018; Kretzschmar et\u00a0al., 2019; McBride, 2020; Morch \net\u00a0 al., 2019). Measures of success included better outcomes for the patients than \nwithout AI (Abramoff et\u00a0al., 2020; Carter et\u00a0al., 2020; Channa et\u00a0al., 2020), efficacy \n(Abramoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019; Poulsen et\u00a0al., 2018), safety (Abra-\nmoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019), equity (Abramoff et\u00a0al., 2020; Carter \net\u00a0al., 2020), effectiveness (Poulsen et\u00a0al., 2018), cost-effectiveness and reduction \n(Carter et\u00a0 al., 2020; Hasenauer et\u00a0 al., 2019), privacy (Kretzschmar et\u00a0 al., 2019), \nclear professional responsibilities, autonomy, usage in relevance settings, and that \ntrust is maintained (Carter et\u00a0al., 2020). The approach, whether a design framework, \nquestionnaire or stakeholders\u2019 consultations, acts as an evaluation methodology. One \narticle featuring an actual care robot application reported a successful evaluation as \nthe robot behaved as expected (Anderson et\u00a0al., 2019). One article concluded mixed \nresults in their evaluations of different mental health mobile applications (Kretzsch-\nmar et\u00a0al., 2019).\nDiscussion\nDearth of\u00a0Studies About the\u00a0Implementation of\u00a0Ethics Frameworks in\u00a0AIHA\nIn this review, we aimed to examine the implementation of ethics frameworks in \nAIHA. While 4444 articles were screened out of which 480 were eligible for full-\ntext review, only 0.75% (n\u2009=\u200933) were ultimately included. The attrition of articles \nbetween screening, eligibility and inclusion indicates a dearth of studies on how eth-\nics are implemented in AIHA. Additionally, only a third of the articles were about \ndeployed applications. This lack of data could be due to the novelty of the ethics \nframeworks for AI as according to Jobin et\u00a0al. (2019), 88% of them have been issued \nafter 2016, the culture of non-disclosure of business practices in the private sector, \nand/or indicative of the complexity of the task (assuming failures don\u2019t get reported).\nLimited Uptake of\u00a0AI\u2011Specific Ethics Frameworks\nThe review reveals that while there have been at least 84 AI-specific ethics frame-\nworks published in the last 6\u00a0years (see (Jobin et\u00a0al., 2019) for full list), the adop-\ntion of these frameworks in healthcare has been limited. Eight or 24% of the \narticles (Beil et\u00a0al., 2019; Buston et\u00a0al., 2019; Cawthorne & Robbins-van Wyns-\nberghe, 2020; Channa et\u00a0al., 2020; CPAIS, 2019; Joerin et\u00a0al., 2020; Milosevic, \n", []], "Evaluation of\u00a0the\u00a0Implementations of\u00a0Ethics Frameworks into\u00a0AIHA": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 13 of 53\u2003\n61\nperform very differently in another context. It is therefore important to prove the \nsystem before deploying it into a clinical setting.\nEvaluation of\u00a0the\u00a0Implementations of\u00a0Ethics Frameworks into\u00a0AIHA\nLess than a third of the proposed implementation strategies included specifics about \nthe evaluations of the implementation (n\u2009=\u20099, 27%) (Beil et\u00a0al., 2019; Cawthorne & \nRobbins-van Wynsberghe, 2020; CPAIS, 2019; Dantas et\u00a0al., 2017; Hasenauer et\u00a0al., \n2019; Klein & Schl\u00f6mer, 2018; Kretzschmar et\u00a0al., 2019; McBride, 2020; Morch \net\u00a0 al., 2019). Measures of success included better outcomes for the patients than \nwithout AI (Abramoff et\u00a0al., 2020; Carter et\u00a0al., 2020; Channa et\u00a0al., 2020), efficacy \n(Abramoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019; Poulsen et\u00a0al., 2018), safety (Abra-\nmoff et\u00a0al., 2020; Kretzschmar et\u00a0al., 2019), equity (Abramoff et\u00a0al., 2020; Carter \net\u00a0al., 2020), effectiveness (Poulsen et\u00a0al., 2018), cost-effectiveness and reduction \n(Carter et\u00a0 al., 2020; Hasenauer et\u00a0 al., 2019), privacy (Kretzschmar et\u00a0 al., 2019), \nclear professional responsibilities, autonomy, usage in relevance settings, and that \ntrust is maintained (Carter et\u00a0al., 2020). The approach, whether a design framework, \nquestionnaire or stakeholders\u2019 consultations, acts as an evaluation methodology. One \narticle featuring an actual care robot application reported a successful evaluation as \nthe robot behaved as expected (Anderson et\u00a0al., 2019). One article concluded mixed \nresults in their evaluations of different mental health mobile applications (Kretzsch-\nmar et\u00a0al., 2019).\nDiscussion\nDearth of\u00a0Studies About the\u00a0Implementation of\u00a0Ethics Frameworks in\u00a0AIHA\nIn this review, we aimed to examine the implementation of ethics frameworks in \nAIHA. While 4444 articles were screened out of which 480 were eligible for full-\ntext review, only 0.75% (n\u2009=\u200933) were ultimately included. The attrition of articles \nbetween screening, eligibility and inclusion indicates a dearth of studies on how eth-\nics are implemented in AIHA. Additionally, only a third of the articles were about \ndeployed applications. This lack of data could be due to the novelty of the ethics \nframeworks for AI as according to Jobin et\u00a0al. (2019), 88% of them have been issued \nafter 2016, the culture of non-disclosure of business practices in the private sector, \nand/or indicative of the complexity of the task (assuming failures don\u2019t get reported).\nLimited Uptake of\u00a0AI\u2011Specific Ethics Frameworks\nThe review reveals that while there have been at least 84 AI-specific ethics frame-\nworks published in the last 6\u00a0years (see (Jobin et\u00a0al., 2019) for full list), the adop-\ntion of these frameworks in healthcare has been limited. Eight or 24% of the \narticles (Beil et\u00a0al., 2019; Buston et\u00a0al., 2019; Cawthorne & Robbins-van Wyns-\nberghe, 2020; Channa et\u00a0al., 2020; CPAIS, 2019; Joerin et\u00a0al., 2020; Milosevic, \n", []], "Evidence-Based": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 12 of 53\n2018; Klein & Schl\u00f6mer, 2018; D. Peters et\u00a0 al., 2020; Poulsen et\u00a0 al., 2018; van \nRysewyk & Pontier, 2015). Another proposed way is to translate a professional code \nof conduct into capabilities (Cooney & Menezes, 2018).\nBeing Contextual\nFour articles stressed the importance of a deep understanding of the context of the \nAIHA (Ienca et\u00a0al., 2017; McBride, 2020; Sendak et\u00a0al., 2020; Van Wynsberghe, \n2016). van Wynsberghe gives poignant situations where a care robot\u2019s purpose is to \nprotect nurses from toxins when collecting urine samples from children undergoing \nchemotherapy or from injuries when lifting a patient off his/her bed. The rapport of \ncare between nurse and patient needs to be respected when programming the robot.\nTechnological Approach\nAnother approach is technological and included coding ethics in the operational \nsystem (Anderson et\u00a0 al., 2019; Milosevic, 2019), embedding ethics principles \nin the algorithm, and the collection of data of the AIHA (Abramoff et\u00a0al., 2020; \nRajkomar et\u00a0al., 2018), monitoring and evaluating the applications (Abramoff et\u00a0al., \n2020; Joerin et\u00a0al., 2020; Kortner, 2016; McCradden et\u00a0al., 2020; Rajkomar et\u00a0al., \n2018), and adopting a continuous improvement approach (Joerin et\u00a0al., 2020; Kort-\nner, 2016; McCradden et\u00a0al., 2020). User eXperience (UX) techniques were recom-\nmended in particular for respecting privacy and transparency (Joerin et\u00a0al., 2020; \nKretzschmar et\u00a0al., 2019).\nChecking and\u00a0Verifying\nChecklists have been developed to guide the dialogues between stakeholders \n(CPAIS, 2019; Stahl & Coeckelbergh, 2016), raise awareness about potential con-\nflicts of interest including funding (Morch et\u00a0al., 2019), and focus on the patient, \nthe system, the technical and the medical aspects of an AIHA (Beil et\u00a0al., 2019). \nAt the evaluation stage, a verification methodology of ethical compliance was rec-\nommended which involves case studies and experts\u2019 consultations (Anderson et\u00a0al., \n2019; Dantas et\u00a0al., 2017).\nOrganisational Checks and\u00a0Balances\nThe importance of the responsibility of the AIHA creators and the need for counsel-\nling through an advisory board was highlighted by authors from a private company \n(Joerin et\u00a0al., 2020).\nEvidence\u2011Based\nAnother recommendation was to follow an evidence-based introduction of a com-\nplex intervention (Carter et\u00a0 al., 2020). AI systems trained within a context can \n", []], "Organisational Checks and\u00a0Balances": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 12 of 53\n2018; Klein & Schl\u00f6mer, 2018; D. Peters et\u00a0 al., 2020; Poulsen et\u00a0 al., 2018; van \nRysewyk & Pontier, 2015). Another proposed way is to translate a professional code \nof conduct into capabilities (Cooney & Menezes, 2018).\nBeing Contextual\nFour articles stressed the importance of a deep understanding of the context of the \nAIHA (Ienca et\u00a0al., 2017; McBride, 2020; Sendak et\u00a0al., 2020; Van Wynsberghe, \n2016). van Wynsberghe gives poignant situations where a care robot\u2019s purpose is to \nprotect nurses from toxins when collecting urine samples from children undergoing \nchemotherapy or from injuries when lifting a patient off his/her bed. The rapport of \ncare between nurse and patient needs to be respected when programming the robot.\nTechnological Approach\nAnother approach is technological and included coding ethics in the operational \nsystem (Anderson et\u00a0 al., 2019; Milosevic, 2019), embedding ethics principles \nin the algorithm, and the collection of data of the AIHA (Abramoff et\u00a0al., 2020; \nRajkomar et\u00a0al., 2018), monitoring and evaluating the applications (Abramoff et\u00a0al., \n2020; Joerin et\u00a0al., 2020; Kortner, 2016; McCradden et\u00a0al., 2020; Rajkomar et\u00a0al., \n2018), and adopting a continuous improvement approach (Joerin et\u00a0al., 2020; Kort-\nner, 2016; McCradden et\u00a0al., 2020). User eXperience (UX) techniques were recom-\nmended in particular for respecting privacy and transparency (Joerin et\u00a0al., 2020; \nKretzschmar et\u00a0al., 2019).\nChecking and\u00a0Verifying\nChecklists have been developed to guide the dialogues between stakeholders \n(CPAIS, 2019; Stahl & Coeckelbergh, 2016), raise awareness about potential con-\nflicts of interest including funding (Morch et\u00a0al., 2019), and focus on the patient, \nthe system, the technical and the medical aspects of an AIHA (Beil et\u00a0al., 2019). \nAt the evaluation stage, a verification methodology of ethical compliance was rec-\nommended which involves case studies and experts\u2019 consultations (Anderson et\u00a0al., \n2019; Dantas et\u00a0al., 2017).\nOrganisational Checks and\u00a0Balances\nThe importance of the responsibility of the AIHA creators and the need for counsel-\nling through an advisory board was highlighted by authors from a private company \n(Joerin et\u00a0al., 2020).\nEvidence\u2011Based\nAnother recommendation was to follow an evidence-based introduction of a com-\nplex intervention (Carter et\u00a0 al., 2020). AI systems trained within a context can \n", []], "Checking and\u00a0Verifying": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 12 of 53\n2018; Klein & Schl\u00f6mer, 2018; D. Peters et\u00a0 al., 2020; Poulsen et\u00a0 al., 2018; van \nRysewyk & Pontier, 2015). Another proposed way is to translate a professional code \nof conduct into capabilities (Cooney & Menezes, 2018).\nBeing Contextual\nFour articles stressed the importance of a deep understanding of the context of the \nAIHA (Ienca et\u00a0al., 2017; McBride, 2020; Sendak et\u00a0al., 2020; Van Wynsberghe, \n2016). van Wynsberghe gives poignant situations where a care robot\u2019s purpose is to \nprotect nurses from toxins when collecting urine samples from children undergoing \nchemotherapy or from injuries when lifting a patient off his/her bed. The rapport of \ncare between nurse and patient needs to be respected when programming the robot.\nTechnological Approach\nAnother approach is technological and included coding ethics in the operational \nsystem (Anderson et\u00a0 al., 2019; Milosevic, 2019), embedding ethics principles \nin the algorithm, and the collection of data of the AIHA (Abramoff et\u00a0al., 2020; \nRajkomar et\u00a0al., 2018), monitoring and evaluating the applications (Abramoff et\u00a0al., \n2020; Joerin et\u00a0al., 2020; Kortner, 2016; McCradden et\u00a0al., 2020; Rajkomar et\u00a0al., \n2018), and adopting a continuous improvement approach (Joerin et\u00a0al., 2020; Kort-\nner, 2016; McCradden et\u00a0al., 2020). User eXperience (UX) techniques were recom-\nmended in particular for respecting privacy and transparency (Joerin et\u00a0al., 2020; \nKretzschmar et\u00a0al., 2019).\nChecking and\u00a0Verifying\nChecklists have been developed to guide the dialogues between stakeholders \n(CPAIS, 2019; Stahl & Coeckelbergh, 2016), raise awareness about potential con-\nflicts of interest including funding (Morch et\u00a0al., 2019), and focus on the patient, \nthe system, the technical and the medical aspects of an AIHA (Beil et\u00a0al., 2019). \nAt the evaluation stage, a verification methodology of ethical compliance was rec-\nommended which involves case studies and experts\u2019 consultations (Anderson et\u00a0al., \n2019; Dantas et\u00a0al., 2017).\nOrganisational Checks and\u00a0Balances\nThe importance of the responsibility of the AIHA creators and the need for counsel-\nling through an advisory board was highlighted by authors from a private company \n(Joerin et\u00a0al., 2020).\nEvidence\u2011Based\nAnother recommendation was to follow an evidence-based introduction of a com-\nplex intervention (Carter et\u00a0 al., 2020). AI systems trained within a context can \n", []], "Technological Approach": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 12 of 53\n2018; Klein & Schl\u00f6mer, 2018; D. Peters et\u00a0 al., 2020; Poulsen et\u00a0 al., 2018; van \nRysewyk & Pontier, 2015). Another proposed way is to translate a professional code \nof conduct into capabilities (Cooney & Menezes, 2018).\nBeing Contextual\nFour articles stressed the importance of a deep understanding of the context of the \nAIHA (Ienca et\u00a0al., 2017; McBride, 2020; Sendak et\u00a0al., 2020; Van Wynsberghe, \n2016). van Wynsberghe gives poignant situations where a care robot\u2019s purpose is to \nprotect nurses from toxins when collecting urine samples from children undergoing \nchemotherapy or from injuries when lifting a patient off his/her bed. The rapport of \ncare between nurse and patient needs to be respected when programming the robot.\nTechnological Approach\nAnother approach is technological and included coding ethics in the operational \nsystem (Anderson et\u00a0 al., 2019; Milosevic, 2019), embedding ethics principles \nin the algorithm, and the collection of data of the AIHA (Abramoff et\u00a0al., 2020; \nRajkomar et\u00a0al., 2018), monitoring and evaluating the applications (Abramoff et\u00a0al., \n2020; Joerin et\u00a0al., 2020; Kortner, 2016; McCradden et\u00a0al., 2020; Rajkomar et\u00a0al., \n2018), and adopting a continuous improvement approach (Joerin et\u00a0al., 2020; Kort-\nner, 2016; McCradden et\u00a0al., 2020). User eXperience (UX) techniques were recom-\nmended in particular for respecting privacy and transparency (Joerin et\u00a0al., 2020; \nKretzschmar et\u00a0al., 2019).\nChecking and\u00a0Verifying\nChecklists have been developed to guide the dialogues between stakeholders \n(CPAIS, 2019; Stahl & Coeckelbergh, 2016), raise awareness about potential con-\nflicts of interest including funding (Morch et\u00a0al., 2019), and focus on the patient, \nthe system, the technical and the medical aspects of an AIHA (Beil et\u00a0al., 2019). \nAt the evaluation stage, a verification methodology of ethical compliance was rec-\nommended which involves case studies and experts\u2019 consultations (Anderson et\u00a0al., \n2019; Dantas et\u00a0al., 2017).\nOrganisational Checks and\u00a0Balances\nThe importance of the responsibility of the AIHA creators and the need for counsel-\nling through an advisory board was highlighted by authors from a private company \n(Joerin et\u00a0al., 2020).\nEvidence\u2011Based\nAnother recommendation was to follow an evidence-based introduction of a com-\nplex intervention (Carter et\u00a0 al., 2020). AI systems trained within a context can \n", []], "Being Contextual": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 12 of 53\n2018; Klein & Schl\u00f6mer, 2018; D. Peters et\u00a0 al., 2020; Poulsen et\u00a0 al., 2018; van \nRysewyk & Pontier, 2015). Another proposed way is to translate a professional code \nof conduct into capabilities (Cooney & Menezes, 2018).\nBeing Contextual\nFour articles stressed the importance of a deep understanding of the context of the \nAIHA (Ienca et\u00a0al., 2017; McBride, 2020; Sendak et\u00a0al., 2020; Van Wynsberghe, \n2016). van Wynsberghe gives poignant situations where a care robot\u2019s purpose is to \nprotect nurses from toxins when collecting urine samples from children undergoing \nchemotherapy or from injuries when lifting a patient off his/her bed. The rapport of \ncare between nurse and patient needs to be respected when programming the robot.\nTechnological Approach\nAnother approach is technological and included coding ethics in the operational \nsystem (Anderson et\u00a0 al., 2019; Milosevic, 2019), embedding ethics principles \nin the algorithm, and the collection of data of the AIHA (Abramoff et\u00a0al., 2020; \nRajkomar et\u00a0al., 2018), monitoring and evaluating the applications (Abramoff et\u00a0al., \n2020; Joerin et\u00a0al., 2020; Kortner, 2016; McCradden et\u00a0al., 2020; Rajkomar et\u00a0al., \n2018), and adopting a continuous improvement approach (Joerin et\u00a0al., 2020; Kort-\nner, 2016; McCradden et\u00a0al., 2020). User eXperience (UX) techniques were recom-\nmended in particular for respecting privacy and transparency (Joerin et\u00a0al., 2020; \nKretzschmar et\u00a0al., 2019).\nChecking and\u00a0Verifying\nChecklists have been developed to guide the dialogues between stakeholders \n(CPAIS, 2019; Stahl & Coeckelbergh, 2016), raise awareness about potential con-\nflicts of interest including funding (Morch et\u00a0al., 2019), and focus on the patient, \nthe system, the technical and the medical aspects of an AIHA (Beil et\u00a0al., 2019). \nAt the evaluation stage, a verification methodology of ethical compliance was rec-\nommended which involves case studies and experts\u2019 consultations (Anderson et\u00a0al., \n2019; Dantas et\u00a0al., 2017).\nOrganisational Checks and\u00a0Balances\nThe importance of the responsibility of the AIHA creators and the need for counsel-\nling through an advisory board was highlighted by authors from a private company \n(Joerin et\u00a0al., 2020).\nEvidence\u2011Based\nAnother recommendation was to follow an evidence-based introduction of a com-\nplex intervention (Carter et\u00a0 al., 2020). AI systems trained within a context can \n", []], "Being Proactive": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 11 of 53\u2003\n61\nnecessarily reproducible, and their uncertainty is difficult to quantify, it does not fit \ninto existing medical regulatory ethical settings (Abramoff et\u00a0al., 2020; Beil et\u00a0al., \n2019; Buston et\u00a0al., 2019; Milosevic, 2019).\nStrategies to\u00a0Implement an\u00a0Ethics Framework into\u00a0AIHA\nImplementation strategies from the surveyed articles covered the application lifecy-\ncle with the design stage the most represented (n\u2009=\u200927 or 82%), and commercialisa-\ntion the least represented (n\u2009=\u20096 or 18%) (see Fig.\u00a05). Only one strategy, which is tar-\ngeted at the technology developers and is a grey literature article, spanned the entire \nlifecycle (Buston et\u00a0 al., 2019). The implementation strategies included proactive, \ncontextual, technological, checklist, organisational and evidence-based approaches.\nBeing Proactive\nOne approach is to consider ethics at the design and requirement capture stage by \nembedding ethical values into the application using methodologies such as Value-\nSensitive Design (VSD) (Battistuzzi et\u00a0al., 2018; Cawthorne & Robbins-van Wyns-\nberghe, 2019, 2020; Ienca et\u00a0al., 2017; Ienca et\u00a0al., 2018; D. Peters et\u00a0al., 2020), \nCare-Centred Value-Sensitive-Design (CCVSD) (Van Wynsberghe, 2016), Values \nin Motion Design (Poulsen et\u00a0al., 2018) and Attentive Framework (Poulsen & Bur-\nmeister, 2019). The ethical values can be captured or validated by consultation of \nstakeholders (Battistuzzi et\u00a0al., 2018; Cawthorne & Robbins-van Wynsberghe, 2019, \n2020; Fukuzumi et\u00a0al., 2019; Garner et\u00a0al., 2016; Ienca et\u00a0al., 2017; Ienca et\u00a0al., \n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n1\n0\nRequirement \ncapture\nDesign\nDevelopment\nCommercialisa\u001fon\nProcurement\nTes\u001fng, QA\nDeployment\nEvalua\u001fon\nApplica\u001fon life-cycle stages\nFig.\u202f5\u2002 \u2009Coverage of the implementation of the ethics framework into the life cycle of an application per \narticle\n", []], "Strategies to\u00a0Implement an\u00a0Ethics Framework into\u00a0AIHA": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 11 of 53\u2003\n61\nnecessarily reproducible, and their uncertainty is difficult to quantify, it does not fit \ninto existing medical regulatory ethical settings (Abramoff et\u00a0al., 2020; Beil et\u00a0al., \n2019; Buston et\u00a0al., 2019; Milosevic, 2019).\nStrategies to\u00a0Implement an\u00a0Ethics Framework into\u00a0AIHA\nImplementation strategies from the surveyed articles covered the application lifecy-\ncle with the design stage the most represented (n\u2009=\u200927 or 82%), and commercialisa-\ntion the least represented (n\u2009=\u20096 or 18%) (see Fig.\u00a05). Only one strategy, which is tar-\ngeted at the technology developers and is a grey literature article, spanned the entire \nlifecycle (Buston et\u00a0 al., 2019). The implementation strategies included proactive, \ncontextual, technological, checklist, organisational and evidence-based approaches.\nBeing Proactive\nOne approach is to consider ethics at the design and requirement capture stage by \nembedding ethical values into the application using methodologies such as Value-\nSensitive Design (VSD) (Battistuzzi et\u00a0al., 2018; Cawthorne & Robbins-van Wyns-\nberghe, 2019, 2020; Ienca et\u00a0al., 2017; Ienca et\u00a0al., 2018; D. Peters et\u00a0al., 2020), \nCare-Centred Value-Sensitive-Design (CCVSD) (Van Wynsberghe, 2016), Values \nin Motion Design (Poulsen et\u00a0al., 2018) and Attentive Framework (Poulsen & Bur-\nmeister, 2019). The ethical values can be captured or validated by consultation of \nstakeholders (Battistuzzi et\u00a0al., 2018; Cawthorne & Robbins-van Wynsberghe, 2019, \n2020; Fukuzumi et\u00a0al., 2019; Garner et\u00a0al., 2016; Ienca et\u00a0al., 2017; Ienca et\u00a0al., \n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n1\n1\n0\n0\n1\n0\n1\n0\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n1\n0\n1\n1\n1\n0\n0\n0\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0\n1\n0\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n1\n1\n0\n1\n0\n0\n0\n0\n0\n1\n1\n1\n1\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n1\n1\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n1\n0\nRequirement \ncapture\nDesign\nDevelopment\nCommercialisa\u001fon\nProcurement\nTes\u001fng, QA\nDeployment\nEvalua\u001fon\nApplica\u001fon life-cycle stages\nFig.\u202f5\u2002 \u2009Coverage of the implementation of the ethics framework into the life cycle of an application per \narticle\n", []], "At a\u00a0Regulatory Level": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 10 of 53\ncompromises (Rajkomar et\u00a0 al., 2018). For example, optimal accuracy may trans-\nlate into unequal outcomes for a protected group because the recommended treat-\nment may be less effective due to lack of testing in this population (Rajkomar et\u00a0al., \n2018). The ranking of importance of each principle is context dependent as in the \ncase of the elderly who may feel socially isolated, care robots can provide social \nconnectedness to the detriment of privacy or independence (Poulsen & Burmeister, \n2019). Patients, caregivers and clinicians can have divergent relationships with the \nethics principles. While a caregiver would prioritise safety over privacy, a patient \nmay prefer to preserve his/her privacy over safety when bathing with a shower robot \nfor instance (Klein & Schl\u00f6mer, 2018).\nAt a\u00a0Design Level\nTranslating abstract principles into design requirements, features and functions of an \nAIHA creates conflict between technology and human values (Cawthorne & Rob-\nbins-van Wynsberghe, 2020). It is challenging for a machine to reproduce the com-\nplexity of human ethical decision making which is a desired feature when model-\nling an autonomous decision-making system whether it is part of a robot or software \n(van Rysewyk & Pontier, 2015).\nAt a\u00a0Technology Level\nDepending on its rigidity, the ethics framework implementation could impede \ncontinuous improvement or create blind spots to emergent issues (Cawthorne & \nRobbins-van Wynsberghe, 2020). Iterative processes built into an AIHA applica-\ntion, typical of AI models that keep learning to improve, can affect the relationship \nbetween the ethics principles (Battistuzzi et\u00a0al., 2018).\nAt an\u00a0Organisational Level\nImplementing a framework requires interdisciplinary collaboration among groups \nwith competing interests and disparate backgrounds ranging from clinicians, devel-\nopers, legal counsellors to board members and investors of a technology provider. \nDevelopers tend to reduce ethics to fairness causing tensions with clinicians who \nhave a broader scope for ethics (Abramoff et\u00a0 al., 2020). In the boardroom of an \nAIHA provider, there could be tensions between ethical, financial and legal consid-\nerations (Joerin et\u00a0al., 2020). In the context of a research project, funding and exper-\ntise silos can impede collaboration across disciplines (Stahl & Coeckelbergh, 2016).\nAt a\u00a0Regulatory Level\nRegulatory and legal matters can hinder the application of ethics principles as, for \nexample, seeking legal informed consent from every person entering in contact \nwith a robot could prove impractical (Kortner, 2016). Lack of regulations of AI \ntechnology, and conflicting national and international legislations can pose a chal-\nlenge (Dantas et\u00a0al., 2017; Garner et\u00a0al., 2016). Because outputs of an AIHA are not \n", []], "At an\u00a0Organisational Level": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 10 of 53\ncompromises (Rajkomar et\u00a0 al., 2018). For example, optimal accuracy may trans-\nlate into unequal outcomes for a protected group because the recommended treat-\nment may be less effective due to lack of testing in this population (Rajkomar et\u00a0al., \n2018). The ranking of importance of each principle is context dependent as in the \ncase of the elderly who may feel socially isolated, care robots can provide social \nconnectedness to the detriment of privacy or independence (Poulsen & Burmeister, \n2019). Patients, caregivers and clinicians can have divergent relationships with the \nethics principles. While a caregiver would prioritise safety over privacy, a patient \nmay prefer to preserve his/her privacy over safety when bathing with a shower robot \nfor instance (Klein & Schl\u00f6mer, 2018).\nAt a\u00a0Design Level\nTranslating abstract principles into design requirements, features and functions of an \nAIHA creates conflict between technology and human values (Cawthorne & Rob-\nbins-van Wynsberghe, 2020). It is challenging for a machine to reproduce the com-\nplexity of human ethical decision making which is a desired feature when model-\nling an autonomous decision-making system whether it is part of a robot or software \n(van Rysewyk & Pontier, 2015).\nAt a\u00a0Technology Level\nDepending on its rigidity, the ethics framework implementation could impede \ncontinuous improvement or create blind spots to emergent issues (Cawthorne & \nRobbins-van Wynsberghe, 2020). Iterative processes built into an AIHA applica-\ntion, typical of AI models that keep learning to improve, can affect the relationship \nbetween the ethics principles (Battistuzzi et\u00a0al., 2018).\nAt an\u00a0Organisational Level\nImplementing a framework requires interdisciplinary collaboration among groups \nwith competing interests and disparate backgrounds ranging from clinicians, devel-\nopers, legal counsellors to board members and investors of a technology provider. \nDevelopers tend to reduce ethics to fairness causing tensions with clinicians who \nhave a broader scope for ethics (Abramoff et\u00a0 al., 2020). In the boardroom of an \nAIHA provider, there could be tensions between ethical, financial and legal consid-\nerations (Joerin et\u00a0al., 2020). In the context of a research project, funding and exper-\ntise silos can impede collaboration across disciplines (Stahl & Coeckelbergh, 2016).\nAt a\u00a0Regulatory Level\nRegulatory and legal matters can hinder the application of ethics principles as, for \nexample, seeking legal informed consent from every person entering in contact \nwith a robot could prove impractical (Kortner, 2016). Lack of regulations of AI \ntechnology, and conflicting national and international legislations can pose a chal-\nlenge (Dantas et\u00a0al., 2017; Garner et\u00a0al., 2016). Because outputs of an AIHA are not \n", []], "At a\u00a0Technology Level": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 10 of 53\ncompromises (Rajkomar et\u00a0 al., 2018). For example, optimal accuracy may trans-\nlate into unequal outcomes for a protected group because the recommended treat-\nment may be less effective due to lack of testing in this population (Rajkomar et\u00a0al., \n2018). The ranking of importance of each principle is context dependent as in the \ncase of the elderly who may feel socially isolated, care robots can provide social \nconnectedness to the detriment of privacy or independence (Poulsen & Burmeister, \n2019). Patients, caregivers and clinicians can have divergent relationships with the \nethics principles. While a caregiver would prioritise safety over privacy, a patient \nmay prefer to preserve his/her privacy over safety when bathing with a shower robot \nfor instance (Klein & Schl\u00f6mer, 2018).\nAt a\u00a0Design Level\nTranslating abstract principles into design requirements, features and functions of an \nAIHA creates conflict between technology and human values (Cawthorne & Rob-\nbins-van Wynsberghe, 2020). It is challenging for a machine to reproduce the com-\nplexity of human ethical decision making which is a desired feature when model-\nling an autonomous decision-making system whether it is part of a robot or software \n(van Rysewyk & Pontier, 2015).\nAt a\u00a0Technology Level\nDepending on its rigidity, the ethics framework implementation could impede \ncontinuous improvement or create blind spots to emergent issues (Cawthorne & \nRobbins-van Wynsberghe, 2020). Iterative processes built into an AIHA applica-\ntion, typical of AI models that keep learning to improve, can affect the relationship \nbetween the ethics principles (Battistuzzi et\u00a0al., 2018).\nAt an\u00a0Organisational Level\nImplementing a framework requires interdisciplinary collaboration among groups \nwith competing interests and disparate backgrounds ranging from clinicians, devel-\nopers, legal counsellors to board members and investors of a technology provider. \nDevelopers tend to reduce ethics to fairness causing tensions with clinicians who \nhave a broader scope for ethics (Abramoff et\u00a0 al., 2020). In the boardroom of an \nAIHA provider, there could be tensions between ethical, financial and legal consid-\nerations (Joerin et\u00a0al., 2020). In the context of a research project, funding and exper-\ntise silos can impede collaboration across disciplines (Stahl & Coeckelbergh, 2016).\nAt a\u00a0Regulatory Level\nRegulatory and legal matters can hinder the application of ethics principles as, for \nexample, seeking legal informed consent from every person entering in contact \nwith a robot could prove impractical (Kortner, 2016). Lack of regulations of AI \ntechnology, and conflicting national and international legislations can pose a chal-\nlenge (Dantas et\u00a0al., 2017; Garner et\u00a0al., 2016). Because outputs of an AIHA are not \n", []], "At a\u00a0Design Level": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 10 of 53\ncompromises (Rajkomar et\u00a0 al., 2018). For example, optimal accuracy may trans-\nlate into unequal outcomes for a protected group because the recommended treat-\nment may be less effective due to lack of testing in this population (Rajkomar et\u00a0al., \n2018). The ranking of importance of each principle is context dependent as in the \ncase of the elderly who may feel socially isolated, care robots can provide social \nconnectedness to the detriment of privacy or independence (Poulsen & Burmeister, \n2019). Patients, caregivers and clinicians can have divergent relationships with the \nethics principles. While a caregiver would prioritise safety over privacy, a patient \nmay prefer to preserve his/her privacy over safety when bathing with a shower robot \nfor instance (Klein & Schl\u00f6mer, 2018).\nAt a\u00a0Design Level\nTranslating abstract principles into design requirements, features and functions of an \nAIHA creates conflict between technology and human values (Cawthorne & Rob-\nbins-van Wynsberghe, 2020). It is challenging for a machine to reproduce the com-\nplexity of human ethical decision making which is a desired feature when model-\nling an autonomous decision-making system whether it is part of a robot or software \n(van Rysewyk & Pontier, 2015).\nAt a\u00a0Technology Level\nDepending on its rigidity, the ethics framework implementation could impede \ncontinuous improvement or create blind spots to emergent issues (Cawthorne & \nRobbins-van Wynsberghe, 2020). Iterative processes built into an AIHA applica-\ntion, typical of AI models that keep learning to improve, can affect the relationship \nbetween the ethics principles (Battistuzzi et\u00a0al., 2018).\nAt an\u00a0Organisational Level\nImplementing a framework requires interdisciplinary collaboration among groups \nwith competing interests and disparate backgrounds ranging from clinicians, devel-\nopers, legal counsellors to board members and investors of a technology provider. \nDevelopers tend to reduce ethics to fairness causing tensions with clinicians who \nhave a broader scope for ethics (Abramoff et\u00a0 al., 2020). In the boardroom of an \nAIHA provider, there could be tensions between ethical, financial and legal consid-\nerations (Joerin et\u00a0al., 2020). In the context of a research project, funding and exper-\ntise silos can impede collaboration across disciplines (Stahl & Coeckelbergh, 2016).\nAt a\u00a0Regulatory Level\nRegulatory and legal matters can hinder the application of ethics principles as, for \nexample, seeking legal informed consent from every person entering in contact \nwith a robot could prove impractical (Kortner, 2016). Lack of regulations of AI \ntechnology, and conflicting national and international legislations can pose a chal-\nlenge (Dantas et\u00a0al., 2017; Garner et\u00a0al., 2016). Because outputs of an AIHA are not \n", []], "At the\u00a0Ethics Principles Level": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 9 of 53\u2003\n61\n2019). The list of ethics frameworks used in each article is available in \u201cAppendix \n4\u201d.\nThe process of categorising ethics principles laid out in the articles was not \nclear-cut, so we added an \u201cother\u201d category. Avoiding deception was reported for \n(n\u2009=\u20094) care robots shaped like animals or anthropomorphised to foster an emotional \nbonding (Battistuzzi et\u00a0al., 2018; Cooney & Menezes, 2018; Kortner, 2016; Stahl \n& Coeckelbergh, 2016). The principle of participation or social interconnected-\nness was evoked in four articles (Battistuzzi et\u00a0al., 2018; Hasenauer et\u00a0al., 2019; \nKlein & Schl\u00f6mer, 2018; Poulsen & Burmeister, 2019). For example, care robots for \nthe elderly as a robot can become an enabler or an inhibitor of the ethics principle. \nThree articles mentioned the principle of interdependence between caregiver and \npatient as in the respect for the rapport existing between a caregiver and a patient \n(Ienca et\u00a0al., 2018; Klein & Schl\u00f6mer, 2018), and the danger of having the attention \nof the caregiver focused on the care robot rather than the patient (McBride, 2020). \nTwo articles considered the impact of the AIHA on human skills, and human jobs \n(Cawthorne & Robbins-van Wynsberghe, 2020; Garner et\u00a0al., 2016), and one recom-\nmended that a human therapist should always be prioritised over the AIHA (Cooney \n& Menezes, 2018). Efficacy as in the number of patients without the disease cor-\nrectly identified as healthy was mentioned once (Channa et\u00a0al., 2020). One article \nspecified the relevance of an AIHA based on its context as part of ethics principles \nto follow (McCradden et\u00a0al., 2020). Minimisation of power imbalances including \nin decision making was emphasised in five articles (Battistuzzi et\u00a0al., 2018; Garner \net\u00a0al., 2016; Ienca et\u00a0al., 2017; Joerin et\u00a0al., 2020; Kortner, 2016). The transparency \nof the validation process and integration of the AIHA was cited once (Buston et\u00a0al., \n2019). One article included contestability, the ability to challenge a decision made \nby a CDSS in the set of principles (Milosevic, 2019).\nChallenges Relating to\u00a0the\u00a0Implementation of\u00a0an\u00a0Ethics Framework into\u00a0an\u00a0AIHA\nThe articles identified challenges relating to the implementation of ethics frame-\nworks. We grouped these into five levels: ethics principles, design, technology, \norganisational, and regulatory.\nAt the\u00a0Ethics Principles Level\nThe relationships between ethics principles can be at odds. For example, in CDSS \nbeneficence can compromise autonomy because of the lack of understanding of \nhow decisions are made or managing biases in datasets can oppose justice and non-\nmaleficence (Beil et\u00a0al., 2019). In the case of virtual bots for the elderly, trust may \nbe compromised to ensure safety by being intrusive, and potentially also compro-\nmising privacy (Garner et\u00a0 al., 2016). Because ML systems learn from historical \ndatasets, it embeds historical biases and disparities which means certain groups are \nfavoured by default while others need to be protected by design to ensure fairness for \nall. Fairness in ML is multi-dimensional and can be achieved by optimising equal-\nity of outcomes, accuracy and allocation of resources for each group which requires \n", []], "Challenges Relating to\u00a0the\u00a0Implementation of\u00a0an\u00a0Ethics Framework into\u00a0an\u00a0AIHA": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 9 of 53\u2003\n61\n2019). The list of ethics frameworks used in each article is available in \u201cAppendix \n4\u201d.\nThe process of categorising ethics principles laid out in the articles was not \nclear-cut, so we added an \u201cother\u201d category. Avoiding deception was reported for \n(n\u2009=\u20094) care robots shaped like animals or anthropomorphised to foster an emotional \nbonding (Battistuzzi et\u00a0al., 2018; Cooney & Menezes, 2018; Kortner, 2016; Stahl \n& Coeckelbergh, 2016). The principle of participation or social interconnected-\nness was evoked in four articles (Battistuzzi et\u00a0al., 2018; Hasenauer et\u00a0al., 2019; \nKlein & Schl\u00f6mer, 2018; Poulsen & Burmeister, 2019). For example, care robots for \nthe elderly as a robot can become an enabler or an inhibitor of the ethics principle. \nThree articles mentioned the principle of interdependence between caregiver and \npatient as in the respect for the rapport existing between a caregiver and a patient \n(Ienca et\u00a0al., 2018; Klein & Schl\u00f6mer, 2018), and the danger of having the attention \nof the caregiver focused on the care robot rather than the patient (McBride, 2020). \nTwo articles considered the impact of the AIHA on human skills, and human jobs \n(Cawthorne & Robbins-van Wynsberghe, 2020; Garner et\u00a0al., 2016), and one recom-\nmended that a human therapist should always be prioritised over the AIHA (Cooney \n& Menezes, 2018). Efficacy as in the number of patients without the disease cor-\nrectly identified as healthy was mentioned once (Channa et\u00a0al., 2020). One article \nspecified the relevance of an AIHA based on its context as part of ethics principles \nto follow (McCradden et\u00a0al., 2020). Minimisation of power imbalances including \nin decision making was emphasised in five articles (Battistuzzi et\u00a0al., 2018; Garner \net\u00a0al., 2016; Ienca et\u00a0al., 2017; Joerin et\u00a0al., 2020; Kortner, 2016). The transparency \nof the validation process and integration of the AIHA was cited once (Buston et\u00a0al., \n2019). One article included contestability, the ability to challenge a decision made \nby a CDSS in the set of principles (Milosevic, 2019).\nChallenges Relating to\u00a0the\u00a0Implementation of\u00a0an\u00a0Ethics Framework into\u00a0an\u00a0AIHA\nThe articles identified challenges relating to the implementation of ethics frame-\nworks. We grouped these into five levels: ethics principles, design, technology, \norganisational, and regulatory.\nAt the\u00a0Ethics Principles Level\nThe relationships between ethics principles can be at odds. For example, in CDSS \nbeneficence can compromise autonomy because of the lack of understanding of \nhow decisions are made or managing biases in datasets can oppose justice and non-\nmaleficence (Beil et\u00a0al., 2019). In the case of virtual bots for the elderly, trust may \nbe compromised to ensure safety by being intrusive, and potentially also compro-\nmising privacy (Garner et\u00a0 al., 2016). Because ML systems learn from historical \ndatasets, it embeds historical biases and disparities which means certain groups are \nfavoured by default while others need to be protected by design to ensure fairness for \nall. Fairness in ML is multi-dimensional and can be achieved by optimising equal-\nity of outcomes, accuracy and allocation of resources for each group which requires \n", []], "Ethics Frameworks that\u00a0Have Been Implemented andor\u00a0Are Recommended to\u00a0be Implemented in\u00a0AIHA": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 6 of 53\nJBI critical appraisal tools, selected depending on the type of article. Any disagree-\nment was resolved through discussion. The outcomes were used to map the quality \nof the literature only, not as an exclusion criterion because of the exploratory nature \nof the scoping review.\nData Processing and\u00a0Analysis\nA narrative synthesis was performed for this review due to the qualitative and het-\nerogeneity of the data. First, the characteristics of the applications were analysed \nto draw an overview of the data. Second, an analysis of the ethics frameworks was \nperformed using the principles upon which they relied. Lastly, the characteristics of \nthe operationalisation of the frameworks were analysed, compared and synthesised.\nResults\nThe combined search of databases and targeted websites yielded a total of 8054 \narticles, which after removal of duplicates was reduced to 4444 items. Out of the \n4444 title and abstract screened articles, 480 were flagged potentially relevant and \nunderwent a full-text review. 33 met the inclusion criteria. Figure\u00a02 illustrates the \nPRISMA diagram for the identification, screening, and inclusion processes.\n80% (n\u2009=\u200926) of the articles dated from 2018 onward. 16 articles were theoreti-\ncal, 17 featured actual applications among which 11 were about deployed applica-\ntions, and 6 were about prototypes. The types of application covered by the articles \nincluded generic Machine Learning (ML) (n\u2009=\u20093), Clinical Decisions Support Sys-\ntems (CDSS) (n\u2009=\u20098), drones (n\u2009=\u20092), Intelligent Assistive Technologies (IAT) which \ninclude generic IAT (n\u2009=\u20093), virtual bots (n\u2009=\u20096) and robots (n\u2009=\u200913) (Fig.\u00a03).\nQuality Assessment\nAmong the 33 articles, seven were appraised as qualitative studies, and 26 as text/\nopinion pieces (see \u201cAppendix 3\u201d). The qualitative studies varied in score, while \nthe opinion/text pieces scored highly. While the process is imperfectly suited to the \nnature of the articles, which do not follow traditional medical review protocols, it \nestablishes the credibility of the sources. Two articles were grey literature (Buston \net\u00a0al., 2019; CPAIS, 2019). Two articles had authors with corporate correspondents \n(Joerin et\u00a0al., 2020; Rajkomar et\u00a0al., 2018).\nEthics Frameworks that\u00a0Have Been Implemented and/or\u00a0Are Recommended to\u00a0be \nImplemented in\u00a0AIHA\nThe ethics principles categories identified by Jobin et\u00a0al. (2019) were used to qualify \nthe extent of coverage of each article\u2019s framework (see Fig.\u00a04). The four principles, \nautonomy, non-maleficence, beneficence and fairness have been classified as bioeth-\nics principles following the standard Beauchamp and Childress (2013)\u2019s framework \n", [89, 90]], "Quality Assessment": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 6 of 53\nJBI critical appraisal tools, selected depending on the type of article. Any disagree-\nment was resolved through discussion. The outcomes were used to map the quality \nof the literature only, not as an exclusion criterion because of the exploratory nature \nof the scoping review.\nData Processing and\u00a0Analysis\nA narrative synthesis was performed for this review due to the qualitative and het-\nerogeneity of the data. First, the characteristics of the applications were analysed \nto draw an overview of the data. Second, an analysis of the ethics frameworks was \nperformed using the principles upon which they relied. Lastly, the characteristics of \nthe operationalisation of the frameworks were analysed, compared and synthesised.\nResults\nThe combined search of databases and targeted websites yielded a total of 8054 \narticles, which after removal of duplicates was reduced to 4444 items. Out of the \n4444 title and abstract screened articles, 480 were flagged potentially relevant and \nunderwent a full-text review. 33 met the inclusion criteria. Figure\u00a02 illustrates the \nPRISMA diagram for the identification, screening, and inclusion processes.\n80% (n\u2009=\u200926) of the articles dated from 2018 onward. 16 articles were theoreti-\ncal, 17 featured actual applications among which 11 were about deployed applica-\ntions, and 6 were about prototypes. The types of application covered by the articles \nincluded generic Machine Learning (ML) (n\u2009=\u20093), Clinical Decisions Support Sys-\ntems (CDSS) (n\u2009=\u20098), drones (n\u2009=\u20092), Intelligent Assistive Technologies (IAT) which \ninclude generic IAT (n\u2009=\u20093), virtual bots (n\u2009=\u20096) and robots (n\u2009=\u200913) (Fig.\u00a03).\nQuality Assessment\nAmong the 33 articles, seven were appraised as qualitative studies, and 26 as text/\nopinion pieces (see \u201cAppendix 3\u201d). The qualitative studies varied in score, while \nthe opinion/text pieces scored highly. While the process is imperfectly suited to the \nnature of the articles, which do not follow traditional medical review protocols, it \nestablishes the credibility of the sources. Two articles were grey literature (Buston \net\u00a0al., 2019; CPAIS, 2019). Two articles had authors with corporate correspondents \n(Joerin et\u00a0al., 2020; Rajkomar et\u00a0al., 2018).\nEthics Frameworks that\u00a0Have Been Implemented and/or\u00a0Are Recommended to\u00a0be \nImplemented in\u00a0AIHA\nThe ethics principles categories identified by Jobin et\u00a0al. (2019) were used to qualify \nthe extent of coverage of each article\u2019s framework (see Fig.\u00a04). The four principles, \nautonomy, non-maleficence, beneficence and fairness have been classified as bioeth-\nics principles following the standard Beauchamp and Childress (2013)\u2019s framework \n", []], "Results": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 6 of 53\nJBI critical appraisal tools, selected depending on the type of article. Any disagree-\nment was resolved through discussion. The outcomes were used to map the quality \nof the literature only, not as an exclusion criterion because of the exploratory nature \nof the scoping review.\nData Processing and\u00a0Analysis\nA narrative synthesis was performed for this review due to the qualitative and het-\nerogeneity of the data. First, the characteristics of the applications were analysed \nto draw an overview of the data. Second, an analysis of the ethics frameworks was \nperformed using the principles upon which they relied. Lastly, the characteristics of \nthe operationalisation of the frameworks were analysed, compared and synthesised.\nResults\nThe combined search of databases and targeted websites yielded a total of 8054 \narticles, which after removal of duplicates was reduced to 4444 items. Out of the \n4444 title and abstract screened articles, 480 were flagged potentially relevant and \nunderwent a full-text review. 33 met the inclusion criteria. Figure\u00a02 illustrates the \nPRISMA diagram for the identification, screening, and inclusion processes.\n80% (n\u2009=\u200926) of the articles dated from 2018 onward. 16 articles were theoreti-\ncal, 17 featured actual applications among which 11 were about deployed applica-\ntions, and 6 were about prototypes. The types of application covered by the articles \nincluded generic Machine Learning (ML) (n\u2009=\u20093), Clinical Decisions Support Sys-\ntems (CDSS) (n\u2009=\u20098), drones (n\u2009=\u20092), Intelligent Assistive Technologies (IAT) which \ninclude generic IAT (n\u2009=\u20093), virtual bots (n\u2009=\u20096) and robots (n\u2009=\u200913) (Fig.\u00a03).\nQuality Assessment\nAmong the 33 articles, seven were appraised as qualitative studies, and 26 as text/\nopinion pieces (see \u201cAppendix 3\u201d). The qualitative studies varied in score, while \nthe opinion/text pieces scored highly. While the process is imperfectly suited to the \nnature of the articles, which do not follow traditional medical review protocols, it \nestablishes the credibility of the sources. Two articles were grey literature (Buston \net\u00a0al., 2019; CPAIS, 2019). Two articles had authors with corporate correspondents \n(Joerin et\u00a0al., 2020; Rajkomar et\u00a0al., 2018).\nEthics Frameworks that\u00a0Have Been Implemented and/or\u00a0Are Recommended to\u00a0be \nImplemented in\u00a0AIHA\nThe ethics principles categories identified by Jobin et\u00a0al. (2019) were used to qualify \nthe extent of coverage of each article\u2019s framework (see Fig.\u00a04). The four principles, \nautonomy, non-maleficence, beneficence and fairness have been classified as bioeth-\nics principles following the standard Beauchamp and Childress (2013)\u2019s framework \n", []], "Data Processing and\u00a0Analysis": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 6 of 53\nJBI critical appraisal tools, selected depending on the type of article. Any disagree-\nment was resolved through discussion. The outcomes were used to map the quality \nof the literature only, not as an exclusion criterion because of the exploratory nature \nof the scoping review.\nData Processing and\u00a0Analysis\nA narrative synthesis was performed for this review due to the qualitative and het-\nerogeneity of the data. First, the characteristics of the applications were analysed \nto draw an overview of the data. Second, an analysis of the ethics frameworks was \nperformed using the principles upon which they relied. Lastly, the characteristics of \nthe operationalisation of the frameworks were analysed, compared and synthesised.\nResults\nThe combined search of databases and targeted websites yielded a total of 8054 \narticles, which after removal of duplicates was reduced to 4444 items. Out of the \n4444 title and abstract screened articles, 480 were flagged potentially relevant and \nunderwent a full-text review. 33 met the inclusion criteria. Figure\u00a02 illustrates the \nPRISMA diagram for the identification, screening, and inclusion processes.\n80% (n\u2009=\u200926) of the articles dated from 2018 onward. 16 articles were theoreti-\ncal, 17 featured actual applications among which 11 were about deployed applica-\ntions, and 6 were about prototypes. The types of application covered by the articles \nincluded generic Machine Learning (ML) (n\u2009=\u20093), Clinical Decisions Support Sys-\ntems (CDSS) (n\u2009=\u20098), drones (n\u2009=\u20092), Intelligent Assistive Technologies (IAT) which \ninclude generic IAT (n\u2009=\u20093), virtual bots (n\u2009=\u20096) and robots (n\u2009=\u200913) (Fig.\u00a03).\nQuality Assessment\nAmong the 33 articles, seven were appraised as qualitative studies, and 26 as text/\nopinion pieces (see \u201cAppendix 3\u201d). The qualitative studies varied in score, while \nthe opinion/text pieces scored highly. While the process is imperfectly suited to the \nnature of the articles, which do not follow traditional medical review protocols, it \nestablishes the credibility of the sources. Two articles were grey literature (Buston \net\u00a0al., 2019; CPAIS, 2019). Two articles had authors with corporate correspondents \n(Joerin et\u00a0al., 2020; Rajkomar et\u00a0al., 2018).\nEthics Frameworks that\u00a0Have Been Implemented and/or\u00a0Are Recommended to\u00a0be \nImplemented in\u00a0AIHA\nThe ethics principles categories identified by Jobin et\u00a0al. (2019) were used to qualify \nthe extent of coverage of each article\u2019s framework (see Fig.\u00a04). The four principles, \nautonomy, non-maleficence, beneficence and fairness have been classified as bioeth-\nics principles following the standard Beauchamp and Childress (2013)\u2019s framework \n", []], "Data Extraction and\u00a0Quality Assessment": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 5 of 53\u2003\n61\nmeeting transcripts, education programmes, slides, or conference proceedings; (2) \nthey were not healthcare specific, they did not mention any healthcare application \nor case study, or healthcare was mentioned only as an industry among others; (3) \nthey did not feature any AI applications at all, or featured AI applications with no \nMachine Learning such as rule-based inference engines for example, or discussed \nAI as a technology among others, or discussed data only; (4) they mentioned ethics \nas only a review by an ethics committee, or featured technology as a solution to an \nethical problem such as the usage of AI for data privacy preservation, or discussed \nonly ethical challenges without specifying any strategy, or discussed data ethics \nonly, or featured no operationalisation of an ethics framework.\nData Extraction and\u00a0Quality Assessment\nPairs of reviewers independently extracted the data of the eligible articles based on a \ndata extraction form developed for this purpose. A pilot of the data extraction form \nwas conducted for usability prior to the full extraction. The data extracted were com-\nprised of authors, title, information about the AI-based application, the characteris-\ntics of the ethics framework, the strategies for implementing the ethics framework \nand the challenges and their resolution if any, the assessment of the ethics frame-\nwork implementation if any, and the type of publication (see Fig.\u00a01). The quality \nassessment of the articles was performed by pairs of independent reviewers using \nFig.\u202f1\u2002 \u2009Data capture model\n", [44]], "Study Selection": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 4 of 53\ndiscussion is an AI-based application in healthcare, (2) the AI-based application \nincludes an ML component, (3) the discussion includes ethical considerations and \nhow to implement or operationalise them into the AI-based application. Because AI \ntechnology evolves rapidly, we chose to limit our search to the past five years.\nSearch Strategy\nBecause of the cross-disciplinary nature of the review and the rapid pace of change \nin AI, we adopted a search strategy encompassing traditional medical publication \ndatabases and grey literature. The medical publications databases were Ovid MED-\nLINE, Ovid Embase, Scopus and Web of Science. Databases were queried on 16 \nJuly 2020. Databases were searched using medical subject headings and keywords \nrepresenting the two main concepts of technology and ethics. The search strategy \nwas developed in consultation with a research librarian and devised to capture intelli-\ngent assistive and decision support technologies based on machine learning. The full \nsearch strategy for all databases is available in \u201cAppendix 2\u201d. For the grey literature, \nwe used scienceresearch.com, a deep web technology search engine, as well as a tar-\ngeted hand search of websites of technology providers, think tanks, and government \nbodies involved with AI-based applications for healthcare. The latter were selected \nfrom a list of organisations that released an AI ethics framework (Jobin et\u00a0al., 2019) \nand had activities related to healthcare regardless of their geographic location. Key \nplayers such as Verily or McKinsey & Company, who did not appear in Jobin et\u00a0al. \n(2019)\u2019s list were also part of the search (full list available in \u201cAppendix 1\u201d). Web-\nsites were searched between 9 and 15 July 2020. Websites were searched using the \nlocal search function and a combination of the following keywords: \u201cArtificial Intel-\nligence\u201d, \u201cethics\u201d, and \u201chealthcare\u201d as systematically as possible within the capa-\nbilities of each website. When the search returned a large number of results (i.e., \nmore than two pages of results), the first 20 items at a minimum were screened for \nrelevance and recorded until a decline of relevance was observed (i.e., characterised \nby items becoming unrelated to healthcare). When a local search was not available \nbut there was a list of publications available on the website, a search using the same \nkeywords within each publication was performed to select or reject them. A log of \nthe search strategies and results were kept in an Excel (MS Excel for Mac version \n16, Microsoft, Seattle, WA) spreadsheet.\nStudy Selection\nThe search results from the databases were uploaded into EndNote citation manage-\nment software (version X9.3; Thompson Reuters, New York, NY), and duplicates \nwere removed. Then all references including grey literature results were uploaded \ninto Rayyan web application software (Ouzzani et\u00a0al., 2016) and duplicates further \nremoved. The title and abstract of each article were screened by pairs of independent \nreviewers for inclusion based on pre-established criteria. Inclusion conflicts between \nreviewers were resolved through discussion. Articles were excluded if (1) they \nwere press releases, promotional in nature, biographies, videos, podcasts, speech or \n", [44]], "Search Strategy": ["\t\nM. Goirand et\u00a0al.\n1 3\n61\u2003\nPage 4 of 53\ndiscussion is an AI-based application in healthcare, (2) the AI-based application \nincludes an ML component, (3) the discussion includes ethical considerations and \nhow to implement or operationalise them into the AI-based application. Because AI \ntechnology evolves rapidly, we chose to limit our search to the past five years.\nSearch Strategy\nBecause of the cross-disciplinary nature of the review and the rapid pace of change \nin AI, we adopted a search strategy encompassing traditional medical publication \ndatabases and grey literature. The medical publications databases were Ovid MED-\nLINE, Ovid Embase, Scopus and Web of Science. Databases were queried on 16 \nJuly 2020. Databases were searched using medical subject headings and keywords \nrepresenting the two main concepts of technology and ethics. The search strategy \nwas developed in consultation with a research librarian and devised to capture intelli-\ngent assistive and decision support technologies based on machine learning. The full \nsearch strategy for all databases is available in \u201cAppendix 2\u201d. For the grey literature, \nwe used scienceresearch.com, a deep web technology search engine, as well as a tar-\ngeted hand search of websites of technology providers, think tanks, and government \nbodies involved with AI-based applications for healthcare. The latter were selected \nfrom a list of organisations that released an AI ethics framework (Jobin et\u00a0al., 2019) \nand had activities related to healthcare regardless of their geographic location. Key \nplayers such as Verily or McKinsey & Company, who did not appear in Jobin et\u00a0al. \n(2019)\u2019s list were also part of the search (full list available in \u201cAppendix 1\u201d). Web-\nsites were searched between 9 and 15 July 2020. Websites were searched using the \nlocal search function and a combination of the following keywords: \u201cArtificial Intel-\nligence\u201d, \u201cethics\u201d, and \u201chealthcare\u201d as systematically as possible within the capa-\nbilities of each website. When the search returned a large number of results (i.e., \nmore than two pages of results), the first 20 items at a minimum were screened for \nrelevance and recorded until a decline of relevance was observed (i.e., characterised \nby items becoming unrelated to healthcare). When a local search was not available \nbut there was a list of publications available on the website, a search using the same \nkeywords within each publication was performed to select or reject them. A log of \nthe search strategies and results were kept in an Excel (MS Excel for Mac version \n16, Microsoft, Seattle, WA) spreadsheet.\nStudy Selection\nThe search results from the databases were uploaded into EndNote citation manage-\nment software (version X9.3; Thompson Reuters, New York, NY), and duplicates \nwere removed. Then all references including grey literature results were uploaded \ninto Rayyan web application software (Ouzzani et\u00a0al., 2016) and duplicates further \nremoved. The title and abstract of each article were screened by pairs of independent \nreviewers for inclusion based on pre-established criteria. Inclusion conflicts between \nreviewers were resolved through discussion. Articles were excluded if (1) they \nwere press releases, promotional in nature, biographies, videos, podcasts, speech or \n", []], "Inclusion Criteria": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 3 of 53\u2003\n61\ndevelopment of the application (Jobin et\u00a0 al., 2019). Consequently, concerns have \nbeen raised about the integrity of the translation of ethics guidelines and frameworks \nduring implementation (Metzinger, 2019; Morley et\u00a0 al., 2020). In the context of \nhealthcare, it is not yet clear what ethics frameworks have been adopted in AI devel-\nopment, how they have been implemented, and how successful these implementa-\ntions have been. Gaining an understanding of the strategies and challenges will help \ngain clarity about the operationalisation of AIHA ethics frameworks.\nIn this review, we sought to survey how AI ethics frameworks have been imple-\nmented and evaluated in healthcare applications, and map the scope, challenges and \npractices of these initiatives. This scoping review aimed to examine the implementa-\ntion of ethics frameworks in AI applications in healthcare. The questions addressed \nby the review were:\n1.\t What ethics frameworks have been implemented in AI applications in healthcare?\n2.\t What\u00a0ethics frameworks\u00a0does the existing literature report should be\u00a0imple-\nmented\u00a0(but have not yet been implemented)\u00a0in AI applications in healthcare?\n3.\t What are the challenges identified in the literature relating to the implementation \nof an ethics framework into an AI application in healthcare?\n4.\t What strategies have been used to implement an ethics framework into AI applica-\ntions in healthcare?\n5.\t How have implementations of ethics frameworks been evaluated?\nMethod\nWe conducted a systematic scoping review of the peer-reviewed and grey literature \nrelated to the implementation of ethics frameworks in AI applications in healthcare \npublished between 2015 and 2020. The protocol was prospectively registered in July \n2020 in the Open Science Framework (osf.io/cb8nv). This scoping review followed \nthe Joanna Briggs Institute\u2019s (JBI) guidance for systematic scoping reviews (Peters \net\u00a0 al., 2015), and the preferred reporting items for systematic reviews and meta-\nanalyses statement (PRISMA) (Moher et\u00a0al., 2009). The process is designed to be \nreproducible. It consists of four steps: identification, screening, eligibility and data \ncapture. The identification step consists in collecting the articles as described in the \nsearch strategy. During screening, articles are excluded based on information from \ntheir abstracts and keywords. The full texts of the articles remaining after the first \nscreening are further scanned and if they meet the eligibility criteria are selected for \nthe data capture. The data capture consists in extracting information from each arti-\ncle based on a pre-defined template.\nInclusion Criteria\nPeer-reviewed articles, as well as grey literature articles in English and French \nlanguage (languages spoken by the researchers) published between 2015 and \n2020, were considered when meeting the following criteria: (1) the context of the \n", []], "Method": ["1 3\nImplementing Ethics in Healthcare AI-Based Applications: A\u2026\nPage 3 of 53\u2003\n61\ndevelopment of the application (Jobin et\u00a0 al., 2019). Consequently, concerns have \nbeen raised about the integrity of the translation of ethics guidelines and frameworks \nduring implementation (Metzinger, 2019; Morley et\u00a0 al., 2020). In the context of \nhealthcare, it is not yet clear what ethics frameworks have been adopted in AI devel-\nopment, how they have been implemented, and how successful these implementa-\ntions have been. Gaining an understanding of the strategies and challenges will help \ngain clarity about the operationalisation of AIHA ethics frameworks.\nIn this review, we sought to survey how AI ethics frameworks have been imple-\nmented and evaluated in healthcare applications, and map the scope, challenges and \npractices of these initiatives. This scoping review aimed to examine the implementa-\ntion of ethics frameworks in AI applications in healthcare. The questions addressed \nby the review were:\n1.\t What ethics frameworks have been implemented in AI applications in healthcare?\n2.\t What\u00a0ethics frameworks\u00a0does the existing literature report should be\u00a0imple-\nmented\u00a0(but have not yet been implemented)\u00a0in AI applications in healthcare?\n3.\t What are the challenges identified in the literature relating to the implementation \nof an ethics framework into an AI application in healthcare?\n4.\t What strategies have been used to implement an ethics framework into AI applica-\ntions in healthcare?\n5.\t How have implementations of ethics frameworks been evaluated?\nMethod\nWe conducted a systematic scoping review of the peer-reviewed and grey literature \nrelated to the implementation of ethics frameworks in AI applications in healthcare \npublished between 2015 and 2020. The protocol was prospectively registered in July \n2020 in the Open Science Framework (osf.io/cb8nv). This scoping review followed \nthe Joanna Briggs Institute\u2019s (JBI) guidance for systematic scoping reviews (Peters \net\u00a0 al., 2015), and the preferred reporting items for systematic reviews and meta-\nanalyses statement (PRISMA) (Moher et\u00a0al., 2009). The process is designed to be \nreproducible. It consists of four steps: identification, screening, eligibility and data \ncapture. The identification step consists in collecting the articles as described in the \nsearch strategy. During screening, articles are excluded based on information from \ntheir abstracts and keywords. The full texts of the articles remaining after the first \nscreening are further scanned and if they meet the eligibility criteria are selected for \nthe data capture. The data capture consists in extracting information from each arti-\ncle based on a pre-defined template.\nInclusion Criteria\nPeer-reviewed articles, as well as grey literature articles in English and French \nlanguage (languages spoken by the researchers) published between 2015 and \n2020, were considered when meeting the following criteria: (1) the context of the \n", []], "Introduction": ["Vol.:(0123456789)\nScience and Engineering Ethics (2021) 27:61\nhttps://doi.org/10.1007/s11948-021-00336-3\n1 3\nORIGINAL RESEARCH/SCHOLARSHIP\nImplementing Ethics in\u00a0Healthcare AI\u2011Based Applications: \nA\u00a0Scoping Review\nMagali\u00a0Goirand1\u200a \u00a0\u00b7 Elizabeth\u00a0Austin1\u200a \u00a0\u00b7 Robyn\u00a0Clay\u2011Williams1\u200a\nReceived: 21 January 2021 / Accepted: 4 August 2021 / Published online: 3 September 2021 \n\u00a9 The Author(s), under exclusive licence to Springer Nature B.V. 2021\nAbstract\nA number of Artificial Intelligence (AI) ethics frameworks have been published in \nthe last 6\u00a0years in response to the growing concerns posed by the adoption of AI \nin different sectors, including healthcare. While there is a strong culture of medi-\ncal ethics in healthcare applications, AI-based Healthcare Applications (AIHA) \nare challenging the existing ethics and regulatory frameworks. This scoping review \nexplores how ethics frameworks have been implemented in AIHA, how these imple-\nmentations have been evaluated and whether they have been successful. AI spe-\ncific ethics frameworks in healthcare appear to have a limited adoption and they are \nmostly used in conjunction with other ethics frameworks. The operationalisation of \nethics frameworks is a complex endeavour with challenges at different levels: ethics \nprinciples, design, technology, organisational, and regulatory. Strategies identified in \nthis review are proactive, contextual, technological, checklist, organisational and/or \nevidence-based approaches. While interdisciplinary approaches show promises, how \nan ethics framework is implemented in an AI-based Healthcare Application is not \nwidely reported, and there is a need for transparency for trustworthy AI.\nKeywords\u2002 Ethics\u00a0\u00b7 AI\u00a0\u00b7 Healthcare\u00a0\u00b7 Machine learning\u00a0\u00b7 Bioethics\u00a0\u00b7 Care robots\u00a0\u00b7 \nCDSS\u00a0\u00b7 IAT\nIntroduction\nArtificial Intelligence (AI) emerged in 1955 and is recognised as a technology \nthat can profoundly reshape healthcare and society at large (Crawford et\u00a0al., 2019; \nFloridi & Cowls, 2019). While many definitions for AI have come into use, the \n1955 definition \u201cthe science of making machines do things that would require intel-\nligence if done by people\u201d is still relevant for healthcare (Joshi & Morley, 2019, p. \n15). Because of the nature of AI technology, large high-tech companies are entering \n *\t Magali Goirand \n\t\nmagali.goirandampaire@hdr.mq.edu.au\n1\t\nAustralian Institute of\u00a0Health Innovation, Macquarie University, Sydney, Australia\n", []], "Abstract": ["Vol.:(0123456789)\nScience and Engineering Ethics (2021) 27:61\nhttps://doi.org/10.1007/s11948-021-00336-3\n1 3\nORIGINAL RESEARCH/SCHOLARSHIP\nImplementing Ethics in\u00a0Healthcare AI\u2011Based Applications: \nA\u00a0Scoping Review\nMagali\u00a0Goirand1\u200a \u00a0\u00b7 Elizabeth\u00a0Austin1\u200a \u00a0\u00b7 Robyn\u00a0Clay\u2011Williams1\u200a\nReceived: 21 January 2021 / Accepted: 4 August 2021 / Published online: 3 September 2021 \n\u00a9 The Author(s), under exclusive licence to Springer Nature B.V. 2021\nAbstract\nA number of Artificial Intelligence (AI) ethics frameworks have been published in \nthe last 6\u00a0years in response to the growing concerns posed by the adoption of AI \nin different sectors, including healthcare. While there is a strong culture of medi-\ncal ethics in healthcare applications, AI-based Healthcare Applications (AIHA) \nare challenging the existing ethics and regulatory frameworks. This scoping review \nexplores how ethics frameworks have been implemented in AIHA, how these imple-\nmentations have been evaluated and whether they have been successful. AI spe-\ncific ethics frameworks in healthcare appear to have a limited adoption and they are \nmostly used in conjunction with other ethics frameworks. The operationalisation of \nethics frameworks is a complex endeavour with challenges at different levels: ethics \nprinciples, design, technology, organisational, and regulatory. Strategies identified in \nthis review are proactive, contextual, technological, checklist, organisational and/or \nevidence-based approaches. While interdisciplinary approaches show promises, how \nan ethics framework is implemented in an AI-based Healthcare Application is not \nwidely reported, and there is a need for transparency for trustworthy AI.\nKeywords\u2002 Ethics\u00a0\u00b7 AI\u00a0\u00b7 Healthcare\u00a0\u00b7 Machine learning\u00a0\u00b7 Bioethics\u00a0\u00b7 Care robots\u00a0\u00b7 \nCDSS\u00a0\u00b7 IAT\nIntroduction\nArtificial Intelligence (AI) emerged in 1955 and is recognised as a technology \nthat can profoundly reshape healthcare and society at large (Crawford et\u00a0al., 2019; \nFloridi & Cowls, 2019). While many definitions for AI have come into use, the \n1955 definition \u201cthe science of making machines do things that would require intel-\nligence if done by people\u201d is still relevant for healthcare (Joshi & Morley, 2019, p. \n15). Because of the nature of AI technology, large high-tech companies are entering \n *\t Magali Goirand \n\t\nmagali.goirandampaire@hdr.mq.edu.au\n1\t\nAustralian Institute of\u00a0Health Innovation, Macquarie University, Sydney, Australia\n", []], "Implementing Ethics in\u00a0Healthcare AI-Based Applications: A\u00a0Scoping Review": ["Vol.:(0123456789)\nScience and Engineering Ethics (2021) 27:61\nhttps://doi.org/10.1007/s11948-021-00336-3\n1 3\nORIGINAL RESEARCH/SCHOLARSHIP\nImplementing Ethics in\u00a0Healthcare AI\u2011Based Applications: \nA\u00a0Scoping Review\nMagali\u00a0Goirand1\u200a \u00a0\u00b7 Elizabeth\u00a0Austin1\u200a \u00a0\u00b7 Robyn\u00a0Clay\u2011Williams1\u200a\nReceived: 21 January 2021 / Accepted: 4 August 2021 / Published online: 3 September 2021 \n\u00a9 The Author(s), under exclusive licence to Springer Nature B.V. 2021\nAbstract\nA number of Artificial Intelligence (AI) ethics frameworks have been published in \nthe last 6\u00a0years in response to the growing concerns posed by the adoption of AI \nin different sectors, including healthcare. While there is a strong culture of medi-\ncal ethics in healthcare applications, AI-based Healthcare Applications (AIHA) \nare challenging the existing ethics and regulatory frameworks. This scoping review \nexplores how ethics frameworks have been implemented in AIHA, how these imple-\nmentations have been evaluated and whether they have been successful. AI spe-\ncific ethics frameworks in healthcare appear to have a limited adoption and they are \nmostly used in conjunction with other ethics frameworks. The operationalisation of \nethics frameworks is a complex endeavour with challenges at different levels: ethics \nprinciples, design, technology, organisational, and regulatory. Strategies identified in \nthis review are proactive, contextual, technological, checklist, organisational and/or \nevidence-based approaches. While interdisciplinary approaches show promises, how \nan ethics framework is implemented in an AI-based Healthcare Application is not \nwidely reported, and there is a need for transparency for trustworthy AI.\nKeywords\u2002 Ethics\u00a0\u00b7 AI\u00a0\u00b7 Healthcare\u00a0\u00b7 Machine learning\u00a0\u00b7 Bioethics\u00a0\u00b7 Care robots\u00a0\u00b7 \nCDSS\u00a0\u00b7 IAT\nIntroduction\nArtificial Intelligence (AI) emerged in 1955 and is recognised as a technology \nthat can profoundly reshape healthcare and society at large (Crawford et\u00a0al., 2019; \nFloridi & Cowls, 2019). While many definitions for AI have come into use, the \n1955 definition \u201cthe science of making machines do things that would require intel-\nligence if done by people\u201d is still relevant for healthcare (Joshi & Morley, 2019, p. \n15). Because of the nature of AI technology, large high-tech companies are entering \n *\t Magali Goirand \n\t\nmagali.goirandampaire@hdr.mq.edu.au\n1\t\nAustralian Institute of\u00a0Health Innovation, Macquarie University, Sydney, Australia\n", []]}