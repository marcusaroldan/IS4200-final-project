{"H Negative Results": ["Published as a conference paper at ICLR 2020\nwhich means\npdata(x|c) = D(x, c)pG(x|c)/(a(1 \u2212D(x, c)) + pG(x|c))\nwhere a = (1 \u2212pmask)/pmask is the number of unmasked tokens for every masked token.\nWe can use this expression to evaluate ELECTRA as a masked language model by selecting\nargmaxx\u2208vocabD(x, c)pG(x|c)/(a(1 \u2212D(x, c)) + pG(x|c)) as the model\u2019s prediction for a given\ncontext. In practice, selecting over the whole vocabulary is very expensive, so we instead take\nthe argmax over the top 100 predictions from the generator.10 Using this method, we compared\nELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT\nslightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It\nis possible that the assumption of an optimal discriminator, which is certainly far from correct, is\nharming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surpris-\ning that a model like BERT that is trained speci\ufb01cally for generation performs better at generation\nwhile a model with a discriminative objective like ELECTRA is better at being \ufb01ne-tuned on dis-\ncriminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an\ninteresting way to uncover more about the differences between ELECTRA and BERT encoders in\nfuture work.\nH\nNEGATIVE RESULTS\nWe brie\ufb02y describe a few ideas that did not look promising in our initial experiments:\n\u2022 We initially attempted to make BERT more ef\ufb01cient by strategically masking-out tokens\n(e.g., masking our rarer tokens more frequently, or training a model to guess which tokens\nBERT would struggle to predict if they were masked out). This resulted in fairly minor\nspeedups over regular BERT.\n\u2022 Given that ELECTRA seemed to bene\ufb01t (up to a certain point) from having a weaker gener-\nator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax\nor disallowing the generator from sampling the correct token. Neither of these improved\nresults.\n\u2022 We tried adding a sentence-level contrastive objective. For this task, we kept 20% of input\nsentences unchanged rather than noising them with the generator. We then added a predic-\ntion head to the model that predicted if the entire input was corrupted or not. Surprisingly,\nthis slightly decreased scores on downstream tasks.\n10For ELECTRA-Base, this means the upper-bound for accuracy is around 95%.\n18\n", []], "G Evaluating ELECTRA as a Masked Language Model": ["Published as a conference paper at ICLR 2020\ntokens {xi : i \u0338\u2208m}, i.e., it does not depend on other generated tokens {\u02c6xi : i \u2208m \u2227i \u0338= t}.\nThis isn\u2019t too bad of an assumption because a relatively small number of tokens are replaced, and\nit greatly simpli\ufb01es credit assignment when using reinforcement learning. Notationally, we show\nthis assumption by (in a slight abuse of notation) by writing D(\u02c6xt|xmasked) for the discriminator\npredicting whether the generated token \u02c6xt equals the original token xt given the masked context\nxmasked. A useful consequence of this assumption is that the discriminator score for non-replaced\ntokens (D(xt|xmasked) for t \u0338\u2208m) is independent of pG because we are assuming it does not depend\non any replaced token. Therefore these tokens can be ignored when training G to maximize LDisc.\nDuring training we seek to \ufb01nd\narg max\n\u03b8G\nLDisc = arg max\n\u03b8G\nE\nx,m,\u02c6x\n \nn\nX\nt=1\n\u22121(xcorrupt\nt\n= xt) log D(xcorrupt, t)\u2212\n1(xcorrupt\nt\n\u0338= xt) log(1 \u2212D(xcorrupt, t))\n!\nUsing the simplifying assumption, we approximate the above by \ufb01nding the argmax of\nE\nx,m,\u02c6x\n X\nt\u2208m\n\u22121(\u02c6xt = xt) log D(\u02c6x|xmasked) \u22121(\u02c6xt \u0338= xt) log(1 \u2212D(\u02c6x|xmasked))\n!\n= E\nx,m\nX\nt\u2208m\nE\n\u02c6xt\u223cpG\nR(\u02c6xt, x)\nwhere R(\u02c6xt, x) =\n\u001a\u2212log D(\u02c6xt|xmasked)\nif \u02c6xt = xt\n\u2212log(1 \u2212D(\u02c6xt|xmasked))\notherwise\nIn short, the simplifying assumption allows us to decompose the loss over the individual generated\ntokens. We cannot directly \ufb01nd arg max\u03b8G using gradient ascent because it is impossible to back-\npropagate through discrete sampling of \u02c6x. Instead, we use policy gradient reinforcement learning\n(Williams, 1992). In particular, we use the REINFORCE gradient\n\u2207\u03b8GLDisc \u2248E\nx,m\nX\nt\u2208m\nE\n\u02c6xt\u223cpG\n\u2207\u03b8g log pG(\u02c6xt|xmasked)[R(\u02c6xt, x) \u2212b(xmasked, t)]\nWhere b is a learned baseline implemented as b(xmasked, t) = \u2212log sigmoid(wT hG(xmasked)t)\nwhere hG(xmasked) are the outputs of the generator\u2019s Transformer encoder. The baseline is trained\nwith cross-entropy loss to match the reward for the corresponding position. We approximate the\nexpectations with a single sample and learn \u03b8G with gradient ascent. Despite receiving no explicit\nfeedback about which generated tokens are correct, we found the adversarial training resulted in\na fairly accurate generator (for a 256-hidden-size generator, the adversarially trained one achieves\n58% accuracy at masked language modeling while the same sized MLE generator gets 65%). How-\never, using this generator did not improve over the MLE-trained one on downstream tasks (see the\nright of Figure 3 in the main paper).\nG\nEVALUATING ELECTRA AS A MASKED LANGUAGE MODEL\nThis sections details some initial experiments in evaluating ELECTRA as a masked language model.\nUsing slightly different notation from the main paper, given a context c consisting of a text sequence\nwith one token x masked-out, the discriminator loss can be written as\nLDisc = \u2212\nX\nx\u2208vocab\n\u0010\n(1 \u2212pmask)pdata(x|c) log D(x, c) +\n//unmasked token\npmaskpdata(x|c)pG(x|c) log D(x, c) +\n//generator samples correct token\npmask(1 \u2212pdata(x|c))pG(x|c) log(1 \u2212D(x, c))\n\u0011\n//generator samples incorrect token\nFinding the critical points of this loss with respect to D shows that for a \ufb01xed generator the optimal\ndiscriminator is\nD(x, c) = pdata(x|c)(a + pG(x|c))/(apdata(x|c) + pG(x|c))\n17\n", []], "F Adversarial Training": ["Published as a conference paper at ICLR 2020\nModel\nTrain FLOPs\nParams CoLA SST MRPC STS QQP MNLI QNLI RTE Avg.\nTinyBERT\n6.4e19+ (45x+) 14.5M\n51.1\n93.1 82.6\n83.7 89.1\n84.6\n90.4\n70.0 80.6\nMobileBERT\n6.4e19+ (45x+) 25.3M\n51.1\n92.6 84.5\n84.8 88.3\n84.3\n91.6\n70.4 81.0\nGPT\n4.0e19 (29x)\n117M\n45.4\n91.3 75.7\n80.0 88.5\n82.1\n88.1\n56.0 75.9\nBERT-Base\n6.4e19 (45x)\n110M\n52.1\n93.5 84.8\n85.8 89.2\n84.6\n90.5\n66.4 80.9\nBERT-Large\n1.9e20 (135x)\n335M\n60.5\n94.9 85.4\n86.5 89.3\n86.7\n92.7\n70.1 83.3\nSpanBERT\n7.1e20 (507x)\n335M\n64.3\n94.8 87.9\n89.9 89.5\n87.7\n94.3\n79.0 85.9\nELECTRA-Small\n1.4e18 (1x)\n14M\n54.6\n89.1 83.7\n80.3 88.0\n79.7\n87.7\n60.8 78.0\nELECTRA-Small++ 3.3e19 (18x)\n14M\n55.6\n91.1 84.9\n84.6 88.0\n81.6\n88.3\n63.6 79.7\nELECTRA-Base\n6.4e19 (45x)\n110M\n59.7\n93.4 86.7\n87.7 89.1\n85.8\n92.7\n73.1 83.5\nELECTRA-Base++\n3.3e20 (182x)\n110M\n64.6\n96.0 88.1\n90.2 89.5\n88.5\n93.1\n75.2 85.7\nELECTRA-1.75M\n3.1e21 (2200x) 330M\n68.1\n96.7 89.2\n91.7 90.4\n90.7\n95.5\n86.1 88.6\nTable 8: Results for models on the GLUE test set. Only models with single-task \ufb01netuning (no\nensembling, task-speci\ufb01c tricks, etc.) are shown.\nE\nCOUNTING FLOPS\nWe chose to measure compute usage in terms of \ufb02oating point operations (FLOPs) because it is a\nmeasure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth not-\ning that in some cases abstracting away hardware details is a drawback because hardware-centered\noptimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019)\ngets by tying weights and thus reducing communication overhead between TPU workers. We used\nTensorFlow\u2019s FLOP-counting capabilities9 and checked the results with by-hand computation. We\nmade the following assumptions:\n\u2022 An \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an\nexp is one op like an add, even though in practice the exp might be slower. We believe\nthis assumption does not substantially change compute estimates because matrix-multiplies\ndominate the compute for most models. Similarly, we count matrix-multiplies as 2 \u2217m \u2217n\nFLOPs instead of m \u2217n as one might if considering fused multiply-add operations.\n\u2022 The backwards pass takes the same number of FLOPs as the forward pass. This assumption\nis not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but\nimportantly, the forward/backward pass FLOPs really are the same for matrix-multiplies,\nwhich is most of the compute anyway.\n\u2022 We assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac-\ntice, sparse embedding lookups are much slower than constant time; on some hardware\naccelerators dense operations are actually faster than sparse lookups.\nF\nADVERSARIAL TRAINING\nHere we detail attempts to adversarially train the generator instead of using maximum likelihood. In\nparticular we train the generator G to maximize the discriminator loss LDisc. As our discriminator\nisn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section 2), this method\nis really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative\nAdversarial Training. It is not possible to adversarially train the generator by back-propagating\nthrough the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from\nthe generator, so we use reinforcement learning instead.\nOur generator is different from most text generation models in that it is non-autogregressive: predic-\ntions are made independently. In other words, rather than taking a sequence of actions where each\naction generates a token, the generator takes a single giant action of generating all tokens simulta-\nneously, where the probability for the action factorizes as the product of generator probabilities for\neach token. To deal with this enormous action space, we make the following simplifying assumption:\nthat the discriminator\u2019s prediction D(xcorrupt, t) depends only on the token xt and the non-replaced\n9See https://www.tensorflow.org/api_docs/python/tf/profiler\n16\n", []], "E Counting FLOPs": ["Published as a conference paper at ICLR 2020\nModel\nTrain FLOPs\nParams CoLA SST MRPC STS QQP MNLI QNLI RTE Avg.\nTinyBERT\n6.4e19+ (45x+) 14.5M\n51.1\n93.1 82.6\n83.7 89.1\n84.6\n90.4\n70.0 80.6\nMobileBERT\n6.4e19+ (45x+) 25.3M\n51.1\n92.6 84.5\n84.8 88.3\n84.3\n91.6\n70.4 81.0\nGPT\n4.0e19 (29x)\n117M\n45.4\n91.3 75.7\n80.0 88.5\n82.1\n88.1\n56.0 75.9\nBERT-Base\n6.4e19 (45x)\n110M\n52.1\n93.5 84.8\n85.8 89.2\n84.6\n90.5\n66.4 80.9\nBERT-Large\n1.9e20 (135x)\n335M\n60.5\n94.9 85.4\n86.5 89.3\n86.7\n92.7\n70.1 83.3\nSpanBERT\n7.1e20 (507x)\n335M\n64.3\n94.8 87.9\n89.9 89.5\n87.7\n94.3\n79.0 85.9\nELECTRA-Small\n1.4e18 (1x)\n14M\n54.6\n89.1 83.7\n80.3 88.0\n79.7\n87.7\n60.8 78.0\nELECTRA-Small++ 3.3e19 (18x)\n14M\n55.6\n91.1 84.9\n84.6 88.0\n81.6\n88.3\n63.6 79.7\nELECTRA-Base\n6.4e19 (45x)\n110M\n59.7\n93.4 86.7\n87.7 89.1\n85.8\n92.7\n73.1 83.5\nELECTRA-Base++\n3.3e20 (182x)\n110M\n64.6\n96.0 88.1\n90.2 89.5\n88.5\n93.1\n75.2 85.7\nELECTRA-1.75M\n3.1e21 (2200x) 330M\n68.1\n96.7 89.2\n91.7 90.4\n90.7\n95.5\n86.1 88.6\nTable 8: Results for models on the GLUE test set. Only models with single-task \ufb01netuning (no\nensembling, task-speci\ufb01c tricks, etc.) are shown.\nE\nCOUNTING FLOPS\nWe chose to measure compute usage in terms of \ufb02oating point operations (FLOPs) because it is a\nmeasure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth not-\ning that in some cases abstracting away hardware details is a drawback because hardware-centered\noptimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019)\ngets by tying weights and thus reducing communication overhead between TPU workers. We used\nTensorFlow\u2019s FLOP-counting capabilities9 and checked the results with by-hand computation. We\nmade the following assumptions:\n\u2022 An \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an\nexp is one op like an add, even though in practice the exp might be slower. We believe\nthis assumption does not substantially change compute estimates because matrix-multiplies\ndominate the compute for most models. Similarly, we count matrix-multiplies as 2 \u2217m \u2217n\nFLOPs instead of m \u2217n as one might if considering fused multiply-add operations.\n\u2022 The backwards pass takes the same number of FLOPs as the forward pass. This assumption\nis not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but\nimportantly, the forward/backward pass FLOPs really are the same for matrix-multiplies,\nwhich is most of the compute anyway.\n\u2022 We assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac-\ntice, sparse embedding lookups are much slower than constant time; on some hardware\naccelerators dense operations are actually faster than sparse lookups.\nF\nADVERSARIAL TRAINING\nHere we detail attempts to adversarially train the generator instead of using maximum likelihood. In\nparticular we train the generator G to maximize the discriminator loss LDisc. As our discriminator\nisn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section 2), this method\nis really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative\nAdversarial Training. It is not possible to adversarially train the generator by back-propagating\nthrough the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from\nthe generator, so we use reinforcement learning instead.\nOur generator is different from most text generation models in that it is non-autogregressive: predic-\ntions are made independently. In other words, rather than taking a sequence of actions where each\naction generates a token, the generator takes a single giant action of generating all tokens simulta-\nneously, where the probability for the action factorizes as the product of generator probabilities for\neach token. To deal with this enormous action space, we make the following simplifying assumption:\nthat the discriminator\u2019s prediction D(xcorrupt, t) depends only on the token xt and the non-replaced\n9See https://www.tensorflow.org/api_docs/python/tf/profiler\n16\n", []], "D Further results on GLUE": ["Published as a conference paper at ICLR 2020\nto-apples because different papers implement the tricks differently. We therefore also report results\nfor ELECTRA-1.75M with the only trick being dev-set model selection (best of 10 models), which\nis the setting BERT used to report results, in Table 8.\nFor our SQuAD 2.0 test set submission, we \ufb01ne-tuned 20 models from the same pre-trained check-\npoint and submitted the one with the best dev set score.\nC\nDETAILS ABOUT GLUE\nWe provide further details about the GLUE benchmark tasks below\n\u2022 CoLA: Corpus of Linguistic Acceptability (Warstadt et al., 2018). The task is to determine\nwhether a given sentence is grammatical or not. The dataset contains 8.5k train examples\nfrom books and journal articles on linguistic theory.\n\u2022 SST: Stanford Sentiment Treebank (Socher et al., 2013). The tasks is to determine if the\nsentence is positive or negative in sentiment. The dataset contains 67k train examples from\nmovie reviews.\n\u2022 MRPC: Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005). The task is to\npredict whether two sentences are semantically equivalent or not. The dataset contains 3.7k\ntrain examples from online news sources.\n\u2022 STS: Semantic Textual Similarity (Cer et al., 2017). The tasks is to predict how seman-\ntically similar two sentences are on a 1-5 scale. The dataset contains 5.8k train examples\ndrawn from new headlines, video and image captions, and natural language inference data.\n\u2022 QQP: Quora Question Pairs (Iyer et al., 2017). The task is to determine whether a pair of\nquestions are semantically equivalent. The dataset contains 364k train examples from the\ncommunity question-answering website Quora.\n\u2022 MNLI: Multi-genre Natural Language Inference (Williams et al., 2018). Given a premise\nsentence and a hypothesis sentence, the task is to predict whether the premise entails the\nhypothesis, contradicts the hypothesis, or neither. The dataset contains 393k train examples\ndrawn from ten different sources.\n\u2022 QNLI: Question Natural Language Inference; constructed from SQuAD (Rajpurkar et al.,\n2016). The task is to predict whether a context sentence contains the answer to a question\nsentence. The dataset contains 108k train examples from Wikipedia.\n\u2022 RTE: Recognizing Textual Entailment (Giampiccolo et al., 2007). Given a premise sen-\ntence and a hypothesis sentence, the task is to predict whether the premise entails the hy-\npothesis or not. The dataset contains 2.5k train examples from a series of annual textual\nentailment challenges.\nD\nFURTHER RESULTS ON GLUE\nWe report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8.\nFurthermore, we push the limits of base-sized and small-sized models by training them on the\nXLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called\nELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also in-\ncreased the sequence length to 512; otherwise the hyperparameters are the same as the ones listed\nin Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in\nAppendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large\nwhile ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models\nperform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and Mo-\nbileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation\nprocedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success\nof distilling BERT, we believe it would be possible to build even stronger small pre-trained models\nby distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal\nis to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely\nmatches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s\nstrength at the task.\n15\n", []], "C Details about GLUE": ["Published as a conference paper at ICLR 2020\nto-apples because different papers implement the tricks differently. We therefore also report results\nfor ELECTRA-1.75M with the only trick being dev-set model selection (best of 10 models), which\nis the setting BERT used to report results, in Table 8.\nFor our SQuAD 2.0 test set submission, we \ufb01ne-tuned 20 models from the same pre-trained check-\npoint and submitted the one with the best dev set score.\nC\nDETAILS ABOUT GLUE\nWe provide further details about the GLUE benchmark tasks below\n\u2022 CoLA: Corpus of Linguistic Acceptability (Warstadt et al., 2018). The task is to determine\nwhether a given sentence is grammatical or not. The dataset contains 8.5k train examples\nfrom books and journal articles on linguistic theory.\n\u2022 SST: Stanford Sentiment Treebank (Socher et al., 2013). The tasks is to determine if the\nsentence is positive or negative in sentiment. The dataset contains 67k train examples from\nmovie reviews.\n\u2022 MRPC: Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005). The task is to\npredict whether two sentences are semantically equivalent or not. The dataset contains 3.7k\ntrain examples from online news sources.\n\u2022 STS: Semantic Textual Similarity (Cer et al., 2017). The tasks is to predict how seman-\ntically similar two sentences are on a 1-5 scale. The dataset contains 5.8k train examples\ndrawn from new headlines, video and image captions, and natural language inference data.\n\u2022 QQP: Quora Question Pairs (Iyer et al., 2017). The task is to determine whether a pair of\nquestions are semantically equivalent. The dataset contains 364k train examples from the\ncommunity question-answering website Quora.\n\u2022 MNLI: Multi-genre Natural Language Inference (Williams et al., 2018). Given a premise\nsentence and a hypothesis sentence, the task is to predict whether the premise entails the\nhypothesis, contradicts the hypothesis, or neither. The dataset contains 393k train examples\ndrawn from ten different sources.\n\u2022 QNLI: Question Natural Language Inference; constructed from SQuAD (Rajpurkar et al.,\n2016). The task is to predict whether a context sentence contains the answer to a question\nsentence. The dataset contains 108k train examples from Wikipedia.\n\u2022 RTE: Recognizing Textual Entailment (Giampiccolo et al., 2007). Given a premise sen-\ntence and a hypothesis sentence, the task is to predict whether the premise entails the hy-\npothesis or not. The dataset contains 2.5k train examples from a series of annual textual\nentailment challenges.\nD\nFURTHER RESULTS ON GLUE\nWe report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8.\nFurthermore, we push the limits of base-sized and small-sized models by training them on the\nXLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called\nELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also in-\ncreased the sequence length to 512; otherwise the hyperparameters are the same as the ones listed\nin Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in\nAppendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large\nwhile ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models\nperform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and Mo-\nbileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation\nprocedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success\nof distilling BERT, we believe it would be possible to build even stronger small pre-trained models\nby distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal\nis to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely\nmatches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s\nstrength at the task.\n15\n", []], "B Fine-Tuning Details": ["Published as a conference paper at ICLR 2020\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning, 8(3-4):229\u2013256, 1992.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\nXLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.\nLantao Yu, Weinan Zhang, Jun Wang, and Yingrui Yu. SeqGAN: Sequence generative adversarial\nnets with policy gradient. In AAAI, 2017.\nYizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin.\nAdversarial feature matching for text generation. In ICML, 2017.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. ICCV, 2015.\nA\nPRE-TRAINING DETAILS\nThe following details apply to both our ELECTRA models and BERT baselines. We mostly use the\nsame hyperparameters as BERT. We set \u03bb, the weight for the discriminator objective in the loss to\n50.8 We use dynamic token masking with the masked positions decided on-the-\ufb02y instead of during\npreprocessing. Also, we did not use the next sentence prediction objective proposed in the original\nBERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al.,\n2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because\nwe noticed the generator was achieving high accuracy with 15% masking, resulting in very few\nreplaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4,\n2e-4, 3e-4, 5e-4] and selected \u03bb out of [1, 10, 20, 50, 100] in early experiments. Otherwise we did\nno hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters\nare listed in Table 6.\nB\nFINE-TUNING DETAILS\nFor Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part.\nHowever, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather\nthan 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD,\nwe decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Base-\nsized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise\nlearning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large\nmodels. We found the small models bene\ufb01t from a larger learning rate and searched for the best one\nout of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same\nhyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and\nRoBERTa separately searched for the best hyperparameters for each task. We expect our results\nwould improve slightly if we performed the same sort of additional hyperparameter search. The full\nset of hyperparameters is listed in Table 7.\nFollowing BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is\ndif\ufb01cult to beat even the majority classi\ufb01er using a standard \ufb01ne-tuning-as-classi\ufb01er approach. For\nthe GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard\nsubmissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan\net al., 2019). Speci\ufb01cally:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an\nELECTRA checkpoint that has been \ufb01ne-tuned on MNLI. For RTE, we found it helpful to\ncombine this with a lower learning rate of 2e-5.\n8As a binary classi\ufb01cation task instead of the 30,000-way classi\ufb01cation task in MLM, the discriminator\u2019s\nloss was typically much lower than the generator\u2019s.\n13\n", []], "A Pre-Training Details": ["Published as a conference paper at ICLR 2020\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine Learning, 8(3-4):229\u2013256, 1992.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.\nXLNet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019.\nLantao Yu, Weinan Zhang, Jun Wang, and Yingrui Yu. SeqGAN: Sequence generative adversarial\nnets with policy gradient. In AAAI, 2017.\nYizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin.\nAdversarial feature matching for text generation. In ICML, 2017.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. ICCV, 2015.\nA\nPRE-TRAINING DETAILS\nThe following details apply to both our ELECTRA models and BERT baselines. We mostly use the\nsame hyperparameters as BERT. We set \u03bb, the weight for the discriminator objective in the loss to\n50.8 We use dynamic token masking with the masked positions decided on-the-\ufb02y instead of during\npreprocessing. Also, we did not use the next sentence prediction objective proposed in the original\nBERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al.,\n2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because\nwe noticed the generator was achieving high accuracy with 15% masking, resulting in very few\nreplaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4,\n2e-4, 3e-4, 5e-4] and selected \u03bb out of [1, 10, 20, 50, 100] in early experiments. Otherwise we did\nno hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters\nare listed in Table 6.\nB\nFINE-TUNING DETAILS\nFor Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part.\nHowever, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather\nthan 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD,\nwe decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Base-\nsized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise\nlearning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large\nmodels. We found the small models bene\ufb01t from a larger learning rate and searched for the best one\nout of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same\nhyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and\nRoBERTa separately searched for the best hyperparameters for each task. We expect our results\nwould improve slightly if we performed the same sort of additional hyperparameter search. The full\nset of hyperparameters is listed in Table 7.\nFollowing BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is\ndif\ufb01cult to beat even the majority classi\ufb01er using a standard \ufb01ne-tuning-as-classi\ufb01er approach. For\nthe GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard\nsubmissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan\net al., 2019). Speci\ufb01cally:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an\nELECTRA checkpoint that has been \ufb01ne-tuned on MNLI. For RTE, we found it helpful to\ncombine this with a lower learning rate of 2e-5.\n8As a binary classi\ufb01cation task instead of the 30,000-way classi\ufb01cation task in MLM, the discriminator\u2019s\nloss was typically much lower than the generator\u2019s.\n13\n", []], "5 Conclusion": ["Published as a conference paper at ICLR 2020\nGenerative Adversarial Networks GANs (Goodfellow et al., 2014) are effective at generating\nhigh-quality synthetic data. Radford et al. (2016) propose using the discriminator of a GAN in\ndownstream tasks, which is similar to our method. GANs have been applied to text data (Yu et al.,\n2017; Zhang et al., 2017), although state-of-the-art approaches still lag behind standard maximum-\nlikelihood training (Caccia et al., 2018; Tevet et al., 2018). Although we do not use adversarial\nlearning, our generator is particularly reminiscent of MaskGAN (Fedus et al., 2018), which trains\nthe generator to \ufb01ll in tokens deleted from the input.\nContrastive Learning Broadly, contrastive learning methods distinguish observed data points from\n\ufb01ctitious negative samples. They have been applied to many modalities including text (Smith &\nEisner, 2005), images (Chopra et al., 2005), and video (Wang & Gupta, 2015; Sermanet et al., 2017)\ndata. Common approaches learn embedding spaces where related data points are similar (Saunshi\net al., 2019) or models that rank real data points over negative samples (Collobert et al., 2011; Bordes\net al., 2013). ELECTRA is particularly related to Noise-Contrastive Estimation (NCE) (Gutmann &\nHyv\u00a8arinen, 2010), which also trains a binary classi\ufb01er to distinguish real and fake data points.\nWord2Vec (Mikolov et al., 2013), one of the earliest pre-training methods for NLP, uses contrastive\nlearning. In fact, ELECTRA can be viewed as a massively scaled-up version of Continuous Bag-\nof-Words (CBOW) with Negative Sampling. CBOW also predicts an input token given surrounding\ncontext and negative sampling rephrases the learning task as a binary classi\ufb01cation task on whether\nthe input token comes from the data or proposal distribution. However, CBOW uses a bag-of-\nvectors encoder rather than a transformer and a simple proposal distribution derived from unigram\ntoken frequencies instead of a learned generator.\n5\nCONCLUSION\nWe have proposed replaced token detection, a new self-supervised task for language representation\nlearning. The key idea is training a text encoder to distinguish input tokens from high-quality nega-\ntive samples produced by an small generator network. Compared to masked language modeling, our\npre-training objective is more compute-ef\ufb01cient and results in better performance on downstream\ntasks. It works well even when using relatively small amounts of compute, which we hope will\nmake developing and applying pre-trained text encoders more accessible to researchers and practi-\ntioners with less access to computing resources. We also hope more future work on NLP pre-training\nwill consider ef\ufb01ciency as well as absolute performance, and follow our effort in reporting compute\nusage and parameter counts along with evaluation metrics.\nACKNOWLEDGEMENTS\nWe thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de\nMontr\u00b4eal, and the anonymous reviewers for their thoughtful comments and suggestions. We thank\nMatt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman\nGoyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong\nLan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT.\nKevin is supported by a Google PhD Fellowship.\nREFERENCES\nAntoine Bordes, Nicolas Usunier, Alberto Garc\u00b4\u0131a-Dur\u00b4an, Jason Weston, and Oksana Yakhnenko.\nTranslating embeddings for modeling multi-relational data. In NeurIPS, 2013.\nAvishek Joey Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. In ACL,\n2018.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, and Laurent Char-\nlin. Language GANs falling short. arXiv preprint arXiv:1811.02549, 2018.\nJamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. Clueweb09 data set, 2009. URL https:\n//lemurproject.org/clueweb09.php/.\n10\n", []], "4 Related Work": ["Published as a conference paper at ICLR 2020\nModel\nELECTRA\nAll-Tokens MLM\nReplace MLM\nELECTRA 15%\nBERT\nGLUE score\n85.0\n84.3\n82.4\n82.4\n82.2\nTable 5: Compute-ef\ufb01ciency experiments (see text for details).\n128\n256\n384\n512\n768\nHidden State Size\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\nGLUE Score\nELECTRA\nBERT\n128\n256\n384\n512\n768\nHidden State Size\n0\n1\n2\n3\n4\n5\n6\n7\nELECTRA improvement over BERT\n0\n1\n2\n3\n4\n5\nPre-Train FLOPs 1e18\n65\n70\n75\n80\nGLUE Score\nELECTRA-256\nBERT-256\nFigure 4: Left and Center: Comparison of BERT and ELECTRA for different model sizes. Right:\nA small ELECTRA model converges to higher downstream accuracy than BERT, showing the im-\nprovement comes from more than just faster training.\nsame 10% of the time. However, our results suggest these simple heuristics are insuf\ufb01cient to fully\nsolve the issue. Lastly, we \ufb01nd that All-Tokens MLM, the generative model that makes predictions\nover all tokens instead of a subset, closes most of the gap between BERT and ELECTRA. In total,\nthese results suggest a large amount of ELECTRA\u2019s improvement can be attributed to learning from\nall tokens and a smaller amount can be attributed to alleviating the pre-train \ufb01ne-tune mismatch.\nThe improvement of ELECTRA over All-Tokens MLM suggests that the ELECTRA\u2019s gains come\nfrom more than just faster training. We study this further by comparing BERT to ELECTRA for\nvarious model sizes (see Figure 4, left). We \ufb01nd that the gains from ELECTRA grow larger as the\nmodels get smaller. The small models are trained fully to convergence (see Figure 4, right), showing\nthat ELECTRA achieves higher downstream accuracy than BERT when fully trained. We speculate\nthat ELECTRA is more parameter-ef\ufb01cient than BERT because it does not have to model the full\ndistribution of possible tokens at each position, but we believe more analysis is needed to completely\nexplain ELECTRA\u2019s parameter ef\ufb01ciency.\n4\nRELATED WORK\nSelf-Supervised Pre-training for NLP Self-supervised learning has been used to learn word rep-\nresentations (Collobert et al., 2011; Pennington et al., 2014) and more recently contextual represen-\ntations of words though objectives such as language modeling (Dai & Le, 2015; Peters et al., 2018;\nHoward & Ruder, 2018). BERT (Devlin et al., 2019) pre-trains a large Transformer (Vaswani et al.,\n2017) at the masked-language modeling task. There have been numerous extensions to BERT. For\nexample, MASS (Song et al., 2019) and UniLM (Dong et al., 2019) extend BERT to generation tasks\nby adding auto-regressive generative training objectives. ERNIE (Sun et al., 2019a) and SpanBERT\n(Joshi et al., 2019) mask out contiguous sequences of token for improved span representations. This\nidea may be complementary to ELECTRA; we think it would be interesting to make ELECTRA\u2019s\ngenerator auto-regressive and add a \u201creplaced span detection\u201d task. Instead of masking out input\ntokens, XLNet (Yang et al., 2019) masks attention weights such that the input sequence is auto-\nregressively generated in a random order. However, this method suffers from the same inef\ufb01ciencies\nas BERT because XLNet only generates 15% of the input tokens in this way. Like ELECTRA, XL-\nNet may alleviate BERT\u2019s pretrain-\ufb01netune discrepancy by not requiring [MASK] tokens, although\nthis isn\u2019t entirely clear because XLNet uses two \u201cstreams\u201d of attention during pre-training but only\none for \ufb01ne-tuning. Recently, models such as TinyBERT (Jiao et al., 2019) and MobileBERT (Sun\net al., 2019b) show that BERT can effectively be distilled down to a smaller model. In contrast, we\nfocus more on pre-training speed rather than inference speed, so we train ELECTRA-Small from\nscratch.\n9\n", []], "3.5 Efficiency Analysis": ["Published as a conference paper at ICLR 2020\nModel\nTrain FLOPs\nParams\nSQuAD 1.1 dev\nSQuAD 2.0 dev\nSQuAD 2.0 test\nEM\nF1\nEM\nF1\nEM\nF1\nBERT-Base\n6.4e19 (0.09x)\n110M\n80.8\n88.5\n\u2013\n\u2013\n\u2013\n\u2013\nBERT\n1.9e20 (0.27x)\n335M\n84.1\n90.9\n79.0\n81.8\n80.0\n83.0\nSpanBERT\n7.1e20 (1x)\n335M\n88.8\n94.6\n85.7\n88.7\n85.7\n88.7\nXLNet-Base\n6.6e19 (0.09x)\n117M\n81.3\n\u2013\n78.5\n\u2013\n\u2013\n\u2013\nXLNet\n3.9e21 (5.4x)\n360M\n89.7\n95.1\n87.9\n90.6\n87.9\n90.7\nRoBERTa-100K\n6.4e20 (0.90x)\n356M\n\u2013\n94.0\n\u2013\n87.7\n\u2013\n\u2013\nRoBERTa-500K\n3.2e21 (4.5x)\n356M\n88.9\n94.6\n86.5\n89.4\n86.8\n89.8\nALBERT\n3.1e22 (44x)\n235M\n89.3\n94.8\n87.4\n90.2\n88.1\n90.9\nBERT (ours)\n7.1e20 (1x)\n335M\n88.0\n93.7\n84.7\n87.5\n\u2013\n\u2013\nELECTRA-Base\n6.4e19 (0.09x)\n110M\n84.5\n90.8\n80.5\n83.3\n\u2013\n\u2013\nELECTRA-400K\n7.1e20 (1x)\n335M\n88.7\n94.2\n86.9\n89.6\n\u2013\n\u2013\nELECTRA-1.75M\n3.1e21 (4.4x)\n335M\n89.7\n94.9\n88.0\n90.6\n88.7\n91.4\nTable 4: Results on the SQuAD for non-ensemble models.\nmark. ELECTRA-Base also yields strong results, scoring substantially better than BERT-Base and\nXLNet-Base, and even surpassing BERT-Large according to most metrics. ELECTRA generally\nperforms better at SQuAD 2.0 than 1.1. Perhaps replaced token detection, in which the model\ndistinguishes real tokens from plausible fakes, is particularly transferable to the answerability clas-\nsi\ufb01cation of SQuAD 2.0, in which the model must distinguish answerable questions from fake unan-\nswerable questions.\n3.5\nEFFICIENCY ANALYSIS\nWe have suggested that posing the training objective over a small subset of tokens makes masked\nlanguage modeling inef\ufb01cient. However, it isn\u2019t entirely obvious that this is the case. After all, the\nmodel still receives a large number of input tokens even though it predicts only a small number of\nmasked tokens. To better understand where the gains from ELECTRA are coming from, we compare\na series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between\nBERT and ELECTRA.\n\u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only\ncomes from the 15% of the tokens that were masked out of the input. In other words, the\nsum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of\nreplacing masked-out tokens with [MASK], they are replaced with tokens from a generator\nmodel. This objective tests to what extent ELECTRA\u2019s gains come from solving the dis-\ncrepancy of exposing the model to [MASK] tokens during pre-training but not \ufb01ne-tuning.\n\u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator sam-\nples. Furthermore, the model predicts the identity of all tokens in the input, not just ones\nthat were masked out. We found it improved results to train this model with an explicit\ncopy mechanism that outputs a copy probability D for each token using a sigmoid layer.\nThe model\u2019s output distribution puts D weight on the input token plus 1 \u2212D times the\noutput of the MLM softmax. This model is essentially a combination of BERT and ELEC-\nTRA. Note that without generator replacements, the model would trivially learn to make\npredictions from the vocabulary for [MASK] tokens and copy the input for other ones.\nResults are shown in Table 5. First, we \ufb01nd that ELECTRA is greatly bene\ufb01ting from having a loss\nde\ufb01ned over all input tokens rather than just a subset: ELECTRA 15% performs much worse than\nELECTRA. Secondly, we \ufb01nd that BERT performance is being slightly harmed from the pre-train\n\ufb01ne-tune mismatch from [MASK] tokens, as Replace MLM slightly outperforms BERT. We note\nthat BERT (including our implementation) already includes a trick to help with the pre-train/\ufb01ne-\ntune discrepancy: masked tokens are replaced with a random token 10% of the time and are kept the\n7We also trained a discriminator that learns from a random 15% of the input tokens distinct from the subset\nthat was originally masked out; this model performed slightly worse.\n8\n", []], "3.4 Large Models": ["Published as a conference paper at ICLR 2020\nModel\nTrain FLOPs Params CoLA SST MRPC STS QQP MNLI QNLI RTE Avg.\nBERT\n1.9e20 (0.27x) 335M\n60.6\n93.2 88.0\n90.0 91.3\n86.6\n92.3\n70.4\n84.0\nRoBERTa-100K\n6.4e20 (0.90x) 356M\n66.1\n95.6 91.4\n92.2 92.0\n89.3\n94.0\n82.7\n87.9\nRoBERTa-500K\n3.2e21 (4.5x)\n356M\n68.0\n96.4 90.9\n92.1 92.2\n90.2\n94.7\n86.6\n88.9\nXLNet\n3.9e21 (5.4x)\n360M\n69.0\n97.0 90.8\n92.2 92.3\n90.8\n94.9\n85.9\n89.1\nBERT (ours)\n7.1e20 (1x)\n335M\n67.0\n95.9 89.1\n91.2 91.5\n89.6\n93.5\n79.5\n87.2\nELECTRA-400K\n7.1e20 (1x)\n335M\n69.3\n96.0 90.6\n92.1 92.4\n90.5\n94.5\n86.8\n89.0\nELECTRA-1.75M 3.1e21 (4.4x)\n335M\n69.1\n96.9 90.8\n92.6 92.4\n90.9\n95.0\n88.0\n89.5\nTable 2: Comparison of large models on the GLUE dev set. ELECTRA and RoBERTa are shown\nfor different numbers of pre-training steps, indicated by the numbers after the dashes. ELECTRA\nperforms comparably to XLNet and RoBERTa when using less than 1/4 of their pre-training compute\nand outperforms them when given a similar amount of pre-training compute. BERT dev results are\nfrom Clark et al. (2019).\nModel\nTrain FLOPs CoLA SST MRPC STS QQP MNLI QNLI RTE WNLI Avg.* Score\nBERT\n1.9e20 (0.06x) 60.5\n94.9 85.4\n86.5 89.3\n86.7\n92.7\n70.1\n65.1\n79.8\n80.5\nRoBERTa\n3.2e21 (1.02x) 67.8\n96.7 89.8\n91.9 90.2\n90.8\n95.4\n88.2\n89.0\n88.1\n88.1\nALBERT\n3.1e22 (10x)\n69.1\n97.1 91.2\n92.0 90.5\n91.3\n\u2013\n89.2\n91.8\n89.0\n\u2013\nXLNet\n3.9e21 (1.26x) 70.2\n97.1 90.5\n92.6 90.4\n90.9\n\u2013\n88.5\n92.5\n89.1\n\u2013\nELECTRA 3.1e21 (1x)\n71.7\n97.1 90.7\n92.5 90.8\n91.3\n95.8\n89.8\n92.5\n89.5\n89.4\nTable 3: GLUE test-set results for large models. Models in this table incorporate additional tricks\nsuch as ensembling to improve scores (see Appendix B for details). Some models do not have\nQNLI scores because they treat QNLI as a ranking task, which has recently been disallowed by the\nGLUE benchmark. To compare against these models, we report the average score excluding QNLI\n(Avg.*) in addition to the GLUE leaderboard score (Score). \u201cELECTRA\u201d and \u201cRoBERTa\u201d refer to\nthe fully-trained ELECTRA-1.75M and RoBERTa-500K models.\n3.4\nLARGE MODELS\nWe train big ELECTRA models to measure the effectiveness of the replaced token detection pre-\ntraining task at the large scale of current state-of-the-art pre-trained Transformers. Our ELECTRA-\nLarge models are the same size as BERT-Large but are trained for much longer. In particular, we\ntrain a model for 400k steps (ELECTRA-400K; roughly 1/4 the pre-training compute of RoBERTa)\nand one for 1.75M steps (ELECTRA-1.75M; similar compute to RoBERTa). We use a batch size\n2048 and the XLNet pre-training data. We note that although the XLNet data is similar to the data\nused to train RoBERTa, the comparison is not entirely direct. As a baseline, we trained our own\nBERT-Large model using the same hyperparameters and training time as ELECTRA-400K.\nResults on the GLUE dev set are shown in Table 2. ELECTRA-400K performs comparably to\nRoBERTa and XLNet. However, it took less than 1/4 of the compute to train ELECTRA-400K\nas it did to train RoBERTa and XLNet, demonstrating that ELECTRA\u2019s sample-ef\ufb01ciency gains\nhold at large scale. Training ELECTRA for longer (ELECTRA-1.75M) results in a model that\noutscores them on most GLUE tasks while still requiring less pre-training compute. Surprisingly,\nour baseline BERT model scores notably worse than RoBERTa-100K, suggesting our models may\nbene\ufb01t from more hyperparameter tuning or using the RoBERTa training data. ELECTRA\u2019s gains\nhold on the GLUE test set (see Table 3), although these comparisons are less apples-to-apples due\nto the additional tricks employed by the models (see Appendix B).\nResults on SQuAD are shown in Table 4. Consistent, with the GLUE results, ELECTRA scores\nbetter than masked-language-modeling-based methods given the same compute resources. For ex-\nample, ELECTRA-400K outperforms RoBERTa-100k and our BERT baseline, which use similar\namounts of pre-training compute. ELECTRA-400K also performs comparably to RoBERTa-500K\ndespite using less than 1/4th of the compute. Unsurprisingly, training ELECTRA longer improves\nresults further: ELECTRA-1.75M scores higher than previous models on the SQuAD 2.0 bench-\n7\n", []], "3.3 Small Models": ["Published as a conference paper at ICLR 2020\nModel\nTrain / Infer FLOPs\nSpeedup\nParams\nTrain Time + Hardware\nGLUE\nELMo\n3.3e18 / 2.6e10\n19x / 1.2x\n96M\n14d on 3 GTX 1080 GPUs\n71.2\nGPT\n4.0e19 / 3.0e10\n1.6x / 0.97x\n117M\n25d on 8 P6000 GPUs\n78.8\nBERT-Small\n1.4e18 / 3.7e9\n45x / 8x\n14M\n4d on 1 V100 GPU\n75.1\nBERT-Base\n6.4e19 / 2.9e10\n1x / 1x\n110M\n4d on 16 TPUv3s\n82.2\nELECTRA-Small\n1.4e18 / 3.7e9\n45x / 8x\n14M\n4d on 1 V100 GPU\n79.9\n50% trained\n7.1e17 / 3.7e9\n90x / 8x\n14M\n2d on 1 V100 GPU\n79.0\n25% trained\n3.6e17 / 3.7e9\n181x / 8x\n14M\n1d on 1 V100 GPU\n77.7\n12.5% trained\n1.8e17 / 3.7e9\n361x / 8x\n14M\n12h on 1 V100 GPU\n76.0\n6.25% trained\n8.9e16 / 3.7e9\n722x / 8x\n14M\n6h on 1 V100 GPU\n74.1\nELECTRA-Base\n6.4e19 / 2.9e10\n1x / 1x\n110M\n4d on 16 TPUv3s\n85.1\nTable 1: Comparison of small models on the GLUE dev set. BERT-Small/Base are our implemen-\ntation and use the same hyperparameters as ELECTRA-Small/Base. Infer FLOPs assumes single\nlength-128 input. Training times should be taken with a grain of salt as they are for different hard-\nware and with sometimes un-optimized code. ELECTRA performs well even when trained on a\nsingle GPU, scoring 5 GLUE points higher than a comparable BERT model and even outscoring the\nmuch larger GPT model.\nproblems with adversarial training. First, the adversarial generator is simply worse at masked lan-\nguage modeling; it achieves 58% accuracy at masked language modeling compared to 65% accuracy\nfor an MLE-trained one. We believe the worse accuracy is mainly due to the poor sample ef\ufb01ciency\nof reinforcement learning when working in the large action space of generating text. Secondly, the\nadversarially trained generator produces a low-entropy output distribution where most of the proba-\nbility mass is on a single token, which means there is not much diversity in the generator samples.\nBoth of these problems have been observed in GANs for text in prior work (Caccia et al., 2018).\n3.3\nSMALL MODELS\nAs a goal of this work is to improve the ef\ufb01ciency of pre-training, we develop a small model that can\nbe quickly trained on a single GPU. Starting with the BERT-Base hyperparameters, we shortened the\nsequence length (from 512 to 128), reduced the batch size (from 256 to 128), reduced the model\u2019s\nhidden dimension size (from 768 to 256), and used smaller token embeddings (from 768 to 128). To\nprovide a fair comparison, we also train a BERT-Small model using the same hyperparameters. We\ntrain BERT-Small for 1.5M steps, so it uses the same training FLOPs as ELECTRA-Small, which\nwas trained for 1M steps.5 In addition to BERT, we compare against two less resource-intensive\npre-training methods based on language modeling: ELMo (Peters et al., 2018) and GPT (Radford\net al., 2018).6 We also show results for a base-sized ELECTRA model comparable to BERT-Base.\nResults are shown in Table 1. See Appendix D for additional results, including stronger small-sized\nand base-sized models trained with more compute. ELECTRA-Small performs remarkably well\ngiven its size, achieving a higher GLUE score than other methods using substantially more compute\nand parameters. For example, it scores 5 points higher than a comparable BERT-Small model and\neven outperforms the much larger GPT model. ELECTRA-Small is trained mostly to convergence,\nwith models trained for even less time (as little as 6 hours) still achieving reasonable performance.\nWhile small models distilled from larger pre-trained transformers can also achieve good GLUE\nscores (Sun et al., 2019b; Jiao et al., 2019), these models require \ufb01rst expending substantial compute\nto pre-train the larger teacher model. The results also demonstrate the strength of ELECTRA at a\nmoderate size; our base-sized ELECTRA model substantially outperforms BERT-Base and even\noutperforms BERT-Large (which gets 84.0 GLUE score). We hope ELECTRA\u2019s ability to achieve\nstrong results with relatively little compute will broaden the accessibility of developing and applying\npre-trained models in NLP.\n5ELECTRA requires more FLOPs per step because it consists of the generator as well as the discriminator.\n6GPT is similar in size to BERT-Base, but is trained for fewer steps.\n6\n", []], "3.2 Model Extensions": ["Published as a conference paper at ICLR 2020\nby using reinforcement learning to train the generator (see Appendix F), this performed worse than\nmaximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input,\nas is typical with a GAN.\nWe minimize the combined loss\nmin\n\u03b8G,\u03b8D\nX\nx\u2208X\nLMLM(x, \u03b8G) + \u03bbLDisc(x, \u03b8D)\nover a large corpus X of raw text. We approximate the expectations in the losses with a single\nsample. We don\u2019t back-propagate the discriminator loss through the generator (indeed, we can\u2019t\nbecause of the sampling step). After pre-training, we throw out the generator and \ufb01ne-tune the\ndiscriminator on downstream tasks.\n3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETUP\nWe evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al.,\n2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains\na variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI),\nparaphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST), and lin-\nguistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation\nmetrics are Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the\nother GLUE tasks; we generally report the average score over all tasks. For SQuAD, we evaluate on\nversions 1.1, in which models select the span of text answering a question, and 2.0, in which some\nquestions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match\n(EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists\nof 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large\nmodel we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT\ndataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and\nGigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although\nwe think it would be interesting to apply our methods to multilingual data in the future.\nOur model architecture and most hyperparameters are the same as BERT\u2019s. For \ufb01ne-tuning on\nGLUE, we add simple linear classi\ufb01ers on top of ELECTRA. For SQuAD, we add the question-\nanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than\nBERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a\n\u201canswerability\u201d classi\ufb01er added for SQuAD 2.0. Some of our evaluation datasets are small, which\nmeans accuracies of \ufb01ne-tuned models can vary substantially depending on the random seed. We\ntherefore report the median of 10 \ufb01ne-tuning runs from the same pre-trained checkpoint for each\nresult. Unless stated otherwise, results are on the dev set. See the appendix for further training\ndetails and hyperparameter values.\n3.2\nMODEL EXTENSIONS\nWe improve our method by proposing and evaluating several extensions to the model. Unless stated\notherwise, these experiments use the same model size and training data as BERT-Base.\nWeight Sharing We propose improving the ef\ufb01ciency of the pre-training by sharing weights be-\ntween the generator and discriminator. If the generator and discriminator are the same size, all of the\ntransformer weights can be tied. However, we found it to be more ef\ufb01cient to have a small genera-\ntor, in which case we only share the embeddings (both the token and positional embeddings) of the\ngenerator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden\nstates.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT.\nWe compare the weight tying strategies when the generator is the same size as the discriminator.\nWe train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying\ntoken embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA bene\ufb01ts from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations.\n4\n", []], "3.1 Experimental Setup": ["Published as a conference paper at ICLR 2020\nby using reinforcement learning to train the generator (see Appendix F), this performed worse than\nmaximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input,\nas is typical with a GAN.\nWe minimize the combined loss\nmin\n\u03b8G,\u03b8D\nX\nx\u2208X\nLMLM(x, \u03b8G) + \u03bbLDisc(x, \u03b8D)\nover a large corpus X of raw text. We approximate the expectations in the losses with a single\nsample. We don\u2019t back-propagate the discriminator loss through the generator (indeed, we can\u2019t\nbecause of the sampling step). After pre-training, we throw out the generator and \ufb01ne-tune the\ndiscriminator on downstream tasks.\n3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETUP\nWe evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al.,\n2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains\na variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI),\nparaphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST), and lin-\nguistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation\nmetrics are Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the\nother GLUE tasks; we generally report the average score over all tasks. For SQuAD, we evaluate on\nversions 1.1, in which models select the span of text answering a question, and 2.0, in which some\nquestions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match\n(EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists\nof 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large\nmodel we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT\ndataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and\nGigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although\nwe think it would be interesting to apply our methods to multilingual data in the future.\nOur model architecture and most hyperparameters are the same as BERT\u2019s. For \ufb01ne-tuning on\nGLUE, we add simple linear classi\ufb01ers on top of ELECTRA. For SQuAD, we add the question-\nanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than\nBERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a\n\u201canswerability\u201d classi\ufb01er added for SQuAD 2.0. Some of our evaluation datasets are small, which\nmeans accuracies of \ufb01ne-tuned models can vary substantially depending on the random seed. We\ntherefore report the median of 10 \ufb01ne-tuning runs from the same pre-trained checkpoint for each\nresult. Unless stated otherwise, results are on the dev set. See the appendix for further training\ndetails and hyperparameter values.\n3.2\nMODEL EXTENSIONS\nWe improve our method by proposing and evaluating several extensions to the model. Unless stated\notherwise, these experiments use the same model size and training data as BERT-Base.\nWeight Sharing We propose improving the ef\ufb01ciency of the pre-training by sharing weights be-\ntween the generator and discriminator. If the generator and discriminator are the same size, all of the\ntransformer weights can be tied. However, we found it to be more ef\ufb01cient to have a small genera-\ntor, in which case we only share the embeddings (both the token and positional embeddings) of the\ngenerator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden\nstates.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT.\nWe compare the weight tying strategies when the generator is the same size as the discriminator.\nWe train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying\ntoken embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA bene\ufb01ts from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations.\n4\n", []], "3 Experiments": ["Published as a conference paper at ICLR 2020\nby using reinforcement learning to train the generator (see Appendix F), this performed worse than\nmaximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input,\nas is typical with a GAN.\nWe minimize the combined loss\nmin\n\u03b8G,\u03b8D\nX\nx\u2208X\nLMLM(x, \u03b8G) + \u03bbLDisc(x, \u03b8D)\nover a large corpus X of raw text. We approximate the expectations in the losses with a single\nsample. We don\u2019t back-propagate the discriminator loss through the generator (indeed, we can\u2019t\nbecause of the sampling step). After pre-training, we throw out the generator and \ufb01ne-tune the\ndiscriminator on downstream tasks.\n3\nEXPERIMENTS\n3.1\nEXPERIMENTAL SETUP\nWe evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al.,\n2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains\na variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI),\nparaphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST), and lin-\nguistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation\nmetrics are Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the\nother GLUE tasks; we generally report the average score over all tasks. For SQuAD, we evaluate on\nversions 1.1, in which models select the span of text answering a question, and 2.0, in which some\nquestions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match\n(EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists\nof 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large\nmodel we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT\ndataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and\nGigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although\nwe think it would be interesting to apply our methods to multilingual data in the future.\nOur model architecture and most hyperparameters are the same as BERT\u2019s. For \ufb01ne-tuning on\nGLUE, we add simple linear classi\ufb01ers on top of ELECTRA. For SQuAD, we add the question-\nanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than\nBERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a\n\u201canswerability\u201d classi\ufb01er added for SQuAD 2.0. Some of our evaluation datasets are small, which\nmeans accuracies of \ufb01ne-tuned models can vary substantially depending on the random seed. We\ntherefore report the median of 10 \ufb01ne-tuning runs from the same pre-trained checkpoint for each\nresult. Unless stated otherwise, results are on the dev set. See the appendix for further training\ndetails and hyperparameter values.\n3.2\nMODEL EXTENSIONS\nWe improve our method by proposing and evaluating several extensions to the model. Unless stated\notherwise, these experiments use the same model size and training data as BERT-Base.\nWeight Sharing We propose improving the ef\ufb01ciency of the pre-training by sharing weights be-\ntween the generator and discriminator. If the generator and discriminator are the same size, all of the\ntransformer weights can be tied. However, we found it to be more ef\ufb01cient to have a small genera-\ntor, in which case we only share the embeddings (both the token and positional embeddings) of the\ngenerator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden\nstates.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT.\nWe compare the weight tying strategies when the generator is the same size as the discriminator.\nWe train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying\ntoken embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA bene\ufb01ts from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations.\n4\n", []], "2 Method": ["Published as a conference paper at ICLR 2020\n0\n1\n2\n3\n4\n5\n6\n7\n8\nPre-train FLOPs\n1e20\n70\n75\n80\n85\n90\nGLUE Score\nELECTRA-Small\nGloVe\nELECTRA-Large\n100k steps\n300k steps\nGPT\n400k steps\nBERT-Base\nBERT-Small\n200k steps\nELMo\nELECTRA-Base\nBERT-Large\nRoBERTa\n100k steps\nXLNet\nReplaced Token Detection Pre-training\nMasked Language Model Pre-training\n0\n1\n2\n3\n4\nPre-train FLOPs\n1e21\n70\n75\n80\n85\n90\nRoBERTa\n300k steps\nRoBERTa\n500k steps\nXLNet\nFigure 1: Replaced token detection pre-training consistently outperforms masked language model\npre-training given the same compute budget. The left \ufb01gure is a zoomed-in view of the dashed box.\napproach is reminiscent of training the discriminator of a GAN, our method is not adversarial in that\nthe generator producing corrupted tokens is trained with maximum likelihood due to the dif\ufb01culty\nof applying GANs to text (Caccia et al., 2018).\nWe call our approach ELECTRA1 for \u201cEf\ufb01ciently Learning an Encoder that Classi\ufb01es Token Re-\nplacements Accurately.\u201d As in prior work, we apply it to pre-train Transformer text encoders\n(Vaswani et al., 2017) that can be \ufb01ne-tuned on downstream tasks. Through a series of ablations, we\nshow that learning from all input positions causes ELECTRA to train much faster than BERT. We\nalso show ELECTRA achieves higher accuracy on downstream tasks when fully trained.\nMost current pre-training methods require large amounts of compute to be effective, raising con-\ncerns about their cost and accessibility. Since pre-training with more compute almost always re-\nsults in better downstream accuracies, we argue an important consideration for pre-training methods\nshould be compute ef\ufb01ciency as well as absolute downstream performance. From this viewpoint,\nwe train ELECTRA models of various sizes and evaluate their downstream performance vs. their\ncompute requirement. In particular, we run experiments on the GLUE natural language understand-\ning benchmark (Wang et al., 2019) and SQuAD question answering benchmark (Rajpurkar et al.,\n2016). ELECTRA substantially outperforms MLM-based methods such as BERT and XLNet given\nthe same model size, data, and compute (see Figure 1). For example, we build an ELECTRA-Small\nmodel that can be trained on 1 GPU in 4 days.2 ELECTRA-Small outperforms a comparably small\nBERT model by 5 points on GLUE, and even outperforms the much larger GPT model (Radford\net al., 2018). Our approach also works well at large scale, where we train an ELECTRA-Large\nmodel that performs comparably to RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019), de-\nspite having fewer parameters and using 1/4 of the compute for training. Training ELECTRA-Large\nfurther results in an even stronger model that outperforms ALBERT (Lan et al., 2019) on GLUE\nand sets a new state-of-the-art for SQuAD 2.0. Taken together, our results indicate that the discrim-\ninative task of distinguishing real data from challenging negative samples is more compute-ef\ufb01cient\nand parameter-ef\ufb01cient than existing generative approaches for language representation learning.\n2\nMETHOD\nWe \ufb01rst describe the replaced token detection pre-training task; see Figure 2 for an overview. We\nsuggest and evaluate several modeling improvements for this method in Section 3.2.\n1Code and pre-trained weights will be released at https://github.com/google-research/\nelectra\n2It has 1/20th the parameters and requires 1/135th the pre-training compute of BERT-Large.\n2\n", [218]], "1 Introduction": ["Published as a conference paper at ICLR 2020\nELECTRA: PRE-TRAINING TEXT ENCODERS\nAS DISCRIMINATORS RATHER THAN GENERATORS\nKevin Clark\nStanford University\nkevclark@cs.stanford.edu\nMinh-Thang Luong\nGoogle Brain\nthangluong@google.com\nQuoc V. Le\nGoogle Brain\nqvl@google.com\nChristopher D. Manning\nStanford University & CIFAR Fellow\nmanning@cs.stanford.edu\nABSTRACT\nMasked language modeling (MLM) pre-training methods such as BERT corrupt\nthe input by replacing some tokens with [MASK] and then train a model to re-\nconstruct the original tokens. While they produce good results when transferred\nto downstream NLP tasks, they generally require large amounts of compute to be\neffective. As an alternative, we propose a more sample-ef\ufb01cient pre-training task\ncalled replaced token detection. Instead of masking the input, our approach cor-\nrupts it by replacing some tokens with plausible alternatives sampled from a small\ngenerator network. Then, instead of training a model that predicts the original\nidentities of the corrupted tokens, we train a discriminative model that predicts\nwhether each token in the corrupted input was replaced by a generator sample\nor not. Thorough experiments demonstrate this new pre-training task is more ef-\n\ufb01cient than MLM because the task is de\ufb01ned over all input tokens rather than\njust the small subset that was masked out. As a result, the contextual representa-\ntions learned by our approach substantially outperform the ones learned by BERT\ngiven the same model size, data, and compute. The gains are particularly strong\nfor small models; for example, we train a model on one GPU for 4 days that\noutperforms GPT (trained using 30x more compute) on the GLUE natural lan-\nguage understanding benchmark. Our approach also works well at scale, where it\nperforms comparably to RoBERTa and XLNet while using less than 1/4 of their\ncompute and outperforms them when using the same amount of compute.\n1\nINTRODUCTION\nCurrent state-of-the-art representation learning methods for language can be viewed as learning\ndenoising autoencoders (Vincent et al., 2008). They select a small subset of the unlabeled input\nsequence (typically 15%), mask the identities of those tokens (e.g., BERT; Devlin et al. (2019)) or\nattention to those tokens (e.g., XLNet; Yang et al. (2019)), and then train the network to recover the\noriginal input. While more effective than conventional language-model pre-training due to learning\nbidirectional representations, these masked language modeling (MLM) approaches incur a substan-\ntial compute cost because the network only learns from 15% of the tokens per example.\nAs an alternative, we propose replaced token detection, a pre-training task in which the model learns\nto distinguish real input tokens from plausible but synthetically generated replacements. Instead of\nmasking, our method corrupts the input by replacing some tokens with samples from a proposal\ndistribution, which is typically the output of a small masked language model. This corruption proce-\ndure solves a mismatch in BERT (although not in XLNet) where the network sees arti\ufb01cial [MASK]\ntokens during pre-training but not when being \ufb01ne-tuned on downstream tasks. We then pre-train the\nnetwork as a discriminator that predicts for every token whether it is an original or a replacement. In\ncontrast, MLM trains the network as a generator that predicts the original identities of the corrupted\ntokens. A key advantage of our discriminative task is that the model learns from all input tokens\ninstead of just the small masked-out subset, making it more computationally ef\ufb01cient. Although our\n1\narXiv:2003.10555v1  [cs.CL]  23 Mar 2020\n", []]}