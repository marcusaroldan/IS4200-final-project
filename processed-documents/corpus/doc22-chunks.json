{"Scores on Every Task for All Experiments": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nAppendix E. Scores on Every Task for All Experiments\nThe following table lists the scores achieved on every task in the experiments described in\nSections 3.2 to 3.6.\n56\n", []], "WMT English to Romanian": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German\nOriginal input: \"Luigi often said to me that he never wanted the brothers to end\nup in court,\" she wrote.\nProcessed input: translate English to German: \"Luigi often said to me that he\nnever wanted the brothers to end up in court,\" she wrote.\nOriginal target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder\nvor Gericht landen\", schrieb sie.\nProcessed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder\nvor Gericht landen\", schrieb sie.\nD.17 WMT English to French\nOriginal input: This image section from an infrared recording by the Spitzer telescope\nshows a \"family portrait\" of countless generations of stars: the oldest stars\nare seen as blue dots, while more difficult to identify are the pink-coloured\n\"new-borns\" in the star delivery room.\nProcessed input: translate English to French: This image section from an infrared\nrecording by the Spitzer telescope shows a \"family portrait\" of countless\ngenerations of stars: the oldest stars are seen as blue dots, while more difficult\nto identify are the pink-coloured \"new-borns\" in the star delivery room.\nOriginal target: Ce d\u00e9tail d\u2019une photographie infrarouge prise par le t\u00e9lescope\nSpitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d\u2019\u00e9toiles:\nles plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles\n\u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d\u2019accouchement de l\u2019univers.\nProcessed target: Ce d\u00e9tail d\u2019une photographie infrarouge prise par le t\u00e9lescope\nSpitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d\u2019\u00e9toiles:\nles plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles\n\u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d\u2019accouchement de l\u2019univers.\nD.18 WMT English to Romanian\nOriginal input: Taco Bell said it plans to add 2,000 locations in the US by 2022.\nProcessed input: translate English to Romanian: Taco Bell said it plans to add\n2,000 locations in the US by 2022.\nOriginal target: Taco Bell a afirmat c\u0103, p\u00e2n\u0103 \u00een 2022, intent,ioneaz\u0103 s\u0103 deschid\u0103\n2000 de restaurante \u00een SUA.\n54\n", []], "WMT English to French": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German\nOriginal input: \"Luigi often said to me that he never wanted the brothers to end\nup in court,\" she wrote.\nProcessed input: translate English to German: \"Luigi often said to me that he\nnever wanted the brothers to end up in court,\" she wrote.\nOriginal target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder\nvor Gericht landen\", schrieb sie.\nProcessed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder\nvor Gericht landen\", schrieb sie.\nD.17 WMT English to French\nOriginal input: This image section from an infrared recording by the Spitzer telescope\nshows a \"family portrait\" of countless generations of stars: the oldest stars\nare seen as blue dots, while more difficult to identify are the pink-coloured\n\"new-borns\" in the star delivery room.\nProcessed input: translate English to French: This image section from an infrared\nrecording by the Spitzer telescope shows a \"family portrait\" of countless\ngenerations of stars: the oldest stars are seen as blue dots, while more difficult\nto identify are the pink-coloured \"new-borns\" in the star delivery room.\nOriginal target: Ce d\u00e9tail d\u2019une photographie infrarouge prise par le t\u00e9lescope\nSpitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d\u2019\u00e9toiles:\nles plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles\n\u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d\u2019accouchement de l\u2019univers.\nProcessed target: Ce d\u00e9tail d\u2019une photographie infrarouge prise par le t\u00e9lescope\nSpitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d\u2019\u00e9toiles:\nles plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles\n\u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d\u2019accouchement de l\u2019univers.\nD.18 WMT English to Romanian\nOriginal input: Taco Bell said it plans to add 2,000 locations in the US by 2022.\nProcessed input: translate English to Romanian: Taco Bell said it plans to add\n2,000 locations in the US by 2022.\nOriginal target: Taco Bell a afirmat c\u0103, p\u00e2n\u0103 \u00een 2022, intent,ioneaz\u0103 s\u0103 deschid\u0103\n2000 de restaurante \u00een SUA.\n54\n", []], "WMT English to German": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German\nOriginal input: \"Luigi often said to me that he never wanted the brothers to end\nup in court,\" she wrote.\nProcessed input: translate English to German: \"Luigi often said to me that he\nnever wanted the brothers to end up in court,\" she wrote.\nOriginal target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder\nvor Gericht landen\", schrieb sie.\nProcessed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Br\u00fcder\nvor Gericht landen\", schrieb sie.\nD.17 WMT English to French\nOriginal input: This image section from an infrared recording by the Spitzer telescope\nshows a \"family portrait\" of countless generations of stars: the oldest stars\nare seen as blue dots, while more difficult to identify are the pink-coloured\n\"new-borns\" in the star delivery room.\nProcessed input: translate English to French: This image section from an infrared\nrecording by the Spitzer telescope shows a \"family portrait\" of countless\ngenerations of stars: the oldest stars are seen as blue dots, while more difficult\nto identify are the pink-coloured \"new-borns\" in the star delivery room.\nOriginal target: Ce d\u00e9tail d\u2019une photographie infrarouge prise par le t\u00e9lescope\nSpitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d\u2019\u00e9toiles:\nles plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles\n\u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d\u2019accouchement de l\u2019univers.\nProcessed target: Ce d\u00e9tail d\u2019une photographie infrarouge prise par le t\u00e9lescope\nSpitzer montre un \"portrait de famille\" des innombrables g\u00e9n\u00e9rations d\u2019\u00e9toiles:\nles plus vieilles \u00e9toiles sont en bleu et les points roses, plus difficiles\n\u00e0 identifier, sont les \"nouveau-n\u00e9s\" dans la salle d\u2019accouchement de l\u2019univers.\nD.18 WMT English to Romanian\nOriginal input: Taco Bell said it plans to add 2,000 locations in the US by 2022.\nProcessed input: translate English to Romanian: Taco Bell said it plans to add\n2,000 locations in the US by 2022.\nOriginal target: Taco Bell a afirmat c\u0103, p\u00e2n\u0103 \u00een 2022, intent,ioneaz\u0103 s\u0103 deschid\u0103\n2000 de restaurante \u00een SUA.\n54\n", []], "SQuAD": ["Exploring the Limits of Transfer Learning\nfrom manchester united\u2019s dutch manager louis van gaal .\njanuzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame .\nOriginal target: the belgian duo took to the dance floor on monday night with\nsome friends .\nmanchester united face newcastle in the premier league on\nwednesday .\nred devils will be looking for just their second league away win\nin seven .\nlouis van gaal\u2019s side currently sit two points clear of liverpool\nin fourth .\nProcessed target: the belgian duo took to the dance floor on monday night with\nsome friends .\nmanchester united face newcastle in the premier league on\nwednesday .\nred devils will be looking for just their second league away win\nin seven .\nlouis van gaal\u2019s side currently sit two points clear of liverpool\nin fourth .\nD.15 SQuAD\nOriginal input:\nQuestion: What does increased oxygen concentrations in the patient\u2019s lungs\ndisplace?\nContext: Hyperbaric (high-pressure) medicine uses special oxygen chambers\nto increase the partial pressure of O 2 around the patient and, when needed,\nthe medical staff.\nCarbon monoxide poisoning, gas gangrene, and decompression\nsickness (the \u2019bends\u2019) are sometimes treated using these devices.\nIncreased\nO 2 concentration in the lungs helps to displace carbon monoxide from the\nheme group of hemoglobin.\nOxygen gas is poisonous to the anaerobic bacteria\nthat cause gas gangrene, so increasing its partial pressure helps kill them.\nDecompression sickness occurs in divers who decompress too quickly after\na dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming\nin their blood.\nIncreasing the pressure of O 2 as soon as possible is part\nof the treatment.\nProcessed input: question: What does increased oxygen concentrations in the patient\u2019s\nlungs displace?\ncontext: Hyperbaric (high-pressure) medicine uses special\noxygen chambers to increase the partial pressure of O 2 around the patient\nand, when needed, the medical staff.\nCarbon monoxide poisoning, gas gangrene,\nand decompression sickness (the \u2019bends\u2019) are sometimes treated using these\ndevices.\nIncreased O 2 concentration in the lungs helps to displace carbon\nmonoxide from the heme group of hemoglobin.\nOxygen gas is poisonous to the\nanaerobic bacteria that cause gas gangrene, so increasing its partial pressure\nhelps kill them.\nDecompression sickness occurs in divers who decompress too\nquickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and\nhelium, forming in their blood.\nIncreasing the pressure of O 2 as soon as\npossible is part of the treatment.\n53\n", []], "CNN/Daily Mail": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: 1\nProcessed target: stable\nD.14 CNN/Daily Mail\nOriginal input: marouane fellaini and adnan januzaj continue to show the world\nthey are not just teammates but also best mates.\nthe manchester united and\nbelgium duo both posted pictures of themselves out at a restaurant on monday\nnight ahead of their game against newcastle on wednesday .\njanuzaj poses\nin the middle of fellaini and a friend looking like somebody who failed to\nreceive the memo about it being a jackson 5 themed night.\npremier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n.\nmanchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch .\nmanchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj .\nthe disco effect\nin the background adds to the theory, but januzaj doesn\u2019t seem to mind as\nthey later pose on the dance floor with other friends.\nunited haven\u2019t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release.\nhowever, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom .\njanuzaj receives some words of wisdom\nfrom manchester united\u2019s dutch manager louis van gaal .\njanuzaj and fellaini\nare joined by some friends as they take to the dance floor ahead of the newcastle\ngame .\nProcessed input: summarize: marouane fellaini and adnan januzaj continue to show\nthe world they are not just teammates but also best mates.\nthe manchester\nunited and belgium duo both posted pictures of themselves out at a restaurant\non monday night ahead of their game against newcastle on wednesday .\njanuzaj\nposes in the middle of fellaini and a friend looking like somebody who failed\nto receive the memo about it being a jackson 5 themed night.\npremier league\nduo adnan januzaj and marouane fellaini pose with a friend on the dance floor\n.\nmanchester united and belgium duo fellaini and januzaj are good friends\nboth on and off the pitch .\nmanchester united ace fellaini runs over to the\nbench to celebrate his goal against qpr with friend januzaj .\nthe disco effect\nin the background adds to the theory, but januzaj doesn\u2019t seem to mind as\nthey later pose on the dance floor with other friends.\nunited haven\u2019t had\ntoo many reasons to have a song and dance this season so it seems they may\nbe hitting the discotheques as another form of release.\nhowever, victory against\nnewcastle on wednesday would leave manager louis van gaal at least tapping\nhis toes as they continue to fight for a champions league spot this season.\njanuzaj and robin van persie join fellaini in celebrating in front of the\nmanchester united fans at west brom .\njanuzaj receives some words of wisdom\n52\n", []], "WSC and DPR": ["Exploring the Limits of Transfer Learning\nat their Aunt Julie\u2019s pond.<br><b>Sent 5: </b>Joey woke up early in the morning\nto eat some food before they left.<br><b>Sent 6: </b>He couldn\u2019t find anything\nto eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit\n(a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and\nJimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their\nfriend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for\nseveral hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The\ntwo squirrels ate some food that Joey\u2019s mom, Jasmine, made and went off to\nbed.<br>\nOriginal target: 1\nProcessed target: True\nD.12 WiC\nOriginal input:\nPOS: N\nSentence 1: It was the deliberation of his act that was insulting .\nSentence 2: The deliberations of the jury .\nWord: deliberation\nProcessed input: wic pos: N sentence1: It was the deliberation of his act that\nwas insulting .\nsentence2: The deliberations of the jury .\nword: deliberation\nOriginal target: 0\nProcessed target: False\nD.13 WSC and DPR\nOriginal input:\nSpan 2 text: it\nSpan 1 text: stable\nSpan 2 index: 20\nSpan 1 index: 1\nText: The stable was very roomy, with four good stalls; a large swinging window\nopened into the yard , which made it pleasant and airy.\nProcessed input: wsc:\nThe stable was very roomy, with four good stalls; a large\nswinging window opened into the yard , which made *it* pleasant and airy.\n51\n", []], "WiC": ["Exploring the Limits of Transfer Learning\nat their Aunt Julie\u2019s pond.<br><b>Sent 5: </b>Joey woke up early in the morning\nto eat some food before they left.<br><b>Sent 6: </b>He couldn\u2019t find anything\nto eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit\n(a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and\nJimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their\nfriend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for\nseveral hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The\ntwo squirrels ate some food that Joey\u2019s mom, Jasmine, made and went off to\nbed.<br>\nOriginal target: 1\nProcessed target: True\nD.12 WiC\nOriginal input:\nPOS: N\nSentence 1: It was the deliberation of his act that was insulting .\nSentence 2: The deliberations of the jury .\nWord: deliberation\nProcessed input: wic pos: N sentence1: It was the deliberation of his act that\nwas insulting .\nsentence2: The deliberations of the jury .\nword: deliberation\nOriginal target: 0\nProcessed target: False\nD.13 WSC and DPR\nOriginal input:\nSpan 2 text: it\nSpan 1 text: stable\nSpan 2 index: 20\nSpan 1 index: 1\nText: The stable was very roomy, with four good stalls; a large swinging window\nopened into the yard , which made it pleasant and airy.\nProcessed input: wsc:\nThe stable was very roomy, with four good stalls; a large\nswinging window opened into the yard , which made *it* pleasant and airy.\n51\n", []], "MultiRC": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nD.10 COPA\nOriginal input:\nQuestion: effect\nPremise: Political violence broke out in the nation.\nChoice 1: Many citizens relocated to the capitol.\nChoice 2: Many citizens took refuge in other territories.\nProcessed input: copa choice1: Many citizens relocated to the capitol.\nchoice2:\nMany citizens took refuge in other territories.\npremise: Political violence\nbroke out in the nation.\nquestion: effect\nOriginal target: 1\nProcessed target: True\nD.11 MultiRC\nOriginal input:\nAnswer: There was only pie to eat, rather than traditional breakfast foods\nParagraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent\n2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent\n3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent\n4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie\u2019s\npond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food\nbefore they left.<br><b>Sent 6: </b>He couldn\u2019t find anything to eat except\nfor pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear),\nor oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went\nto the pond.<br><b>Sent 9: </b>On their way there they saw their friend\nJack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several\nhours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\n</b>The two squirrels ate some food that Joey\u2019s mom, Jasmine, made and went\noff to bed.<br>\nQuestion: Why was Joey surprised the morning he woke up for breakfast?\nProcessed input: multirc question: Why was Joey surprised the morning he woke\nup for breakfast?\nanswer: There was only pie to eat, rather than traditional\nbreakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel\nnamed Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin\nJimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were\nalways laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together\n50\n", []], "COPA": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nD.10 COPA\nOriginal input:\nQuestion: effect\nPremise: Political violence broke out in the nation.\nChoice 1: Many citizens relocated to the capitol.\nChoice 2: Many citizens took refuge in other territories.\nProcessed input: copa choice1: Many citizens relocated to the capitol.\nchoice2:\nMany citizens took refuge in other territories.\npremise: Political violence\nbroke out in the nation.\nquestion: effect\nOriginal target: 1\nProcessed target: True\nD.11 MultiRC\nOriginal input:\nAnswer: There was only pie to eat, rather than traditional breakfast foods\nParagraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent\n2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent\n3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent\n4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie\u2019s\npond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food\nbefore they left.<br><b>Sent 6: </b>He couldn\u2019t find anything to eat except\nfor pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear),\nor oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went\nto the pond.<br><b>Sent 9: </b>On their way there they saw their friend\nJack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several\nhours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent\n12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent\n13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When\nthey got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent\n15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16:\n</b>The two squirrels ate some food that Joey\u2019s mom, Jasmine, made and went\noff to bed.<br>\nQuestion: Why was Joey surprised the morning he woke up for breakfast?\nProcessed input: multirc question: Why was Joey surprised the morning he woke\nup for breakfast?\nanswer: There was only pie to eat, rather than traditional\nbreakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel\nnamed Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin\nJimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were\nalways laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together\n50\n", []], "CB": ["Exploring the Limits of Transfer Learning\nOriginal target: 0\nProcessed target: not_duplicate\nD.7 SST2\nOriginal input:\nSentence: it confirms fincher \u2019s status as a film maker who artfully bends\ntechnical know-how to the service of psychological insight .\nProcessed input: sst2 sentence: it confirms fincher \u2019s status as a film maker\nwho artfully bends technical know-how to the service of psychological insight\n.\nOriginal target: 1\nProcessed target: positive\nD.8 STSB\nOriginal input:\nSentence 1: Representatives for Puretunes could not immediately be reached\nfor comment Wednesday.\nSentence 2: Puretunes representatives could not be located Thursday to comment\non the suit.\nProcessed input: stsb sentence1: Representatives for Puretunes could not immediately\nbe reached for comment Wednesday.\nsentence2: Puretunes representatives could\nnot be located Thursday to comment on the suit.\nOriginal target: 3.25\nProcessed target: 3.2\nD.9 CB\nOriginal input:\nHypothesis: Valence was helping\nPremise: Valence the void-brain, Valence the virtuous valet.\nWhy couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft?\nDid he think\nhe was helping?\nProcessed input: cb hypothesis: Valence was helping premise: Valence the void-brain,\nValence the virtuous valet.\nWhy couldn\u2019t the figger choose his own portion\nof titanic anatomy to shaft?\nDid he think he was helping?\nOriginal target: 1\nProcessed target: contradiction\n49\n", []], "STSB": ["Exploring the Limits of Transfer Learning\nOriginal target: 0\nProcessed target: not_duplicate\nD.7 SST2\nOriginal input:\nSentence: it confirms fincher \u2019s status as a film maker who artfully bends\ntechnical know-how to the service of psychological insight .\nProcessed input: sst2 sentence: it confirms fincher \u2019s status as a film maker\nwho artfully bends technical know-how to the service of psychological insight\n.\nOriginal target: 1\nProcessed target: positive\nD.8 STSB\nOriginal input:\nSentence 1: Representatives for Puretunes could not immediately be reached\nfor comment Wednesday.\nSentence 2: Puretunes representatives could not be located Thursday to comment\non the suit.\nProcessed input: stsb sentence1: Representatives for Puretunes could not immediately\nbe reached for comment Wednesday.\nsentence2: Puretunes representatives could\nnot be located Thursday to comment on the suit.\nOriginal target: 3.25\nProcessed target: 3.2\nD.9 CB\nOriginal input:\nHypothesis: Valence was helping\nPremise: Valence the void-brain, Valence the virtuous valet.\nWhy couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft?\nDid he think\nhe was helping?\nProcessed input: cb hypothesis: Valence was helping premise: Valence the void-brain,\nValence the virtuous valet.\nWhy couldn\u2019t the figger choose his own portion\nof titanic anatomy to shaft?\nDid he think he was helping?\nOriginal target: 1\nProcessed target: contradiction\n49\n", []], "SST2": ["Exploring the Limits of Transfer Learning\nOriginal target: 0\nProcessed target: not_duplicate\nD.7 SST2\nOriginal input:\nSentence: it confirms fincher \u2019s status as a film maker who artfully bends\ntechnical know-how to the service of psychological insight .\nProcessed input: sst2 sentence: it confirms fincher \u2019s status as a film maker\nwho artfully bends technical know-how to the service of psychological insight\n.\nOriginal target: 1\nProcessed target: positive\nD.8 STSB\nOriginal input:\nSentence 1: Representatives for Puretunes could not immediately be reached\nfor comment Wednesday.\nSentence 2: Puretunes representatives could not be located Thursday to comment\non the suit.\nProcessed input: stsb sentence1: Representatives for Puretunes could not immediately\nbe reached for comment Wednesday.\nsentence2: Puretunes representatives could\nnot be located Thursday to comment on the suit.\nOriginal target: 3.25\nProcessed target: 3.2\nD.9 CB\nOriginal input:\nHypothesis: Valence was helping\nPremise: Valence the void-brain, Valence the virtuous valet.\nWhy couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft?\nDid he think\nhe was helping?\nProcessed input: cb hypothesis: Valence was helping premise: Valence the void-brain,\nValence the virtuous valet.\nWhy couldn\u2019t the figger choose his own portion\nof titanic anatomy to shaft?\nDid he think he was helping?\nOriginal target: 1\nProcessed target: contradiction\n49\n", []], "QQP": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: 2\nProcessed target: contradiction\nD.4 MRPC\nOriginal input:\nSentence 1: We acted because we saw the existing evidence in a new light ,\nthrough the prism of our experience on 11 September , \" Rumsfeld said .\nSentence 2: Rather , the US acted because the administration saw \" existing\nevidence in a new light , through the prism of our experience on September\n11 \" .\nProcessed input: mrpc sentence1: We acted because we saw the existing evidence\nin a new light , through the prism of our experience on 11 September , \" Rumsfeld\nsaid .\nsentence2: Rather , the US acted because the administration saw \"\nexisting evidence in a new light , through the prism of our experience on\nSeptember 11 \" .\nOriginal target: 1\nProcessed target: equivalent\nD.5 QNLI\nOriginal input:\nQuestion: Where did Jebe die?\nSentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and\nJebe died on the road back to Samarkand.\nProcessed input: qnli question: Where did Jebe die?\nsentence: Genghis Khan recalled\nSubutai back to Mongolia soon afterwards, and Jebe died on the road back to\nSamarkand.\nOriginal target: 0\nProcessed target: entailment\nD.6 QQP\nOriginal input:\nQuestion 1: What attributes would have made you highly desirable in ancient\nRome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable\nin ancient Rome?\nquestion2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A\nFRESHER?\n48\n", []], "QNLI": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: 2\nProcessed target: contradiction\nD.4 MRPC\nOriginal input:\nSentence 1: We acted because we saw the existing evidence in a new light ,\nthrough the prism of our experience on 11 September , \" Rumsfeld said .\nSentence 2: Rather , the US acted because the administration saw \" existing\nevidence in a new light , through the prism of our experience on September\n11 \" .\nProcessed input: mrpc sentence1: We acted because we saw the existing evidence\nin a new light , through the prism of our experience on 11 September , \" Rumsfeld\nsaid .\nsentence2: Rather , the US acted because the administration saw \"\nexisting evidence in a new light , through the prism of our experience on\nSeptember 11 \" .\nOriginal target: 1\nProcessed target: equivalent\nD.5 QNLI\nOriginal input:\nQuestion: Where did Jebe die?\nSentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and\nJebe died on the road back to Samarkand.\nProcessed input: qnli question: Where did Jebe die?\nsentence: Genghis Khan recalled\nSubutai back to Mongolia soon afterwards, and Jebe died on the road back to\nSamarkand.\nOriginal target: 0\nProcessed target: entailment\nD.6 QQP\nOriginal input:\nQuestion 1: What attributes would have made you highly desirable in ancient\nRome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable\nin ancient Rome?\nquestion2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A\nFRESHER?\n48\n", []], "MRPC": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nOriginal target: 2\nProcessed target: contradiction\nD.4 MRPC\nOriginal input:\nSentence 1: We acted because we saw the existing evidence in a new light ,\nthrough the prism of our experience on 11 September , \" Rumsfeld said .\nSentence 2: Rather , the US acted because the administration saw \" existing\nevidence in a new light , through the prism of our experience on September\n11 \" .\nProcessed input: mrpc sentence1: We acted because we saw the existing evidence\nin a new light , through the prism of our experience on 11 September , \" Rumsfeld\nsaid .\nsentence2: Rather , the US acted because the administration saw \"\nexisting evidence in a new light , through the prism of our experience on\nSeptember 11 \" .\nOriginal target: 1\nProcessed target: equivalent\nD.5 QNLI\nOriginal input:\nQuestion: Where did Jebe die?\nSentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and\nJebe died on the road back to Samarkand.\nProcessed input: qnli question: Where did Jebe die?\nsentence: Genghis Khan recalled\nSubutai back to Mongolia soon afterwards, and Jebe died on the road back to\nSamarkand.\nOriginal target: 0\nProcessed target: entailment\nD.6 QQP\nOriginal input:\nQuestion 1: What attributes would have made you highly desirable in ancient\nRome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable\nin ancient Rome?\nquestion2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A\nFRESHER?\n48\n", []], "MNLI": ["Exploring the Limits of Transfer Learning\nof a hot dog bar on saturday .\nalves is yet to be offered a new deal at\nthe nou camp .\nclick here for all the latest barcelona news .\nAppendix D. Preprocessed Examples\nIn this section, we provide examples of our preprocessing for each of the data sets we consider.\nD.1 CoLA\nOriginal input:\nSentence: John made Bill master of himself.\nProcessed input: cola sentence: John made Bill master of himself.\nOriginal target: 1\nProcessed target: acceptable\nD.2 RTE\nOriginal input:\nSentence 1: A smaller proportion of Yugoslavia\u2019s Italians were settled in Slovenia\n(at the 1991 national census, some 3000 inhabitants of Slovenia declared\nthemselves as ethnic Italians).\nSentence 2: Slovenia has 3,000 inhabitants.\nProcessed input: rte sentence1: A smaller proportion of Yugoslavia\u2019s Italians\nwere settled in Slovenia (at the 1991 national census, some 3000 inhabitants\nof Slovenia declared themselves as ethnic Italians).\nsentence2: Slovenia\nhas 3,000 inhabitants.\nOriginal target: 1\nProcessed target: not_entailment\nD.3 MNLI\nOriginal input:\nHypothesis: The St. Louis Cardinals have always won.\nPremise: yeah well losing is i mean i\u2019m i\u2019m originally from Saint Louis and\nSaint Louis Cardinals when they were there were uh a mostly a losing team\nbut\nProcessed input: mnli hypothesis: The St.\nLouis Cardinals have always won.\npremise:\nyeah well losing is i mean i\u2019m i\u2019m originally from Saint Louis and Saint Louis\nCardinals when they were there were uh a mostly a losing team but\n47\n", []], "RTE": ["Exploring the Limits of Transfer Learning\n4.2 Outlook\nThe inconvenience of large models An unsurprising but important result from our\nstudy is that larger models tend to perform better. The fact that the hardware used for\nrunning these models is continually getting cheaper and more powerful suggests that\nscaling up may continue to be a promising way to achieve better performance (Sutton,\n2019). However, it will always be the case that there are applications and scenarios\nwhere using a smaller or less expensive model is helpful, for example when performing\nclient-side inference or federated learning (Kone\u010dn`y et al., 2015, 2016). Relatedly, one\nbeneficial use of transfer learning is the possibility of attaining good performance on\nlow-resource tasks. Low-resource tasks often occur (by definition) in settings where\none lacks the assets to label more data. It follows that low-resource applications often\nalso have limited access to computational resources which can incur additional costs.\nAs a result, we advocate for research on methods that achieve stronger performance\nwith cheaper models so that transfer learning can be applied where it will have the\nmost impact. Some current work along these lines include distillation (Hinton et al.,\n2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and\nconditional computation (Shazeer et al., 2017).\nMore efficient knowledge extraction Recall that one of the goals of pre-training is\n(loosely speaking) to provide the model with general-purpose \u201cknowledge\u201d that improves\nits performance on downstream tasks. The method we use in this work, which is\ncurrently common practice, is to train the model to denoise corrupted spans of text.\nWe suspect that this simplistic technique may not be a very efficient way to teach the\nmodel general-purpose knowledge. More concretely, it would be useful to be able to\nattain good fine-tuning performance without needing to train our models on 1 trillion\ntokens of text first. Some concurrent work along these lines improves efficiency by\npre-training a model to distinguish between real and machine-generated text (Clark\net al., 2020).\nFormalizing the similarity between tasks We observed that pre-training on unlabeled\nin-domain data can improve performance on downstream tasks (Section 3.4). This\nfinding mostly relies on basic observations like the fact that SQuAD was created using\ndata from Wikipedia. It would be useful to formulate a more rigorous notion of the\n\u201csimilarity\u201d between the pre-training and downstream tasks, so that we could make\nmore principled choices about what source of unlabeled data to use. There is some\nearly empirical work along these lines in the field of computer vision (Huh et al., 2016;\nKornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could\nalso help choose supervised pre-training tasks, which has been shown to be helpful for\nthe GLUE benchmark (Phang et al., 2018).\nLanguage-agnostic models We were disappointed to find that English-only pre-training\ndid not achieve state-of-the-art results on the translation tasks we studied. We also\nare interested in avoiding the logistical difficulty of needing to specify which languages\na vocabulary can encode ahead of time. To address these issues, we are interested in\nfurther investigating language-agnostic models, i.e. models that can perform a given\nNLP task with good performance regardless of the text\u2019s language. This is an especially\n43\n", []], "CoLA": ["Exploring the Limits of Transfer Learning\nModel\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline-1T\n84.80\n19.62\n83.01\n73.90\n27.46\n40.30\n28.34\nT5-Base\n85.97\n20.90\n85.44\n75.64\n28.37\n41.37\n28.98\nTable 15: Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. \u201cBaseline-1T\u201d\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of 235 \u224834B\ntokens (as was used for the baseline).\n\u201cnon-scaling\u201d changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline.\nNevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5\u2019s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors.\n4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field.\n4.1 Takeaways\nText-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale.\nArchitectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as \u201cencoder-only\u201d (e.g. BERT) or \u201cdecoder-only\u201d (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count.\n41\n", []], "Preprocessed Examples": ["Exploring the Limits of Transfer Learning\nModel\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline-1T\n84.80\n19.62\n83.01\n73.90\n27.46\n40.30\n28.34\nT5-Base\n85.97\n20.90\n85.44\n75.64\n28.37\n41.37\n28.98\nTable 15: Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. \u201cBaseline-1T\u201d\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of 235 \u224834B\ntokens (as was used for the baseline).\n\u201cnon-scaling\u201d changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline.\nNevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5\u2019s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors.\n4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field.\n4.1 Takeaways\nText-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale.\nArchitectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as \u201cencoder-only\u201d (e.g. BERT) or \u201cdecoder-only\u201d (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count.\n41\n", []], "Example Predictions on CNN/Daily Mail": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nthe words in the model\u2019s output are a subset of the words in the candidate noun phrase\n(or vice versa) and assign a \u201cFalse\u201d label otherwise. This removes roughly half of the WSC\ntraining set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples\nfrom DPR are annotated with the correct referent noun, making it easy to use this data set\nin the format listed above.\nThe WNLI training and validation sets have a significant overlap with the WSC training\nset. To avoid leaking validation examples into our training data (a particular issue in the\nmulti-task experiments of Section 3.5.2), we therefore never train on WNLI and never report\nresults on the WNLI validation set. Omitting results on the WNLI validation set is standard\npractice (Devlin et al., 2018) due to the fact that it is \u201cadversarial\u201d with respect to the\ntraining set, i.e. validation examples are all slightly-perturbed versions of training examples\nwith the opposite label. As such, we do not include WNLI in the average GLUE score\nwhenever we report on the validation set (all sections except Section 3.7 where results\nare presented on the test sets). Converting examples from WNLI to the \u201creferent noun\nprediction\u201d variant described above is a little more involved; we describe this process in\nAppendix B.\n3. Experiments\nRecent advances in transfer learning for NLP have come from a wide variety of developments,\nsuch as new pre-training objectives, model architectures, unlabeled data sets, and more.\nIn this section, we carry out an empirical survey of these techniques in hopes of teasing\napart their contribution and significance. We then combine the insights gained to attain\nstate-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly\ngrowing area of research, it is not feasible for us to cover every possible technique or idea\nin our empirical study. For a broader literature review, we recommend a recent survey by\nRuder et al. (2019).\nWe systematically study these contributions by taking a reasonable baseline (described\nin Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3\nwe measure the performance of different unsupervised objectives while keeping the rest of\nour experimental pipeline fixed. This \u201ccoordinate ascent\u201d approach might miss second-order\neffects (for example, some particular unsupervised objective may work best on a model\nlarger than our baseline setting), but performing a combinatorial exploration of all of the\nfactors in our study would be prohibitively expensive. In future work, we expect it could be\nfruitful to more thoroughly consider combinations of the approaches we study.\nOur goal is to compare a variety of different approaches on a diverse set of tasks while\nkeeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do\nnot exactly replicate existing approaches. For example, \u201cencoder-only\u201d models like BERT\n(Devlin et al., 2018) are designed to produce a single prediction per input token or a single\nprediction for an entire input sequence. This makes them applicable for classification or span\nprediction tasks but not for generative tasks like translation or abstractive summarization.\nAs such, none of the model architectures we consider are identical to BERT or consist of an\nencoder-only structure. Instead, we test approaches that are similar in spirit\u2014for example,\nwe consider an analogous objective to BERT\u2019s \u201cmasked language modeling\u201d objective in\n10\n", []], "Converting WNLI to Our Text-to-Text Format": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n2. Setup\nBefore presenting the results from our large-scale empirical study, we review the necessary\nbackground topics required to understand our results, including the Transformer model\narchitecture and the downstream tasks we evaluate on. We also introduce our approach\nfor treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled\nCorpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text\ndata. We refer to our model and framework as the \u201cText-to-Text Transfer Transformer\u201d\n(T5).\n2.1 Model\nEarly results on transfer learning for NLP leveraged recurrent neural networks (Peters\net al., 2018; Howard and Ruder, 2018), but it has recently become more common to use\nmodels based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017). The Transformer\nwas initially shown to be effective for machine translation, but it has subsequently been\nused in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann\net al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are\nbased on the Transformer architecture. Apart from the details mentioned below and the\nvariants we explore in Section 3.2, we do not deviate significantly from this architecture as\noriginally proposed. Instead of providing a comprehensive definition of this model, we refer\nthe interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for\na more detailed introduction.\nThe primary building block of the Transformer is self-attention (Cheng et al., 2016).\nSelf-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes\na sequence by replacing each element by a weighted average of the rest of the sequence.\nThe original Transformer consisted of an encoder-decoder architecture and was intended\nfor sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has\nrecently also become common to use models consisting of a single Transformer layer stack,\nwith varying forms of self-attention used to produce architectures appropriate for language\nmodeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction\ntasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural\nvariants in Section 3.2.\nOverall, our encoder-decoder Transformer implementation closely follows its originally-\nproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to\na sequence of embeddings, which is then passed into the encoder. The encoder consists\nof a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer\nfollowed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to\nthe input of each subcomponent. We use a simplified version of layer normalization where\nthe activations are only rescaled and no additive bias is applied. After layer normalization,\na residual skip connection (He et al., 2016) adds each subcomponent\u2019s input to its output.\nDropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip\nconnection, on the attention weights, and at the input and output of the entire stack. The\ndecoder is similar in structure to the encoder except that it includes a standard attention\n3. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n4. http://jalammar.github.io/illustrated-transformer/\n4\n", []], "Contributions": ["Journal of Machine Learning Research 21 (2020) 1-67\nSubmitted 1/20; Revised 6/20; Published 6/20\nExploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer\nColin Raffel\u2217\ncraffel@gmail.com\nNoam Shazeer\u2217\nnoam@google.com\nAdam Roberts\u2217\nadarob@google.com\nKatherine Lee\u2217\nkatherinelee@google.com\nSharan Narang\nsharannarang@google.com\nMichael Matena\nmmatena@google.com\nYanqi Zhou\nyanqiz@google.com\nWei Li\nmweili@google.com\nPeter J. Liu\npeterjliu@google.com\nGoogle, Mountain View, CA 94043, USA\nEditor: Ivan Titov\nAbstract\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-\ntuned on a downstream task, has emerged as a powerful technique in natural language\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\nlearning techniques for NLP by introducing a unified framework that converts all text-based\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\ndozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results\non many benchmarks covering summarization, question answering, text classification, and\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.1\nKeywords:\ntransfer learning, natural language processing, multi-task learning, attention-\nbased models, deep learning\n1. Introduction\nTraining a machine learning model to perform natural language processing (NLP) tasks\noften requires that the model can process text in a way that is amenable to downstream\nlearning. This can be loosely viewed as developing general-purpose knowledge that allows\nthe model to \u201cunderstand\u201d text. This knowledge can range from low-level (e.g. the spelling\n\u2217. Equal contribution. A description of each author\u2019s contribution is available in Appendix A. Correspondence\nto craffel@gmail.com.\n1. https://github.com/google-research/text-to-text-transfer-transformer\n\u00a92020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v21/20-074.html.\narXiv:1910.10683v4  [cs.LG]  19 Sep 2023\n", []], "Outlook": ["Exploring the Limits of Transfer Learning\n4.2 Outlook\nThe inconvenience of large models An unsurprising but important result from our\nstudy is that larger models tend to perform better. The fact that the hardware used for\nrunning these models is continually getting cheaper and more powerful suggests that\nscaling up may continue to be a promising way to achieve better performance (Sutton,\n2019). However, it will always be the case that there are applications and scenarios\nwhere using a smaller or less expensive model is helpful, for example when performing\nclient-side inference or federated learning (Kone\u010dn`y et al., 2015, 2016). Relatedly, one\nbeneficial use of transfer learning is the possibility of attaining good performance on\nlow-resource tasks. Low-resource tasks often occur (by definition) in settings where\none lacks the assets to label more data. It follows that low-resource applications often\nalso have limited access to computational resources which can incur additional costs.\nAs a result, we advocate for research on methods that achieve stronger performance\nwith cheaper models so that transfer learning can be applied where it will have the\nmost impact. Some current work along these lines include distillation (Hinton et al.,\n2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and\nconditional computation (Shazeer et al., 2017).\nMore efficient knowledge extraction Recall that one of the goals of pre-training is\n(loosely speaking) to provide the model with general-purpose \u201cknowledge\u201d that improves\nits performance on downstream tasks. The method we use in this work, which is\ncurrently common practice, is to train the model to denoise corrupted spans of text.\nWe suspect that this simplistic technique may not be a very efficient way to teach the\nmodel general-purpose knowledge. More concretely, it would be useful to be able to\nattain good fine-tuning performance without needing to train our models on 1 trillion\ntokens of text first. Some concurrent work along these lines improves efficiency by\npre-training a model to distinguish between real and machine-generated text (Clark\net al., 2020).\nFormalizing the similarity between tasks We observed that pre-training on unlabeled\nin-domain data can improve performance on downstream tasks (Section 3.4). This\nfinding mostly relies on basic observations like the fact that SQuAD was created using\ndata from Wikipedia. It would be useful to formulate a more rigorous notion of the\n\u201csimilarity\u201d between the pre-training and downstream tasks, so that we could make\nmore principled choices about what source of unlabeled data to use. There is some\nearly empirical work along these lines in the field of computer vision (Huh et al., 2016;\nKornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could\nalso help choose supervised pre-training tasks, which has been shown to be helpful for\nthe GLUE benchmark (Phang et al., 2018).\nLanguage-agnostic models We were disappointed to find that English-only pre-training\ndid not achieve state-of-the-art results on the translation tasks we studied. We also\nare interested in avoiding the logistical difficulty of needing to specify which languages\na vocabulary can encode ahead of time. To address these issues, we are interested in\nfurther investigating language-agnostic models, i.e. models that can perform a given\nNLP task with good performance regardless of the text\u2019s language. This is an especially\n43\n", []], "Takeaways": ["Exploring the Limits of Transfer Learning\nModel\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline-1T\n84.80\n19.62\n83.01\n73.90\n27.46\n40.30\n28.34\nT5-Base\n85.97\n20.90\n85.44\n75.64\n28.37\n41.37\n28.98\nTable 15: Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. \u201cBaseline-1T\u201d\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of 235 \u224834B\ntokens (as was used for the baseline).\n\u201cnon-scaling\u201d changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline.\nNevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5\u2019s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors.\n4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field.\n4.1 Takeaways\nText-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale.\nArchitectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as \u201cencoder-only\u201d (e.g. BERT) or \u201cdecoder-only\u201d (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count.\n41\n", []], "Reflection": ["Exploring the Limits of Transfer Learning\nModel\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Baseline\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nBaseline-1T\n84.80\n19.62\n83.01\n73.90\n27.46\n40.30\n28.34\nT5-Base\n85.97\n20.90\n85.44\n75.64\n28.37\n41.37\n28.98\nTable 15: Performance comparison of T5-Base to our baseline experimental setup used in\nthe rest of the paper. Results are reported on the validation set. \u201cBaseline-1T\u201d\nrefers to the performance achieved by pre-training the baseline model on 1 trillion\ntokens (the same number used for the T5 model variants) instead of 235 \u224834B\ntokens (as was used for the baseline).\n\u201cnon-scaling\u201d changes we made when designing T5. As such, comparing the performance of\nthese two models gives us a concrete measurement of the impact of the insights from our\nsystematic study.\nThe performance of these three model configurations is shown in Table 15. Consistent\nwith the findings in Section 3.6, we find that additional pre-training improves performance\nover the baseline.\nNevertheless, T5-Base substantially outperforms baseline-1T on all\ndownstream tasks. This suggests that scale is not the only factor that contributes to T5\u2019s\nsuccess. We hypothesize that the larger models benefit not only from their increased size\nbut also from these non-scaling factors.\n4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most\nsignificant findings. Our results provide some high-level perspective on which avenues of\nresearch might be more or less promising. To conclude, we outline some topics we think\nmight provide effective approaches for further progressing the field.\n4.1 Takeaways\nText-to-text Our text-to-text framework provides a simple way to train a single model\non a wide variety of text tasks using the same loss function and decoding procedure.\nWe showed how this approach can be successfully applied to generative tasks like\nabstractive summarization, classification tasks like natural language inference, and\neven regression tasks like STS-B. In spite of its simplicity, we found the text-to-\ntext framework obtained comparable performance to task-specific architectures and\nultimately produced state-of-the-art results when combined with scale.\nArchitectures While some work on transfer learning for NLP has considered architectural\nvariants of the Transformer, we found the original encoder-decoder form worked\nbest in our text-to-text framework. Though an encoder-decoder model uses twice as\nmany parameters as \u201cencoder-only\u201d (e.g. BERT) or \u201cdecoder-only\u201d (language model)\narchitectures, it has a similar computational cost. We also showed that sharing the\nparameters in the encoder and decoder did not result in a substantial performance\ndrop while halving the total parameter count.\n41\n", []], "Putting It All Together": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\ninference more expensive. In contrast, the cost of pre-training a small model for longer is\neffectively amortized if it is applied to many downstream tasks. Separately, we note that\nensembling N separate models has a similar cost to using a model that has an N\u00d7 higher\ncomputational cost. As a result, some consideration for the eventual use of the model is\nimportant when choosing between scaling methods.\n3.7 Putting It All Together\nWe now leverage the insights from our systematic study to determine how far we can push\nperformance on popular NLP benchmarks. We are also interested in exploring the current\nlimits of transfer learning for NLP by training larger models on large amounts of data. We\nstart with our baseline training approach and make the following changes:\nObjective We swap out the i.i.d. denoising objective in our baseline for the span-corruption\nobjective described in Section 3.3.4, which was loosely inspired by SpanBERT (Joshi\net al., 2019). Specifically, we use a mean span length of 3 and corrupt 15% of the\noriginal sequence. We found that this objective produced marginally better performance\n(Table 7) while being slightly more computationally efficient due to shorter target\nsequence lengths.\nLonger training Our baseline model uses a relatively small amount of pre-training (1\u20444 as\nmuch as BERT (Devlin et al., 2018), 1\u204416 as much as XLNet (Yang et al., 2019), 1\u204464 as\nmuch as RoBERTa (Liu et al., 2019c), etc.). Fortunately, C4 is big enough that we\ncan train for substantially longer without repeating data (which can be detrimental,\nas shown in Section 3.4.2). We found in Section 3.6 that additional pre-training can\nindeed be helpful, and that both increasing the batch size and increasing the number of\ntraining steps can confer this benefit. We therefore pre-train our models for 1 million\nsteps on a batch size of 211 sequences of length 512, corresponding to a total of about\n1 trillion pre-training tokens (about 32\u00d7 as many as our baseline). In Section 3.4.1, we\nshowed that pre-training on the RealNews-like, WebText-like, and Wikipedia + TBC\ndata sets outperformed pre-training on C4 on a few downstream tasks. However, these\ndata set variants are sufficiently small that they would be repeated hundreds of times\nover the course of pre-training on 1 trillion tokens. Since we showed in Section 3.4.2\nthat this repetition could be harmful, we opted instead to continue using the C4 data\nset.\nModel sizes In Section 3.6 we also showed how scaling up the baseline model size improved\nperformance. However, using smaller models can be helpful in settings where limited\ncomputational resources are available for fine-tuning or inference. Based on these\nfactors, we train models with a wide range of sizes:\n\u2022 Base.\nThis is our baseline model, whose hyperparameters are described in\nSection 3.1.1. It has roughly 220 million parameters.\n\u2022 Small. We consider a smaller model, which scales the baseline down by using\ndmodel = 512, dff = 2,048, 8-headed attention, and only 6 layers each in the\nencoder and decoder. This variant has about 60 million parameters.\n36\n", []], "Scaling": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nTraining strategy\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Unsupervised pre-training + fine-tuning\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nMulti-task training\n81.42\n19.24\n79.78\n67.30\n25.21\n36.30\n27.76\nMulti-task pre-training + fine-tuning\n83.11\n19.12\n80.26\n71.03\n27.08\n39.80\n28.07\nLeave-one-out multi-task training\n81.98\n19.05\n79.97\n71.68\n26.93\n39.79\n27.87\nSupervised multi-task pre-training\n79.93\n18.96\n77.38\n65.36\n26.81\n40.13\n28.04\nTable 12: Comparison of unsupervised pre-training, multi-task learning, and various forms\nof multi-task pre-training.\ncould suggest that the translation tasks benefit less from (English) pre-training, whereas\nunsupervised pre-training is an important factor in the other tasks.\n3.6 Scaling\nThe \u201cbitter lesson\u201d of machine learning research argues that general methods that can\nleverage additional computation ultimately win out against methods that rely on human\nexpertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016;\nMahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a).\nRecent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c;\nRadford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown\nthat scaling up produces improved performance compared to more carefully-engineered\nmethods. However, there are a variety of possible ways to scale, including using a bigger\nmodel, training the model for more steps, and ensembling. In this section, we compare these\ndifferent approaches by addressing the following premise: \u201cYou were just given 4\u00d7 more\ncompute. How should you use it?\u201d\nWe start with our baseline model, which has 220M parameters and is pre-trained and\nfine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized\nsimilarly to \u201cBERTBASE\u201d. To experiment with increased model size, we follow the guidelines\nof \u201cBERTLARGE\u201d Devlin et al. (2018) and use dff = 4096, dmodel = 1024, dkv = 64 and\n16-head attention mechanisms. We then generate two variants with 16 and 32 layers each in\nthe encoder and decoder, producing models with 2\u00d7 and 4\u00d7 as many parameters as our\noriginal model. These two variants also have a roughly 2\u00d7 and 4\u00d7 the computational cost.\nUsing our baseline and these two larger models, we consider three ways of using 4\u00d7 as much\ncomputation: Training for 4\u00d7 as many steps, training for 2\u00d7 as many steps with the 2\u00d7\nbigger model, and training the 4\u00d7 bigger model for the \u201cbaseline\u201d number of training steps.\nWhen we increase the training steps, we scale both the pre-train and fine-tune steps for\nsimplicity. Note that when increasing the number of pre-training steps, we are effectively\nincluding more pre-training data as C4 is so large that we do not complete one pass over\nthe data even when training for 223 steps.\nAn alternative way for the model to see 4\u00d7 as much data is to increase the batch size by a\nfactor of 4. This can potentially result in faster training due to more efficient parallelization.\nHowever, training with a 4\u00d7 larger batch size can yield a different outcome than training\n34\n", []], "Combining Multi-Task Learning with Fine-Tuning": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\neach task\u2019s mixing rate rm to the power of 1\u2044T and renormalize the rates so that they\nsum to 1. When T = 1, this approach is equivalent to examples-proportional mixing\nand as T increases the proportions become closer to equal mixing. We retain the data\nset size limit K (applied to obtain rm before temperature scaling) but set it to a large\nvalue of K = 221. We use a large value of K because increasing the temperature will\ndecrease the mixing rate of the largest data sets.\nEqual mixing In this case, we sample examples from each task with equal probability.\nSpecifically, each example in each batch is sampled uniformly at random from one of\nthe data sets we train on. This is most likely a suboptimal strategy, as the model will\noverfit quickly on low-resource tasks and underfit on high-resource tasks. We mainly\ninclude it as a point of reference of what might go wrong when the proportions are set\nsuboptimally.\nTo compare these mixing strategies on equal footing with our baseline pre-train-then-\nfine-tune results, we train multi-task models for the same total number of steps: 219 + 218 =\n786,432. The results are shown in Table 11.\nIn general, we find that multi-task training underperforms pre-training followed by\nfine-tuning on most tasks. The \u201cequal\u201d mixing strategy in particular results in dramatically\ndegraded performance, which may be because the low-resource tasks have overfit, the high-\nresource tasks have not seen enough data, or the model has not seen enough unlabeled data to\nlearn general-purpose language capabilities. For examples-proportional mixing, we find that\nfor most tasks there is a \u201csweet spot\u201d for K where the model obtains the best performance,\nand larger or smaller values of K tend to result in worse performance. The exception (for the\nrange of K values we considered) was WMT English to French translation, which is such a\nhigh-resource task that it always benefits from a higher mixing proportion. Finally, we note\nthat temperature-scaled mixing also provides a means of obtaining reasonable performance\nfrom most tasks, with T = 2 performing the best in most cases. The finding that a multi-task\nmodel is outperformed by separate models trained on each individual task has previously\nbeen observed e.g. by Arivazhagan et al. (2019) and McCann et al. (2018), though it has\nbeen shown that the multi-task setup can confer benefits across very similar tasks Liu et al.\n(2019b); Ratner et al. (2018). In the following section, we explore ways to close the gap\nbetween multi-task training and the pre-train-then-fine-tune approach.\n3.5.3 Combining Multi-Task Learning with Fine-Tuning\nRecall that we are studying a relaxed version of multi-task learning where we train a single\nmodel on a mixture of tasks but are allowed to evaluate performance using different parameter\nsettings (checkpoints) for the model. We can extend this approach by considering the case\nwhere the model is pre-trained on all tasks at once but is then fine-tuned on the individual\nsupervised tasks. This is the method used by the \u201cMT-DNN\u201d (Liu et al., 2015, 2019b),\nwhich achieved state-of-the-art performance on GLUE and other benchmarks when it was\nintroduced. We consider three variants of this approach: In the first, we simply pre-train the\nmodel on an examples-proportional mixture with an artificial data set size limit of K = 219\nbefore fine-tuning it on each individual downstream task. This helps us measure whether\nincluding the supervised tasks alongside the unsupervised objective during pre-training\n32\n", []], "Multi-task Learning": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nFine-tuning method\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6All parameters\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nAdapter layers, d = 32\n80.52\n15.08\n79.32\n60.40\n13.84\n17.88\n15.54\nAdapter layers, d = 128\n81.51\n16.62\n79.47\n63.03\n19.83\n27.50\n22.63\nAdapter layers, d = 512\n81.54\n17.78\n79.18\n64.30\n23.45\n33.98\n25.81\nAdapter layers, d = 2048\n81.51\n16.62\n79.47\n63.03\n19.83\n27.50\n22.63\nGradual unfreezing\n82.50\n18.95\n79.17\n70.79\n26.71\n39.02\n26.93\nTable 10: Comparison of different alternative fine-tuning methods that only update a subset\nof the model\u2019s parameters. For adapter layers, d refers to the inner dimensionality\nof the adapters.\nparameters of the final layer are updated, then after training for a certain number of updates\nthe parameters of the second-to-last layer are also included, and so on until the entire\nnetwork\u2019s parameters are being fine-tuned. To adapt this approach to our encoder-decoder\nmodel, we gradually unfreeze layers in the encoder and decoder in parallel, starting from\nthe top in both cases. Since the parameters of our input embedding matrix and output\nclassification matrix are shared, we update them throughout fine-tuning. Recall that our\nbaseline model consists of 12 layers each in the encoder and decoder and is fine-tuned for\n218 steps. As such, we subdivide the fine-tuning process into 12 episodes of 218/12 steps each\nand train from layers 12 \u2212n to 12 in the nth episode. We note that Howard and Ruder\n(2018) suggested fine-tuning an additional layer after each epoch of training. However, since\nour supervised data sets vary so much in size and since some of our downstream tasks are\nactually mixtures of many tasks (GLUE and SuperGLUE), we instead adopt the simpler\nstrategy of fine-tuning an additional layer after every 218/12 steps.\nA comparison of the performance of these fine-tuning approaches is shown in Table 10.\nFor adapter layers, we report the performance using an inner dimensionality d of 32, 128,\n512, 2048. Pursuant with past results (Houlsby et al., 2019; Bapna et al., 2019) we find that\nlower-resource tasks like SQuAD work well with a small value of d whereas higher resource\ntasks require a large dimensionality to achieve reasonable performance. This suggests that\nadapter layers could be a promising technique for fine-tuning on fewer parameters as long as\nthe dimensionality is scaled appropriately to the task size. Note that in our case we treat\nGLUE and SuperGLUE each as a single \u201ctask\u201d by concatenating their constituent data\nsets, so although they comprise some low-resource data sets the combined data set is large\nenough that it necessitates a large value of d. We found that gradual unfreezing caused\na minor degradation in performance across all tasks, though it did provide some speedup\nduring fine-tuning. Better results may be attainable by more carefully tuning the unfreezing\nschedule.\n3.5.2 Multi-task Learning\nSo far, we have been pre-training our model on a single unsupervised learning task before\nfine-tuning it individually on each downstream task. An alternative approach, called \u201cmulti-\ntask learning\u201d (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a\ntime. This approach typically has the goal of training a single model that can simultaneously\n30\n", []], "Fine-tuning Methods": ["Exploring the Limits of Transfer Learning\n0\n100\n200\n300\n400\n500\nStep \u00d7 1,000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTraining loss\nDataset size\nFull dataset\n229\n227\n225\n223\nFigure 6: Pre-training loss for our original C4 data set as well as 4 artificially truncated\nversions. The sizes listed refer to the number of tokens in each data set. The four\nsizes considered correspond to repeating the data set between 64 and 4,096 times\nover the course of pre-training. Using a smaller data set size results in smaller\ntraining loss values, which may suggest some memorization of the unlabeled data\nset.\n3.5.1 Fine-tuning Methods\nIt has been argued that fine-tuning all of the model\u2019s parameters can lead to suboptimal\nresults, particularly on low-resource tasks (Peters et al., 2019). Early results on transfer\nlearning for text classification tasks advocated fine-tuning only the parameters of a small\nclassifier that was fed sentence embeddings produced by a fixed pre-trained model (Subra-\nmanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau\net al., 2017). This approach is less applicable to our encoder-decoder model because the\nentire decoder must be trained to output the target sequences for a given task. Instead, we\nfocus on two alternative fine-tuning approaches that update only a subset of the parameters\nof our encoder-decoder model.\nThe first, \u201cadapter layers\u201d (Houlsby et al., 2019; Bapna et al., 2019), is motivated by\nthe goal of keeping most of the original model fixed while fine-tuning. Adapter layers are\nadditional dense-ReLU-dense blocks that are added after each of the preexisting feed-forward\nnetworks in each block of the Transformer. These new feed-forward networks are designed\nso that their output dimensionality matches their input. This allows them to be inserted\ninto the network with no additional changes to the structure or parameters. When fine-\ntuning, only the adapter layer and layer normalization parameters are updated. The main\nhyperparameter of this approach is the inner dimensionality d of the feed-forward network,\nwhich changes the number of new parameters added to the model. We experiment with\nvarious values for d.\nThe second alternative fine-tuning method we consider is \u201cgradual unfreezing\u201d (Howard\nand Ruder, 2018). In gradual unfreezing, more and more of the model\u2019s parameters are fine-\ntuned over time. Gradual unfreezing was originally applied to a language model architecture\nconsisting of a single stack of layers. In this setting, at the start of fine-tuning only the\n29\n", []], "Training Strategy": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nNumber of tokens\nRepeats\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Full data set\n0\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n229\n64\n82.87\n19.19\n80.97\n72.03\n26.83\n39.74\n27.63\n227\n256\n82.62\n19.20\n79.78\n69.97\n27.02\n39.71\n27.33\n225\n1,024\n79.55\n18.57\n76.27\n64.76\n26.38\n39.56\n26.80\n223\n4,096\n76.34\n18.33\n70.92\n59.29\n26.37\n38.84\n25.81\nTable 9: Measuring the effect of repeating data during pre-training. In these experiments,\nwe only use the first N tokens from C4 (with varying values of N shown in the\nfirst column) but still pre-train over 235 tokens. This results in the data set being\nrepeated over the course of pre-training (with the number of repeats for each\nexperiment shown in the second column), which may result in memorization (see\nFigure 6).\nTo test the effect of limited unlabeled data set sizes, we pre-trained our baseline model\non artificially truncated versions of C4. Recall that we pre-train our baseline model on\n235 \u224834B tokens (a small fraction of the total size of C4). We consider training on truncated\nvariants of C4 consisting of 229, 227, 225 and 223 tokens. These sizes correspond to repeating\nthe data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training.\nThe resulting downstream performance is shown in Table 9. As expected, performance\ndegrades as the data set size shrinks. We suspect this may be due to the fact that the model\nbegins to memorize the pre-training data set. To measure if this is true, we plot the training\nloss for each of these data set sizes in Figure 6. Indeed, the model attains significantly\nsmaller training losses as the size of the pre-training data set shrinks, suggesting possible\nmemorization. Baevski et al. (2019) similarly observed that truncating the pre-training data\nset size can degrade downstream task performance.\nWe note that these effects are limited when the pre-training data set is repeated only\n64 times. This suggests that some amount of repetition of pre-training data might not be\nharmful. However, given that additional pre-training can be beneficial (as we will show in\nSection 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest\nusing large pre-training data sets whenever possible. We also note that this effect may be\nmore pronounced for larger model sizes, i.e. a bigger model may be more prone to overfitting\nto a smaller pre-training data set.\n3.5 Training Strategy\nSo far we have considered the setting where all parameters of a model are pre-trained on\nan unsupervised task before being fine-tuned on individual supervised tasks. While this\napproach is straightforward, various alternative methods for training the model on down-\nstream/supervised tasks have been proposed. In this section, we compare different schemes\nfor fine-tuning the model in addition to the approach of training the model simultaneously\non multiple tasks.\n28\n", []], "Pre-training Data set Size": ["Exploring the Limits of Transfer Learning\nData set\nSize\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6C4\n745GB\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nC4, unfiltered\n6.1TB\n81.46\n19.14\n78.78\n68.04\n26.55\n39.34\n27.21\nRealNews-like\n35GB\n83.83\n19.23\n80.39\n72.38\n26.75\n39.90\n27.48\nWebText-like\n17GB\n84.03\n19.31\n81.42\n71.40\n26.80\n39.74\n27.59\nWikipedia\n16GB\n81.85\n19.31\n81.29\n68.01\n26.94\n39.69\n27.67\nWikipedia + TBC\n20GB\n83.65\n19.28\n82.08\n73.24\n26.77\n39.63\n27.57\nTable 8: Performance resulting from pre-training on different data sets. The first four\nvariants are based on our new C4 data set.\nproduced a SuperGLUE score of 73.24, beating our baseline\u2019s score (using C4) of 71.36.\nThis is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to\n50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC\nis a reading comprehension data set whose largest source of data comes from fiction books,\nwhich is exactly the domain covered by TBC. Similarly, using the RealNews-like data set\nfor pre-training conferred an increase from 68.16 to 73.72 on the Exact Match score for\nReCoRD, a data set that measures reading comprehension on news articles. As a final\nexample, using data from Wikipedia produced significant (but less dramatic) gains on\nSQuAD, which is a question-answering data set with passages sourced from Wikipedia.\nSimilar observations have been made in prior work, e.g. Beltagy et al. (2019) found that\npre-training BERT on text from research papers improved its performance on scientific tasks.\nThe main lesson behind these findings is that pre-training on in-domain unlabeled data can\nimprove performance on downstream tasks. This is unsurprising but also unsatisfying if\nour goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary\ndomains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded\nimprovements on downstream tasks. This observation also motivates the parallel line of\nresearch on domain adaptation for natural language processing; for surveys of this field see\ne.g. Ruder (2019); Li (2012).\nA drawback to only pre-training on a single domain is that the resulting data sets are\noften substantially smaller. Similarly, while the WebText-like variant performed as well or\nbetter than the C4 data set in our baseline setting, the Reddit-based filtering produced a\ndata set that was about 40\u00d7 smaller than C4 despite being based on 12\u00d7 more data from\nCommon Crawl. Note, however, that in our baseline setup we only pre-train on 235 \u224834B\ntokens, which is only about 8 times larger than the smallest pre-training data set we consider.\nWe investigate at what point using a smaller pre-training data sets poses an issue in the\nfollowing section.\n3.4.2 Pre-training Data set Size\nThe pipeline we use to create C4 was designed to be able to create extremely large pre-\ntraining data sets. The access to so much data allows us to pre-train our models without\nrepeating examples. It is not clear whether repeating examples during pre-training would\nbe helpful or harmful to downstream performance because our pre-training objective is itself\nstochastic and can help prevent the model from seeing the same exact data multiple times.\n27\n", []], "Unlabeled Data Sets": ["Exploring the Limits of Transfer Learning\nFigure 5: A flow chart of our exploration of unsupervised objectives. We first consider a\nfew disparate approaches in Section 3.3.1 and find that a BERT-style denoising\nobjective performs best. Then, we consider various methods for simplifying the\nBERT objective so that it produces shorter target sequences in Section 3.3.2.\nGiven that replacing dropped-out spans with sentinel tokens performs well and\nresults in short target sequences, in Section 3.3.3 we experiment with different\ncorruption rates. Finally, we evaluate an objective that intentionally corrupts\ncontiguous spans of tokens in Section 3.3.4.\nreleased alongside pre-trained models and code. Instead, they are typically introduced in\nthe course of presenting a new method or model. As a result, there has been relatively little\ncomparison of different pre-training data sets as well as a lack of a \u201cstandard\u201d data set used\nfor pre-training. Some recent notable exceptions (Baevski et al., 2019; Liu et al., 2019c;\nYang et al., 2019) have compared pre-training on a new large (often Common Crawl-sourced)\ndata set to using a smaller preexisting data set (often Wikipedia). To probe more deeply\ninto the impact of the pre-training data set on performance, in this section we compare\nvariants of our C4 data set and other potential sources of pre-training data. We release all\nof the C4 data set variants we consider as part of TensorFlow Datasets.11\n3.4.1 Unlabeled Data Sets\nIn creating C4, we developed various heuristics to filter the web-extracted text from Common\nCrawl (see Section 2.2 for a description). We are interested in measuring whether this\nfiltering results in improved performance on downstream tasks, in addition to comparing\nit to other filtering approaches and common pre-training data sets. Towards this end, we\ncompare the performance of our baseline model after pre-training on the following data sets:\nC4 As a baseline, we first consider pre-training on our proposed unlabeled data set as\ndescribed in Section 2.2.\nUnfiltered C4 To measure the effect of the heuristic filtering we used in creating C4\n(deduplication, removing bad words, only retaining sentences, etc.), we also generate\nan alternate version of C4 that forgoes this filtering. Note that we still use langdetect\n11. https://www.tensorflow.org/datasets/catalog/c4\n25\n", []], "Pre-training Data set": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nSpan length\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Baseline (i.i.d.)\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n2\n83.54\n19.39\n82.09\n72.20\n26.76\n39.99\n27.63\n3\n83.49\n19.62\n81.84\n72.53\n26.86\n39.65\n27.62\n5\n83.40\n19.24\n82.05\n72.23\n26.88\n39.40\n27.53\n10\n82.85\n19.33\n81.84\n70.44\n26.79\n39.49\n27.69\nTable 7: Performance of the span-corruption objective (inspired by Joshi et al. (2019)) for\ndifferent average span lengths. In all cases, we corrupt 15% of the original text\nsequence.\nrandomly to satisfy these specified parameters. For example, if we are processing a sequence\nof 500 tokens and we have specified that 15% of tokens should be corrupted and that there\nshould be 25 total spans, then the total number of corrupted tokens would be 500\u00d70.15 = 75\nand the average span length would be 75/25 = 3. Note that given the original sequence\nlength and corruption rate, we can equivalently parametrize this objective either by the\naverage span length or the total number of spans.\nWe compare the span-corruption objective to the i.i.d-corruption objective in Table 7.\nWe use a corruption rate of 15% in all cases and compare using average span lengths of 2, 3,\n5 and 10. Again, we find a limited difference between these objectives, though the version\nwith an average span length of 10 slightly underperforms the other values in some cases.\nWe also find in particular that using an average span length of 3 slightly (but significantly)\noutperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the\nspan-corruption objective also provides some speedup during training compared to the i.i.d.\nnoise approach because span corruption produces shorter sequences on average.\n3.3.5 Discussion\nFigure 5 shows a flow chart of the choices made during our exploration of unsupervised\nobjectives. Overall, the most significant difference in performance we observed was that\ndenoising objectives outperformed language modeling and deshuffling for pre-training. We\ndid not observe a remarkable difference across the many variants of the denoising objectives\nwe explored. However, different objectives (or parameterizations of objectives) can lead to\ndifferent sequence lengths and thus different training speeds. This implies that choosing\namong the denoising objectives we considered here should mainly be done according to\ntheir computational cost. Our results also suggest that additional exploration of objectives\nsimilar to the ones we consider here may not lead to significant gains for the tasks and model\nwe consider. Instead, it may be fortuitous to explore entirely different ways of leveraging\nunlabeled data.\n3.4 Pre-training Data set\nLike the unsupervised objective, the pre-training data set itself is a crucial component of\nthe transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training\ndata sets are usually not treated as significant contributions on their own and are often not\n24\n", []], "Discussion": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nSpan length\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Baseline (i.i.d.)\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n2\n83.54\n19.39\n82.09\n72.20\n26.76\n39.99\n27.63\n3\n83.49\n19.62\n81.84\n72.53\n26.86\n39.65\n27.62\n5\n83.40\n19.24\n82.05\n72.23\n26.88\n39.40\n27.53\n10\n82.85\n19.33\n81.84\n70.44\n26.79\n39.49\n27.69\nTable 7: Performance of the span-corruption objective (inspired by Joshi et al. (2019)) for\ndifferent average span lengths. In all cases, we corrupt 15% of the original text\nsequence.\nrandomly to satisfy these specified parameters. For example, if we are processing a sequence\nof 500 tokens and we have specified that 15% of tokens should be corrupted and that there\nshould be 25 total spans, then the total number of corrupted tokens would be 500\u00d70.15 = 75\nand the average span length would be 75/25 = 3. Note that given the original sequence\nlength and corruption rate, we can equivalently parametrize this objective either by the\naverage span length or the total number of spans.\nWe compare the span-corruption objective to the i.i.d-corruption objective in Table 7.\nWe use a corruption rate of 15% in all cases and compare using average span lengths of 2, 3,\n5 and 10. Again, we find a limited difference between these objectives, though the version\nwith an average span length of 10 slightly underperforms the other values in some cases.\nWe also find in particular that using an average span length of 3 slightly (but significantly)\noutperforms the i.i.d. objective on most non-translation benchmarks. Fortunately, the\nspan-corruption objective also provides some speedup during training compared to the i.i.d.\nnoise approach because span corruption produces shorter sequences on average.\n3.3.5 Discussion\nFigure 5 shows a flow chart of the choices made during our exploration of unsupervised\nobjectives. Overall, the most significant difference in performance we observed was that\ndenoising objectives outperformed language modeling and deshuffling for pre-training. We\ndid not observe a remarkable difference across the many variants of the denoising objectives\nwe explored. However, different objectives (or parameterizations of objectives) can lead to\ndifferent sequence lengths and thus different training speeds. This implies that choosing\namong the denoising objectives we considered here should mainly be done according to\ntheir computational cost. Our results also suggest that additional exploration of objectives\nsimilar to the ones we consider here may not lead to significant gains for the tasks and model\nwe consider. Instead, it may be fortuitous to explore entirely different ways of leveraging\nunlabeled data.\n3.4 Pre-training Data set\nLike the unsupervised objective, the pre-training data set itself is a crucial component of\nthe transfer learning pipeline. However, unlike objectives and benchmarks, new pre-training\ndata sets are usually not treated as significant contributions on their own and are often not\n24\n", []], "Corrupting Spans": ["Exploring the Limits of Transfer Learning\nCorruption rate\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n10%\n82.82\n19.00\n80.38\n69.55\n26.87\n39.28\n27.44\n\u22c615%\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n25%\n83.00\n19.54\n80.96\n70.48\n27.04\n39.83\n27.47\n50%\n81.27\n19.32\n79.80\n70.33\n27.01\n39.90\n27.49\nTable 6:\nPerformance of the i.i.d. corruption objective with different corruption rates.\nbaseline average of 53.84, see Table 16). This may be due to the fact that CoLA involves\nclassifying whether a given sentence is grammatically and syntactically acceptable, and\nbeing able to determine when tokens are missing is closely related to detecting acceptability.\nHowever, dropping tokens completely performed worse than replacing them with sentinel\ntokens on SuperGLUE. The two variants that do not require predicting the full original\nsequence (\u201creplace corrupted spans\u201d and \u201cdrop corrupted spans\u201d) are both potentially\nattractive since they make the target sequences shorter and consequently make training\nfaster. Going forward, we will explore variants where we replace corrupted spans with\nsentinel tokens and only predict the corrupted tokens (as in our baseline objective).\n3.3.3 Varying the Corruption Rate\nSo far, we have been corrupting 15% of the tokens, the value used in BERT (Devlin et al.,\n2018). Again, since our text-to-text framework differs from BERT\u2019s, we are interested to\nsee if a different corruption rate works better for us. We compare corruption rates of 10%,\n15%, 25%, and 50% in Table 6. Overall, we find that the corruption rate had a limited\neffect on the model\u2019s performance. The only exception is that the largest corruption rate we\nconsider (50%) results in a significant degradation of performance on GLUE and SQuAD.\nUsing a larger corruption rate also results in longer targets, which can potentially slow down\ntraining. Based on these results and the historical precedent set by BERT, we will use a\ncorruption rate of 15% going forward.\n3.3.4 Corrupting Spans\nWe now turn towards the goal of speeding up training by predicting shorter targets. The\napproach we have used so far makes an i.i.d. decision for each input token as to whether\nto corrupt it or not. When multiple consecutive tokens have been corrupted, they are\ntreated as a \u201cspan\u201d and a single unique mask token is used to replace the entire span.\nReplacing entire spans with a single token results in unlabeled text data being processed into\nshorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case\nthat a significant number of corrupted tokens appear consecutively. As a result, we might\nobtain additional speedup by specifically corrupting spans of tokens rather than corrupting\nindividual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a\npre-training objective for BERT, where it was found to improve performance (Joshi et al.,\n2019).\nTo test this idea, we consider an objective that specifically corrupts contiguous, randomly-\nspaced spans of tokens. This objective can be parametrized by the proportion of tokens to\nbe corrupted and the total number of corrupted spans. The span lengths are then chosen\n23\n", []], "Varying the Corruption Rate": ["Exploring the Limits of Transfer Learning\nCorruption rate\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n10%\n82.82\n19.00\n80.38\n69.55\n26.87\n39.28\n27.44\n\u22c615%\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\n25%\n83.00\n19.54\n80.96\n70.48\n27.04\n39.83\n27.47\n50%\n81.27\n19.32\n79.80\n70.33\n27.01\n39.90\n27.49\nTable 6:\nPerformance of the i.i.d. corruption objective with different corruption rates.\nbaseline average of 53.84, see Table 16). This may be due to the fact that CoLA involves\nclassifying whether a given sentence is grammatically and syntactically acceptable, and\nbeing able to determine when tokens are missing is closely related to detecting acceptability.\nHowever, dropping tokens completely performed worse than replacing them with sentinel\ntokens on SuperGLUE. The two variants that do not require predicting the full original\nsequence (\u201creplace corrupted spans\u201d and \u201cdrop corrupted spans\u201d) are both potentially\nattractive since they make the target sequences shorter and consequently make training\nfaster. Going forward, we will explore variants where we replace corrupted spans with\nsentinel tokens and only predict the corrupted tokens (as in our baseline objective).\n3.3.3 Varying the Corruption Rate\nSo far, we have been corrupting 15% of the tokens, the value used in BERT (Devlin et al.,\n2018). Again, since our text-to-text framework differs from BERT\u2019s, we are interested to\nsee if a different corruption rate works better for us. We compare corruption rates of 10%,\n15%, 25%, and 50% in Table 6. Overall, we find that the corruption rate had a limited\neffect on the model\u2019s performance. The only exception is that the largest corruption rate we\nconsider (50%) results in a significant degradation of performance on GLUE and SQuAD.\nUsing a larger corruption rate also results in longer targets, which can potentially slow down\ntraining. Based on these results and the historical precedent set by BERT, we will use a\ncorruption rate of 15% going forward.\n3.3.4 Corrupting Spans\nWe now turn towards the goal of speeding up training by predicting shorter targets. The\napproach we have used so far makes an i.i.d. decision for each input token as to whether\nto corrupt it or not. When multiple consecutive tokens have been corrupted, they are\ntreated as a \u201cspan\u201d and a single unique mask token is used to replace the entire span.\nReplacing entire spans with a single token results in unlabeled text data being processed into\nshorter sequences. Since we are using an i.i.d. corruption strategy, it is not always the case\nthat a significant number of corrupted tokens appear consecutively. As a result, we might\nobtain additional speedup by specifically corrupting spans of tokens rather than corrupting\nindividual tokens in an i.i.d. manner. Corrupting spans was also previously considered as a\npre-training objective for BERT, where it was found to improve performance (Joshi et al.,\n2019).\nTo test this idea, we consider an objective that specifically corrupts contiguous, randomly-\nspaced spans of tokens. This objective can be parametrized by the proportion of tokens to\nbe corrupted and the total number of corrupted spans. The span lengths are then chosen\n23\n", []], "Simplifying the BERT Objective": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nObjective\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\nPrefix language modeling\n80.69\n18.94\n77.99\n65.27\n26.86\n39.73\n27.49\nBERT-style (Devlin et al., 2018)\n82.96\n19.17\n80.65\n69.85\n26.78\n40.03\n27.41\nDeshuffling\n73.17\n18.59\n67.61\n58.47\n26.11\n39.30\n25.62\nTable 4: Performance of the three disparate pre-training objectives described in Section 3.3.1.\nObjective\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\nBERT-style (Devlin et al., 2018)\n82.96\n19.17\n80.65\n69.85\n26.78\n40.03\n27.41\nMASS-style (Song et al., 2019)\n82.32\n19.16\n80.10\n69.28\n26.79\n39.89\n27.55\n\u22c6Replace corrupted spans\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nDrop corrupted tokens\n84.44\n19.31\n80.52\n68.67\n27.07\n39.76\n27.82\nTable 5: Comparison of variants of the BERT-style pre-training objective. In the first two\nvariants, the model is trained to reconstruct the original uncorrupted text segment.\nIn the latter two, the model only predicts the sequence of corrupted tokens.\n3.3.2 Simplifying the BERT Objective\nBased on the results in the prior section, we will now focus on exploring modifications to\nthe BERT-style denoising objective. This objective was originally proposed as a pre-training\ntechnique for an encoder-only model trained for classification and span prediction. As\nsuch, it may be possible to modify it so that it performs better or is more efficient in our\nencoder-decoder text-to-text setup.\nFirst, we consider a simple variant of the BERT-style objective where we don\u2019t include the\nrandom token swapping step. The resulting objective simply replaces 15% of the tokens in\nthe input with a mask token and the model is trained to reconstruct the original uncorrupted\nsequence. A similar masking objective was used by Song et al. (2019) where it was referred to\nas \u201cMASS\u201d, so we call this variant the \u201cMASS-style\u201d objective. Second, we were interested\nto see if it was possible to avoid predicting the entire uncorrupted text span since this\nrequires self-attention over long sequences in the decoder. We consider two strategies to\nachieve this: First, instead of replacing each corrupted token with a mask token, we replace\nthe entirety of each consecutive span of corrupted tokens with a unique mask token. Then,\nthe target sequence becomes the concatenation of the \u201ccorrupted\u201d spans, each prefixed by\nthe mask token used to replace it in the input. This is the pre-training objective we use in\nour baseline, described in Section 3.1.4. Second, we also consider a variant where we simply\ndrop the corrupted tokens from the input sequence completely and task the model with\nreconstructing the dropped tokens in order. Examples of these approaches are shown in the\nfifth and sixth rows of Table 3.\nAn empirical comparison of the original BERT-style objective to these three alternatives\nis shown in Table 5. We find that in our setting, all of these variants perform similarly. The\nonly exception was that dropping corrupted tokens completely produced a small improvement\nin the GLUE score thanks to a significantly higher score on CoLA (60.04, compared to our\n22\n", []], "Disparate High-Level Approaches": ["Exploring the Limits of Transfer Learning\nObjective\nInputs\nTargets\nPrefix language modeling\nThank you for inviting\nme to your party last week .\nBERT-style Devlin et al. (2018)\nThank you <M> <M> me to your party apple week .\n(original text)\nDeshuffling\nparty me for your to . last fun you inviting week Thank\n(original text)\nMASS-style Song et al. (2019)\nThank you <M> <M> me to your party <M> week .\n(original text)\nI.i.d. noise, replace spans\nThank you <X> me to your party <Y> week .\n<X> for inviting <Y> last <Z>\nI.i.d. noise, drop tokens\nThank you me to your party week .\nfor inviting last\nRandom spans\nThank you <X> to <Y> week .\n<X> for inviting me <Y> your party last <Z>\nTable 3: Examples of inputs and targets produced by some of the unsupervised objectives\nwe consider applied to the input text \u201cThank you for inviting me to your party last\nweek .\u201d Note that all of our objectives process tokenized text. For this particular\nsentence, all words were mapped to a single token by our vocabulary. We write\n(original text) as a target to denote that the model is tasked with reconstructing the\nentire input text. <M> denotes a shared mask token and <X>, <Y>, and <Z> denote\nsentinel tokens that are assigned unique token IDs. The BERT-style objective\n(second row) includes a corruption where some tokens are replaced by a random\ntoken ID; we show this via the greyed-out word apple.\nwith maximum likelihood to predict the target sequence. We provide illustrative examples\nof many of the objectives we consider in Table 3.\n3.3.1 Disparate High-Level Approaches\nTo begin with, we compare three techniques that are inspired by commonly-used objectives\nbut differ significantly in their approach. First, we include a basic \u201cprefix language modeling\u201d\nobjective as was used in Section 3.2.3.\nThis technique splits a span of text into two\ncomponents, one to use as inputs to the encoder and the other to use as a target sequence\nto be predicted by the decoder. Second, we consider an objective inspired by the \u201cmasked\nlanguage modeling\u201d (MLM) objective used in BERT (Devlin et al., 2018). MLM takes a\nspan of text and corrupts 15% of the tokens. 90% of the corrupted tokens are replaced\nwith a special mask token and 10% are replaced with a random token. Since BERT is an\nencoder-only model, its goal during pre-training is to reconstruct masked tokens at the\noutput of the encoder. In the encoder-decoder case, we simply use the entire uncorrupted\nsequence as the target. Note that this differs from our baseline objective, which uses only\nthe corrupted tokens as targets; we compare these two approaches in Section 3.3.2. Finally,\nwe also consider a basic deshuffling objective as used e.g. in (Liu et al., 2019a) where it was\napplied to a denoising sequential autoencoder. This approach takes a sequence of tokens,\nshuffles it, and then uses the original deshuffled sequence as a target. We provide examples\nof the inputs and targets for these three methods in the first three rows of Table 3.\nThe performance of these three objectives is shown in Table 4. Overall, we find that the\nBERT-style objective performs best, though the prefix language modeling objective attains\nsimilar performance on the translation tasks. Indeed, the motivation for the BERT objective\nwas to outperform language model-based pre-training. The deshuffling objective performs\nconsiderably worse than both prefix language modeling and the BERT-style objective.\n21\n", []], "Unsupervised Objectives": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nArchitecture\nObjective\nParams\nCost\nGLUE\nCNNDM\nSQuAD\nSGLUE\nEnDe\nEnFr\nEnRo\n\u22c6Encoder-decoder\nDenoising\n2P\nM\n83.28\n19.24\n80.88\n71.36\n26.98\n39.82\n27.65\nEnc-dec, shared\nDenoising\nP\nM\n82.81\n18.78\n80.63\n70.73\n26.72\n39.03\n27.46\nEnc-dec, 6 layers\nDenoising\nP\nM/2\n80.88\n18.97\n77.59\n68.42\n26.38\n38.40\n26.95\nLanguage model\nDenoising\nP\nM\n74.70\n17.93\n61.14\n55.02\n25.09\n35.28\n25.86\nPrefix LM\nDenoising\nP\nM\n81.82\n18.61\n78.94\n68.11\n26.43\n37.98\n27.39\nEncoder-decoder\nLM\n2P\nM\n79.56\n18.59\n76.02\n64.29\n26.27\n39.17\n26.86\nEnc-dec, shared\nLM\nP\nM\n79.60\n18.13\n76.35\n63.50\n26.62\n39.17\n27.05\nEnc-dec, 6 layers\nLM\nP\nM/2\n78.67\n18.26\n75.32\n64.06\n26.13\n38.42\n26.89\nLanguage model\nLM\nP\nM\n73.78\n17.54\n53.81\n56.51\n25.23\n34.31\n25.38\nPrefix LM\nLM\nP\nM\n79.68\n17.84\n76.87\n64.86\n26.28\n37.51\n26.76\nTable 2: Performance of the different architectural variants described in Section 3.2.2. We\nuse P to refer to the number of parameters in a 12-layer base Transformer layer\nstack and M to refer to the FLOPs required to process a sequence using the encoder-\ndecoder model. We evaluate each architectural variant using a denoising objective\n(described in Section 3.1.4) and an autoregressive objective (as is commonly used\nto train language models).\nthe encoder and decoder stacks significantly hurt performance. Concurrent work (Lan et al.,\n2019) also found that sharing parameters across Transformer blocks can be an effective means\nof lowering the total parameter count without sacrificing much performance. XLNet also\nbears some resemblance to the shared encoder-decoder approach with a denoising objective\n(Yang et al., 2019). We also note that the shared parameter encoder-decoder outperforms\nthe decoder-only prefix LM, suggesting that the addition of an explicit encoder-decoder\nattention is beneficial. Finally, we confirm the widely-held conception that using a denoising\nobjective always results in better downstream task performance compared to a language\nmodeling objective. This observation has been previously made by Devlin et al. (2018),\nVoita et al. (2019), and Lample and Conneau (2019) among others. We undertake a more\ndetailed exploration of unsupervised objectives in the following section.\n3.3 Unsupervised Objectives\nThe choice of unsupervised objective is of central importance as it provides the mechanism\nthrough which the model gains general-purpose knowledge to apply to downstream tasks.\nThis has led to the development of a wide variety of pre-training objectives (Dai and Le,\n2015; Ramachandran et al., 2016; Radford et al., 2018; Devlin et al., 2018; Yang et al., 2019;\nLiu et al., 2019b; Wang et al., 2019a; Song et al., 2019; Dong et al., 2019; Joshi et al., 2019).\nIn this section, we perform a procedural exploration of the space of unsupervised objectives.\nIn many cases, we will not replicate an existing objective exactly\u2014some will be modified to\nfit our text-to-text encoder-decoder framework and, in other cases, we will use objectives\nthat combine concepts from multiple common approaches.\nOverall, all of our objectives ingest a sequence of token IDs corresponding to a tokenized\nspan of text from our unlabeled text data set. The token sequence is processed to produce a\n(corrupted) input sequence and a corresponding target. Then, the model is trained as usual\n20\n", []], "Results": ["Exploring the Limits of Transfer Learning\ntimes for L-layer language models versus L + L-layer encoder-decoder models, suggesting a\nroughly equivalent computational cost. Further, for the model sizes we consider, the number\nof parameters in the encoder-decoder attention layers is about 10% of the total parameter\ncount, so we make the simplifying assumption that an L + L-layer encoder-decoder model\nhas the same number of parameters as an 2L-layer language model.\nTo provide a reasonable means of comparison, we consider multiple configurations for\nour encoder-decoder model. We will refer to the number of layers and parameters in a\nBERTBASE-sized layer stack as L and P, respectively. We will use M to refer to the number\nof FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model\nto process a given input-target pair. In total, we will compare:\n\u2022 An encoder-decoder model with L layers in the encoder and L layers in the decoder.\nThis model has 2P parameters and a computation cost of M FLOPs.\n\u2022 An equivalent model, but with parameters shared across the encoder and decoder,\nresulting in P parameters and an M-FLOP computational cost.\n\u2022 An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P\nparameters and an M/2-FLOP cost.\n\u2022 A decoder-only language model with L layers and P parameters and a resulting\ncomputational cost of M FLOPs.\n\u2022 A decoder-only prefix LM with the same architecture (and thus the same number\nof parameters and computational cost), but with fully-visible self-attention over the\ninput.\n3.2.3 Objectives\nAs an unsupervised objective, we will consider both a basic language modeling objective as\nwell as our baseline denoising objective described in Section 3.1.4. We include the language\nmodeling objective due to its historic use as a pre-training objective (Dai and Le, 2015;\nRamachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al.,\n2018) as well as its natural fit for the language model architectures we consider. For models\nthat ingest a prefix before making predictions (the encoder-decoder model and prefix LM),\nwe sample a span of text from our unlabeled data set and choose a random point to split\nit into prefix and target portions. For the standard language model, we train the model\nto predict the entire span from beginning to end. Our unsupervised denoising objective is\ndesigned for text-to-text models; to adapt it for use with a language model we concatenate\nthe inputs and targets as described in Section 3.2.1.\n3.2.4 Results\nThe scores achieved by each of the architectures we compare are shown in Table 2. For\nall tasks, the encoder-decoder architecture with the denoising objective performed best.\nThis variant has the highest parameter count (2P) but the same computational cost as the\nP-parameter decoder-only models. Surprisingly, we found that sharing parameters across the\nencoder and decoder performed nearly as well. In contrast, halving the number of layers in\n19\n", []], "Objectives": ["Exploring the Limits of Transfer Learning\ntimes for L-layer language models versus L + L-layer encoder-decoder models, suggesting a\nroughly equivalent computational cost. Further, for the model sizes we consider, the number\nof parameters in the encoder-decoder attention layers is about 10% of the total parameter\ncount, so we make the simplifying assumption that an L + L-layer encoder-decoder model\nhas the same number of parameters as an 2L-layer language model.\nTo provide a reasonable means of comparison, we consider multiple configurations for\nour encoder-decoder model. We will refer to the number of layers and parameters in a\nBERTBASE-sized layer stack as L and P, respectively. We will use M to refer to the number\nof FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model\nto process a given input-target pair. In total, we will compare:\n\u2022 An encoder-decoder model with L layers in the encoder and L layers in the decoder.\nThis model has 2P parameters and a computation cost of M FLOPs.\n\u2022 An equivalent model, but with parameters shared across the encoder and decoder,\nresulting in P parameters and an M-FLOP computational cost.\n\u2022 An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P\nparameters and an M/2-FLOP cost.\n\u2022 A decoder-only language model with L layers and P parameters and a resulting\ncomputational cost of M FLOPs.\n\u2022 A decoder-only prefix LM with the same architecture (and thus the same number\nof parameters and computational cost), but with fully-visible self-attention over the\ninput.\n3.2.3 Objectives\nAs an unsupervised objective, we will consider both a basic language modeling objective as\nwell as our baseline denoising objective described in Section 3.1.4. We include the language\nmodeling objective due to its historic use as a pre-training objective (Dai and Le, 2015;\nRamachandran et al., 2016; Howard and Ruder, 2018; Radford et al., 2018; Peters et al.,\n2018) as well as its natural fit for the language model architectures we consider. For models\nthat ingest a prefix before making predictions (the encoder-decoder model and prefix LM),\nwe sample a span of text from our unlabeled data set and choose a random point to split\nit into prefix and target portions. For the standard language model, we train the model\nto predict the entire span from beginning to end. Our unsupervised denoising objective is\ndesigned for text-to-text models; to adapt it for use with a language model we concatenate\nthe inputs and targets as described in Section 3.2.1.\n3.2.4 Results\nThe scores achieved by each of the architectures we compare are shown in Table 2. For\nall tasks, the encoder-decoder architecture with the denoising objective performed best.\nThis variant has the highest parameter count (2P) but the same computational cost as the\nP-parameter decoder-only models. Surprisingly, we found that sharing parameters across the\nencoder and decoder performed nearly as well. In contrast, halving the number of layers in\n19\n", []], "Comparing Different Model Structures": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nThis issue can be avoided in a Transformer-based language model simply by changing\nthe masking pattern. Instead of using a causal mask, we use fully-visible masking during\nthe prefix portion of the sequence. This masking pattern and a schematic of the resulting\n\u201cprefix LM\u201d (the third model structure we consider) are illustrated in the rightmost panels of\nFigures 3 and 4, respectively. In the English to German translation example mentioned above,\nfully-visible masking would be applied to the prefix \u201ctranslate English to German: That is\ngood. target:\u201d and causal masking would be used during training for predicting the target\n\u201cDas ist gut.\u201d Using a prefix LM in the text-to-text framework was originally proposed by\nLiu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective\non a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder\nmodel with parameters shared across the encoder and decoder and with the encoder-decoder\nattention replaced with full attention across the input and target sequence.\nWe note that when following our text-to-text framework, the prefix LM architecture\nclosely resembles BERT (Devlin et al., 2018) for classification tasks. To see why, consider an\nexample from the MNLI benchmark where the premise is \u201cI hate pigeons.\u201d, the hypothesis is\n\u201cMy feelings towards pigeons are filled with animosity.\u201d and the correct label is \u201centailment\u201d.\nTo feed this example into a language model, we would transform it into the sequence \u201cmnli\npremise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\ntarget: entailment\u201d. In this case, the fully-visible prefix would correspond to the entire input\nsequence up to the word \u201ctarget:\u201d, which can be seen as being analogous to the \u201cclassification\u201d\ntoken used in BERT. So, our model would have full visibility over the entire input, and then\nwould be tasked with making a classification by outputting the word \u201centailment\u201d. It is easy\nfor the model to learn to output one of the valid class labels given the task prefix (\u201cmnli\u201d in\nthis case). As such, the main difference between a prefix LM and the BERT architecture is\nthat the classifier is simply integrated into the output layer of the Transformer decoder in\nthe prefix LM.\n3.2.2 Comparing Different Model Structures\nIn the interest of experimentally comparing these architectural variants, we would like each\nmodel we consider to be equivalent in some meaningful way. We might say that two models\nare equivalent if they either have the same number of parameters or they require roughly\nthe same amount of computation to process a given (input-sequence, target-sequence) pair.\nUnfortunately, it is not possible to compare an encoder-decoder model to a language model\narchitecture (comprising a single Transformer stack) according to both of these criteria\nat the same time. To see why, first note an encoder-decoder model with L layers in the\nencoder and L layers in the decoder has approximately the same number of parameters as a\nlanguage model with 2L layers. However, the same L + L encoder-decoder model will have\napproximately the same computational cost as a language model with only L layers. This\nis a consequence of the fact that the L layers in the language model must be applied to\nboth the input and output sequence, while the encoder is only applied to the input sequence\nand the decoder is only applied to the output sequence. Note that these equivalences are\napproximate\u2014there are some extra parameters in the decoder due to the encoder-decoder\nattention and there are also some computational costs in the attention layers that are\nquadratic in the sequence lengths. In practice, however, we observed nearly identical step\n18\n", []], "Model Structures": ["Exploring the Limits of Transfer Learning\nenough data set that gains from pre-training tend to be marginal. We include this task in\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\nbetween our baseline and \u201cno pre-training\u201d emphasize how much pre-training improves\nperformance on tasks with limited data. While we do not explicitly measure improvements\nin data efficiency in this paper, we emphasize that this is one of the primary benefits of the\ntransfer learning paradigm.\nAs for inter-run variance, we find that for most tasks the standard deviation across runs\nis smaller than 1% of the task\u2019s baseline score. Exceptions to this rule include CoLA, CB,\nand COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks.\nFor example, on CB our baseline model had an average F1 score of 91.22 with a standard\ndeviation of 3.237 (see Table 16), which may be partly due to the fact that CB\u2019s validation\nset contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed\nas the average of scores across the tasks comprising each benchmark. As a result, we caution\nthat the high inter-run variance of CoLA, CB, and COPA can make it harder to compare\nmodels using the GLUE and SuperGLUE scores alone.\n3.2 Architectures\nWhile the Transformer was originally introduced with an encoder-decoder architecture, much\nmodern work on transfer learning for NLP uses alternative architectures. In this section, we\nreview and compare these architectural variants.\n3.2.1 Model Structures\nA major distinguishing factor for different architectures is the \u201cmask\u201d used by different\nattention mechanisms in the model. Recall that the self-attention operation in a Transformer\ntakes a sequence as input and outputs a new sequence of the same length. Each entry of\nthe output sequence is produced by computing a weighted average of entries of the input\nsequence. Specifically, let yi refer to the ith element of the output sequence and xj refer to\nthe jth entry of the input sequence. yi is computed as P\nj wi,jxj, where wi,j is the scalar\nweight produced by the self-attention mechanism as a function of xi and xj. The attention\nmask is then used to zero out certain weights in order to constrain which entries of the input\ncan be attended to at a given output timestep. Diagrams of the masks we will consider are\nshown in Figure 3. For example, the causal mask (Figure 3, middle) sets any wi,j to zero if\nj > i.\nThe first model structure we consider is an an encoder-decoder Transformer, which\nconsists of two layer stacks: The encoder, which is fed an input sequence, and the decoder,\nwhich produces a new output sequence. A schematic of this architectural variant is shown\nin the left panel of Figure 4.\nThe encoder uses a \u201cfully-visible\u201d attention mask. Fully-visible masking allows a self-\nattention mechanism to attend to any entry of the input when producing each entry of\nits output. We visualize this masking pattern in Figure 3, left. This form of masking is\nappropriate when attending over a \u201cprefix\u201d, i.e. some context provided to the model that\nis later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible\nmasking pattern and appends a special \u201cclassification\u201d token to the input. BERT\u2019s output\n15\n", []], "Architectures": ["Exploring the Limits of Transfer Learning\nenough data set that gains from pre-training tend to be marginal. We include this task in\nour experiments to test the behavior of transfer learning in the high-resource regime. Since\nwe perform early stopping by selecting the best-performing checkpoint, the large disparity\nbetween our baseline and \u201cno pre-training\u201d emphasize how much pre-training improves\nperformance on tasks with limited data. While we do not explicitly measure improvements\nin data efficiency in this paper, we emphasize that this is one of the primary benefits of the\ntransfer learning paradigm.\nAs for inter-run variance, we find that for most tasks the standard deviation across runs\nis smaller than 1% of the task\u2019s baseline score. Exceptions to this rule include CoLA, CB,\nand COPA, which are all low-resource tasks from the GLUE and SuperGLUE benchmarks.\nFor example, on CB our baseline model had an average F1 score of 91.22 with a standard\ndeviation of 3.237 (see Table 16), which may be partly due to the fact that CB\u2019s validation\nset contains only 56 examples. Note that the GLUE and SuperGLUE scores are computed\nas the average of scores across the tasks comprising each benchmark. As a result, we caution\nthat the high inter-run variance of CoLA, CB, and COPA can make it harder to compare\nmodels using the GLUE and SuperGLUE scores alone.\n3.2 Architectures\nWhile the Transformer was originally introduced with an encoder-decoder architecture, much\nmodern work on transfer learning for NLP uses alternative architectures. In this section, we\nreview and compare these architectural variants.\n3.2.1 Model Structures\nA major distinguishing factor for different architectures is the \u201cmask\u201d used by different\nattention mechanisms in the model. Recall that the self-attention operation in a Transformer\ntakes a sequence as input and outputs a new sequence of the same length. Each entry of\nthe output sequence is produced by computing a weighted average of entries of the input\nsequence. Specifically, let yi refer to the ith element of the output sequence and xj refer to\nthe jth entry of the input sequence. yi is computed as P\nj wi,jxj, where wi,j is the scalar\nweight produced by the self-attention mechanism as a function of xi and xj. The attention\nmask is then used to zero out certain weights in order to constrain which entries of the input\ncan be attended to at a given output timestep. Diagrams of the masks we will consider are\nshown in Figure 3. For example, the causal mask (Figure 3, middle) sets any wi,j to zero if\nj > i.\nThe first model structure we consider is an an encoder-decoder Transformer, which\nconsists of two layer stacks: The encoder, which is fed an input sequence, and the decoder,\nwhich produces a new output sequence. A schematic of this architectural variant is shown\nin the left panel of Figure 4.\nThe encoder uses a \u201cfully-visible\u201d attention mask. Fully-visible masking allows a self-\nattention mechanism to attend to any entry of the input when producing each entry of\nits output. We visualize this masking pattern in Figure 3, left. This form of masking is\nappropriate when attending over a \u201cprefix\u201d, i.e. some context provided to the model that\nis later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible\nmasking pattern and appends a special \u201cclassification\u201d token to the input. BERT\u2019s output\n15\n", []], "Baseline Performance": ["Exploring the Limits of Transfer Learning\n<X>\n<Y>\n<X>\n<Y>\n<Z>\nFigure 2: Schematic of the objective we use in our baseline model. In this example, we\nprocess the sentence \u201cThank you for inviting me to your party last week.\u201d The\nwords \u201cfor\u201d, \u201cinviting\u201d and \u201clast\u201d (marked with an \u00d7) are randomly chosen for\ncorruption. Each consecutive span of corrupted tokens is replaced by a sentinel\ntoken (shown as <X> and <Y>) that is unique over the example. Since \u201cfor\u201d and\n\u201cinviting\u201d occur consecutively, they are replaced by a single sentinel <X>. The\noutput sequence then consists of the dropped-out spans, delimited by the sentinel\ntokens used to replace them in the input plus a final sentinel token <Z>.\nuseful in downstream tasks. Preliminary work that applied the transfer learning paradigm\nof pre-training and fine-tuning all of the model\u2019s parameters to NLP problems used a\ncausal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018;\nRadford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that\n\u201cdenoising\u201d objectives (Devlin et al., 2018; Taylor, 1953) (also called \u201cmasked language\nmodeling\u201d) produce better performance and as a result they have quickly become standard.\nIn a denoising objective, the model is trained to predict missing or otherwise corrupted\ntokens in the input. Inspired by BERT\u2019s \u201cmasked language modeling\u201d objective and the\n\u201cword dropout\u201d regularization technique (Bowman et al., 2015), we design an objective that\nrandomly samples and then drops out 15% of tokens in the input sequence. All consecutive\nspans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token\nis assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens\nwhich are added to our vocabulary and do not correspond to any wordpiece. The target\nthen corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel\ntokens used in the input sequence plus a final sentinel token to mark the end of the target\nsequence. Our choices to mask consecutive spans of tokens and only predict dropped-out\ntokens were made to reduce the computational cost of pre-training. We perform thorough\ninvestigation into pre-training objectives in Section 3.3. An example of the transformation\nresulting from applying this objective is shown in Figure 2. We empirically compare this\nobjective to many other variants in Section 3.3.\n3.1.5 Baseline Performance\nIn this section, we present results using the baseline experimental procedure described above\nto get a sense of what kind of performance to expect on our suite of downstream tasks.\nIdeally, we would repeat every experiment in our study multiple times to get a confidence\ninterval on our results. Unfortunately, this would be prohibitively expensive due to the large\n13\n", []], "Unsupervised Objective": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nwe \u201cpack\u201d multiple sequences into each entry of the batch10 so that our batches contain\nroughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds\nto pre-training on 235 \u224834B tokens. This is considerably less than BERT (Devlin et al.,\n2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly\n2.2T tokens. Using only 235 tokens results in a reasonable computational budget while still\nproviding a sufficient amount of pre-training for acceptable performance. We consider the\neffect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235 tokens only covers\na fraction of the entire C4 data set, so we never repeat any data during pre-training.\nDuring pre-training, we use an \u201cinverse square root\u201d learning rate schedule: 1\n\u000ep\nmax(n, k)\nwhere n is the current training iteration and k is the number of warm-up steps (set to 104\nin all of our experiments). This sets a constant learning rate of 0.01 for the first 104 steps,\nthen exponentially decays the learning rate until pre-training is over. We also experimented\nwith using a triangular learning rate (Howard and Ruder, 2018), which produced slightly\nbetter results but requires knowing the total number of training steps ahead of time. Since\nwe will be varying the number of training steps in some of our experiments, we opt for the\nmore generic inverse square root schedule.\nOur models are fine-tuned for 218 = 262,144 steps on all tasks. This value was chosen\nas a trade-off between the high-resource tasks (i.e. those with large data sets), which\nbenefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit\nquickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e.\n216 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save\na checkpoint every 5,000 steps and report results on the model checkpoint corresponding\nto the highest validation performance. For models fine-tuned on multiple tasks, we choose\nthe best checkpoint for each task independently. For all of the experiments except those in\nSection 3.7, we report results in the validation set to avoid performing model selection on\nthe test set.\n3.1.3 Vocabulary\nWe use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens\n(Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000\nwordpieces. Since we ultimately fine-tune our model on English to German, French, and\nRomanian translation, we also require that our vocabulary covers these non-English languages.\nTo address this, we classified pages from the Common Crawl scrape used in C4 as German,\nFrench, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts\nof English C4 data with 1 part each of data classified as German, French or Romanian.\nThis vocabulary was shared across both the input and output of our model. Note that\nour vocabulary makes it so that our model can only process a predetermined, fixed set of\nlanguages.\n3.1.4 Unsupervised Objective\nLeveraging unlabeled data to pre-train our model necessitates an objective that does not\nrequire labels but (loosely speaking) teaches the model generalizable knowledge that will be\n10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/\nindex.html#data_generators.generator_utils.pack_examples\n12\n", []], "Vocabulary": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nwe \u201cpack\u201d multiple sequences into each entry of the batch10 so that our batches contain\nroughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds\nto pre-training on 235 \u224834B tokens. This is considerably less than BERT (Devlin et al.,\n2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly\n2.2T tokens. Using only 235 tokens results in a reasonable computational budget while still\nproviding a sufficient amount of pre-training for acceptable performance. We consider the\neffect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235 tokens only covers\na fraction of the entire C4 data set, so we never repeat any data during pre-training.\nDuring pre-training, we use an \u201cinverse square root\u201d learning rate schedule: 1\n\u000ep\nmax(n, k)\nwhere n is the current training iteration and k is the number of warm-up steps (set to 104\nin all of our experiments). This sets a constant learning rate of 0.01 for the first 104 steps,\nthen exponentially decays the learning rate until pre-training is over. We also experimented\nwith using a triangular learning rate (Howard and Ruder, 2018), which produced slightly\nbetter results but requires knowing the total number of training steps ahead of time. Since\nwe will be varying the number of training steps in some of our experiments, we opt for the\nmore generic inverse square root schedule.\nOur models are fine-tuned for 218 = 262,144 steps on all tasks. This value was chosen\nas a trade-off between the high-resource tasks (i.e. those with large data sets), which\nbenefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit\nquickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e.\n216 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save\na checkpoint every 5,000 steps and report results on the model checkpoint corresponding\nto the highest validation performance. For models fine-tuned on multiple tasks, we choose\nthe best checkpoint for each task independently. For all of the experiments except those in\nSection 3.7, we report results in the validation set to avoid performing model selection on\nthe test set.\n3.1.3 Vocabulary\nWe use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens\n(Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000\nwordpieces. Since we ultimately fine-tune our model on English to German, French, and\nRomanian translation, we also require that our vocabulary covers these non-English languages.\nTo address this, we classified pages from the Common Crawl scrape used in C4 as German,\nFrench, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts\nof English C4 data with 1 part each of data classified as German, French or Romanian.\nThis vocabulary was shared across both the input and output of our model. Note that\nour vocabulary makes it so that our model can only process a predetermined, fixed set of\nlanguages.\n3.1.4 Unsupervised Objective\nLeveraging unlabeled data to pre-train our model necessitates an objective that does not\nrequire labels but (loosely speaking) teaches the model generalizable knowledge that will be\n10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/\nindex.html#data_generators.generator_utils.pack_examples\n12\n", []], "Training": ["Exploring the Limits of Transfer Learning\nSection 3.3 and we consider a model architecture that behaves similarly to BERT on text\nclassification tasks in Section 3.2.\nAfter outlining our baseline experimental setup in the following subsection, we undertake\nan empirical comparison of model architectures (Section 3.2), unsupervised objectives\n(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and\nscaling (Section 3.6). At the culmination of this section, we combine insights from our study\nwith scale to obtain state-of-the-art results in many tasks we consider (Section 3.7).\n3.1 Baseline\nOur goal for our baseline is to reflect typical, modern practice. We pre-train a standard\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\nfine-tune on each of our downstream tasks. We describe the details of this experimental\nsetup in the following subsections.\n3.1.1 Model\nFor our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\narchitecture consisting of only a single \u201cstack\u201d (e.g. for language modeling (Radford et al.,\n2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al.,\n2019)), we found that using a standard encoder-decoder structure achieved good results\non both generative and classification tasks. We explore the performance of different model\narchitectures in Section 3.2.\nOur baseline model is designed so that the encoder and decoder are each similar in\nsize and configuration to a \u201cBERTBASE\u201d (Devlin et al., 2018) stack. Specifically, both the\nencoder and decoder consist of 12 blocks (each block comprising self-attention, optional\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality of dff = 3072 followed by a\nReLU nonlinearity and another dense layer. The \u201ckey\u201d and \u201cvalue\u201d matrices of all attention\nmechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12\nheads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,\nthis results in a model with about 220 million parameters. This is roughly twice the number\nof parameters of BERTBASE since our baseline model contains two layer stacks instead of\none. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied\nin the model.\n3.1.2 Training\nAs described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\nlogit at every timestep).\nWe pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a\nmaximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,\n11\n", []], "Model": ["Exploring the Limits of Transfer Learning\nSection 3.3 and we consider a model architecture that behaves similarly to BERT on text\nclassification tasks in Section 3.2.\nAfter outlining our baseline experimental setup in the following subsection, we undertake\nan empirical comparison of model architectures (Section 3.2), unsupervised objectives\n(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and\nscaling (Section 3.6). At the culmination of this section, we combine insights from our study\nwith scale to obtain state-of-the-art results in many tasks we consider (Section 3.7).\n3.1 Baseline\nOur goal for our baseline is to reflect typical, modern practice. We pre-train a standard\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\nfine-tune on each of our downstream tasks. We describe the details of this experimental\nsetup in the following subsections.\n3.1.1 Model\nFor our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\narchitecture consisting of only a single \u201cstack\u201d (e.g. for language modeling (Radford et al.,\n2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al.,\n2019)), we found that using a standard encoder-decoder structure achieved good results\non both generative and classification tasks. We explore the performance of different model\narchitectures in Section 3.2.\nOur baseline model is designed so that the encoder and decoder are each similar in\nsize and configuration to a \u201cBERTBASE\u201d (Devlin et al., 2018) stack. Specifically, both the\nencoder and decoder consist of 12 blocks (each block comprising self-attention, optional\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality of dff = 3072 followed by a\nReLU nonlinearity and another dense layer. The \u201ckey\u201d and \u201cvalue\u201d matrices of all attention\nmechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12\nheads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,\nthis results in a model with about 220 million parameters. This is roughly twice the number\nof parameters of BERTBASE since our baseline model contains two layer stacks instead of\none. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied\nin the model.\n3.1.2 Training\nAs described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\nlogit at every timestep).\nWe pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a\nmaximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,\n11\nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n2. Setup\nBefore presenting the results from our large-scale empirical study, we review the necessary\nbackground topics required to understand our results, including the Transformer model\narchitecture and the downstream tasks we evaluate on. We also introduce our approach\nfor treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled\nCorpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text\ndata. We refer to our model and framework as the \u201cText-to-Text Transfer Transformer\u201d\n(T5).\n2.1 Model\nEarly results on transfer learning for NLP leveraged recurrent neural networks (Peters\net al., 2018; Howard and Ruder, 2018), but it has recently become more common to use\nmodels based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017). The Transformer\nwas initially shown to be effective for machine translation, but it has subsequently been\nused in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann\net al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are\nbased on the Transformer architecture. Apart from the details mentioned below and the\nvariants we explore in Section 3.2, we do not deviate significantly from this architecture as\noriginally proposed. Instead of providing a comprehensive definition of this model, we refer\nthe interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for\na more detailed introduction.\nThe primary building block of the Transformer is self-attention (Cheng et al., 2016).\nSelf-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes\na sequence by replacing each element by a weighted average of the rest of the sequence.\nThe original Transformer consisted of an encoder-decoder architecture and was intended\nfor sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has\nrecently also become common to use models consisting of a single Transformer layer stack,\nwith varying forms of self-attention used to produce architectures appropriate for language\nmodeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction\ntasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural\nvariants in Section 3.2.\nOverall, our encoder-decoder Transformer implementation closely follows its originally-\nproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to\na sequence of embeddings, which is then passed into the encoder. The encoder consists\nof a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer\nfollowed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to\nthe input of each subcomponent. We use a simplified version of layer normalization where\nthe activations are only rescaled and no additive bias is applied. After layer normalization,\na residual skip connection (He et al., 2016) adds each subcomponent\u2019s input to its output.\nDropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip\nconnection, on the attention weights, and at the input and output of the entire stack. The\ndecoder is similar in structure to the encoder except that it includes a standard attention\n3. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n4. http://jalammar.github.io/illustrated-transformer/\n4\n", []], "Baseline": ["Exploring the Limits of Transfer Learning\nSection 3.3 and we consider a model architecture that behaves similarly to BERT on text\nclassification tasks in Section 3.2.\nAfter outlining our baseline experimental setup in the following subsection, we undertake\nan empirical comparison of model architectures (Section 3.2), unsupervised objectives\n(Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and\nscaling (Section 3.6). At the culmination of this section, we combine insights from our study\nwith scale to obtain state-of-the-art results in many tasks we consider (Section 3.7).\n3.1 Baseline\nOur goal for our baseline is to reflect typical, modern practice. We pre-train a standard\nTransformer (described in Section 2.1) using a simple denoising objective and then separately\nfine-tune on each of our downstream tasks. We describe the details of this experimental\nsetup in the following subsections.\n3.1.1 Model\nFor our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al.\n(2017). While many modern approaches to transfer learning for NLP use a Transformer\narchitecture consisting of only a single \u201cstack\u201d (e.g. for language modeling (Radford et al.,\n2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al.,\n2019)), we found that using a standard encoder-decoder structure achieved good results\non both generative and classification tasks. We explore the performance of different model\narchitectures in Section 3.2.\nOur baseline model is designed so that the encoder and decoder are each similar in\nsize and configuration to a \u201cBERTBASE\u201d (Devlin et al., 2018) stack. Specifically, both the\nencoder and decoder consist of 12 blocks (each block comprising self-attention, optional\nencoder-decoder attention, and a feed-forward network). The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality of dff = 3072 followed by a\nReLU nonlinearity and another dense layer. The \u201ckey\u201d and \u201cvalue\u201d matrices of all attention\nmechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12\nheads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total,\nthis results in a model with about 220 million parameters. This is roughly twice the number\nof parameters of BERTBASE since our baseline model contains two layer stacks instead of\none. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied\nin the model.\n3.1.2 Training\nAs described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to\nalways train using standard maximum likelihood, i.e. using teacher forcing (Williams and\nZipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and\nStern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability\nlogit at every timestep).\nWe pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a\nmaximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,\n11\n", []], "Experiments": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nthe words in the model\u2019s output are a subset of the words in the candidate noun phrase\n(or vice versa) and assign a \u201cFalse\u201d label otherwise. This removes roughly half of the WSC\ntraining set, but the DPR data set adds about 1,000 pronoun resolution examples. Examples\nfrom DPR are annotated with the correct referent noun, making it easy to use this data set\nin the format listed above.\nThe WNLI training and validation sets have a significant overlap with the WSC training\nset. To avoid leaking validation examples into our training data (a particular issue in the\nmulti-task experiments of Section 3.5.2), we therefore never train on WNLI and never report\nresults on the WNLI validation set. Omitting results on the WNLI validation set is standard\npractice (Devlin et al., 2018) due to the fact that it is \u201cadversarial\u201d with respect to the\ntraining set, i.e. validation examples are all slightly-perturbed versions of training examples\nwith the opposite label. As such, we do not include WNLI in the average GLUE score\nwhenever we report on the validation set (all sections except Section 3.7 where results\nare presented on the test sets). Converting examples from WNLI to the \u201creferent noun\nprediction\u201d variant described above is a little more involved; we describe this process in\nAppendix B.\n3. Experiments\nRecent advances in transfer learning for NLP have come from a wide variety of developments,\nsuch as new pre-training objectives, model architectures, unlabeled data sets, and more.\nIn this section, we carry out an empirical survey of these techniques in hopes of teasing\napart their contribution and significance. We then combine the insights gained to attain\nstate-of-the-art in many of the tasks we consider. Since transfer learning for NLP is a rapidly\ngrowing area of research, it is not feasible for us to cover every possible technique or idea\nin our empirical study. For a broader literature review, we recommend a recent survey by\nRuder et al. (2019).\nWe systematically study these contributions by taking a reasonable baseline (described\nin Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3\nwe measure the performance of different unsupervised objectives while keeping the rest of\nour experimental pipeline fixed. This \u201ccoordinate ascent\u201d approach might miss second-order\neffects (for example, some particular unsupervised objective may work best on a model\nlarger than our baseline setting), but performing a combinatorial exploration of all of the\nfactors in our study would be prohibitively expensive. In future work, we expect it could be\nfruitful to more thoroughly consider combinations of the approaches we study.\nOur goal is to compare a variety of different approaches on a diverse set of tasks while\nkeeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do\nnot exactly replicate existing approaches. For example, \u201cencoder-only\u201d models like BERT\n(Devlin et al., 2018) are designed to produce a single prediction per input token or a single\nprediction for an entire input sequence. This makes them applicable for classification or span\nprediction tasks but not for generative tasks like translation or abstractive summarization.\nAs such, none of the model architectures we consider are identical to BERT or consist of an\nencoder-only structure. Instead, we test approaches that are similar in spirit\u2014for example,\nwe consider an analogous objective to BERT\u2019s \u201cmasked language modeling\u201d objective in\n10\n", []], "Input and Output Format": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\nWe use the data sets as distributed by the GLUE and SuperGLUE benchmarks.\nFor\nsimplicity, when fine-tuning we treat all of the tasks in the GLUE benchmark (and similarly\nfor SuperGLUE) as a single task by concatenating all of the constituent data sets. As\nsuggested by Kocijan et al. (2019) we also include the Definite Pronoun Resolution (DPR)\ndata set (Rahman and Ng, 2012) in the combined SuperGLUE task.\nThe CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a question-\nanswering task but was adapted for text summarization by Nallapati et al. (2016); we\nuse the non-anonymized version from See et al. (2017) as an abstractive summarization\ntask. SQuAD (Rajpurkar et al., 2016) is a common question-answering benchmark. In our\nexperiments, the model is fed the question and its context and asked to generate the answer\ntoken-by-token. For WMT English to German, we use the same training data as (Vaswani\net al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013\nas a validation set (Bojar et al., 2014). For English to French, we use the standard training\ndata from 2015 and newstest2014 as a validation set (Bojar et al., 2015). For English to\nRomanian, which is a standard lower-resource machine translation benchmark, we use the\ntrain and validation sets from WMT 2016 (Bojar et al., 2016). Note that we only pre-train\non English data, so in order to learn to translate a given model will need to learn to generate\ntext in a new language.\n2.4 Input and Output Format\nIn order to train a single model on the diverse set of tasks described above, we cast all of\nthe tasks we consider into a \u201ctext-to-text\u201d format\u2014that is, a task where the model is fed\nsome text for context or conditioning and is then asked to produce some output text. This\nframework provides a consistent training objective both for pre-training and fine-tuning.\nSpecifically, the model is trained with a maximum likelihood objective (using \u201cteacher forcing\u201d\n(Williams and Zipser, 1989)) regardless of the task. To specify which task the model should\nperform, we add a task-specific (text) prefix to the original input sequence before feeding it\nto the model.\nAs an example, to ask the model to translate the sentence \u201cThat is good.\u201d from English\nto German, the model would be fed the sequence \u201ctranslate English to German: That is\ngood.\u201d and would be trained to output \u201cDas ist gut.\u201d For text classification tasks, the\nmodel simply predicts a single word corresponding to the target label. For example, on the\nMNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies\n(\u201centailment\u201d), contradicts (\u201ccontradiction\u201d), or neither (\u201cneutral\u201d) a hypothesis. With\nour preprocessing, the input sequence becomes \u201cmnli premise: I hate pigeons. hypothesis:\nMy feelings towards pigeons are filled with animosity.\u201d with the corresponding target word\n\u201centailment\u201d. Note that an issue arises if our model outputs text on a text classification\ntask that does not correspond to any of the possible labels (for example if the model\noutputs \u201chamburger\u201d when the only possible labels for a task were \u201centailment\u201d, \u201cneutral\u201d,\nor \u201ccontradiction\u201d). In this case, we always count the model\u2019s output as wrong, though we\nnever observed this behavior in any of our trained models. Note that the choice of text prefix\nused for a given task is essentially a hyperparameter; we found that changing the exact\nwording of the prefix had limited impact and so did not perform extensive experiments into\ndifferent prefix choices. A diagram of our text-to-text framework with a few input/output\n8\n", []], "Downstream Tasks": ["Exploring the Limits of Transfer Learning\nCrawl as a source of data for NLP: For example, Grave et al. (2018) also filter text using an\nautomatic language detector and discard short lines and Smith et al. (2013); Grave et al.\n(2018) both perform line-level deduplication. However, we opted to create a new data set\nbecause prior data sets use a more limited set of filtering heuristics, are not publicly available,\nand/or are different in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al.,\n2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on\nparallel training data for machine translation (Smith et al., 2013)).\nTo assemble our base data set, we downloaded the web extracted text from April 2019\nand applied the aforementioned filtering. This produces a collection of text that is not only\norders of magnitude larger than most data sets used for pre-training (about 750 GB) but also\ncomprises reasonably clean and natural English text. We dub this data set the \u201cColossal\nClean Crawled Corpus\u201d (or C4 for short) and release it as part of TensorFlow Datasets.8\nWe consider the impact of using various alternative versions of this data set in Section 3.4.\n2.3 Downstream Tasks\nOur goal in this paper is to measure general language learning abilities. As such, we study\ndownstream performance on a diverse set of benchmarks, including machine translation,\nquestion answering, abstractive summarization, and text classification. Specifically, we\nmeasure performance on the GLUE and SuperGLUE text classification meta-benchmarks;\nCNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English\nto German, French, and Romanian translation. All data was sourced from TensorFlow\nDatasets.9\nGLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) each comprise a\ncollection of text classification tasks meant to test general language understanding abilities:\n\u2022 Sentence acceptability judgment (CoLA (Warstadt et al., 2018))\n\u2022 Sentiment analysis (SST-2 (Socher et al., 2013))\n\u2022 Paraphrasing/sentence similarity (MRPC (Dolan and Brockett, 2005), STS-B (Cer\net al., 2017), QQP (Iyer et al., 2017))\n\u2022 Natural language inference (MNLI (Williams et al., 2017), QNLI (Rajpurkar et al.,\n2016), RTE (Dagan et al., 2005), CB (De Marneff et al., 2019))\n\u2022 Coreference resolution (WNLI and WSC (Levesque et al., 2012))\n\u2022 Sentence completion (COPA (Roemmele et al., 2011))\n\u2022 Word sense disambiguation (WIC (Pilehvar and Camacho-Collados, 2018))\n\u2022 Question answering (MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018),\nBoolQ (Clark et al., 2019))\n8. https://www.tensorflow.org/datasets/catalog/c4\n9. https://www.tensorflow.org/datasets\n7\n", []], "The Colossal Clean Crawled Corpus": ["Exploring the Limits of Transfer Learning\nmechanism after each self-attention layer that attends to the output of the encoder. The\nself-attention mechanism in the decoder also uses a form of autoregressive or causal self-\nattention, which only allows the model to attend to past outputs. The output of the final\ndecoder block is fed into a dense layer with a softmax output, whose weights are shared with\nthe input embedding matrix. All attention mechanisms in the Transformer are split up into\nindependent \u201cheads\u201d whose outputs are concatenated before being further processed.\nSince self-attention is order-independent (i.e. it is an operation on sets), it is common\nto provide an explicit position signal to the Transformer. While the original Transformer\nused a sinusoidal position signal or learned position embeddings, it has recently become\nmore common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a).\nInstead of using a fixed embedding for each position, relative position embeddings produce\na different learned embedding according to the offset between the \u201ckey\u201d and \u201cquery\u201d being\ncompared in the self-attention mechanism. We use a simplified form of position embeddings\nwhere each \u201cembedding\u201d is simply a scalar that is added to the corresponding logit used\nfor computing the attention weights. For efficiency, we also share the position embedding\nparameters across all layers in our model, though within a given layer each attention head\nuses a different learned position embedding. Typically, a fixed number of embeddings are\nlearned, each corresponding to a range of possible key-query offsets. In this work, we use 32\nembeddings for all of our models with ranges that increase in size logarithmically up to an\noffset of 128 beyond which we assign all relative positions to the same embedding. Note\nthat a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers\ncan build a sensitivity to larger offsets by combining local information from previous layers.\nTo summarize, our model is roughly equivalent to the original Transformer proposed by\nVaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer\nnormalization outside the residual path, and using a different position embedding scheme.\nSince these architectural changes are orthogonal to the experimental factors we consider in\nour empirical survey of transfer learning, we leave the ablation of their impact for future\nwork.\nAs part of our study, we experiment with the scalability of these models, i.e. how their\nperformance changes as they are made to have more parameters or layers. Training large\nmodels can be non-trivial since they might not fit on a single machine and require a great deal\nof computation. As a result, we use a combination of model and data parallelism and train\nmodels on \u201cslices\u201d of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers\nthat contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with\nsupporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al.,\n2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky,\n2014).\n2.2 The Colossal Clean Crawled Corpus\nMuch of the previous work on transfer learning for NLP makes use of large unlabeled data\nsets for unsupervised learning. In this paper, we are interested in measuring the effect of the\nquality, characteristics, and size of this unlabeled data. To generate data sets that satisfy\nour needs, we leverage Common Crawl as a source of text scraped from the web. Common\n5. https://cloud.google.com/tpu/\n5\n", []], "Setup": ["Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu\n2. Setup\nBefore presenting the results from our large-scale empirical study, we review the necessary\nbackground topics required to understand our results, including the Transformer model\narchitecture and the downstream tasks we evaluate on. We also introduce our approach\nfor treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled\nCorpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text\ndata. We refer to our model and framework as the \u201cText-to-Text Transfer Transformer\u201d\n(T5).\n2.1 Model\nEarly results on transfer learning for NLP leveraged recurrent neural networks (Peters\net al., 2018; Howard and Ruder, 2018), but it has recently become more common to use\nmodels based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017). The Transformer\nwas initially shown to be effective for machine translation, but it has subsequently been\nused in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann\net al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are\nbased on the Transformer architecture. Apart from the details mentioned below and the\nvariants we explore in Section 3.2, we do not deviate significantly from this architecture as\noriginally proposed. Instead of providing a comprehensive definition of this model, we refer\nthe interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for\na more detailed introduction.\nThe primary building block of the Transformer is self-attention (Cheng et al., 2016).\nSelf-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes\na sequence by replacing each element by a weighted average of the rest of the sequence.\nThe original Transformer consisted of an encoder-decoder architecture and was intended\nfor sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has\nrecently also become common to use models consisting of a single Transformer layer stack,\nwith varying forms of self-attention used to produce architectures appropriate for language\nmodeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction\ntasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural\nvariants in Section 3.2.\nOverall, our encoder-decoder Transformer implementation closely follows its originally-\nproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to\na sequence of embeddings, which is then passed into the encoder. The encoder consists\nof a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer\nfollowed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to\nthe input of each subcomponent. We use a simplified version of layer normalization where\nthe activations are only rescaled and no additive bias is applied. After layer normalization,\na residual skip connection (He et al., 2016) adds each subcomponent\u2019s input to its output.\nDropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip\nconnection, on the attention weights, and at the input and output of the entire stack. The\ndecoder is similar in structure to the encoder except that it includes a standard attention\n3. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n4. http://jalammar.github.io/illustrated-transformer/\n4\n", []], "Introduction": ["Journal of Machine Learning Research 21 (2020) 1-67\nSubmitted 1/20; Revised 6/20; Published 6/20\nExploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer\nColin Raffel\u2217\ncraffel@gmail.com\nNoam Shazeer\u2217\nnoam@google.com\nAdam Roberts\u2217\nadarob@google.com\nKatherine Lee\u2217\nkatherinelee@google.com\nSharan Narang\nsharannarang@google.com\nMichael Matena\nmmatena@google.com\nYanqi Zhou\nyanqiz@google.com\nWei Li\nmweili@google.com\nPeter J. Liu\npeterjliu@google.com\nGoogle, Mountain View, CA 94043, USA\nEditor: Ivan Titov\nAbstract\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-\ntuned on a downstream task, has emerged as a powerful technique in natural language\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\nlearning techniques for NLP by introducing a unified framework that converts all text-based\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\ndozens of language understanding tasks. By combining the insights from our exploration\nwith scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results\non many benchmarks covering summarization, question answering, text classification, and\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\npre-trained models, and code.1\nKeywords:\ntransfer learning, natural language processing, multi-task learning, attention-\nbased models, deep learning\n1. Introduction\nTraining a machine learning model to perform natural language processing (NLP) tasks\noften requires that the model can process text in a way that is amenable to downstream\nlearning. This can be loosely viewed as developing general-purpose knowledge that allows\nthe model to \u201cunderstand\u201d text. This knowledge can range from low-level (e.g. the spelling\n\u2217. Equal contribution. A description of each author\u2019s contribution is available in Appendix A. Correspondence\nto craffel@gmail.com.\n1. https://github.com/google-research/text-to-text-transfer-transformer\n\u00a92020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\nhttp://jmlr.org/papers/v21/20-074.html.\narXiv:1910.10683v4  [cs.LG]  19 Sep 2023\n", []]}